[
    {
        "Question_title":"Unable to load .ipynb file or terminal in Azure Machine Learning Workspace",
        "Question_body":"<p>Hey all,<\/p>\n<p>Our team experienced the issue below. To summarize, I cannot open .ipynb files or the terminal in my azure machine learning workspace. <\/p>\n<p>This also happens with new created clusters and while using different Browsers and private modes. <\/p>\n<p>Can anyone give suggestions\/guidance on how to resolve the issue?<\/p>\n<p>I also get a lot of Failed to load resource errors.<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/46bb729a-dd22-4a1f-9f81-e9a148670c45?platform=QnA\" alt=\"User's image\" \/><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_created_time":1674455749663,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1163217\/unable-to-load-ipynb-file-or-terminal-in-azure-mac",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unabl load ipynb file termin workspac hei team experienc issu summar open ipynb file termin workspac happen new creat cluster differ browser privat mode suggest guidanc resolv issu lot fail load resourc error",
        "Question_preprocessed_content":"unabl load ipynb file termin workspac hei team experienc issu summar open ipynb file termin workspac happen new creat cluster differ browser privat mode resolv issu lot fail load resourc error",
        "Question_gpt_summary_original":"The user is unable to open .ipynb files or the terminal in their Azure Machine Learning Workspace, even with new created clusters and different browsers. They are seeking suggestions on how to resolve the issue and are also experiencing \"Failed to load resource\" errors.",
        "Question_gpt_summary":"user unabl open ipynb file termin workspac new creat cluster differ browser seek suggest resolv issu experienc fail load resourc error"
    },
    {
        "Question_title":"How to tune conditional objective function using optuna or hyperopt",
        "Question_body":"<p>I tried to use optuna to tune hyperparameters. But my objective function is conditional which creates issues in getting optimal parameters.<\/p>\n\n<p>i want to get cwc only if the condtion is met otherwise continue trial for next hyperparameters.\nBut i guess since the condition is not met and objective func reurns cwc it gives error<\/p>\n\n<p>UnboundLocalError: local variable 'cwc_train' referenced before assignment<\/p>\n\n<pre><code>define objective (trial):\n    k_dis = trial.suggest_uniform('k_dis', 0.0, 5.0)\n    l_dis = trial.suggest_uniform('l_dis', 0.0, 5.0)\n    k_bound = trial.suggest_uniform('k_bound', 0.0, 5.0)\n    l_bound = trial.suggest_uniform('l_bound', 0.0, 5.0) \n\n    picp = .....\n    pinrw = .....\n\n\n    if picp_train &gt;= 0.8 and pinrw_train &lt; 0.18: \n        cwc_train = fc.CWC_proposed(predict_bound_train, Y_train)\n    else:\n        print(\"error = \")\n    return  cwc_train\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100)\n<\/code><\/pre>\n\n<p>UnboundLocalError: local variable 'cwc_train' referenced before assignment<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1564371554813,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1564379241136,
        "Question_score_count":1.0,
        "Question_view_count":814.0,
        "Poster_created_time":1564369929972,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57246939",
        "Tool":"Optuna",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"tune condit object function hyperopt tri us tune hyperparamet object function condit creat issu get optim paramet want cwc condtion met continu trial hyperparamet guess condit met object func reurn cwc give error unboundlocalerror local variabl cwc train referenc assign defin object trial di trial suggest uniform di di trial suggest uniform di bound trial suggest uniform bound bound trial suggest uniform bound picp pinrw picp train pinrw train cwc train cwc propos predict bound train train print error return cwc train studi creat studi studi optim object trial unboundlocalerror local variabl cwc train referenc assign",
        "Question_preprocessed_content":"tune condit object function hyperopt tri us tune hyperparamet object function condit creat issu get optim paramet want cwc condtion met continu trial hyperparamet guess condit met object func reurn cwc give error unboundlocalerror local variabl referenc assign unboundlocalerror local variabl referenc assign",
        "Question_gpt_summary_original":"The user is trying to use optuna to tune hyperparameters for an objective function that is conditional. They want to get a specific value only if a certain condition is met, otherwise, they want to continue the trial for the next set of hyperparameters. However, the code is giving an error \"UnboundLocalError: local variable 'cwc_train' referenced before assignment\" because the condition is not met and the objective function returns 'cwc_train'.",
        "Question_gpt_summary":"user try us tune hyperparamet object function condit want specif valu certain condit met want continu trial set hyperparamet code give error unboundlocalerror local variabl cwc train referenc assign condit met object function return cwc train"
    },
    {
        "Question_title":"Guild compare \/ view \/ tensorboard hangs",
        "Question_body":"<p>I have about 15 runs which where I perform about 60000 steps and log a loss for each step. When I try to view these using <code>guild runs<\/code> it works fine, but trying to extract the best run using compare or viewing the results using view \/ tensorboard it loads for a long time until I can actually view the information.<\/p>\n<p>What could be the reason for this? Am I logging too much per run?<\/p>\n<p>EDIT: I realised that <a class=\"mention\" href=\"\/u\/garrett\">@garrett<\/a> has said elsewhere that the view is due for an overhaul, perhaps that will fix this problem.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1603449092502,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":414.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-compare-view-tensorboard-hangs\/427",
        "Tool":"Guild AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"compar view tensorboard hang run perform step log loss step try view run work fine try extract best run compar view result view tensorboard load long time actual view inform reason log run edit realis garrett said view overhaul fix problem",
        "Question_preprocessed_content":"compar view tensorboard hang run perform step log loss step try view work fine try extract best run compar view result view tensorboard load long time actual view inform reason log run edit realis said view overhaul fix problem",
        "Question_gpt_summary_original":"The user has 15 runs with 60,000 steps and logs a loss for each step. While using guild runs, it works fine, but when trying to extract the best run using compare or viewing the results using view\/tensorboard, it takes a long time to load. The user is unsure if logging too much per run is the reason for this issue.",
        "Question_gpt_summary":"user run step log loss step run work fine try extract best run compar view result view tensorboard take long time load user unsur log run reason issu"
    },
    {
        "Question_title":"S3 Dataset versioning with SageMaker?",
        "Question_body":"Is there any standard for ML S3 dataset tracking or versioning?\nBasically, what setup allows to track a given model training execution to a given dataset?\nInterested to hear about proven or state-of-the-art ideas",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1549396058000,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668615929200,
        "Question_score_count":0.0,
        "Question_view_count":754.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"Unfortunately, managing versions of datasets and which models used them is not embedded in SageMaker. But, you can use SageMaker search to manage the differences in data location between experiments. In that case, if your dataset isn't too big, my recommendation will be to create a standard for data structure in S3. i.e. for each new dataset, create a new prefix in S3 with your logic.\nUsing SageMaker search you'll be able to find all your jobs and compare between datasets.",
        "Answer_comment_count":0.0,
        "Question_closed_time":1549437509000,
        "Answer_last_edit_time":1667925558875,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhYC1EJQuSWqpwTByAtB_fg\/s3-dataset-versioning-with-sagemaker",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":1.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":1.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_original_content":"dataset version standard dataset track version basic setup allow track given model train execut given dataset interest hear proven state art idea",
        "Question_preprocessed_content":"dataset version standard dataset track version basic setup allow track given model train execut given dataset interest hear proven idea",
        "Question_gpt_summary_original":"The user is looking for a standard or setup that allows for tracking or versioning of S3 datasets used in machine learning model training. They are seeking proven or state-of-the-art ideas for this purpose.",
        "Question_gpt_summary":"user look standard setup allow track version dataset machin learn model train seek proven state art idea purpos"
    },
    {
        "Question_title":"Span filtering prodigy datasets using W&B Tables",
        "Question_body":"<p>Hi! I am currently testing the new prodigy integration to visualize NER datasets and it works great, thank you! But I have some problems.<br>\nI would like to filter  <code>row[\"spans\"][\"label\"] <\/code> by the special entities and I have problems here. My use case:<\/p>\n<ul>\n<li>Which texts contain only people (PERSON)? Which organisations (ORG) or locations (LOCATION)? How many people do I find in total?<\/li>\n<li>I would like to create new rows\/columns to visualize the results.<\/li>\n<\/ul>\n<p>My problem: I can\u2019t manage to filter according to the entities. What am I doing wrong?<br>\nYour example here is also ok for the test:<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/kshen\/prodigy\/reports\/Visualizing-Prodigy-Datasets-Using-W-B-Tables--Vmlldzo5NDE2MTc\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/7a7a7077833cb4ec4be6e63ad7c2db322d3e15a6.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/kshen\/prodigy\/reports\/Visualizing-Prodigy-Datasets-Using-W-B-Tables--Vmlldzo5NDE2MTc\" target=\"_blank\" rel=\"noopener\" title=\"02:05PM - 17 August 2021\">W&amp;B \u2013 17 Aug 21<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/6f5cc42f5be5838eaa024b6166848b71312afb4e.png\" class=\"thumbnail onebox-avatar\" width=\"300\" height=\"300\">\n\n<h3><a href=\"https:\/\/wandb.ai\/kshen\/prodigy\/reports\/Visualizing-Prodigy-Datasets-Using-W-B-Tables--Vmlldzo5NDE2MTc\" target=\"_blank\" rel=\"noopener\">Visualizing Prodigy Datasets Using W&amp;B Tables<\/a><\/h3>\n\n  <p>Use the W&amp;B\/Prodigy integration to upload your Prodigy annotated datasets to W&amp;B for easier visualization. Made by Kevin Shen using Weights &amp; Biases<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0.png\" data-download-href=\"\/uploads\/short-url\/quowLbwH5KL7ZM97JjbPxcjnfNe.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0_2_517x186.png\" alt=\"image\" data-base62-sha1=\"quowLbwH5KL7ZM97JjbPxcjnfNe\" width=\"517\" height=\"186\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0_2_517x186.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0_2_775x279.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0_2_1034x372.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1762\u00d7637 64 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Thank you<br>\nAlfred<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1631795821780,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":4.0,
        "Question_view_count":303.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/span-filtering-prodigy-datasets-using-w-b-tables\/646",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"span filter prodigi dataset tabl current test new prodigi integr visual ner dataset work great thank problem like filter row span label special entiti problem us case text contain peopl person organis org locat locat peopl total like creat new row column visual result problem manag filter accord entiti wrong exampl test aug visual prodigi dataset tabl us prodigi integr upload prodigi annot dataset easier visual kevin shen imag thank alfr",
        "Question_preprocessed_content":"span filter prodigi dataset tabl current test new prodigi integr visual ner dataset work great thank problem like filter special entiti problem us case text contain peopl organis locat peopl total like creat new visual result problem manag filter accord entiti wrong exampl test aug visual prodigi dataset tabl us integr upload prodigi annot dataset easier visual kevin shen imag thank alfr",
        "Question_gpt_summary_original":"The user is encountering difficulties in filtering prodigy datasets using W&B tables. They want to filter the spans by special entities such as people, organizations, and locations, and create new rows\/columns to visualize the results. However, they are unable to filter according to the entities and are seeking assistance.",
        "Question_gpt_summary":"user encount difficulti filter prodigi dataset tabl want filter span special entiti peopl organ locat creat new row column visual result unabl filter accord entiti seek assist"
    },
    {
        "Question_title":"Send request as Json on UWP",
        "Question_body":"<p>I have deployed an AzureML published experiment with deployed web service. I tried to use the <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-consume-web-services\/\" rel=\"nofollow\">sample code provided in the configuration page<\/a>, but universal apps do not implement Http.Formatting yet, thus I couldn't use <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/hh944521(v=vs.118).aspx\" rel=\"nofollow\">postasjsonasync<\/a>.<\/p>\n\n<p>I tried to follow the sample code as much as possible, but I'm getting statuscode of 415 \"Unsupported Media Type\", What's the mistake I'm doing?<\/p>\n\n<pre><code>var client = new HttpClient();\nclient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\n\/\/ client.BaseAddress = uri;\n\nvar scoreRequest = new\n{\n            Inputs = new Dictionary&lt;string, StringTable&gt;() {\n                    {\n                        \"dataInput\",\n                        new StringTable()\n                        {\n                            ColumnNames = new [] {\"Direction\", \"meanX\", \"meanY\", \"meanZ\"},\n                            Values = new [,] {  { \"\", x.ToString(), y.ToString(), z.ToString() },  }\n                        }\n                    },\n                },\n            GlobalParameters = new Dictionary&lt;string, string&gt;() { }\n };\n var stringContent = new StringContent(scoreRequest.ToString());\n HttpResponseMessage response = await client.PostAsync(uri, stringContent);\n<\/code><\/pre>\n\n<p>Many Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1452005613750,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":3194.0,
        "Poster_created_time":1352139399460,
        "Poster_location":"Cyprus",
        "Poster_reputation_count":820.0,
        "Poster_view_count":256.0,
        "Answer_body":"<p>You'll need to serialize the object to a JSON string (I recommend using NewtonSoft.Json to make it easier) and set the content type accordingly. Here's an implementation I'm using in my UWP apps (note that <code>_client<\/code> is an <code>HttpClient<\/code>):<\/p>\n\n<pre><code>    public async Task&lt;HttpResponseMessage&gt; PostAsJsonAsync&lt;T&gt;(Uri uri, T item)\n    {\n        var itemAsJson = JsonConvert.SerializeObject(item);\n        var content = new StringContent(itemAsJson);\n        content.Headers.ContentType = new MediaTypeHeaderValue(\"application\/json\");\n\n        return await _client.PostAsync(uri, content);\n    }\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1452007973623,
        "Answer_last_edit_time":null,
        "Answer_score_count":3.0,
        "Answerer_created_time":1395422283667,
        "Answerer_location":null,
        "Answerer_reputation_count":1411.0,
        "Answerer_view_count":45.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34614582",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"send request json uwp deploi publish experi deploi web servic tri us sampl code provid configur page univers app implement http format couldn us postasjsonasync tri follow sampl code possibl get statuscod unsupport media type mistak var client new httpclient client defaultrequesthead author new authenticationheadervalu bearer apikei client baseaddress uri var scorerequest new input new dictionari datainput new stringtabl columnnam new direct meanx meani meanz valu new tostr tostr tostr globalparamet new dictionari var stringcont new stringcont scorerequest tostr httpresponsemessag respons await client postasync uri stringcont thank",
        "Question_preprocessed_content":"send request json uwp deploi publish experi deploi web servic tri us sampl code provid configur page univers app implement couldn us postasjsonasync tri follow sampl code possibl get statuscod unsupport media type mistak thank",
        "Question_gpt_summary_original":"The user is facing challenges while trying to send a request as JSON on UWP for an AzureML published experiment with a deployed web service. The sample code provided in the configuration page is not working as universal apps do not implement Http.Formatting yet, resulting in a status code of 415 \"Unsupported Media Type\". The user is seeking help to identify the mistake in the code.",
        "Question_gpt_summary":"user face challeng try send request json uwp publish experi deploi web servic sampl code provid configur page work univers app implement http format result statu code unsupport media type user seek help identifi mistak code"
    },
    {
        "Question_title":"How to pre deploy the instances used by sagemaker for training?",
        "Question_body":"<p>Sagemaker takes ~3 mins for <code>preparing the instances for training<\/code> and around ~1 min for the actual training and artifact publishing.<\/p>\n<p>I want to pre-deploy some ec2 instances which it can use for training. However I couldn't find an option to specify pre deployed instances in the <code>ResourceConfig<\/code> part of the <code>createTrainingJob<\/code>.<\/p>\n<p>Is this possible?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1642316278607,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":30.0,
        "Poster_created_time":1551701216487,
        "Poster_location":null,
        "Poster_reputation_count":735.0,
        "Poster_view_count":106.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70727988",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"pre deploi instanc train take min prepar instanc train min actual train artifact publish want pre deploi instanc us train couldn option specifi pre deploi instanc resourceconfig createtrainingjob possibl",
        "Question_preprocessed_content":"pre deploi instanc train take min min actual train artifact publish want instanc us train couldn option specifi pre deploi instanc possibl",
        "Question_gpt_summary_original":"The user is facing a challenge with the time taken by Sagemaker to prepare instances for training. They are looking for a way to pre-deploy EC2 instances for Sagemaker to use, but are unable to find an option to specify pre-deployed instances in the ResourceConfig part of the createTrainingJob. They are seeking advice on whether this is possible.",
        "Question_gpt_summary":"user face challeng time taken prepar instanc train look wai pre deploi instanc us unabl option specifi pre deploi instanc resourceconfig createtrainingjob seek advic possibl"
    },
    {
        "Question_title":"Amazon SageMaker: TrainingJobAnalytics returns only one timestamp for inbuilt xgboost",
        "Question_body":"<p>I am trying to use <code>TrainingJobAnalytics<\/code> to plot the training and validation loss curves for a training job using XGBoost on SageMaker. The training job completes successfully and I can see the training and validation rmse values in the CloudWatch logs. <\/p>\n\n<p>However when I try to get them in my notebook using <code>TrainingJobAnalytics<\/code>, I only get the metrics for a single timestamp and not all of them.<\/p>\n\n<p>My code is as below:<\/p>\n\n<pre><code>metrics_dataframe = TrainingJobAnalytics(training_job_name=job_name).dataframe()\n<\/code><\/pre>\n\n<p>What's going wrong and how can I fix it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1575578099463,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":346.0,
        "Poster_created_time":1368812548267,
        "Poster_location":"New York, NY, USA",
        "Poster_reputation_count":1499.0,
        "Poster_view_count":210.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59202898",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"trainingjobanalyt return timestamp inbuilt xgboost try us trainingjobanalyt plot train valid loss curv train job xgboost train job complet successfulli train valid rmse valu cloudwatch log try notebook trainingjobanalyt metric singl timestamp code metric datafram trainingjobanalyt train job job datafram go wrong fix",
        "Question_preprocessed_content":"trainingjobanalyt return timestamp inbuilt xgboost try us plot train valid loss curv train job xgboost train job complet successfulli train valid rmse valu cloudwatch log try notebook metric singl timestamp code go wrong fix",
        "Question_gpt_summary_original":"The user is facing an issue with Amazon SageMaker's TrainingJobAnalytics while trying to plot the training and validation loss curves for a training job using XGBoost. Although the training job completes successfully and the user can see the training and validation rmse values in the CloudWatch logs, they are only able to retrieve metrics for a single timestamp and not all of them using TrainingJobAnalytics. The user is seeking help to fix this issue.",
        "Question_gpt_summary":"user face issu trainingjobanalyt try plot train valid loss curv train job xgboost train job complet successfulli user train valid rmse valu cloudwatch log abl retriev metric singl timestamp trainingjobanalyt user seek help fix issu"
    },
    {
        "Question_title":"JAVA API for DVC",
        "Question_body":"<p>Hi,<\/p>\n<p>Is there a Java API similar to Python to access DVC?<\/p>\n<p>Regards<br>\nManju<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1675200951194,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":82.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/java-api-for-dvc\/1486",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"java api java api similar python access regard manju",
        "Question_preprocessed_content":"java api java api similar python access regard manju",
        "Question_gpt_summary_original":"The user is inquiring about the availability of a Java API that can be used to access DVC, similar to the Python API.",
        "Question_gpt_summary":"user inquir avail java api access similar python api"
    },
    {
        "Question_title":"Sagemaker: How to debug Model monitoring(data quality and model quality)?",
        "Question_body":"<p>I have created a Data Quality monitoring from Sagemaker Studio UI and also created using sagemaker SDK <a href=\"https:\/\/stackoverflow.com\/q\/69179914\/11844406\">code<\/a>, I referred to create model Data Quality monitoring job.<\/p>\n<p><strong>Errors:<\/strong><\/p>\n<ol>\n<li>when there is no captured data (this is expected)<\/li>\n<\/ol>\n<blockquote>\n<p>Monitoring job failure reason:<\/p>\n<p>Job inputs had no data<\/p>\n<\/blockquote>\n<ol start=\"2\">\n<li>From logs, I can see that it is using <code>Java<\/code> in background. Not sure how to debug?<\/li>\n<\/ol>\n<blockquote>\n<p>org.json4s.package$MappingException: Do not know how to convert\nJObject(List(0,JDouble(38.0))) into class java.lang.String.<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3da3i.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3da3i.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Once we create the DataQuality monitoring job using Sagemaker Studio UI or Sagemkaer python sdk, it is taking a hour to start. I would like to know is there a way to debug monitoring job without waiting for a hour every time we get a error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1643780042067,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":88.0,
        "Poster_created_time":1564208933767,
        "Poster_location":null,
        "Poster_reputation_count":491.0,
        "Poster_view_count":59.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70950914",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"debug model monitor data qualiti model qualiti creat data qualiti monitor studio creat sdk code refer creat model data qualiti monitor job error captur data expect monitor job failur reason job input data log java background sure debug org json packag mappingexcept know convert jobject list jdoubl class java lang string creat dataqu monitor job studio sagemka python sdk take hour start like know wai debug monitor job wait hour time error",
        "Question_preprocessed_content":"debug model monitor creat data qualiti monitor studio creat sdk code refer creat model data qualiti monitor job error captur data monitor job failur reason job input data log background sure debug know convert class creat dataqu monitor job studio sagemka python sdk take hour start like know wai debug monitor job wait hour time error",
        "Question_gpt_summary_original":"The user has encountered issues with debugging model monitoring for data quality and model quality in Sagemaker. They have created a Data Quality monitoring job using Sagemaker Studio UI and SDK, but are facing errors when there is no captured data and are unsure how to debug the job as it uses Java in the background. Additionally, the job takes an hour to start, and the user is looking for a way to debug it without waiting for an hour every time an error occurs.",
        "Question_gpt_summary":"user encount issu debug model monitor data qualiti model qualiti creat data qualiti monitor job studio sdk face error captur data unsur debug job us java background addition job take hour start user look wai debug wait hour time error occur"
    },
    {
        "Question_title":"Navigating to azure ml studio",
        "Question_body":"<p>I already set up a free account by didn't know how to navigate to azure machine learning studio to conduct my analysis. Can you show me how? Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1602782362910,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/127890\/navigating-to-azure-ml-studio",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"navig studio set free account know navig studio conduct analysi thank",
        "Question_preprocessed_content":"navig studio set free account know navig studio conduct analysi thank",
        "Question_gpt_summary_original":"The user has set up a free account but is unsure how to navigate to Azure Machine Learning Studio to conduct their analysis. They are seeking guidance on how to access the platform.",
        "Question_gpt_summary":"user set free account unsur navig studio conduct analysi seek guidanc access platform"
    },
    {
        "Question_title":"botocore.exceptions.ClientError: An error occurred (ValidationException)",
        "Question_body":"Hi,  \nI want to deploy an MLflow image to an AWS Sagemaker endpoint that contains a machine learning model. I executed the following code, which I found in  <https:\/\/towardsdatascience.com\/deploying-models-to-production-with-mlflow-and-amazon-sagemaker-d21f67909198> .  \n  \n    import mlflow.sagemaker as mfs  \n      \n    run_id = run_id # the model you want to deploy - this run_id was saved when we trained our model  \n    region = \"us-east-1\" # region of your account  \n    aws_id = \"XXXXXXXXXXX\" # from the aws-cli output  \n    arn = \"arn:aws:iam::XXXXXXXXXXX:role\/your-role\"  \n    app_name = \"iris-rf-1\"  \n    model_uri = \"mlruns\/%s\/%s\/artifacts\/random-forest-model\" % (experiment_id,run_id) # edit this path based on your working directory  \n    image_url = aws_id _ \".dkr.ecr.\" _ region + \".amazonaws.com\/mlflow-pyfunc:1.2.0\" # change to your mlflow version  \n      \n    mfs.deploy(app_name=app_name,   \n               model_uri=model_uri,   \n               region_name=region,   \n               mode=\"create\",  \n               execution_role_arn=arn,  \n               image_url=image_url)  \n  \nBut I got the following error. I checked all policies and permissions attached to the IAM role. They all comply with what the error message complains about. I don't know what to do next. I'd appreciate your help. Thanks.  \n  \nbotocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not access model data at https:\/\/s3.amazonaws.com\/mlflow-sagemaker-us-east-1-xxx\/mlflow-xgb-demo-model-eqktjeoit5mxhmjn-abpanw\/model.tar.gz. Please ensure that the role \"arn:aws:iam::xxx:role\/mlflow-sagemaker-dev\" exists and that its trust relationship policy allows the action \"sts:AssumeRole\" for the service principal \"sagemaker.amazonaws.com\". Also ensure that the role has \"s3:GetObject\" permissions and that the object is located in us-east-1.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1611849385000,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668629395731,
        "Question_score_count":0.0,
        "Question_view_count":1587.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSzifL4nASM60UV1hVI6e_A\/botocore-exceptions-clienterror-an-error-occurred-validationexception",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"botocor except clienterror error occur validationexcept want deploi imag endpoint contain machin learn model execut follow code import mf run run model want deploi run save train model region east region account aw aw cli output arn arn aw iam role role app iri model uri mlrun artifact random forest model experi run edit path base work directori imag url aw dkr ecr region amazonaw com pyfunc chang version mf deploi app app model uri model uri region region mode creat execut role arn arn imag url imag url got follow error check polici permiss attach iam role compli error messag complain know appreci help thank botocor except clienterror error occur validationexcept call createmodel oper access model data http amazonaw com east xgb demo model eqktjeoitmxhmjn abpanw model tar ensur role arn aw iam role dev exist trust relationship polici allow action st assumerol servic princip amazonaw com ensur role getobject permiss object locat east",
        "Question_preprocessed_content":"error occur want deploi imag endpoint contain machin learn model execut follow code import mf model want deploi save train model region region account output arn edit path base work directori region chang version mode creat got follow error check polici permiss attach iam role compli error messag complain know appreci help thank error occur call createmodel oper access model data ensur role exist trust relationship polici allow action st assumerol servic princip ensur role getobject permiss object locat",
        "Question_gpt_summary_original":"The user encountered an error while trying to deploy an MLflow image to an AWS Sagemaker endpoint that contains a machine learning model. The error message indicates that the IAM role does not exist or its trust relationship policy does not allow the action \"sts:AssumeRole\" for the service principal \"sagemaker.amazonaws.com\". Additionally, the role needs to have \"s3:GetObject\" permissions and the object should be located in us-east-1.",
        "Question_gpt_summary":"user encount error try deploi imag endpoint contain machin learn model error messag indic iam role exist trust relationship polici allow action st assumerol servic princip amazonaw com addition role need getobject permiss object locat east"
    },
    {
        "Question_title":"Azure ML Experiment Scheduling - No web service",
        "Question_body":"<p>I want to schedule an Azure ML experiment to run everyday without creating a web service. Is that possible. Is there no scheduler in Azure ML itself.<\/p>\n\n<p>I basically import the latest data from Azuresql and then export the predictions again into AzureSql.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1493034890893,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":116.0,
        "Poster_created_time":1493034212608,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/43587399",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"experi schedul web servic want schedul experi run everydai creat web servic possibl schedul basic import latest data azuresql export predict azuresql",
        "Question_preprocessed_content":"experi schedul web servic want schedul experi run everydai creat web servic possibl schedul basic import latest data azuresql export predict azuresql",
        "Question_gpt_summary_original":"The user wants to schedule an Azure ML experiment to run daily without creating a web service. They are wondering if there is a scheduler in Azure ML itself. The experiment involves importing the latest data from AzureSql and exporting predictions back into AzureSql.",
        "Question_gpt_summary":"user want schedul experi run daili creat web servic wonder schedul experi involv import latest data azuresql export predict azuresql"
    },
    {
        "Question_title":"Google Cloud Platform - Vertex AI - Workbench JupyterLab - Spark\/Hadoop - JAVA_HOME is not set error",
        "Question_body":"Hi All,\n\nI am trying to connect to a SparkSession on Vertex AI's Workbench JupyterLab, but receive this error. Locally, my JAVA_HOME system environments and path environments are already set, and can work when I run Jupyter locally. But only on\u00a0Vertex AI's Workbench JupyterLab I get this error.\n\n\u00a0\n\nCode:\u00a0\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n.appName('Jupyter BigQuery Storage')\\\n.config('spark.jars', 'gs:\/\/spark-lib\/bigquery\/spark-bigquery-latest_2.12.jar') \\\n.getOrCreate()\n\nFull Error:\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n\/tmp\/ipykernel_3404\/1949393828.py in <module>\n      9 spark = SparkSession.builder \\\n     10   .appName('Jupyter BigQuery Storage')\\\n---> 11   .config('spark.jars', 'gs:\/\/spark-lib\/bigquery\/spark-bigquery-latest_2.12.jar') \\\n     12   .getOrCreate()\n     13 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/sql\/session.py in getOrCreate(self)\n    226                             sparkConf.set(key, value)\n    227                         # This SparkContext may be an existing one.\n--> 228                         sc = SparkContext.getOrCreate(sparkConf)\n    229                     # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n    230                     # by all sessions.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in getOrCreate(cls, conf)\n    390         with SparkContext._lock:\n    391             if SparkContext._active_spark_context is None:\n--> 392                 SparkContext(conf=conf or SparkConf())\n    393             return SparkContext._active_spark_context\n    394 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\n    142                 \" is not allowed as it is a security risk.\")\n    143 \n--> 144         SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    145         try:\n    146             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in _ensure_initialized(cls, instance, gateway, conf)\n    337         with SparkContext._lock:\n    338             if not SparkContext._gateway:\n--> 339                 SparkContext._gateway = gateway or launch_gateway(conf)\n    340                 SparkContext._jvm = SparkContext._gateway.jvm\n    341 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/java_gateway.py in launch_gateway(conf, popen_kwargs)\n    106 \n    107             if not os.path.isfile(conn_info_file):\n--> 108                 raise RuntimeError(\"Java gateway process exited before sending its port number\")\n    109 \n    110             with open(conn_info_file, \"rb\") as info:\n\nRuntimeError: Java gateway process exited before sending its port number\n\nDo let me know if you have advice or help, thank you!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1649862720000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":648.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-Platform-Vertex-AI-Workbench-JupyterLab-Spark\/td-p\/413482\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"googl cloud platform workbench jupyterlab spark hadoop java home set error try connect sparksess workbench jupyterlab receiv error local java home environ path environ set work run jupyt local workbench jupyterlab error code pyspark sql import sparksess spark sparksess builder appnam jupyt bigqueri storag config spark jar spark lib bigqueri spark bigqueri latest jar getorcr error runtimeerror traceback recent tmp ipykernel spark sparksess builder appnam jupyt bigqueri storag config spark jar spark lib bigqueri spark bigqueri latest jar getorcr opt conda lib python site packag pyspark sql session getorcr self sparkconf set kei valu sparkcontext exist sparkcontext getorcr sparkconf updat sparkconf exist sparkcontext share session opt conda lib python site packag pyspark context getorcr cl conf sparkcontext lock sparkcontext activ spark context sparkcontext conf conf sparkconf return sparkcontext activ spark context opt conda lib python site packag pyspark context init self master appnam sparkhom pyfil environ batchsiz serial conf gatewai jsc profil cl allow secur risk sparkcontext ensur initi self gatewai gatewai conf conf try self init master appnam sparkhom pyfil environ batchsiz serial opt conda lib python site packag pyspark context ensur initi cl instanc gatewai conf sparkcontext lock sparkcontext gatewai sparkcontext gatewai gatewai launch gatewai conf sparkcontext jvm sparkcontext gatewai jvm opt conda lib python site packag pyspark java gatewai launch gatewai conf popen kwarg path isfil conn info file rais runtimeerror java gatewai process exit send port number open conn info file info runtimeerror java gatewai process exit send port number let know advic help thank",
        "Question_preprocessed_content":"googl cloud platform workbench jupyterlab set error try connect sparksess workbench jupyterlab receiv error local environ path environ set work run jupyt local workbench jupyterlab error code import sparksess spark appnam getorcr error runtimeerror traceback spark appnam getorcr getorcr valu sparkcontext exist updat exist share session getorcr sparkcontext return master appnam sparkhom pyfil environ batchsiz serial conf gatewai jsc allow secur gatewai gatewai conf conf try appnam sparkhom pyfil environ batchsiz serial instanc gatewai conf gatewai rais runtimeerror info runtimeerror java gatewai process exit send port number let know advic help thank",
        "Question_gpt_summary_original":"The user is encountering a \"JAVA_HOME is not set\" error while trying to connect to a SparkSession on Vertex AI's Workbench JupyterLab, despite having the JAVA_HOME system environments and path environments already set locally. The error occurs only on Vertex AI's Workbench JupyterLab. The user has provided the code and the full error message for reference.",
        "Question_gpt_summary":"user encount java home set error try connect sparksess workbench jupyterlab despit have java home environ path environ set local error occur workbench jupyterlab user provid code error messag refer"
    },
    {
        "Question_title":"Wandb.plot.confusion_matrix() just show a Table!",
        "Question_body":"<p>Hello,<\/p>\n<p>I used this code to create a confusion matrix:<\/p>\n<pre><code class=\"lang-auto\"># confusion matrix\n        wandb.log({\"confusion-matrix-test\": wandb.plot.confusion_matrix(\n            probs=None,\n            y_true=all_gt, preds=all_pre,\n            class_names=classes_names)})\n<\/code><\/pre>\n<p>However, Wanda\u2019s website only shows a table instead of the confusion matrix. This is a screenshot from the issue:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046.png\" data-download-href=\"\/uploads\/short-url\/8UGiFwpsOZ6Pivp7qmFXgL1OuvI.png?dl=1\" title=\"Screenshot from 2022-01-09 20-58-35\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_690x249.png\" alt=\"Screenshot from 2022-01-09 20-58-35\" data-base62-sha1=\"8UGiFwpsOZ6Pivp7qmFXgL1OuvI\" width=\"690\" height=\"249\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_690x249.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_1035x373.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_1380x498.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2022-01-09 20-58-35<\/span><span class=\"informations\">1741\u00d7629 29.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1641781829922,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":677.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<aside class=\"quote no-group\" data-username=\"fdaliran\" data-post=\"1\" data-topic=\"1744\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/f\/73ab20\/40.png\" class=\"avatar\"> fdaliran:<\/div>\n<blockquote>\n<pre><code class=\"lang-auto\">        wandb.log({\"confusion-matrix-test\": wandb.plot.confusion_matrix(\n            probs=None,\n            y_true=all_gt, preds=all_pre,\n            class_names=classes_names)})\n<\/code><\/pre>\n<\/blockquote>\n<\/aside>\n<p>If you click the section called \u201cCustom Charts\u201d above the Table, it\u2019ll show the line plot that you\u2019ve logged.<\/p>\n<p>Logging the Table also is expected behaviour because this will allow users to interactively explore the logged data in a W&amp;B Table after logging it.<\/p>",
        "Answer_comment_count":null,
        "Question_closed_time":1641809553312,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"plot confus matrix tabl hello code creat confus matrix confus matrix log confus matrix test plot confus matrix prob true pred pre class name class name wanda websit show tabl instead confus matrix screenshot issu screenshot",
        "Question_preprocessed_content":"tabl hello code creat confus matrix wanda websit show tabl instead confus matrix screenshot issu screenshot",
        "Question_gpt_summary_original":"The user is facing an issue with Wandb.plot.confusion_matrix() function as it only displays a table instead of a confusion matrix on Wanda's website. The user has shared a screenshot of the issue.",
        "Question_gpt_summary":"user face issu plot confus matrix function displai tabl instead confus matrix wanda websit user share screenshot issu"
    },
    {
        "Question_title":"Forecasting in AWS Autopilot",
        "Question_body":"Hey all, \nI am used to using an AutoML solution where you can do forecasting algorithms, by uploading a time-series dataset and choose a time-period for which you want to predict (e.g. 6 months). The time-series dataset can be both multivariate, multiple time-series, include categorical\/numericla values and include parameters known in advance (e.g. time since product launch, country, product etc.). In AWS AutoPilot I can only choose regression\/multiclass classification - but not really a prediction model for a set number of months. When I do a google search, I cant find any guides to how to do forecasting in AutoPilot - even if it claims that e.g., stock predictions are possible. Any comments, thought and ideas to this? Or any link to a guide that can help we set up AutoPilot for forecasting?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1670920730192,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1671268065468,
        "Question_score_count":0.0,
        "Question_view_count":57.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlcwt3TxNSqyzhsxszvXuSw\/forecasting-in-aws-autopilot",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"forecast aw autopilot hei automl solut forecast algorithm upload time seri dataset choos time period want predict month time seri dataset multivari multipl time seri includ categor numericla valu includ paramet known advanc time product launch countri product aw autopilot choos regress multiclass classif predict model set number month googl search guid forecast autopilot claim stock predict possibl comment thought idea link guid help set autopilot forecast",
        "Question_preprocessed_content":"forecast aw autopilot hei automl solut forecast algorithm upload dataset choos want predict dataset multivari multipl includ valu includ paramet known advanc aw autopilot choos classif predict model set number month googl search guid forecast autopilot claim stock predict possibl comment thought idea link guid help set autopilot forecast",
        "Question_gpt_summary_original":"The user is facing challenges with forecasting in AWS Autopilot. They are used to using an AutoML solution for forecasting algorithms, but in AWS Autopilot, they can only choose regression\/multiclass classification. The user is unable to find any guides on how to do forecasting in Autopilot, even though it claims to support stock predictions. The user is seeking comments, thoughts, and ideas or a guide to help them set up Autopilot for forecasting.",
        "Question_gpt_summary":"user face challeng forecast aw autopilot automl solut forecast algorithm aw autopilot choos regress multiclass classif user unabl guid forecast autopilot claim support stock predict user seek comment thought idea guid help set autopilot forecast"
    },
    {
        "Question_title":"Install gym atari in Amazon SageMaker",
        "Question_body":"<p>I am trying to install Gym Atari environments on a Amazon SageMaker instance. I run the following script in a jupyter notebook:<\/p>\n<pre><code>!python -m pip install --upgrade pip\n!pip install gym[atari]\nimport urllib.request\nurllib.request.urlretrieve('http:\/\/www.atarimania.com\/roms\/Roms.rar', 'Roms.rar')\nurllib.request.urlretrieve('https:\/\/www.rarlab.com\/rar\/rarlinux-x64-6.0.2.tar.gz', 'rarlinux-x64-6.0.2.tar.gz')\n!tar -xvzf rarlinux-x64-6.0.2.tar.gz\n%cd rar\n!make\n!sudo make install\n%cd ..\n!unrar x Roms.rar\n!mkdir rars\n!mv HC\\ ROMS.zip rars\n!mv ROMS.zip rars\n!python -m atari_py.import_roms rars\n<\/code><\/pre>\n<p>The script upgrades pip, install gym with the Atari dependencies, fetch the Roms, install unrar to extract the file Roms.rar and then run the command <code>python -m atari_py.import_roms rars<\/code>.<\/p>\n<p>An issue arise when I run <code>python -m atari_py.import_roms rars<\/code>. The output is the following:<\/p>\n<pre><code>rar\/\nrar\/unrar\nrar\/acknow.txt\nrar\/whatsnew.txt\nrar\/order.htm\nrar\/readme.txt\nrar\/rar.txt\nrar\/makefile\nrar\/default.sfx\nrar\/rar\nrar\/rarfiles.lst\nrar\/license.txt\n\/home\/ec2-user\/SageMaker\/rar\nmkdir -p \/usr\/local\/bin\nmkdir -p \/usr\/local\/lib\ncp rar unrar \/usr\/local\/bin\ncp: cannot create regular file \u2018\/usr\/local\/bin\/rar\u2019: Permission denied\ncp: cannot create regular file \u2018\/usr\/local\/bin\/unrar\u2019: Permission denied\nmake: *** [install] Error 1\nmkdir -p \/usr\/local\/bin\nmkdir -p \/usr\/local\/lib\ncp rar unrar \/usr\/local\/bin\ncp rarfiles.lst \/etc\ncp default.sfx \/usr\/local\/lib\n\/home\/ec2-user\/SageMaker\n\nUNRAR 6.02 freeware      Copyright (c) 1993-2021 Alexander Roshal\n\n\nExtracting from Roms.rar\n\nExtracting  HC ROMS.zip                                                 36  OK \nExtracting  ROMS.zip                                                    7 99  OK \nAll OK\ncopying adventure.bin from HC ROMS\/BY ALPHABET (PAL)\/A-G\/Adventure (PAL).bin to \/home\/ec2-user\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/site-packages\/atari_py\/atari_roms\/adventure.bin\nTraceback (most recent call last):\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/runpy.py&quot;, line 193, in _run_module_as_main\n    &quot;__main__&quot;, mod_spec)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/site-packages\/atari_py\/import_roms.py&quot;, line 93, in &lt;module&gt;\n    main()\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/site-packages\/atari_py\/import_roms.py&quot;, line 89, in main\n    import_roms(args.dirpath)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/site-packages\/atari_py\/import_roms.py&quot;, line 78, in import_roms\n    _check_zipfile(f, save_if_matches)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/site-packages\/atari_py\/import_roms.py&quot;, line 23, in _check_zipfile\n    process_f(innerf)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/site-packages\/atari_py\/import_roms.py&quot;, line 67, in save_if_matches\n    f.seek(0)\nio.UnsupportedOperation: seek\n<\/code><\/pre>\n<p>I did some research but found nothing. I would appreciate some help, thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1627045779903,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":148.0,
        "Poster_created_time":1539639589447,
        "Poster_location":null,
        "Poster_reputation_count":21.0,
        "Poster_view_count":8.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68499721",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"instal gym atari try instal gym atari environ instanc run follow script jupyt notebook python pip instal upgrad pip pip instal gym atari import urllib request urllib request urlretriev http atarimania com rom rom rar rom rar urllib request urlretriev http rarlab com rar rarlinux tar rarlinux tar tar xvzf rarlinux tar rar sudo instal unrar rom rar mkdir rar rom zip rar rom zip rar python atari import rom rar script upgrad pip instal gym atari depend fetch rom instal unrar extract file rom rar run command python atari import rom rar issu aris run python atari import rom rar output follow rar rar unrar rar acknow txt rar whatsnew txt rar order htm rar readm txt rar rar txt rar makefil rar default sfx rar rar rar rarfil lst rar licens txt home user rar mkdir usr local bin mkdir usr local lib rar unrar usr local bin creat regular file usr local bin rar permiss deni creat regular file usr local bin unrar permiss deni instal error mkdir usr local bin mkdir usr local lib rar unrar usr local bin rarfil lst default sfx usr local lib home user unrar freewar copyright alexand roshal extract rom rar extract rom zip extract rom zip copi adventur bin rom alphabet pal adventur pal bin home user anaconda env amazonei pytorch latest lib python site packag atari atari rom adventur bin traceback recent file home user anaconda env amazonei pytorch latest lib python runpi line run modul main main mod spec file home user anaconda env amazonei pytorch latest lib python runpi line run code exec code run global file home user anaconda env amazonei pytorch latest lib python site packag atari import rom line main file home user anaconda env amazonei pytorch latest lib python site packag atari import rom line main import rom arg dirpath file home user anaconda env amazonei pytorch latest lib python site packag atari import rom line import rom check zipfil save match file home user anaconda env amazonei pytorch latest lib python site packag atari import rom line check zipfil process innerf file home user anaconda env amazonei pytorch latest lib python site packag atari import rom line save match seek unsupportedoper seek research appreci help thank",
        "Question_preprocessed_content":"instal gym atari try instal gym atari environ instanc run follow script jupyt notebook script upgrad pip instal gym atari depend fetch rom instal unrar extract file run command issu aris run output follow research appreci help thank",
        "Question_gpt_summary_original":"The user is trying to install Gym Atari environments on an Amazon SageMaker instance using a script that upgrades pip, installs gym with the Atari dependencies, fetches the Roms, installs unrar to extract the file Roms.rar, and then runs the command python -m atari_py.import_roms rars. However, the user encounters an issue when running the command, which results in a permission denied error and an io.UnsupportedOperation error. The user has researched the issue but has not found a solution.",
        "Question_gpt_summary":"user try instal gym atari environ instanc script upgrad pip instal gym atari depend fetch rom instal unrar extract file rom rar run command python atari import rom rar user encount issu run command result permiss deni error unsupportedoper error user research issu solut"
    },
    {
        "Question_title":"Custom Argument pass to Docker Container Azure ML inference",
        "Question_body":"<p>Hello Team,   <\/p>\n<p>I'm trying to pass the arguments to Azure ML docker. I have created an environment like this.  <\/p>\n<pre><code>env = Environment.from_conda_specification(name='pytorch-1.6-gpu', file_path='curated_env\/conda_dependencies.yml' )\n<\/code><\/pre>\n<p>Am I passing the arguments correct?   <\/p>\n<pre><code>DOCKER_ARGUMENTS = [&quot;--shm-size&quot;,&quot;32G&quot;]  # increase shared memory\nenv.docker.arguments = DOCKER_ARGUMENTS\n<\/code><\/pre>\n<p>The main goal of this project is to deploy a model on the AKS inference cluster. I have successfully deployed the model. When I try to get predictions from the model I got this error   <\/p>\n<blockquote>\n<p>It is possible that data loaders workers are out of shared memory. Please try to raise your shared memory limit  <\/p>\n<\/blockquote>\n<p>How can I do that if that's not the correct way to pass arguments?  <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1632856020243,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/569816\/custom-argument-pass-to-docker-container-azure-ml",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"custom argument pass docker contain infer hello team try pass argument docker creat environ like env environ conda specif pytorch gpu file path curat env conda depend yml pass argument correct docker argument shm size increas share memori env docker argument docker argument main goal project deploi model ak infer cluster successfulli deploi model try predict model got error possibl data loader worker share memori try rais share memori limit correct wai pass argument",
        "Question_preprocessed_content":"custom argument pass docker contain infer hello team try pass argument docker creat environ like pass argument correct main goal project deploi model ak infer cluster successfulli deploi model try predict model got error possibl data loader worker share memori try rais share memori limit correct wai pass argument",
        "Question_gpt_summary_original":"The user is trying to pass arguments to Azure ML docker using a created environment and Docker arguments. The goal is to deploy a model on the AKS inference cluster, which has been successfully done. However, when trying to get predictions from the model, an error occurred suggesting that the data loaders workers are out of shared memory, and the user is seeking advice on how to raise the shared memory limit.",
        "Question_gpt_summary":"user try pass argument docker creat environ docker argument goal deploi model ak infer cluster successfulli try predict model error occur suggest data loader worker share memori user seek advic rais share memori limit"
    },
    {
        "Question_title":"DVC + Github Actions + GCP Storage",
        "Question_body":"<p>I\u2019m trying triggering a pipeline to run DVC and download the data from GCP Storage but the log of GitHub Actions returns the following error:<\/p>\n<pre><code class=\"lang-auto\">ERROR: unexpected error - Anonymous caller does not have storage.objects.get access to the Google Cloud Storage object., 401\n<\/code><\/pre>\n<p>I think this happens due to giving the right permissions to the Service Account but the one that I\u2019m using has the <em><strong>Storage Object Viewer<\/strong><\/em>, which gives the permission I need.<\/p>\n<p>Here is part of my pipeline file<\/p>\n<pre><code class=\"lang-auto\">- name: Setup Cloud SDK\n  uses: google-github-actions\/setup-gcloud@v0.2.0\n  with:\n    project_id: ${{ secrets.GCP_PROJECT }}\n    service_account_key: ${{ secrets.GCP_KEY }}\n    export_default_credentials: true\n\n- name: CML Run\n  shell: bash\n  env:\n    repo_token: ${{ secrets.GITHUB_TOKEN }}\n    GOOGLE_APPLICATION_CREDENTIALS: ${{ secrets.GCP_KEY }}\n  run: |\n    # run-cache and reproduce pipeline\n    dvc remote add -d -f myremote gs:\/\/myproject\/\n    dvc pull mypath\/data.csv.zip.dvc\n    dvc repro -m\n    \n    # Report metrics\n    echo \"## Metrics\" &gt;&gt; report.md\n    git fetch --prune\n    dvc metrics diff main --show-md &gt;&gt; report.md\n    \n    # Publish confusion matrix diff\n    echo -e \"## Plots\\n### Confusion Matrix\" &gt;&gt; report.md\n    cml-publish $PWD\/mypath\/reports\/confusion-matrix.png --md &gt;&gt; report.md\n    cml-send-comment report.md\n<\/code><\/pre>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_created_time":1628712122880,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":828.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-github-actions-gcp-storage\/840",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"github action gcp storag try trigger pipelin run download data gcp storag log github action return follow error error unexpect error anonym caller storag object access googl cloud storag object think happen give right permiss servic account storag object viewer give permiss need pipelin file setup cloud sdk us googl github action setup gcloud project secret gcp project servic account kei secret gcp kei export default credenti true cml run shell bash env repo token secret github token googl applic credenti secret gcp kei run run cach reproduc pipelin remot add myremot myproject pull mypath data csv zip repro report metric echo metric report git fetch prune metric diff main report publish confus matrix diff echo plot confus matrix report cml publish pwd mypath report confus matrix png report cml send comment report",
        "Question_preprocessed_content":"github action gcp storag try trigger pipelin run download data gcp storag log github action return follow error think happen give right permiss servic account storag object viewer give permiss need pipelin file",
        "Question_gpt_summary_original":"The user is encountering an error while triggering a pipeline to run DVC and download data from GCP Storage. The error message indicates that the anonymous caller does not have storage.objects.get access to the Google Cloud Storage object. The user suspects that the issue is related to the Service Account permissions, but the Service Account being used has the Storage Object Viewer permission. The user has shared a part of their pipeline file for reference.",
        "Question_gpt_summary":"user encount error trigger pipelin run download data gcp storag error messag indic anonym caller storag object access googl cloud storag object user suspect issu relat servic account permiss servic account storag object viewer permiss user share pipelin file refer"
    },
    {
        "Question_title":"Vertex AI: Workbench - Failing to Created a schedule based recurrent execution",
        "Question_body":"Hello,\n\nI am trying to create a schedule-based recurrent execution via the Vertex AI workbench. It fails with error code 2- unknown error. Attaching the error snippet.\u00a0\n\nAny help to understand the root cause is highly appreciated.\u00a0\nappreciated.\u00a0\n\nNote: I can do standalone execution successfully. It's only when trying to create a schedule based event, this error occurs.",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_created_time":1670887320000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":251.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Workbench-Failing-to-Created-a-schedule-based\/td-p\/498980\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"workbench fail creat schedul base recurr execut hello try creat schedul base recurr execut workbench fail error code unknown error attach error snippet help understand root caus highli appreci appreci note standalon execut successfulli try creat schedul base event error occur",
        "Question_preprocessed_content":"workbench fail creat schedul base recurr execut hello try creat recurr execut workbench fail error code unknown error attach error snippet help understand root caus highli appreci appreci note standalon execut successfulli try creat schedul base event error occur",
        "Question_gpt_summary_original":"The user is encountering an error (error code 2) while attempting to create a schedule-based recurrent execution via the Vertex AI workbench. The error only occurs when trying to create a schedule-based event, as standalone execution is successful. The user is seeking assistance in understanding the root cause of the issue.",
        "Question_gpt_summary":"user encount error error code attempt creat schedul base recurr execut workbench error occur try creat schedul base event standalon execut success user seek assist understand root caus issu"
    },
    {
        "Question_title":"Are Graviton instance types not supported for SageMaker multimodel endpoints?",
        "Question_body":"I am trying to create a multimodel endpoint using an ml.c7g.xlarge instance type. The error I get is: \u201cMultiModel mode is not supported for instance type ml.c7g.xlarge.\u201d However, according to the documentation on AWS, \u201cMulti-model endpoints are currently supported for all CPU instances types and on single-GPU instance types.\u201d (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html#multi-model-endpoint-instance). Subsequently, I tried ml.c6g.xlarge and see that that also is not supported. Are Graviton instance types not supported in general? If so, why?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1681313292601,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1681660217836,
        "Question_score_count":0.0,
        "Question_view_count":75.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHoRNBe1XQA2srtenVsUJsg\/are-graviton-instance-types-not-supported-for-sagemaker-multimodel-endpoints",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"graviton instanc type support multimodel endpoint try creat multimodel endpoint xlarg instanc type error multimodel mode support instanc type xlarg accord document aw multi model endpoint current support cpu instanc type singl gpu instanc type http doc aw amazon com latest multi model endpoint html multi model endpoint instanc subsequ tri xlarg support graviton instanc type support gener",
        "Question_preprocessed_content":"graviton instanc type support multimodel endpoint try creat multimodel endpoint instanc type error multimodel mode support instanc type accord document aw endpoint current support cpu instanc type instanc type subsequ tri support graviton instanc type support gener",
        "Question_gpt_summary_original":"The user is trying to create a multimodel endpoint using ml.c7g.xlarge instance type but is receiving an error message stating that the mode is not supported. The user has checked the AWS documentation and found that multi-model endpoints are supported for all CPU instances and single-GPU instance types. The user has also tried ml.c6g.xlarge but it is also not supported. The user is questioning if Graviton instance types are not supported in general and is seeking an explanation for this.",
        "Question_gpt_summary":"user try creat multimodel endpoint xlarg instanc type receiv error messag state mode support user check aw document multi model endpoint support cpu instanc singl gpu instanc type user tri xlarg support user question graviton instanc type support gener seek explan"
    },
    {
        "Question_title":"How to combine flags from multiple operations in pipelines to enable parameter searches across multiple operations?",
        "Question_body":"<p>Let say I have 2 operations preprocessing and training.<br>\nEach has a series of flags that I want to do a hyper parameter search over.<br>\nHere is a rough idea of what my operations would look like<\/p>\n<pre><code>preprocessing:\n  flags:\n    a:\n    b:\ntraining:\n  flags:\n    c:\n    d:\n<\/code><\/pre>\n<p>Neglecting the practicality of this scenario, I want to run my preprocessing operation every time I run train.  Is there a way to achieve this with pipelines or a similar method?  I know I could do<\/p>\n<pre><code>mypipeline:\n  flags:\n    a: [1,2,3]\n    b: [4,5,6]\n    c: [12,13,14]\n    d: [15,16,17]\n  steps:\n    - preprocessing a=${a} b=${b}\n    - train c=${c} d=${d}\n<\/code><\/pre>\n<p>and this would solve my problem, but when both operations have a lot of parameters it would end up requiring a lot of copying and pasting between the pipeline and operation flags.  Is there a better\/more efficient way to achieve this?  I basically want the pipline to \u201cinherit\u201d the flags of the steps its performing if possible.  The documentation seems to hint at something like this<\/p>\n<aside class=\"quote no-group\" data-username=\"guildai\" data-post=\"1\" data-topic=\"197\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard11\/user_avatar\/my.guild.ai\/guildai\/40\/103_2.png\" class=\"avatar\"><a href=\"https:\/\/my.guild.ai\/t\/guild-file-reference\/197\/1\">Guild File Reference<\/a>\n<\/div>\n<blockquote>\n<p>You can include references to step flag values as needed to pass through user-specified values.<\/p>\n<\/blockquote>\n<\/aside>\n<p>But when I create a pipeline with no flags and attempt to pass a flag which is defined by one of the steps I get an error saying that the flag does not exists.  Is what I am trying to do possible with Guild?<\/p>\n<p>Thanks.<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_created_time":1598537665027,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":850.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/how-to-combine-flags-from-multiple-operations-in-pipelines-to-enable-parameter-searches-across-multiple-operations\/328",
        "Tool":"Guild AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"combin flag multipl oper pipelin enabl paramet search multipl oper let oper preprocess train seri flag want hyper paramet search rough idea oper look like preprocess flag train flag neglect practic scenario want run preprocess oper time run train wai achiev pipelin similar method know mypipelin flag step preprocess train solv problem oper lot paramet end requir lot copi past pipelin oper flag better effici wai achiev basic want piplin inherit flag step perform possibl document hint like file refer includ refer step flag valu need pass user specifi valu creat pipelin flag attempt pass flag defin step error sai flag exist try possibl thank",
        "Question_preprocessed_content":"combin flag multipl oper pipelin enabl paramet search multipl oper let oper preprocess train seri flag want hyper paramet search rough idea oper look like neglect practic scenario want run preprocess oper time run train wai achiev pipelin similar method know solv problem oper lot paramet end requir lot copi past pipelin oper flag effici wai achiev basic want piplin inherit flag step perform possibl document hint like file refer includ refer step flag valu need pass valu creat pipelin flag attempt pass flag defin step error sai flag exist try possibl thank",
        "Question_gpt_summary_original":"The user wants to perform a hyper parameter search over flags in two operations, but wants to avoid copying and pasting between the pipeline and operation flags. They are looking for a way to have the pipeline inherit the flags of the steps it is performing. The user has tried to reference step flag values but encountered an error saying that the flag does not exist. They are seeking advice on whether this is possible with Guild.",
        "Question_gpt_summary":"user want perform hyper paramet search flag oper want avoid copi past pipelin oper flag look wai pipelin inherit flag step perform user tri refer step flag valu encount error sai flag exist seek advic possibl"
    },
    {
        "Question_title":"Trying to invoke an AWS sagemaker endpoint for a locally trained Object detection model (with .pb file). Facing this issue while invoking",
        "Question_body":"<p>I have created a multi model endpoint and facing issue while invoking the object detection model alone. The OD model has only .pb file and no variables folder in the saved model dir. While trying to invoke the endpoint with a sample image, I am getting this error:\n<a href=\"https:\/\/i.stack.imgur.com\/H4I3v.png\" rel=\"nofollow noreferrer\">Error in Cloudwatch:<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1631791432093,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":32.0,
        "Poster_created_time":1605887877220,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69207550",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"try invok endpoint local train object detect model file face issu invok creat multi model endpoint face issu invok object detect model model file variabl folder save model dir try invok endpoint sampl imag get error error cloudwatch",
        "Question_preprocessed_content":"try invok endpoint local train object detect model face issu invok creat multi model endpoint face issu invok object detect model model file variabl folder save model dir try invok endpoint sampl imag get error error cloudwatch",
        "Question_gpt_summary_original":"The user is facing an issue while trying to invoke an AWS Sagemaker endpoint for a locally trained object detection model with only a .pb file and no variables folder in the saved model directory. The user is receiving an error while trying to invoke the endpoint with a sample image.",
        "Question_gpt_summary":"user face issu try invok endpoint local train object detect model file variabl folder save model directori user receiv error try invok endpoint sampl imag"
    },
    {
        "Question_title":"Azure ML Endpoint Deployment Error (No matching distribution found for en-core-web-sm==2.1.0)",
        "Question_body":"<p>I have trained a model with Automated ML. When deploying the model to an endpoint, I came across with this issue: <\/p>\n<blockquote>\n<p><strong>ResourceOperationFailure<\/strong>:\u00a0ResourceNotReady: User container has crashed or terminated. Please see troubleshooting guide, available here: <a href=\"https:\/\/aka.ms\/oe-tsg#error-resourcenotready\">https:\/\/aka.ms\/oe-tsg#error-resourcenotready <\/a><em><a href=\"https:\/\/aka.ms\/oe-tsg#error-resourcenotready\">\uf35f<\/a><\/em><\/p>\n<\/blockquote>\n<p>When checking the deployment log I got the following error:<\/p>\n<pre><code>alling pip dependencies: ...working... Ran pip subprocess with arguments:['\/opt\/miniconda\/envs\/userenv\/bin\/python', '-m', 'pip', 'install', '-U', '-r', '\/tmp\/condaenv.n3tlurp8.requirements.txt']Pip subprocess output:Collecting adal==1.2.7  Downloading adal-1.2.7-py2.py3-none-any.whl (55 kB)Pip subprocess error:ERROR: Ignored the following versions that require a different python version: 0.7 Requires-Python &gt;=3.6, &lt;3.7; 0.8 Requires-Python &gt;=3.6, &lt;3.7; 1.19.0 Requires-Python &gt;=3.5,&lt;3.8; 1.20.0 Requires-Python &gt;=3.5,&lt;3.8; 1.20.0.post1 Requires-Python &gt;=3.5,&lt;3.8; 1.21.0 Requires-Python &gt;=3.5,&lt;3.8; 1.21.0.post1 Requires-Python &gt;=3.5,&lt;3.8; 1.22.0 Requires-Python &gt;=3.5,&lt;3.8; 1.22.0.post1 Requires-Python &gt;=3.5,&lt;3.8; 1.22.0.post2 Requires-Python &gt;=3.5,&lt;3.8; 1.22.0.post2 Requires-Python &gt;=3.6,&lt;3.8; 1.23.0 Requires-Python &gt;=3.5,&lt;3.8; 1.23.0.post1 Requires-Python &gt;=3.5,&lt;3.8; 1.24.0 Requires-Python &gt;=3.5,&lt;3.8; 1.25.0 Requires-Python &gt;=3.5,&lt;3.8; 1.25.0.post1 Requires-Python &gt;=3.5,&lt;3.8; 1.26.0 Requires-Python &gt;=3.5,&lt;3.8; 1.26.0.post1 Requires-Python &gt;=3.5,&lt;3.8; 1.27.0 Requires-Python &gt;=3.5,&lt;3.8; 1.27.0.post1 Requires-Python &gt;=3.5,&lt;3.8; 1.27.0.post2 Requires-Python &gt;=3.5,&lt;3.8; 1.28.0 Requires-Python &gt;=3.5,&lt;3.8; 1.28.0.post1 Requires-Python &gt;=3.5,&lt;3.8; 1.28.0.post2 Requires-Python &gt;=3.5,&lt;3.8; 1.29.0 Requires-Python &gt;=3.6,&lt;3.8; 1.29.0.post1 Requires-Python &gt;=3.6,&lt;3.8; 1.30.0 Requires-Python &gt;=3.6,&lt;3.8; 1.31.0 Requires-Python &gt;=3.6,&lt;3.8; 1.32.0 Requires-Python &gt;=3.6,&lt;3.8; 1.33.0 Requires-Python &gt;=3.6,&lt;3.8; 1.33.1 Requires-Python &gt;=3.6,&lt;3.8; 1.33.1.post1 Requires-Python &gt;=3.6,&lt;3.8; 1.34.0 Requires-Python &gt;=3.6,&lt;3.8; 1.34.0.post1 Requires-Python &gt;=3.6,&lt;3.8; 1.34.1 Requires-Python &gt;=3.6,&lt;3.8; 1.34.1.post1 Requires-Python &gt;=3.6,&lt;3.8; 1.35.0 Requires-Python &gt;=3.6,&lt;3.8; 1.35.1 Requires-Python &gt;=3.6,&lt;3.8; 1.36.0 Requires-Python &gt;=3.6,&lt;3.8; 1.36.1 Requires-Python &gt;=3.6,&lt;3.8; 1.37.0 Requires-Python &gt;=3.6,&lt;3.8; 1.38.0 Requires-Python &gt;=3.6,&lt;3.8\nERROR: Could not find a version that satisfies the requirement en-core-web-sm==2.1.0 (from versions: none)\nERROR: No matching distribution found for en-core-web-sm==2.1.0\n<\/code><\/pre>\n<p>I'm wondering if anyone had the same issue? How can I change the requirements.txt in an automated ML model? Thanks.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_created_time":1678794182460,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1189423\/azure-ml-endpoint-deployment-error-(no-matching-di",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"endpoint deploy error match distribut core web train model autom deploi model endpoint came issu resourceoperationfailur resourcenotreadi user contain crash termin troubleshoot guid avail http aka tsg error resourcenotreadi check deploy log got follow error all pip depend work ran pip subprocess argument opt miniconda env userenv bin python pip instal tmp condaenv ntlurp requir txt pip subprocess output collect adal download adal whl pip subprocess error error ignor follow version requir differ python version requir python error version satisfi requir core web version error match distribut core web wonder issu chang requir txt autom model thank",
        "Question_preprocessed_content":"endpoint deploy error train model autom deploi model endpoint came issu resourceoperationfailur resourcenotreadi user contain crash termin troubleshoot guid avail check deploy log got follow error wonder issu chang autom model thank",
        "Question_gpt_summary_original":"The user encountered an error while deploying a model to an endpoint in Azure ML. The error message indicates that the user container has crashed or terminated. The deployment log shows an error related to the en-core-web-sm==2.1.0 package, which could not be found. The user is seeking advice on how to modify the requirements.txt file in an automated ML model.",
        "Question_gpt_summary":"user encount error deploi model endpoint error messag indic user contain crash termin deploy log show error relat core web packag user seek advic modifi requir txt file autom model"
    },
    {
        "Question_title":"Why does AWS SageMaker create an S3 Bucket",
        "Question_body":"<p>Upon deploying a custom pytorch model with the boto3 client in python. I noticed that a new S3 bucket had been created with no visible objects. Is there a reason for this?<\/p>\n<p>The bucket that contained my model was named with the keyword &quot;sagemaker&quot; included, so I don't any issue there.<\/p>\n<p>Here is the code that I used for deployment:<\/p>\n<pre><code>remote_model = PyTorchModel(\n                     name = model_name, \n                     model_data=model_url,\n                     role=role,\n                     sagemaker_session = sess,\n                     entry_point=&quot;inference.py&quot;,\n                     # image=image, \n                     framework_version=&quot;1.5.0&quot;,\n                     py_version='py3'\n                    )\n\nremote_predictor = remote_model.deploy(\n                         instance_type='ml.g4dn.xlarge', \n                         initial_instance_count=1,\n                         #update_endpoint = True, # comment or False if endpoint doesns't exist\n                         endpoint_name=endpoint_name, # define a unique endpoint name; if ommited, Sagemaker will generate it based on used container\n                         wait=True\n                         )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1607931328823,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":1047.0,
        "Poster_created_time":1577873077020,
        "Poster_location":"Perth WA, Australia",
        "Poster_reputation_count":438.0,
        "Poster_view_count":67.0,
        "Answer_body":"<p>It was likely created as a default bucket by the SageMaker Python SDK. Note that the code you wrote about is not <code>boto3<\/code> (AWS python SDK), but <code>sagemaker<\/code> (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">link<\/a>), the SageMaker-specific Python SDK, that is higher-level than boto3.<\/p>\n<p>The SageMaker Python SDK uses S3 at multiple places, for example to stage training code when using a Framework Estimator, and to stage inference code when deployment with a Framework Model (your case). It gives you control of the S3 location to use, but if you don't specify it, it may use an automatically generated bucket, if it has the permissions to do so.<\/p>\n<p>To control code staging S3 location, you can use the parameter <code>code_location<\/code> in either your <code>PyTorchEstimator<\/code> (training) or your <code>PyTorchModel<\/code> (serving)<\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1608132873676,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65285203",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"creat bucket deploi custom pytorch model boto client python notic new bucket creat visibl object reason bucket contain model name keyword includ issu code deploy remot model pytorchmodel model model data model url role role session sess entri point infer imag imag framework version version remot predictor remot model deploi instanc type gdn xlarg initi instanc count updat endpoint true comment fals endpoint doesn exist endpoint endpoint defin uniqu endpoint ommit gener base contain wait true",
        "Question_preprocessed_content":"creat bucket deploi custom pytorch model boto client python notic new bucket creat visibl object reason bucket contain model name keyword includ issu code deploy",
        "Question_gpt_summary_original":"The user has noticed that a new S3 bucket is created with no visible objects upon deploying a custom PyTorch model with the boto3 client in Python using AWS SageMaker. The bucket name contained the keyword \"sagemaker,\" and the user is unsure if this is expected behavior. The user has provided the code used for deployment.",
        "Question_gpt_summary":"user notic new bucket creat visibl object deploi custom pytorch model boto client python bucket contain keyword user unsur expect behavior user provid code deploy"
    },
    {
        "Question_title":"On tensorflow on sagemaker receiving error as input must be a vector, got shape: [1,2]",
        "Question_body":"<p>I have built a tensorflow model on Sagemaker. The model uses universal sentence encoder <\/p>\n\n<pre><code>import json\nclass MixEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return json.JSONEncoder.default(self, obj)\n\nserialize_data = json.dumps({'input1': np.array((\"Hello\",\"World\")) } , cls = MixEncoder  )\nprint(serialize_data)\n\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,ContentType='application\/json' ,Body=serialize_data)-- \n<\/code><\/pre>\n\n<p>This line throws an error <\/p>\n\n<blockquote>\n  <p>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message \"{ \"error\": \"input must be a vector, got shape: [1,2]\\n\\t <\/p>\n  \n  <p>The model expects only one tensor as input . It is defined as \n  input_model_placeholder = tf.placeholder(tf.string, shape=[None], name=\"tensor_input_model_1\").<\/p>\n<\/blockquote>\n\n<p>Please help<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1580548991430,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1580549977087,
        "Question_score_count":3.0,
        "Question_view_count":222.0,
        "Poster_created_time":1419691849220,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":4.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60015562",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"tensorflow receiv error input vector got shape built tensorflow model model us univers sentenc encod import json class mixencod json jsonencod def default self obj isinst obj ndarrai return obj tolist return json jsonencod default self obj serial data json dump input arrai hello world cl mixencod print serial data respons client invok endpoint endpointnam endpoint contenttyp applic json bodi serial data line throw error modelerror error occur modelerror call invokeendpoint oper receiv client error model messag error input vector got shape model expect tensor input defin input model placehold placehold string shape tensor input model help",
        "Question_preprocessed_content":"tensorflow receiv error input vector got shape built tensorflow model model us univers sentenc encod line throw error modelerror error occur call invokeendpoint oper receiv client error model messag error input vector got shape model expect tensor input defin shape help",
        "Question_gpt_summary_original":"The user has encountered an error while building a TensorFlow model on Sagemaker. The error message states that the input must be a vector, but the model received a shape of [1,2]. The model expects only one tensor as input, defined as input_model_placeholder = tf.placeholder(tf.string, shape=[None], name=\"tensor_input_model_1\"). The user is seeking assistance to resolve the issue.",
        "Question_gpt_summary":"user encount error build tensorflow model error messag state input vector model receiv shape model expect tensor input defin input model placehold placehold string shape tensor input model user seek assist resolv issu"
    },
    {
        "Question_title":"Error in deploying PyTorch model using SageMaker Pipeline and RegisterModel",
        "Question_body":"<p>Can anyone provide an example for deploying a pytorch model using <strong>SageMaker Pipeline<\/strong>?<\/p>\n<p>I've used the MLOps template (MLOps template for model building, traing and deployment) of SageMaker Studio to build a MLOps project.<\/p>\n<p>The template is using sagemaker pipelines to build a pipeline for preprocessing and training and registering the model.\nAnd deployment script is implemented in the YAML file and employing CloudFormation to run. The deployment script will be triggered automatically when the model is registered.<\/p>\n<p>The template is using xgboost model to train the data and deploy the model. I want to use Pytorch and deploy it.\nI successfully replaced the pytorch with xgboost and successfully preprocessed the data, trained the model and registered the model. But I didn't use inference.py in my model. So I get error for the model deployment.<\/p>\n<p><strong>The error log in updating the endpoint is:<\/strong><\/p>\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/model\/code\/inference.py'\n<\/code><\/pre>\n<p>I tried to find example of using inference.py for pytorch model, but I couldn't find any example which uses <strong>sagemaker pipelines<\/strong> and <strong>RegisterModel<\/strong>.<\/p>\n<p>Any help would be appreciated.<\/p>\n<p>Below you can see a part of the pipeline for training and registering the model.<\/p>\n<pre><code>from sagemaker.pytorch.estimator import PyTorch\nfrom sagemaker.workflow.pipeline import Pipeline\nfrom sagemaker.workflow.steps import (\n    ProcessingStep,\n    TrainingStep,\n)\nfrom sagemaker.workflow.step_collections import RegisterModel\n\npytorch_estimator = PyTorch(entry_point= os.path.join(BASE_DIR, 'train.py'),\n                            instance_type= &quot;ml.m5.xlarge&quot;,\n                            instance_count=1,\n                            role=role,\n                            framework_version='1.8.0',\n                            py_version='py3',\n                            hyperparameters = {'epochs': 5, 'batch-size': 64, 'learning-rate': 0.1})\n\nstep_train = TrainingStep(\n        name=&quot;TrainModel&quot;,\n        estimator=pytorch_estimator,\n\n        inputs={\n                &quot;train&quot;: sagemaker.TrainingInput(\n                            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                            &quot;train_data&quot;\n                            ].S3Output.S3Uri,\n                            content_type=&quot;text\/csv&quot;,\n                        ),\n                &quot;dev&quot;: sagemaker.TrainingInput(\n                            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                            &quot;dev_data&quot;\n                            ].S3Output.S3Uri,\n                            content_type=&quot;text\/csv&quot;\n                        ),\n                &quot;test&quot;: sagemaker.TrainingInput(\n                            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                            &quot;test_data&quot;\n                            ].S3Output.S3Uri,\n                            content_type=&quot;text\/csv&quot;\n                        ),\n        },\n)\nstep_register = RegisterModel(\n            name=&quot;RegisterModel&quot;,\n            estimator=pytorch_estimator,\n            model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n            content_types=[&quot;text\/csv&quot;],\n            response_types=[&quot;text\/csv&quot;],\n            inference_instances=[&quot;ml.t2.medium&quot;, &quot;ml.m5.large&quot;],\n            transform_instances=[&quot;ml.m5.large&quot;],\n            model_package_group_name=model_package_group_name,\n            approval_status=model_approval_status,\n        )\n    \npipeline = Pipeline(\n            name=pipeline_name,\n            parameters=[\n                processing_instance_type,\n                processing_instance_count,\n                training_instance_type,\n                model_approval_status,\n                input_data,\n            ],\n            steps=[step_process, step_train, step_register],\n            sagemaker_session=sagemaker_session,\n        )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1633941357330,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1633953506383,
        "Question_score_count":0.0,
        "Question_view_count":258.0,
        "Poster_created_time":1450889293150,
        "Poster_location":"Finland",
        "Poster_reputation_count":398.0,
        "Poster_view_count":28.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69523257",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error deploi pytorch model pipelin registermodel provid exampl deploi pytorch model pipelin mlop templat mlop templat model build tra deploy studio build mlop project templat pipelin build pipelin preprocess train regist model deploy script implement yaml file emploi cloudform run deploy script trigger automat model regist templat xgboost model train data deploi model want us pytorch deploi successfulli replac pytorch xgboost successfulli preprocess data train model regist model us infer model error model deploy error log updat endpoint filenotfounderror errno file directori opt model code infer tri exampl infer pytorch model couldn exampl us pipelin registermodel help appreci pipelin train regist model pytorch estim import pytorch workflow pipelin import pipelin workflow step import processingstep trainingstep workflow step collect import registermodel pytorch estim pytorch entri point path join base dir train instanc type xlarg instanc count role role framework version version hyperparamet epoch batch size learn rate step train trainingstep trainmodel estim pytorch estim input train traininginput data step process properti processingoutputconfig output train data soutput suri content type text csv dev traininginput data step process properti processingoutputconfig output dev data soutput suri content type text csv test traininginput data step process properti processingoutputconfig output test data soutput suri content type text csv step regist registermodel registermodel estim pytorch estim model data step train properti modelartifact smodelartifact content type text csv respons type text csv infer instanc medium larg transform instanc larg model packag group model packag group approv statu model approv statu pipelin pipelin pipelin paramet process instanc type process instanc count train instanc type model approv statu input data step step process step train step regist session session",
        "Question_preprocessed_content":"error deploi pytorch model pipelin registermodel provid exampl deploi pytorch model pipelin mlop templat studio build mlop project templat pipelin build pipelin preprocess train regist model deploy script implement yaml file emploi cloudform run deploy script trigger automat model regist templat xgboost model train data deploi model want us pytorch deploi successfulli replac pytorch xgboost successfulli preprocess data train model regist model us model error model deploy error log updat endpoint tri exampl pytorch model couldn exampl us pipelin registermodel help appreci pipelin train regist model",
        "Question_gpt_summary_original":"The user is encountering an error while deploying a PyTorch model using SageMaker Pipeline and RegisterModel. They have successfully replaced XGBoost with PyTorch and preprocessed, trained, and registered the model, but they did not use inference.py in their model, resulting in a FileNotFoundError when updating the endpoint. The user is seeking help in finding an example of using inference.py for PyTorch model with SageMaker Pipeline and RegisterModel.",
        "Question_gpt_summary":"user encount error deploi pytorch model pipelin registermodel successfulli replac xgboost pytorch preprocess train regist model us infer model result filenotfounderror updat endpoint user seek help find exampl infer pytorch model pipelin registermodel"
    },
    {
        "Question_title":"ROC and PR curves logging",
        "Question_body":"<p>Hi!<br>\nI am using (and loving) Wandb so far <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>Today I wanted to log my validation roc and pr curves, and I used the command:<\/p>\n<pre><code class=\"lang-auto\">wandb.log({\"val_roc\" : wandb.plot.roc_curve(target_list.numpy(), pred_list.numpy(), labels=None, classes_to_plot=None)})\n<\/code><\/pre>\n<p>My task is a binary classification, and my data is in numpy array in the format [m,n], with m the number of samples and n the number of classes, my case 1 (i.e. [128,1]).<\/p>\n<p>I am encountering the following error:<\/p>\n<pre><code class=\"lang-auto\">  File \"\/home\/mgiordano\/.pyenv\/versions\/3.8.11\/envs\/sepsis\/lib\/python3.8\/site-packages\/wandb\/plot\/roc_curve.py\", line 74, in roc_curve\n    y_true, y_probas[..., i], pos_label=classes[i]\nIndexError: index 1 is out of bounds for axis 1 with size 1\n<\/code><\/pre>\n<p>I think Wandb is trying to compute the curves on other classes, that are not there. Am I missing something?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":14,
        "Question_comment_count":0,
        "Question_created_time":1673480583638,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":232.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/roc-and-pr-curves-logging\/3686",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"roc curv log love far todai want log valid roc curv command log val roc plot roc curv target list numpi pred list numpi label class plot task binari classif data numpi arrai format number sampl number class case encount follow error file home mgiordano pyenv version env sepsi lib python site packag plot roc curv line roc curv true proba po label class indexerror index bound axi size think try comput curv class miss thank",
        "Question_preprocessed_content":"roc curv log far todai want log valid roc curv command task binari classif data numpi arrai format number sampl number class case encount follow error think try comput curv class miss thank",
        "Question_gpt_summary_original":"The user is trying to log their validation ROC and PR curves using Wandb, but encounters an error due to the format of their data. The error suggests that Wandb is trying to compute the curves on other classes that are not present in the data. The user is seeking clarification on whether they are missing something.",
        "Question_gpt_summary":"user try log valid roc curv encount error format data error suggest try comput curv class present data user seek clarif miss"
    },
    {
        "Question_title":"Unable to use private docker registry with latest Azure ML release",
        "Question_body":"<p>Since the latest Azure ML release, we have been unable to submit any job using a private docker registry. Same jobs were working before the new release.  <br \/>\nWe configure the job as follows (all of this is automated and the code has not changed):<\/p>\n<p>base_image_name = 'REDACTED.azurecr.io\/lb\/learning_box_azure_compute:0.1.15_1601582281'<\/p>\n<pre><code># Set the container registry information  \n\nmyenv = Environment(name=&amp;#34;lb&amp;#34;)  \n\nmyenv.docker.enabled = True  \nmyenv.docker.base_image = base_image_name  \nmyenv.docker.base_image_registry.address = &amp;#39;REDACTED.azurecr.io\/lb\/&amp;#39;  \nmyenv.docker.base_image_registry.username, myenv.docker.base_image_registry.password = get_docker_secrets()  \nmyenv.python.user_managed_dependencies = True  \n\nmyenv.python.interpreter_path = &amp;#34;\/opt\/miniconda\/bin\/python&amp;#34;  \n<\/code><\/pre>\n<p>Instead of successful job submission, we are instead getting:  <br \/>\n{  <br \/>\n&quot;error&quot;: {  <br \/>\n&quot;message&quot;: &quot;Activity Failed:\\n{\\n \\&quot;error\\&quot;: {\\n \\&quot;code\\&quot;: \\&quot;UserError\\&quot;,\\n \\&quot;message\\&quot;: \\&quot;Unable to get image details : Specified base docker image REDACTED.azurecr.io\/lb\/learning_box_azure_compute:0.1.15_16\\&quot;,\\n \\&quot;details\\&quot;: []\\n },\\n \\&quot;correlation\\&quot;: {\\n \\&quot;operation\\&quot;: null,\\n \\&quot;request\\&quot;: \\&quot;c41448d429f9c80b\\&quot;\\n },\\n \\&quot;environment\\&quot;: \\&quot;eastus\\&quot;,\\n \\&quot;location\\&quot;: \\&quot;eastus\\&quot;,\\n \\&quot;time\\&quot;: \\&quot;2020-11-09T21:40:39.699533Z\\&quot;,\\n \\&quot;componentName\\&quot;: \\&quot;execution-worker\\&quot;\\n}&quot;  <br \/>\n}  <br \/>\n}  <br \/>\nThe image has not changed (we tried a few different ones from prior successful jobs) and the use of the SDK has not changed.  <br \/>\nHas anybody else encountered a similar problem since the Nov 5 upgrade (<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes<\/a>)?  <br \/>\nThis is a major block as we cannot proceed with any project that depend on Azure ML at this time.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_created_time":1604958506767,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p><a href=\"\/users\/na\/?userid=d02d6aeb-d51f-460e-9be5-e8da649952cc\">@Fabien Campagne  <\/a>  Thanks for the details, with fully qualified base image name you do not need to specify container registry address. container registry address itself should be just a host name.    <\/p>\n",
        "Answer_comment_count":2.0,
        "Question_closed_time":1605016029612,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/157021\/unable-to-use-private-docker-registry-with-latest",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unabl us privat docker registri latest releas latest releas unabl submit job privat docker registri job work new releas configur job follow autom code chang base imag redact azurecr learn box azur comput set contain registri inform myenv environ myenv docker enabl true myenv docker base imag base imag myenv docker base imag registri address redact azurecr myenv docker base imag registri usernam myenv docker base imag registri password docker secret myenv python user manag depend true myenv python interpret path opt miniconda bin python instead success job submiss instead get error messag activ fail error code usererror messag unabl imag detail specifi base docker imag redact azurecr learn box azur comput detail correl oper null request cdfcb environ eastu locat eastu time componentnam execut worker imag chang tri differ on prior success job us sdk chang anybodi encount similar problem nov upgrad http learn microsoft com azur machin learn azur machin learn releas note major block proce project depend time",
        "Question_preprocessed_content":"unabl us privat docker registri latest releas latest releas unabl submit job privat docker registri job work new releas configur job follow instead success job submiss instead get error imag chang us sdk chang anybodi encount similar problem nov upgrad major block proce project depend time",
        "Question_gpt_summary_original":"The user is unable to submit any job using a private docker registry since the latest Azure ML release. The error message received is \"Unable to get image details\". The image and the use of the SDK have not changed. The user is seeking help from others who may have encountered a similar problem since the Nov 5 upgrade.",
        "Question_gpt_summary":"user unabl submit job privat docker registri latest releas error messag receiv unabl imag detail imag us sdk chang user seek help encount similar problem nov upgrad"
    },
    {
        "Question_title":"ValidationException Importing from Redshift into Data Wrangler",
        "Question_body":"<p>I'm trying to build a model workflow in AWS SageMaker using Data Wrangler for preprocessing. I'm loading data from various tables in a Redshift instance, before mutating and joining them as required to build the model input data.<\/p>\n<p>I'm a contractor working for a company who has provisioned some resource in their AWS environment for me to work, and am reading from a production database. If I do not load open the Data Wrangler flow early enough in the day (which I suspect is related to load on their system), some of the nodes which I have created will not validate, and instead show a red cross and the following error message:<\/p>\n<p><code>RedshiftQueryExecutionIdValidationError: An error occurred when trying to invoke `describe_statement`: An error occurred (ValidationException) when calling the DescribeStatement operation: Could not retrieve the query result as it has expired after 1655759552.<\/code><\/p>\n<p>The remaining un-errored nodes appear to hang in a loading\/validating state. Here's a screenshot of part of the flow in this state:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/AewzK.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AewzK.png\" alt=\"An image showing some currently loading AWS Data Wrangler nodes, with some errored nodes joining in towards the end of the flow, rendering the end product errored and thus unusable\" \/><\/a><\/p>\n<p>I'm not sure if it's related, but I occasionally see error messages pop up saying something about &quot;too many inflight requests&quot;.<\/p>\n<p>My main issue, I think, is a lack of context. I have not worked in this environment before, and am finding it difficult to diagnose the issue. It might be possible to provision more resource, and I could likely trim down some of the information before reading it in, but I'd like to be able to read the error messages and understand what's <em>causing<\/em> the nodes to error, so that I can decide on the appropriate course of action.<\/p>\n<p>Can somebody please help explain what's going on here?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_created_time":1655774625443,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":57.0,
        "Poster_created_time":1427947978892,
        "Poster_location":"Auckland, New Zealand",
        "Poster_reputation_count":25.0,
        "Poster_view_count":4.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72694415",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"validationexcept import redshift data wrangler try build model workflow data wrangler preprocess load data tabl redshift instanc mutat join requir build model input data contractor work compani provis resourc aw environ work read product databas load open data wrangler flow earli dai suspect relat load node creat valid instead red cross follow error messag redshiftqueryexecutionidvalidationerror error occur try invok statement error occur validationexcept call describestat oper retriev queri result expir remain error node appear hang load valid state screenshot flow state sure relat occasion error messag pop sai inflight request main issu think lack context work environ find difficult diagnos issu possibl provis resourc like trim inform read like abl read error messag understand caus node error decid appropri cours action somebodi help explain go",
        "Question_preprocessed_content":"validationexcept import redshift data wrangler try build model workflow data wrangler preprocess load data tabl redshift instanc mutat join requir build model input data contractor work compani provis resourc aw environ work read product databas load open data wrangler flow earli dai node creat valid instead red cross follow error messag remain node appear hang state screenshot flow state sure relat occasion error messag pop sai inflight request main issu think lack context work environ find difficult diagnos issu possibl provis resourc like trim inform read like abl read error messag understand caus node error decid appropri cours action somebodi help explain go",
        "Question_gpt_summary_original":"The user is encountering a ValidationException error while importing data from Redshift into Data Wrangler for preprocessing in AWS SageMaker. Some nodes are not validating and showing a red cross with an error message related to query result expiration. The un-errored nodes are hanging in a loading\/validating state, and the user occasionally sees error messages about too many inflight requests. The user is seeking help to understand the cause of the error messages and decide on the appropriate course of action.",
        "Question_gpt_summary":"user encount validationexcept error import data redshift data wrangler preprocess node valid show red cross error messag relat queri result expir error node hang load valid state user occasion see error messag inflight request user seek help understand caus error messag decid appropri cours action"
    },
    {
        "Question_title":"Wandb process not getting terminated properly",
        "Question_body":"<p>My process is not getting terminated properly (running in a multi-GPU setting). It is trying to upload information but gets stuck for some reason. I am facing this problem since yesterday, and haven\u2019t made any changes to the version of the library (although this didn\u2019t get resolved after upgrading the library to the latest version). Any help will be highly appreciated. I can disable wandb completely by passing <code>mode = \"disabled\"<\/code> in the test setting, but need it while running sweeps or logging training metrics.<br>\nP.S.: Same code was running just fine till yesterday.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/342fa335656c0535a9ca21307507f30ef53feb1d.png\" data-download-href=\"\/uploads\/short-url\/7rETgzwUz7nuCquITds6NyYKXVj.png?dl=1\" title=\"Screenshot 2021-10-30 at 6.52.11 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/342fa335656c0535a9ca21307507f30ef53feb1d_2_690x69.png\" alt=\"Screenshot 2021-10-30 at 6.52.11 PM\" data-base62-sha1=\"7rETgzwUz7nuCquITds6NyYKXVj\" width=\"690\" height=\"69\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/342fa335656c0535a9ca21307507f30ef53feb1d_2_690x69.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/342fa335656c0535a9ca21307507f30ef53feb1d_2_1035x103.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/342fa335656c0535a9ca21307507f30ef53feb1d.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/342fa335656c0535a9ca21307507f30ef53feb1d_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2021-10-30 at 6.52.11 PM<\/span><span class=\"informations\">1048\u00d7106 27.4 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1635600298677,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":337.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-process-not-getting-terminated-properly\/1166",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"process get termin properli process get termin properli run multi gpu set try upload inform get stuck reason face problem yesterdai havent chang version librari didnt resolv upgrad librari latest version help highli appreci disabl complet pass mode disabl test set need run sweep log train metric code run fine till yesterdai screenshot",
        "Question_preprocessed_content":"process get termin properli process get termin properli try upload inform get stuck reason face problem yesterdai havent chang version librari help highli appreci disabl complet pass test set need run sweep log train metric code run fine till yesterdai screenshot",
        "Question_gpt_summary_original":"The user is facing an issue with the Wandb process not getting terminated properly while running in a multi-GPU setting. The process gets stuck while trying to upload information, and the problem persists even after upgrading the library to the latest version. The user seeks help to resolve the issue as they need Wandb while running sweeps or logging training metrics.",
        "Question_gpt_summary":"user face issu process get termin properli run multi gpu set process get stuck try upload inform problem persist upgrad librari latest version user seek help resolv issu need run sweep log train metric"
    },
    {
        "Question_title":"Guild run hangs \/ very slow",
        "Question_body":"<p>I have successfully used guild in some older project, but in this new project I am having a hard time debugging what is going on.<\/p>\n<p>I am trying to do:<\/p>\n<pre><code>guild run model:train -y\n<\/code><\/pre>\n<p>But the operation hangs without any output to the console.<\/p>\n<p>If I do<\/p>\n<pre><code>guild run model:train -y --print-cmd\n<\/code><\/pre>\n<p>And execute it directly with python and exact same arguments it gets executed right away.<\/p>\n<p>How do I debug this behavior?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1601685221068,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":443.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-run-hangs-very-slow\/362",
        "Tool":"Guild AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"run hang slow successfulli older project new project have hard time debug go try run model train oper hang output consol run model train print cmd execut directli python exact argument get execut right awai debug behavior",
        "Question_preprocessed_content":"run hang slow successfulli older project new project have hard time debug go try oper hang output consol execut directli python exact argument get execut right awai debug behavior",
        "Question_gpt_summary_original":"The user is experiencing a problem with Guild where a guild run hangs or runs very slowly without any output to the console. The user has tried to debug the issue by running the command with the \"--print-cmd\" option and executing it directly with Python, which works fine. The user is seeking advice on how to debug this behavior.",
        "Question_gpt_summary":"user experienc problem run hang run slowli output consol user tri debug issu run command print cmd option execut directli python work fine user seek advic debug behavior"
    },
    {
        "Question_title":"Object Detection in AWS + Sagemaker Neo",
        "Question_body":"<p>I am trying the inbuilt object detection algorithm available on AWS for a computer vision problem. The training job ran successfully and I have received the model artifacts in the .tar.gz format in an S3 bucket.<\/p>\n<p>To reduce the model footprint, we need to use Sagemaker Neo - a compilation job on the available model artifacts. The compilation job fails with the error - &quot;ClientError: OperatorNotImplemented:('One or more operators are not supported in frontend MXNet:\\n_contrib_MultiBoxTarget: 1\\nMakeLoss: 3&quot;<\/p>\n<p>How can this be resolved?\nAround August 2019, Sagemaker Neo did not support models trained with built-in Sagemaker Object Detection Algorithm. Is there any change in this status today ?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1607416900990,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":165.0,
        "Poster_created_time":1577092851692,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":13.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65195685",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"object detect aw neo try inbuilt object detect algorithm avail aw vision problem train job ran successfulli receiv model artifact tar format bucket reduc model footprint need us neo compil job avail model artifact compil job fail error clienterror operatornotimpl oper support frontend mxnet contrib multiboxtarget nmakeloss resolv august neo support model train built object detect algorithm chang statu todai thank",
        "Question_preprocessed_content":"object detect aw neo try inbuilt object detect algorithm avail aw vision problem train job ran successfulli receiv model artifact format bucket reduc model footprint need us neo compil job avail model artifact compil job fail error clienterror operatornotimpl oper support frontend resolv august neo support model train object detect algorithm chang statu todai thank",
        "Question_gpt_summary_original":"The user has encountered an error while trying to use Sagemaker Neo to reduce the model footprint of an object detection algorithm trained on AWS. The compilation job fails with the error message \"ClientError: OperatorNotImplemented\". The user is seeking a solution to this issue and also inquiring about any updates on whether Sagemaker Neo now supports models trained with the built-in Sagemaker Object Detection Algorithm.",
        "Question_gpt_summary":"user encount error try us neo reduc model footprint object detect algorithm train aw compil job fail error messag clienterror operatornotimpl user seek solut issu inquir updat neo support model train built object detect algorithm"
    },
    {
        "Question_title":"How should I mount docker volumes in mlflow project?",
        "Question_body":"<p>I use <code>mlflow<\/code> in a docker environment as described in this <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker\" rel=\"nofollow noreferrer\">example<\/a> and I start my runs with <code>mlflow run .<\/code>.<\/p>\n\n<p>I get output like this<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>2019\/07\/17 16:08:16 INFO mlflow.projects: === Building docker image mlflow-myproject-ab8e0e4 ===\n2019\/07\/17 16:08:18 INFO mlflow.projects: === Created directory \/var\/folders\/93\/xt2vz36s7jd1fh9bkhkk9sgc0000gn\/T\/tmp1lxyqqw9 for downloading remote URIs passed to arguments of type 'path' ===\n2019\/07\/17 16:08:18 INFO mlflow.projects: === Running command 'docker run \n--rm -v \/Users\/foo\/bar\/mlruns:\/mlflow\/tmp\/mlruns -e \nMLFLOW_RUN_ID=ef21de61d8a6436b97b643e5cee64ae1 -e MLFLOW_TRACKING_URI=file:\/\/\/mlflow\/tmp\/mlruns -e MLFLOW_EXPERIMENT_ID=0 mlflow-myproject-ab8e0e4 python train.py' in run with ID 'ef21de61d8a6436b97b643e5cee64ae1' ===\n<\/code><\/pre>\n\n<p>I would like to mount a docker volume named <code>my_docker_volume<\/code> to the container\n at \nthe path <code>\/data<\/code>. So instead of the <code>docker run<\/code> shown above, I would like to\n use<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>docker run --rm --mount source=my_docker_volume,target=\/data -v \/Users\/foo\/bar\/mlruns:\/mlflow\/tmp\/mlruns -e MLFLOW_RUN_ID=ef21de61d8a6436b97b643e5cee64ae1 -e MLFLOW_TRACKING_URI=file:\/\/\/mlflow\/tmp\/mlruns -e MLFLOW_EXPERIMENT_ID=0 mlflow-myproject-ab8e0e4 python train.py\n<\/code><\/pre>\n\n<p>I see that I could in principle run it once without mounted volume and then \ncopy the <code>docker run ...<\/code> and add <code>--mount source=my_volume,target=\/data<\/code> but\n I'd rather use something like<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>mlflow run --mount source=my_docker_volume,target=\/data .\n<\/code><\/pre>\n\n<p>but this obviously doesn't work because --mount is not a parameter for \n<code>mlflow run<\/code>.\nWhat's the recommened way of mounting a docker volume then?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1563373349400,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1564754919900,
        "Question_score_count":4.0,
        "Question_view_count":1301.0,
        "Poster_created_time":1375955724343,
        "Poster_location":"Freiburg im Breisgau, Germany",
        "Poster_reputation_count":191.0,
        "Poster_view_count":11.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57078147",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"mount docker volum project us docker environ describ exampl start run run output like info project build docker imag myproject abe info project creat directori var folder xtvzsjdfhbkhkksgcgn tmplxyqqw download remot uri pass argument type path info project run command docker run user foo bar mlrun tmp mlrun run efdedabbeceea track uri file tmp mlrun experi myproject abe python train run efdedabbeceea like mount docker volum name docker volum contain path data instead docker run shown like us docker run mount sourc docker volum target data user foo bar mlrun tmp mlrun run efdedabbeceea track uri file tmp mlrun experi myproject abe python train principl run mount volum copi docker run add mount sourc volum target data us like run mount sourc docker volum target data obvious work mount paramet run recommen wai mount docker volum",
        "Question_preprocessed_content":"mount docker volum project us docker environ describ exampl start run output like like mount docker volum name contain path instead shown like us principl run mount volum copi add us like obvious work paramet recommen wai mount docker volum",
        "Question_gpt_summary_original":"The user is trying to mount a docker volume named \"my_docker_volume\" to the container at the path \"\/data\" while using mlflow in a docker environment. The user is looking for a recommended way to mount the docker volume as the \"--mount\" parameter is not available for \"mlflow run\" command.",
        "Question_gpt_summary":"user try mount docker volum name docker volum contain path data docker environ user look recommend wai mount docker volum mount paramet avail run command"
    },
    {
        "Question_title":"Kedro airflow on spark",
        "Question_body":"<p>Looking for kedro+ airflow implementation on spark. Is the plugin now available for spark ?<\/p>\n<p>Looked at PipelineX but couldn't find relevant examples on spark ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1597352928350,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":186.0,
        "Poster_created_time":1422274976252,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":33.0,
        "Poster_view_count":14.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63403226",
        "Tool":"Kedro",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"airflow spark look airflow implement spark plugin avail spark look pipelinex couldn relev exampl spark",
        "Question_preprocessed_content":"airflow spark look airflow implement spark plugin avail spark look pipelinex couldn relev exampl spark",
        "Question_gpt_summary_original":"The user is looking for a Kedro and Airflow implementation on Spark and is inquiring if a plugin is available for Spark. They have looked at PipelineX but could not find relevant examples on Spark.",
        "Question_gpt_summary":"user look airflow implement spark inquir plugin avail spark look pipelinex relev exampl spark"
    },
    {
        "Question_title":"ERROR: The provided hyperparameter space cannot be interpreted",
        "Question_body":"<p>I'm training a KNN classifier from sklearn, and I want to use BayersianParameterSampling for hypertununing parameters. I have this code:   <\/p>\n<pre><code>run_config = ScriptRunConfig(\n    source_directory='.', script='train.py', arguments=['--input-data', input_ds.as_named_input('data')], \n    environment=_env, compute_target=cluster\n)\n\nhyper_params = BayesianParameterSampling(parameter_space={\n    '--n_neighbors': choice(range(5, 11)),\n    '--weights': choice('uniform', 'distance'),\n    '--leaf_size': choice(range(30, 101)),\n    '--p': choice(1, 2)\n})\n\nhd_config = HyperDriveConfig(\n    run_config=run_config, hyperparameter_sampling=hyper_params, policy=None, \n    primary_metric_name='AUC', primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n    max_total_runs=80, max_concurrent_runs=2)\n\nexperiment = Experiment(ws, 'churn-hyperdrive')\nhyperdrive_run = experiment.submit(hd_config)\n\nhyperdrive_run.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>When I submit the experiment, I get an error saying there's something wrong with the parameter space I have. I'm passing the parameters as arguments to a simple script, train.py, which only parses the args, sets the values in the KNN classifier, logs a few metrics, and saves the model.  <\/p>\n<p>What am I doing wrong here? I've went over everything multiple times, and I don't think there's a mistake. The error I'm getting:  <\/p>\n<pre><code>&quot;&lt;START&gt;[2022-03-05T13:56:17.215074][API][INFO]Experiment created&lt;END&gt;\\n&quot;&quot;&lt;START&gt;[2022-03-05T13:56:18.343781][GENERATOR][ERROR]Exception in creating bayesian optimization: ArgumentException:\\n\\tMessage: Got an invalid parameter space for [Random sampling]: [The provided hyperparameter space cannot be interpreted.]\\n\\tInnerException None\\n\\tErrorResponse \\n{\\n    \\&quot;error\\&quot;: {\\n        \\&quot;code\\&quot;: \\&quot;UserError\\&quot;,\\n        \\&quot;message\\&quot;: \\&quot;Got an invalid parameter space for [Random sampling]: [The provided hyperparameter space cannot be interpreted.]\\&quot;,\\n        \\&quot;inner_error\\&quot;: {\\n            \\&quot;code\\&quot;: \\&quot;BadArgument\\&quot;,\\n            \\&quot;inner_error\\&quot;: {\\n                \\&quot;code\\&quot;: \\&quot;ArgumentInvalid\\&quot;\\n            }\\n        }\\n    }\\n}.&lt;END&gt;\\n&quot;&quot;&lt;START&gt;[2022-03-05T13:56:18.343632][GENERATOR][INFO]Trying to sample '2' jobs from the hyperparameter space&lt;END&gt;\\n&quot;\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_created_time":1646489041427,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/760594\/error-the-provided-hyperparameter-space-cannot-be",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error provid hyperparamet space interpret train knn classifi sklearn want us bayersianparametersampl hypertunun paramet code run config scriptrunconfig sourc directori script train argument input data input name input data environ env comput target cluster hyper param bayesianparametersampl paramet space neighbor choic rang weight choic uniform distanc leaf size choic rang choic config hyperdriveconfig run config run config hyperparamet sampl hyper param polici primari metric auc primari metric goal primarymetricgo maxim max total run max concurr run experi experi churn hyperdr hyperdr run experi submit config hyperdr run wait complet output true submit experi error sai wrong paramet space pass paramet argument simpl script train pars arg set valu knn classifi log metric save model wrong went multipl time think mistak error get api info experi creat gener error except creat bayesian optim argumentexcept tmessag got invalid paramet space random sampl provid hyperparamet space interpret tinnerexcept terrorrespons error code usererror messag got invalid paramet space random sampl provid hyperparamet space interpret inner error code badargu inner error code argumentinvalid gener info try sampl job hyperparamet space",
        "Question_preprocessed_content":"error provid hyperparamet space interpret train knn classifi sklearn want us bayersianparametersampl hypertunun paramet code submit experi error sai wrong paramet space pass paramet argument simpl script pars arg set valu knn classifi log metric save model wrong went multipl time think mistak error get",
        "Question_gpt_summary_original":"The user is encountering an error while using BayesianParameterSampling for hyperparameter tuning in a KNN classifier from sklearn. The error message states that the provided hyperparameter space cannot be interpreted, and the user is unsure of what they are doing wrong. The error occurs when the user submits the experiment.",
        "Question_gpt_summary":"user encount error bayesianparametersampl hyperparamet tune knn classifi sklearn error messag state provid hyperparamet space interpret user unsur wrong error occur user submit experi"
    },
    {
        "Question_title":"Graphs out of sync with each other",
        "Question_body":"<p>This happens to multiple users on other projects as well, not just me. If you look at the graphs here, as an example (<a href=\"https:\/\/wandb.ai\/kaiyotech\/KaiBumBot?workspace=user-kaiyotech\" class=\"inline-onebox\">Weights &amp; Biases<\/a>) you can see that if you move your mouse to the right side of the graph, they show different steps, so they\u2019re not in sync with each other. This makes it really complicated to actually nicely figure out what\u2019s happening with a run, and makes some graphs out of date with others. Occasionally the screen will refresh and some graphs will change to be more in date and others will move out of date, it seems random.<\/p>\n<p>Is there something I can do about this?<\/p>\n<p>Thanks,<br>\nKai<\/p>",
        "Question_answer_count":9,
        "Question_comment_count":0,
        "Question_created_time":1658864529811,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":209.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/graphs-out-of-sync-with-each-other\/2803",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"graph sync happen multipl user project look graph exampl mous right graph differ step theyr sync make complic actual nice figur what happen run make graph date occasion screen refresh graph chang date date random thank kai",
        "Question_preprocessed_content":"graph sync happen multipl user project look graph exampl mous right graph differ step theyr sync make complic actual nice figur what happen run make graph date occasion screen refresh graph chang date date random thank kai",
        "Question_gpt_summary_original":"Multiple users are facing an issue where graphs are out of sync with each other on different projects. The graphs show different steps when the mouse is moved to the right side, making it difficult to figure out what's happening with a run. The issue causes some graphs to be out of date with others, and the screen refreshes randomly. The user is seeking a solution to this problem.",
        "Question_gpt_summary":"multipl user face issu graph sync differ project graph differ step mous move right make difficult figur happen run issu caus graph date screen refresh randomli user seek solut problem"
    },
    {
        "Question_title":"How do you load an existing S3 bucket in Sagemaker using R Programming?",
        "Question_body":"<p>I know how to use python to load an existing s3 bucket in sage maker using R. Something like this:<\/p>\n\n<pre><code>role = get_execution_role()\nregion = boto3.Session().region_name\nbucket='existing S3 Bucket' \ndata_key = 'Data file in the existing s3 bucket'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n<\/code><\/pre>\n\n<p>How can one recreate this using R in Sage maker? All i see in the available documentation is how to create a new bucket but none of it mentions how to use an existing S3 bucket. Help would be appreciated.<\/p>\n\n<p>link to documentation for R in sage maker:\n<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-r-with-amazon-sage\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-r-with-amazon-sage<\/a> maker\/<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1575069896083,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":1591.0,
        "Poster_created_time":1575069492660,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59111930",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"load exist bucket program know us python load exist bucket sage maker like role execut role region boto session region bucket exist bucket data kei data file exist bucket data locat format bucket data kei recreat sage maker avail document creat new bucket mention us exist bucket help appreci link document sage maker http aw amazon com blog machin learn amazon sage maker",
        "Question_preprocessed_content":"load exist bucket program know us python load exist bucket sage maker like recreat sage maker avail document creat new bucket mention us exist bucket help appreci link document sage maker maker",
        "Question_gpt_summary_original":"The user is seeking help to load an existing S3 bucket in SageMaker using R programming. They have provided a Python code snippet for reference but are unable to find any documentation on how to do the same in R. The user is requesting assistance in recreating the Python code in R.",
        "Question_gpt_summary":"user seek help load exist bucket program provid python code snippet refer unabl document user request assist recreat python code"
    },
    {
        "Question_title":"Keras Custom Data Generator - Stuck on First Epoch, No Output?",
        "Question_body":"<p>I've been trying to get a multi-input data generator to work in Keras for a muti-input model. The inputs are in the form of an image and an associated number.<\/p>\n<p>I've tried two different custom data generators, but the simpler one merely uses ImageDataGenerator and flowfromdataframe with two outputs. Later on, I switch one of the outputs into an input and feed it into the model. The relevant code is as follows, where y_col is the output, number_col is the associated number and path_col is the path to the images:<\/p>\n<pre><code># data generator\ndf_gen = img_data_gen.flow_from_dataframe(\n        **all_args,\n        x_col=path_col,\n        y_col=[y_col, number_col],\n        shuffle=False,\n        class_mode='raw')\n\n  \n\n\n# sending data to model, wrapped in a larger function\nwhile True:\n    data_batch = next(df_gen)\n\n    #fake data, works in the model perfectly\n    number_labels = np.random.randint(1,219,len(data_batch[1]))\n  \n    outputdata, numberdata = data_batch[1].T\n    outputdata = np.asarray(outputdata).astype('float32')\n\n    #this code never works, the model freezes\n    numberdata = np.asarray(numberdata).astype(np.int32)\n  \n    yield [numberdata, data_batch[0]], outputdata\n\n#fitting the model\nhistory = model.fit(\n            train_generator,\n            steps_per_epoch=int(len(train_df) \/ batch_size),\n            validation_data=val_generator,\n            validation_steps=int(len(validation_df) \/ batch_size),\n            epochs=epochs,\n            callbacks=callbacks,\n            verbose = 1\n        )\n<\/code><\/pre>\n<p><strong>When I run this model, the output freezes at 'Epoch 1\/12'.<\/strong> I've checked that the data is in the right format, the right length, and matching properly to the other input.<\/p>\n<p>When I generate a random list of numbers, the model runs perfectly. I can also see that when fake data is generated, the number data is also getting generated.<\/p>\n<p>However, when I use the correct number data as an input into the model, the model <strong>freezes at the second 'next' call.<\/strong> I can also use a smaller sub-dataset with the same data structure and the model runs correctly. But when I use the entire dataset, the problem occurs again.<\/p>\n<p><strong>Do you know what could be causing this problem?<\/strong> I'm using AWS Sagemaker to run the model and can't seem to figure out where this problem is coming from. Thank you for your help!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1629139040917,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":248.0,
        "Poster_created_time":1629137397396,
        "Poster_location":null,
        "Poster_reputation_count":21.0,
        "Poster_view_count":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68807732",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"kera custom data gener stuck epoch output try multi input data gener work kera muti input model input form imag associ number tri differ custom data gener simpler mere us imagedatagener flowfromdatafram output later switch output input feed model relev code follow col output number col associ number path col path imag data gener gen img data gen flow datafram arg col path col col col number col shuffl fals class mode raw send data model wrap larger function true data batch gen fake data work model perfectli number label random randint len data batch outputdata numberdata data batch outputdata asarrai outputdata astyp float code work model freez numberdata asarrai numberdata astyp int yield numberdata data batch outputdata fit model histori model fit train gener step epoch int len train batch size valid data val gener valid step int len valid batch size epoch epoch callback callback verbos run model output freez epoch check data right format right length match properli input gener random list number model run perfectli fake data gener number data get gener us correct number data input model model freez second us smaller sub dataset data structur model run correctli us entir dataset problem occur know caus problem run model figur problem come thank help",
        "Question_preprocessed_content":"kera custom data gener stuck epoch output try data gener work kera model input form imag associ number tri differ custom data gener simpler mere us imagedatagener flowfromdatafram output later switch output input feed model relev code follow output associ number path imag run model output freez epoch check data right format right length match properli input gener random list number model run perfectli fake data gener number data get gener us correct number data input model model freez second us smaller data structur model run correctli us entir dataset problem occur know caus problem run model figur problem come thank help",
        "Question_gpt_summary_original":"The user is facing issues with a custom data generator in Keras for a multi-input model. The model freezes at the second 'next' call when the correct number data is used as input. The user has tried two different custom data generators, and the simpler one uses ImageDataGenerator and flowfromdataframe with two outputs. The user is seeking help to identify the cause of the problem.",
        "Question_gpt_summary":"user face issu custom data gener kera multi input model model freez second correct number data input user tri differ custom data gener simpler us imagedatagener flowfromdatafram output user seek help identifi caus problem"
    },
    {
        "Question_title":"Dynamic parameter import",
        "Question_body":"<p>Hi, I have the following scenario: I\u2019m working with a Computer Vision Deep Learning framework for which I basically can write a config file only and train. This config file is in python. I would like to input this config file as parameter dependency (under params in dvc.yaml) such that I have one place where I can change the parameters. If I don\u2019t I need to maintain two places because some things I have to set in the <code>config.py<\/code> file\u2026<\/p>\n<p>I know I can specify the <code>config.py <\/code>as a params file. However, I have different architectures with potentially different parameters I would like to change. I made a stage using templating that switches the <code>config.py<\/code> based on the architecture name. Is it possible to create a <code>deps.yaml<\/code> (or any file) that contains a list of the keys present in <code>config.py<\/code> that should be considered parameters and ignore all the other valid potential parameters in <code>config.py<\/code>?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1661757180755,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":178.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dynamic-parameter-import\/1312",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"dynam paramet import follow scenario work vision deep learn framework basic write config file train config file python like input config file paramet depend param yaml place chang paramet dont need maintain place thing set config file know specifi config param file differ architectur potenti differ paramet like chang stage templat switch config base architectur possibl creat dep yaml file contain list kei present config consid paramet ignor valid potenti paramet config thank",
        "Question_preprocessed_content":"dynam paramet import follow scenario work vision deep learn framework basic write config file train config file python like input config file paramet depend place chang paramet dont need maintain place thing set file know specifi param file differ architectur potenti differ paramet like chang stage templat switch base architectur possibl creat contain list kei present consid paramet ignor valid potenti paramet thank",
        "Question_gpt_summary_original":"The user is working with a Computer Vision Deep Learning framework and wants to input a config file as a parameter dependency in dvc.yaml to have one place to change parameters. However, they have different architectures with potentially different parameters they would like to change. They made a stage using templating that switches the config.py based on the architecture name and want to know if it's possible to create a deps.yaml file that contains a list of keys present in config.py that should be considered parameters and ignore all other valid potential parameters in config.py.",
        "Question_gpt_summary":"user work vision deep learn framework want input config file paramet depend yaml place chang paramet differ architectur potenti differ paramet like chang stage templat switch config base architectur want know possibl creat dep yaml file contain list kei present config consid paramet ignor valid potenti paramet config"
    },
    {
        "Question_title":"How to avoid data averaging when logging to metric across multiple runs?",
        "Question_body":"<p>I'm trying to log data points for the same metric across multiple runs (<code>wandb.init<\/code> is called repeatedly in between each data point) and I'm unsure how to avoid the behavior seen in the attached screenshot...<\/p>\n<p>Instead of getting a line chart with multiple points, I'm getting a single data point with associated statistics. In the attached e.g., the 1st data point was generated at step 1,470 and the 2nd at step 2,940...rather than seeing two points, I'm instead getting a single point that's the average and appears at step 2,205.\n<a href=\"https:\/\/i.stack.imgur.com\/98Lln.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/98Lln.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>My hunch is that using the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming\" rel=\"nofollow noreferrer\">resume run<\/a> feature may address my problem, but even testing out this hunch is proving to be cumbersome given the constraints of the system I'm working with...<\/p>\n<p>Before I invest more time in my hypothesized solution, could someone confirm that the behavior I'm seeing is, indeed, the result of logging data to the same metric across separate runs without using the resume feature?<\/p>\n<p>If this is the case, can you confirm or deny my conception of how to use resume?<\/p>\n<p>Initial run:<\/p>\n<ol>\n<li><code>run = wandb.init()<\/code><\/li>\n<li><code>wandb_id = run.id<\/code><\/li>\n<li>cache <code>wandb_id<\/code> for successive runs<\/li>\n<\/ol>\n<p>Successive run:<\/p>\n<ol>\n<li>retrieve <code>wandb_id<\/code> from cache<\/li>\n<li><code>wandb.init(id=wandb_id, resume=&quot;must&quot;)<\/code><\/li>\n<\/ol>\n<p>Is it also acceptable \/ preferable to replace <code>1.<\/code> and <code>2.<\/code> of the initial run with:<\/p>\n<ol>\n<li><code>wandb_id = wandb.util.generate_id()<\/code><\/li>\n<li><code>wandb.init(id=wandb_id)<\/code><\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1662443319723,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":35.0,
        "Poster_created_time":1352429442632,
        "Poster_location":null,
        "Poster_reputation_count":740.0,
        "Poster_view_count":63.0,
        "Answer_body":"<blockquote>\n<p>My hunch is that using the resume run feature may address my problem,<\/p>\n<\/blockquote>\n<p>Indeed, providing a cached <code>id<\/code> in combination with <code>resume=&quot;must&quot;<\/code> fixed the issue.\n<a href=\"https:\/\/i.stack.imgur.com\/KooZ9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KooZ9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Corresponding snippet:<\/p>\n<pre><code>import wandb\n\n# wandb run associated with evaluation after first N epochs of training.\nwandb_id = wandb.util.generate_id()\nwandb.init(id=wandb_id, project=&quot;alrichards&quot;, name=&quot;test-run-3\/job-1&quot;, group=&quot;test-run-3&quot;)\nwandb.log({&quot;mean_evaluate_loss_epoch&quot;: 20}, step=1)\nwandb.finish()\n\n# wandb run associated with evaluation after second N epochs of training.\nwandb.init(id=wandb_id, resume=&quot;must&quot;, project=&quot;alrichards&quot;, name=&quot;test-run-3\/job-2&quot;, group=&quot;test-run-3&quot;)\nwandb.log({&quot;mean_evaluate_loss_epoch&quot;: 10}, step=5)\nwandb.finish()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1662569956687,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1352429442632,
        "Answerer_location":null,
        "Answerer_reputation_count":740.0,
        "Answerer_view_count":63.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73617230",
        "Tool":"Weights & Biases",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"avoid data averag log metric multipl run try log data point metric multipl run init call repeatedli data point unsur avoid behavior seen attach screenshot instead get line chart multipl point get singl data point associ statist attach data point gener step step see point instead get singl point averag appear step hunch resum run featur address problem test hunch prove cumbersom given constraint work invest time hypothes solut confirm behavior see result log data metric separ run resum featur case confirm deni concept us resum initi run run init run cach success run success run retriev cach init resum accept prefer replac initi run util gener init",
        "Question_preprocessed_content":"avoid data averag log metric multipl run try log data point metric multipl run unsur avoid behavior seen attach instead get line chart multipl point get singl data point associ statist attach data point gener step step see point instead get singl point averag appear step hunch resum run featur address problem test hunch prove cumbersom given constraint work invest time hypothes solut confirm behavior see result log data metric separ run resum featur case confirm deni concept us resum initi run cach success run success run retriev cach accept prefer replac initi run",
        "Question_gpt_summary_original":"The user is trying to log data points for the same metric across multiple runs using wandb.init, but instead of getting a line chart with multiple points, they are getting a single data point with associated statistics. The user suspects that using the resume run feature may address the problem, but they are unsure. They are seeking confirmation that the behavior they are seeing is the result of logging data to the same metric across separate runs without using the resume feature and guidance on how to use the resume feature.",
        "Question_gpt_summary":"user try log data point metric multipl run init instead get line chart multipl point get singl data point associ statist user suspect resum run featur address problem unsur seek confirm behavior see result log data metric separ run resum featur guidanc us resum featur"
    },
    {
        "Question_title":"Vertex AI model showing failure but 0 bytes in prediction.errors",
        "Question_body":"<p>I'm running a vertex batch job on a custom model with 676 instances in my jsonl.<\/p>\n<p>I get results for all 676 instances but the job shows &quot;Due to one or more errors, this batch prediction job was canceled on Aug 22, 2022 at 09:04PM&quot;<\/p>\n<p>The error file prediction.errors_stats-00000-of-00001 has 0 bytes and there is a directory:<\/p>\n<pre><code>beam-temp-prediction.errors_stats-e289aa8c228c11eda06642010a800fdc \n<\/code><\/pre>\n<p>with 4 files with 0 bytes\u00a0in them.<\/p>\n<pre><code>34e19e3d-e717-4c6f-860c-5c2a177f1e93.prediction.errors_stats\n7c9374c8-65f7-4a97-8a95-6068ba9cd936.prediction.errors_stats\n981a634b-d27c-4285-b871-648b25ca87b8.prediction.errors_stats\nf14dbc29-42d7-480e-8b79-95e799e132bf.prediction.errors_stats\n<\/code><\/pre>\n<p>Any ideas on why the job is showing failure but the results seem ok?<\/p>\n<p>Job params:<\/p>\n<pre><code>{&quot;displayName&quot;: &lt;MY_DISPLAY_NAME&gt;,\n &quot;model&quot;: &lt;MY_MODEL&gt;,\n &quot;inputConfig&quot;: {&quot;instancesFormat&quot;: &quot;jsonl&quot;,\n                 &quot;gcsSource&quot;: {&quot;uris&quot;: [&quot;gs:\/\/&lt;MY_BUCKET&gt;\/MY_INSTANCES.jsonl&quot;]}},\n &quot;outputConfig&quot;: {&quot;predictionsFormat&quot;: &quot;jsonl&quot;,\n                  &quot;gcsDestination&quot;: {&quot;outputUriPrefix&quot;: &quot;gs:\/\/&lt;MY_OUTPUTS&gt;\/outputs\/2022-08-23&quot;}},\n&quot;dedicatedResources&quot;: {&quot;machineSpec&quot;: {&quot;machineType&quot;: &quot;n1-standard-8&quot;, \n&quot;acceleratorType&quot;: null, &quot;acceleratorCount&quot;: null},\n&quot;startingReplicaCount&quot;: 2},\n&quot;manualBatchTuningParameters&quot;: {&quot;batch_size&quot;: 1}}\n<\/code><\/pre>\n<p>Note: I've also tried leaving out the 'startingReplicaCount and manualBatchTuningParameters.<\/p>\n<p>I'm using a curl command to initiate the batch job:<\/p>\n<pre><code>    curl -X POST \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\n-H &quot;Content-Type: application\/json; charset=utf-8&quot; \\\n-d @$INPUT_JSON \\\n&quot;https:\/\/$LOCATION-aiplatform.googleapis.com\/v1\/projects\/$PROJECT\/locations\/$LOCATION\/batchPredictionJobs&quot;\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1661278466123,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1661279037728,
        "Question_score_count":1.0,
        "Question_view_count":42.0,
        "Poster_created_time":1462581330168,
        "Poster_location":"Berkeley, CA, United States",
        "Poster_reputation_count":329.0,
        "Poster_view_count":65.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73463378",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"model show failur byte predict error run vertex batch job custom model instanc jsonl result instanc job show error batch predict job cancel aug error file predict error stat byte directori beam temp predict error stat eaaccedaafdc file bytesin eed cafe predict error stat bacd predict error stat bcab predict error stat fdbc eebf predict error stat idea job show failur result job param displaynam model inputconfig instancesformat jsonl gcssourc uri instanc jsonl outputconfig predictionsformat jsonl gcsdestin outputuriprefix output dedicatedresourc machinespec machinetyp standard acceleratortyp null acceleratorcount null startingreplicacount manualbatchtuningparamet batch size note tri leav startingreplicacount manualbatchtuningparamet curl command initi batch job curl post author bearer gcloud auth applic default print access token content type applic json charset utf input json http locat aiplatform googleapi com project project locat locat batchpredictionjob",
        "Question_preprocessed_content":"model show failur byte run vertex batch job custom model instanc jsonl result instanc job show error batch predict job cancel aug error file byte directori file bytesin idea job show failur result job param note tri leav startingreplicacount manualbatchtuningparamet curl command initi batch job",
        "Question_gpt_summary_original":"The user is running a vertex batch job on a custom model with 676 instances in their jsonl. They get results for all 676 instances but the job shows \"Due to one or more errors, this batch prediction job was canceled\" and the error file prediction.errors_stats-00000-of-00001 has 0 bytes. The user is unsure why the job is showing failure but the results seem okay. They have tried leaving out the 'startingReplicaCount and manualBatchTuningParameters' and are using a curl command to initiate the batch job.",
        "Question_gpt_summary":"user run vertex batch job custom model instanc jsonl result instanc job show error batch predict job cancel error file predict error stat byte user unsur job show failur result okai tri leav startingreplicacount manualbatchtuningparamet curl command initi batch job"
    },
    {
        "Question_title":"Runs never terminating in Dashboard even when requested to stop in UI with wandb-service killed",
        "Question_body":"<p>I\u2019ve been trying to stop my runs for a few days now with no luck. Every new experiment I track using wandb shows a state <em>running<\/em> , and when the experiment is completed, it stays in that state even when requested to stop within the runs overview or the project runs sidebar.<\/p>\n<p>The ones that I have requested to stop manually keep showing the <em>green dot<\/em> to indicate that the run is still active but shows state <em>stopping<\/em> (some of these have been in state <em>stopping<\/em> for more than three days). This occurs in all my projects and all runs. I\u2019ve tested with a new project, same issue.<\/p>\n<p>Any help would be greatly appreciated!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1682345647254,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":47.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/runs-never-terminating-in-dashboard-even-when-requested-to-stop-in-ui-with-wandb-service-killed\/4271",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"run termin dashboard request stop servic kill iv try stop run dai luck new experi track show state run experi complet stai state request stop run overview project run sidebar on request stop manual show green dot indic run activ show state stop state stop dai occur project run iv test new project issu help greatli appreci",
        "Question_preprocessed_content":"run termin dashboard request stop servic kill iv try stop run dai luck new experi track show state run experi complet stai state request stop run overview project run sidebar on request stop manual show green dot indic run activ show state stop occur project run iv test new project issu help greatli appreci",
        "Question_gpt_summary_original":"The user is facing an issue where their runs in the Dashboard are not terminating even when requested to stop in the UI with wandb-service killed. The runs show a state of \"running\" even after completion and those requested to stop manually show a \"green dot\" indicating they are still active but in a \"stopping\" state for more than three days. This issue is occurring in all projects and runs.",
        "Question_gpt_summary":"user face issu run dashboard termin request stop servic kill run state run complet request stop manual green dot indic activ stop state dai issu occur project run"
    },
    {
        "Question_title":"Given the following tree. Apply DFS and BFS as studied in the chapter. Show the state of the data structure Q and the visited list clearly at every",
        "Question_body":"<p>Given the following tree. Apply DFS and BFS as studied in the chapter. Show\nthe state of the data structure Q and the visited list clearly at every step. S is the\ninitial state and D is the goal state. <a href=\"https:\/\/i.stack.imgur.com\/Ya0fH.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_created_time":1653813896830,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":35.0,
        "Poster_created_time":1640023206990,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72422180",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"given follow tree appli df bf studi chapter state data structur visit list clearli given follow tree appli df bf studi chapter state data structur visit list clearli step initi state goal state enter imag descript",
        "Question_preprocessed_content":"given follow tree appli df bf studi chapter state data structur visit list clearli given follow tree appli df bf studi chapter state data structur visit list clearli step initi state goal state enter imag descript",
        "Question_gpt_summary_original":"The user is seeking assistance in applying DFS and BFS on a given tree, and wants to see the state of the data structure Q and the visited list at every step. The initial state is S and the goal state is D.",
        "Question_gpt_summary":"user seek assist appli df bf given tree want state data structur visit list step initi state goal state"
    },
    {
        "Question_title":"How to defined Serializer\/Deserializer while sagemaker create_endpoint using boto3 sdk",
        "Question_body":"<p>How can I defined serializer\/deserializer while using boto3 sdk client to create an sagemaker endpoint the same as passing them while using sagemaker sdk deploy() function like following?<\/p>\n<pre><code>from sagemaker.serializers import JSONSerializer\nfrom sagemaker.deserializers import JSONDeserializer\n\npredictor = estimator.deploy(\n    instance_type='ml.m4.xlarge',\n    initial_instance_count=1,\n    serializer=JSONSerializer(),\n    deserializer=JSONDeserializer(),\n)\n<\/code><\/pre>\n<p>My code:<\/p>\n<pre><code>    ......\n    endpoint_configs = sm_client.list_endpoint_configs(NameContains=endpoint_config_name)\n    if len(endpoint_configs['EndpointConfigs']) &gt; 0:\n        sm_client.delete_endpoint_config(EndpointConfigName = endpoint_config_name)\n    \n    create_endpoint_config_response = sm_client.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name,\n        ProductionVariants=[\n           {\n               &quot;InstanceType&quot;: &quot;ml.m4.xlarge&quot;,\n               &quot;InitialVariantWeight&quot;: 1,\n               &quot;InitialInstanceCount&quot;: 1,\n               &quot;ModelName&quot;: model_name,\n               &quot;VariantName&quot;: &quot;AllTraffic&quot;,\n           }\n        ]\n   )\n\n   endpoints = sm_client.list_endpoints(NameContains=endpoint_name)\n\n   if len(endpoints['Endpoints']) &gt; 0:\n       create_endpoint_response = sm_client.update_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n    else:\n        create_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_created_time":1657611857000,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":34.0,
        "Poster_created_time":1352858984688,
        "Poster_location":null,
        "Poster_reputation_count":213.0,
        "Poster_view_count":25.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72948711",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"defin serial deseri creat endpoint boto sdk defin serial deseri boto sdk client creat endpoint pass sdk deploi function like follow serial import jsonseri deseri import jsondeseri predictor estim deploi instanc type xlarg initi instanc count serial jsonseri deseri jsondeseri code endpoint config client list endpoint config namecontain endpoint config len endpoint config endpointconfig client delet endpoint config endpointconfignam endpoint config creat endpoint config respons client creat endpoint config endpointconfignam endpoint config productionvari instancetyp xlarg initialvariantweight initialinstancecount modelnam model variantnam alltraff endpoint client list endpoint namecontain endpoint len endpoint endpoint creat endpoint respons client updat endpoint endpointnam endpoint endpointconfignam endpoint config creat endpoint respons client creat endpoint endpointnam endpoint endpointconfignam endpoint config",
        "Question_preprocessed_content":"defin boto sdk defin boto sdk client creat endpoint pass sdk deploi function like follow code",
        "Question_gpt_summary_original":"The user is seeking guidance on how to define serializer\/deserializer while using boto3 sdk client to create an sagemaker endpoint, similar to passing them while using sagemaker sdk deploy() function. The user has provided their code for creating an endpoint using boto3 sdk.",
        "Question_gpt_summary":"user seek guidanc defin serial deseri boto sdk client creat endpoint similar pass sdk deploi function user provid code creat endpoint boto sdk"
    },
    {
        "Question_title":"Error while trying to run  data",
        "Question_body":"<p>i am new on Azure ML and trying to get familiar.i imported my data from the Web URL via HTTP. i tried running but it cant be completed as it keeps giving me Error 0030 , error while downloading the file , error 0039, error while completing operations.  <br \/>\nkindly help ou as i cant proceed<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1604177052600,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/147105\/error-while-trying-to-run-data",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error try run data new try familiar import data web url http tri run complet keep give error error download file error error complet oper kindli help proce",
        "Question_preprocessed_content":"error try run data new try import data web url http tri run complet keep give error error download file error error complet oper kindli help proce",
        "Question_gpt_summary_original":"The user is encountering errors while trying to run data on Azure ML. They imported data from a web URL via HTTP but are unable to complete the process due to errors 0030 and 0039 related to downloading and completing operations. The user is seeking assistance to proceed.",
        "Question_gpt_summary":"user encount error try run data import data web url http unabl complet process error relat download complet oper user seek assist proce"
    },
    {
        "Question_title":"Is it possible to run an MLFlow model from Spark Scala using UDFs?",
        "Question_body":"<p>In python you can <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.pyfunc.html#mlflow.pyfunc.spark_udf\" rel=\"nofollow noreferrer\">simply<\/a>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from pyspark.sql.functions import struct\n\npredict = mlflow.pyfunc.spark_udf(spark, &quot;\/my\/local\/model&quot;)\ndf.withColumn(&quot;prediction&quot;, predict(struct(&quot;name&quot;, &quot;age&quot;))).show()\n<\/code><\/pre>\n<p>Is it possible with scala?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1661799943870,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":22.0,
        "Poster_created_time":1581629032807,
        "Poster_location":null,
        "Poster_reputation_count":477.0,
        "Poster_view_count":56.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73533553",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"possibl run model spark scala udf python simpli pyspark sql function import struct predict pyfunc spark udf spark local model withcolumn predict predict struct ag possibl scala",
        "Question_preprocessed_content":"possibl run model spark scala udf python simpli possibl scala",
        "Question_gpt_summary_original":"The user is asking if it is possible to run an MLFlow model from Spark Scala using UDFs, similar to how it is done in Python.",
        "Question_gpt_summary":"user ask possibl run model spark scala udf similar python"
    },
    {
        "Question_title":"Control AWS Sagemaker costs",
        "Question_body":"<p>I want to use GPU capacity for deep learning models. Sagemaker is great in its flexibility of starting on demand clusters for training. However, my department wants to have guarantees we won't overspend on the AWS budget. Is there a way to 'cap' the costs without resorting to using a dedicated machine? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1592212014720,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":933.0,
        "Poster_created_time":1321823573476,
        "Poster_location":"Hilversum, Netherlands",
        "Poster_reputation_count":572.0,
        "Poster_view_count":39.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62384872",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"control cost want us gpu capac deep learn model great flexibl start demand cluster train depart want guarante won overspend aw budget wai cap cost resort dedic machin",
        "Question_preprocessed_content":"control cost want us gpu capac deep learn model great flexibl start demand cluster train depart want guarante won overspend aw budget wai cap cost resort dedic machin",
        "Question_gpt_summary_original":"The user wants to use GPU capacity for deep learning models using AWS Sagemaker, but their department wants to ensure that they don't overspend on the AWS budget. They are looking for a way to cap the costs without using a dedicated machine.",
        "Question_gpt_summary":"user want us gpu capac deep learn model depart want ensur overspend aw budget look wai cap cost dedic machin"
    },
    {
        "Question_title":"Is it possible to check that the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file?",
        "Question_body":"<p>My team has a set up wherein we track datasets and models in DVC, and have a GitLab repository for tracking our code and DVC metadata files. We have a job in our dev GitLab pipeline (run on each push to a merge request) that has the goal of checking to be sure that the developer remembered to run <code>dvc push<\/code> to keep DVC remote storage up-to-date. Right now, the way we do this is by running <code>dvc pull<\/code> on the GitLab runner, which will fail with errors telling you which files (new files or latest versions of existing files) were not found.<\/p>\n<p>The downside to this approach is that we are loading the entirety of our data stored in DVC onto a GitLab runner, and we've run into out-of-memory issues, not to mention lengthy run time to download all that data. Since the path and md5 hash of the objects are stored in the DVC metadata files, I would think that's all the information that DVC would need to be able to answer the question &quot;is the remote storage system up-to-date&quot;.<\/p>\n<p>It seems like <code>dvc status<\/code> is similar to what I'm asking for, but compares the cache or workspace and remote storage. In other words, it requires the files to actually be present on whatever filesystem is making the call.<\/p>\n<p>Is there some way to achieve the goal I laid out above (&quot;inform the developer that they need to run <code>dvc push<\/code>&quot;) without pulling everything from DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1622232629793,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1622257491983,
        "Question_score_count":5.0,
        "Question_view_count":488.0,
        "Poster_created_time":1618255062696,
        "Poster_location":null,
        "Poster_reputation_count":75.0,
        "Poster_view_count":2.0,
        "Answer_body":"<blockquote>\n<p>It seems like dvc status is similar to what I'm asking for<\/p>\n<\/blockquote>\n<p><code>dvc status --cloud<\/code> will give you a list of &quot;new&quot; files if they that haven't been pushed to the (default) remote. It won't error out though, so your CI script should fail depending on the stdout message.<\/p>\n<p>More info: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/status#options\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/status#options<\/a><\/p>\n<p>I'd also ask everyone to run <code>dvc install<\/code>, which will setup some Git hooks, including automatic <code>dvc push<\/code> with <code>git push<\/code>.<\/p>\n<p>See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/install\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/install<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1622257759208,
        "Answer_last_edit_time":1622503453296,
        "Answer_score_count":3.0,
        "Answerer_created_time":1305851487736,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67744934",
        "Tool":"DVC",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"possibl check version file track metadata file exist remot storag pull file team set track dataset model gitlab repositori track code metadata file job dev gitlab pipelin run push merg request goal check sure develop rememb run push remot storag date right wai run pull gitlab runner fail error tell file new file latest version exist file downsid approach load entireti data store gitlab runner run memori issu mention lengthi run time download data path hash object store metadata file think inform need abl answer question remot storag date like statu similar ask compar cach workspac remot storag word requir file actual present filesystem make wai achiev goal laid inform develop need run push pull",
        "Question_preprocessed_content":"possibl check version file track metadata file exist remot storag pull file team set track dataset model gitlab repositori track code metadata file job dev gitlab pipelin goal check sure develop rememb run remot storag right wai run gitlab runner fail error tell file downsid approach load entireti data store gitlab runner run issu mention lengthi run time download data path hash object store metadata file think inform need abl answer question remot storag like similar ask compar cach workspac remot storag word requir file actual present filesystem make wai achiev goal laid pull",
        "Question_gpt_summary_original":"The user's team tracks datasets and models in DVC and has a GitLab repository for tracking code and DVC metadata files. They have a job in their dev GitLab pipeline that checks if the developer has run \"dvc push\" to keep DVC remote storage up-to-date. Currently, they run \"dvc pull\" on the GitLab runner, which loads the entirety of their data stored in DVC onto the runner, causing out-of-memory issues and lengthy run time. The user is looking for a way to check if the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file.",
        "Question_gpt_summary":"user team track dataset model gitlab repositori track code metadata file job dev gitlab pipelin check develop run push remot storag date current run pull gitlab runner load entireti data store runner caus memori issu lengthi run time user look wai check version file track metadata file exist remot storag pull file"
    },
    {
        "Question_title":"How to organize inference results by model ID that produced it?",
        "Question_body":"<p>I am creating a deep ensemble, and I need to keep track of which inference results came from which neural network. However, enabling versioning via the Kedro catalog only organizes my inference results by datetime, with each file having an identical name.<\/p>\n<p>What is the best practice for organizing results by model ID? For example, is it possible to customize the versioned folder names, or the versioned file? E.g. <code>data\/07_model_output\/model_predictions.json\/2020-08-09T20.55.57.237Z-Model-04\/model_predictions.json<\/code> or <code>...\/2020-08-09T20.55.57.237Z\/model_predictions-Model-04.json<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1597007197307,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1597058346200,
        "Question_score_count":0.0,
        "Question_view_count":55.0,
        "Poster_created_time":1415053264667,
        "Poster_location":"USA",
        "Poster_reputation_count":11166.0,
        "Poster_view_count":653.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63331325",
        "Tool":"Kedro",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"organ infer result model produc creat deep ensembl need track infer result came neural network enabl version catalog organ infer result datetim file have ident best practic organ result model exampl possibl custom version folder name version file data model output model predict json model model predict json model predict model json",
        "Question_preprocessed_content":"organ infer result model produc creat deep ensembl need track infer result came neural network enabl version catalog organ infer result datetim file have ident best practic organ result model exampl possibl custom version folder name version file",
        "Question_gpt_summary_original":"The user is creating a deep ensemble and needs to organize inference results by the model ID that produced it. However, enabling versioning via the Kedro catalog only organizes the results by datetime with identical file names. The user is seeking advice on the best practice for organizing results by model ID and customizing versioned folder names or files.",
        "Question_gpt_summary":"user creat deep ensembl need organ infer result model produc enabl version catalog organ result datetim ident file name user seek advic best practic organ result model custom version folder name file"
    },
    {
        "Question_title":"azure ML studio terrible performance",
        "Question_body":"<p>the last day or two Azure ML studio performance is terrible.<\/p>\n\n<p>It can take up to 10 mins to save a simple experiment and another 10 mins to run it.<\/p>\n\n<p>These are simple tutorial experiments, nothing massive, using at maximum 18mb of data.<\/p>\n\n<p>When i finally get the experiment to run and try to view the evaluation results, ML Studio spins for 5 mins before giving the error \"Error producing the visualization of the output \"<\/p>\n\n<p>Note this error also occasionally occurs when i am just trying to view the list of saved experiments.<\/p>\n\n<p>Im in the process of completing the Microsoft data science professional course and this is completely blocking me from making any progress.<\/p>\n\n<p>Any info on what might be wrong would be appreciated. <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_created_time":1499184624330,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":478.0,
        "Poster_created_time":1447682287792,
        "Poster_location":"Nairobi, Kenya",
        "Poster_reputation_count":194.0,
        "Poster_view_count":44.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44910388",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"studio terribl perform dai studio perform terribl min save simpl experi min run simpl tutori experi massiv maximum data final experi run try view evalu result studio spin min give error error produc visual output note error occasion occur try view list save experi process complet microsoft data scienc profession cours complet block make progress info wrong appreci",
        "Question_preprocessed_content":"studio terribl perform dai studio perform terribl min save simpl experi min run simpl tutori experi massiv maximum data final experi run try view evalu result studio spin min give error error produc visual output note error occasion occur try view list save experi process complet microsoft data scienc profession cours complet block make progress info wrong appreci",
        "Question_gpt_summary_original":"The user is experiencing terrible performance on Azure ML studio for the past few days. Simple experiments take up to 10 minutes to save and run, and viewing evaluation results takes up to 5 minutes before giving an error. This is hindering the user's progress in completing the Microsoft data science professional course. The user is seeking information on what might be causing the issue.",
        "Question_gpt_summary":"user experienc terribl perform studio past dai simpl experi minut save run view evalu result take minut give error hinder user progress complet microsoft data scienc profession cours user seek inform caus issu"
    },
    {
        "Question_title":"SageMaker and TensorFlow 2.0",
        "Question_body":"<p>What is the best way to run TensorFlow 2.0 with AWS Sagemeker?<\/p>\n\n<p>As of today (Aug 7th, 2019) AWS does not provide TensorFlow 2.0 <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"noreferrer\">SageMaker containers<\/a>, so my understanding is that I need to build my own.<\/p>\n\n<p>What is the best Base image to use? Example Dockerfile?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_created_time":1565186451297,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1565186790803,
        "Question_score_count":13.0,
        "Question_view_count":4136.0,
        "Poster_created_time":1446859510543,
        "Poster_location":"Toronto, Canada",
        "Poster_reputation_count":3259.0,
        "Poster_view_count":233.0,
        "Answer_body":"<p>EDIT: <strong>Amazon SageMaker does now support TF 2.0 and higher.<\/strong><\/p>\n<ul>\n<li>SageMaker + TensorFlow docs: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html<\/a><\/li>\n<li>Supported Tensorflow versions (and Docker URIs): <a href=\"https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images<\/a><\/li>\n<\/ul>\n<hr \/>\n<p><em>Original answer<\/em><\/p>\n<p>Here is an example Dockerfile that uses <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"nofollow noreferrer\">the underlying SageMaker Containers library<\/a> (this is what is used in the official pre-built Docker images):<\/p>\n<pre><code>FROM tensorflow\/tensorflow:2.0.0b1\n\nRUN pip install sagemaker-containers\n\n# Copies the training code inside the container\nCOPY train.py \/opt\/ml\/code\/train.py\n\n# Defines train.py as script entrypoint\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>For more information on this approach, see <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html<\/a><\/p>",
        "Answer_comment_count":4.0,
        "Question_closed_time":1567635751827,
        "Answer_last_edit_time":1595008443227,
        "Answer_score_count":10.0,
        "Answerer_created_time":1512770138847,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57396212",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"tensorflow best wai run tensorflow aw sagemek todai aug aw provid tensorflow contain understand need build best base imag us exampl dockerfil",
        "Question_preprocessed_content":"tensorflow best wai run tensorflow aw sagemek todai aw provid tensorflow contain understand need build best base imag us exampl dockerfil",
        "Question_gpt_summary_original":"The user is seeking advice on the best way to run TensorFlow 2.0 with AWS SageMaker, as AWS does not currently provide SageMaker containers for TensorFlow 2.0. The user is looking for guidance on the best base image to use and an example Dockerfile.",
        "Question_gpt_summary":"user seek advic best wai run tensorflow aw current provid contain tensorflow user look guidanc best base imag us exampl dockerfil"
    },
    {
        "Question_title":"How do I enable_logging for ModelMonitoringAlertConfig in GCP?",
        "Question_body":"<p>I am trying to <code>enable_logging<\/code> in <code>ModelMonitoringAlertConfig<\/code> I have tried:<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as vertex_ai_beta\n...\n    alerting_config = vertex_ai_beta.ModelMonitoringAlertConfig(\n    enable_logging=True,\n    email_alert_config=vertex_ai_beta.ModelMonitoringAlertConfig.EmailAlertConfig(\n        user_emails=NOTIFY_EMAILS\n    )\n)\n<\/code><\/pre>\n<p>gives:<\/p>\n<pre><code> Unknown field for ModelMonitoringAlertConfig: enable_logging\n<\/code><\/pre>\n<p>but <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1.types.ModelMonitoringAlertConfig\" rel=\"nofollow noreferrer\">this<\/a> suggests it should work. What am I missing?<\/p>\n<p>(I have also tried <code>aiplatform_v1beta1<\/code>.)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1661254728657,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":19.0,
        "Poster_created_time":1351154914716,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73458040",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"enabl log modelmonitoringalertconfig gcp try enabl log modelmonitoringalertconfig tri googl cloud import aiplatform vertex beta alert config vertex beta modelmonitoringalertconfig enabl log true email alert config vertex beta modelmonitoringalertconfig emailalertconfig user email notifi email give unknown field modelmonitoringalertconfig enabl log suggest work miss tri aiplatform vbeta",
        "Question_preprocessed_content":"modelmonitoringalertconfig gcp try tri give suggest work miss tri",
        "Question_gpt_summary_original":"The user is trying to enable logging for ModelMonitoringAlertConfig in GCP using the code provided in the documentation, but is encountering an error message stating that \"Unknown field for ModelMonitoringAlertConfig: enable_logging\". The user is seeking assistance in resolving this issue.",
        "Question_gpt_summary":"user try enabl log modelmonitoringalertconfig gcp code provid document encount error messag state unknown field modelmonitoringalertconfig enabl log user seek assist resolv issu"
    },
    {
        "Question_title":"How do I create a resource group when creating a workspace?",
        "Question_body":"<p>I am going through the Azure AI training and need to create a workspace under machine learning. When it asks me to select a resource group there are no options. When I want to create a new resource group it says I dont have permissions under my subscription. What do I need to do?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1638162185497,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/643801\/how-do-i-create-a-resource-group-when-creating-a-w",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"creat resourc group creat workspac go azur train need creat workspac machin learn ask select resourc group option want creat new resourc group sai dont permiss subscript need",
        "Question_preprocessed_content":"creat resourc group creat workspac go azur train need creat workspac machin learn ask select resourc group option want creat new resourc group sai dont permiss subscript need",
        "Question_gpt_summary_original":"The user is trying to create a workspace under machine learning in Azure AI training but is unable to select a resource group as there are no options available. When attempting to create a new resource group, the user is informed that they do not have the necessary permissions under their subscription. The user is seeking guidance on what to do next.",
        "Question_gpt_summary":"user try creat workspac machin learn azur train unabl select resourc group option avail attempt creat new resourc group user inform necessari permiss subscript user seek guidanc"
    },
    {
        "Question_title":"get metrics out of AutoMLRun based on test_data",
        "Question_body":"<p>I\u2019m using the following script to execute an AutoML run, also passing the test dataset<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>automl_settings = {\n    &quot;n_cross_validations&quot;: 10,\n    &quot;primary_metric&quot;: 'spearman_correlation',\n    &quot;enable_early_stopping&quot;: True,\n    &quot;max_concurrent_iterations&quot;: 10, \n    &quot;max_cores_per_iteration&quot;: -1,   \n    &quot;experiment_timeout_hours&quot;: 1,\n    &quot;featurization&quot;: 'auto',\n    &quot;verbosity&quot;: logging.INFO}\nautoml_config = AutoMLConfig(task = 'regression',\n                             debug_log = 'automl_errors.log',\n                             compute_target = compute_target,\n                             training_data = training_data,\n                             test_data = test_data,\n                             label_column_name = label_column_name,\n                             model_explainability = True,\n                             **automl_settings                            )\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1635954155380,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":85.0,
        "Poster_created_time":1405457120427,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation_count":3359.0,
        "Poster_view_count":555.0,
        "Answer_body":"<p>Note that the TEST DATASET SUPPORT is a feature still in PRIVATE PREVIEW. It'll probably be released as PUBLIC PREVIEW later in NOVEMBER, but until then, you need to be enrolled in the PRIVATE PREVIEW in order to see the &quot;Test runs and metrics&quot; in the UI. You can send me an email to cesardl at microsoft dot com and send me your AZURE SUBSCRIPTION ID to be enabled so you see it in the UI.<\/p>\n<p>You can see further info on how to get started here:\n<a href=\"https:\/\/github.com\/Azure\/automl-testdataset-preview\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/automl-testdataset-preview<\/a><\/p>\n<p>About how to use it, you need to either provide the test_Data (specific Test AML Tabular Dataset that for instance you loaded from a file os split manually previously)\nor you can provide a test_size which is the % (i.e. 0.2 is 20%) to be split from the single\/original dataset.<\/p>\n<p>About the TEST metrics, since you can make multiple TEST runs against a single model, you need to go to the specific TEST run available under the link &quot;Test results&quot;<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3pPPS.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1635964127852,
        "Answer_last_edit_time":null,
        "Answer_score_count":3.0,
        "Answerer_created_time":1463778034387,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69827748",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"metric automlrun base test data follow script execut automl run pass test dataset automl set cross valid primari metric spearman correl enabl earli stop true max concurr iter max core iter experi timeout hour featur auto verbos log info automl config automlconfig task regress debug log automl error log comput target comput target train data train data test data test data label column label column model explain true automl set",
        "Question_preprocessed_content":"metric automlrun base follow script execut automl run pass test dataset",
        "Question_gpt_summary_original":"The user is trying to execute an AutoML run and pass the test dataset to get metrics out of it. They have provided a script with various settings for the AutoML run, including the primary metric, featurization, and verbosity.",
        "Question_gpt_summary":"user try execut automl run pass test dataset metric provid script set automl run includ primari metric featur verbos"
    },
    {
        "Question_title":"Can I make Amazon SageMaker deliver a recommendation based on historic data instead of a probability score?",
        "Question_body":"<p>We have a huge set of data in CSV format, containing a few numeric elements, like this:<\/p>\n\n<pre><code>Year,BinaryDigit,NumberToPredict,JustANumber, ...other stuff\n1954,1,762,16, ...other stuff\n1965,0,142,16, ...other stuff\n1977,1,172,16, ...other stuff\n<\/code><\/pre>\n\n<p>The thing here is that there is a strong correlation between the third column and the columns before that. So I have pre-processed the data and it's now available in a format I think is perfect:<\/p>\n\n<pre><code>1954,1,762\n1965,0,142\n1977,1,172\n<\/code><\/pre>\n\n<p>What I want is a predicition on the value in the third column, using the first two as input. So in the case above, I want the input 1965,0 to return 142. In real life this file is thousands of rows, but since there's a pattern, I'd like to retrieve the most possible value.<\/p>\n\n<p>So far I've setup a train job on the CSV file using the L<em>inear Learner<\/em> algorithm, with the following settings:<\/p>\n\n<pre><code>label_size = 1\nfeature_dim = 2\npredictor_type = regression\n<\/code><\/pre>\n\n<p>I've also created a model from it, and setup an endpoint. When I invoke it, I get a score in return.<\/p>\n\n<pre><code>    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                   ContentType='text\/csv',\n                                   Body=payload)\n<\/code><\/pre>\n\n<p>My goal here is to get the third column prediction instead. How can I achieve that? I have read a lot of the documentation regarding this, but since I'm not very familiar with AWS, I might as well have used the wrong algorithms for what I am trying to do.<\/p>\n\n<p>(Please feel free to edit this question to better suit AWS terminology)<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1552553455710,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1552895653107,
        "Question_score_count":1.0,
        "Question_view_count":271.0,
        "Poster_created_time":1411464641600,
        "Poster_location":"\u00d6rebro, Sverige",
        "Poster_reputation_count":205.0,
        "Poster_view_count":38.0,
        "Answer_body":"<p>For csv input, the label should be in the first column, as mentioned <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-training.html\" rel=\"nofollow noreferrer\">here<\/a>:  So you should preprocess your data to put the label (the column you want to predict) on the left.<\/p>\n\n<p>Next, you need to decide whether this is a regression problem or a classification problem. <\/p>\n\n<p>If you want to predict a number that's as close as possible to the true number, that's regression. For example, the truth might be 4, and the model might predict 4.15. If you need an integer prediction, you could round the model's output.<\/p>\n\n<p>If you want the prediction to be one of a few categories, then you have a classification problem. For example, we might encode 'North America' = 0, 'Europe' = 1, 'Africa' = 2, and so on. In this case, a fractional prediction wouldn't make sense. <\/p>\n\n<p>For regression, use <code>'predictor_type' = 'regressor'<\/code> and for classification with more than 2 classes, use <code>'predictor_type' = 'multiclass_classifier'<\/code> as documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The output of regression will contain only a <code>'score'<\/code> field, which is the model's prediction. The output of multiclass classification will contain a <code>'predicted_label'<\/code> field, which is the model's prediction, as well as a <code>'score'<\/code> field, which is a vector of probabilities representing the model's confidence. The index with the highest probability will be the one that's predicted as the <code>'predicted_label'<\/code>. The output formats are documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/LL-in-formats.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1553880598532,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1373375969332,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":16.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55158307",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"deliv recommend base histor data instead probabl score huge set data csv format contain numer element like year binarydigit numbertopredict justanumb stuff stuff stuff stuff thing strong correl column column pre process data avail format think perfect want predicit valu column input case want input return real life file thousand row pattern like retriev possibl valu far setup train job csv file linear learner algorithm follow set label size featur dim predictor type regress creat model setup endpoint invok score return respons runtim invok endpoint endpointnam endpoint contenttyp text csv bodi payload goal column predict instead achiev read lot document familiar aw wrong algorithm try feel free edit question better suit aw terminolog",
        "Question_preprocessed_content":"deliv recommend base histor data instead probabl score huge set data csv format contain numer element like thing strong correl column column data avail format think perfect want predicit valu column input case want input return real life file thousand row pattern like retriev possibl valu far setup train job csv file linear learner algorithm follow set creat model setup endpoint invok score return goal column predict instead achiev read lot document familiar aw wrong algorithm try feel free edit question better suit aw terminolog",
        "Question_gpt_summary_original":"The user has a large dataset in CSV format with a strong correlation between the third column and the columns before it. They have pre-processed the data and want to use the first two columns as input to predict the value in the third column. The user has set up a train job using the Linear Learner algorithm with regression settings and created a model and endpoint. However, when invoking the endpoint, they receive a score instead of the predicted value. The user is seeking guidance on how to achieve their goal of getting the predicted value instead of a score.",
        "Question_gpt_summary":"user larg dataset csv format strong correl column column pre process data want us column input predict valu column user set train job linear learner algorithm regress set creat model endpoint invok endpoint receiv score instead predict valu user seek guidanc achiev goal get predict valu instead score"
    },
    {
        "Question_title":"How to log the learning rate with pytorch lightning when using a scheduler?",
        "Question_body":"<p>I\u2019ve been trying to find some documentation, I don\u2019t want to save all the hyperparameters each epoch, just the learning rate.<br>\nWould be so great if you can help me out.<\/p>\n<p>Cheers,<\/p>\n<p>Oli<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_created_time":1677518425190,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":4.0,
        "Question_view_count":468.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>I\u2019m also wondering how this is done! Whether within a sweep configuration or not - when using a lr scheduler, I am trying to track the lr at epoch during training, as it is now dynamic. Even within a sweep, you will have some initial lr  determined during the sweep, but it will not stay constant for the duration of training.<\/p>\n<p>edit:<\/p>\n<p>The example on the <a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/1.2.10\/api\/pytorch_lightning.callbacks.lr_monitor.html#learning-rate-monitor\" rel=\"noopener nofollow ugc\">lightning site here<\/a> worked for me:<\/p>\n<pre><code class=\"lang-auto\">&gt;&gt;&gt; from pytorch_lightning.callbacks import LearningRateMonitor\n&gt;&gt;&gt; lr_monitor = LearningRateMonitor(logging_interval='step')\n&gt;&gt;&gt; trainer = Trainer(callbacks=[lr_monitor])\n<\/code><\/pre>\n<p>Passing the <code>WandBLogger<\/code> to the trainer I see my lr is logged on the <code>wandb<\/code> dashboard.<\/p>",
        "Answer_comment_count":null,
        "Question_closed_time":1679602846363,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-log-the-learning-rate-with-pytorch-lightning-when-using-a-scheduler\/3964",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"log learn rate pytorch lightn schedul iv try document dont want save hyperparamet epoch learn rate great help cheer oli",
        "Question_preprocessed_content":"log learn rate pytorch lightn schedul iv try document dont want save hyperparamet epoch learn rate great help cheer oli",
        "Question_gpt_summary_original":"The user is seeking help on how to log the learning rate with PyTorch Lightning when using a scheduler, without saving all the hyperparameters each epoch.",
        "Question_gpt_summary":"user seek help log learn rate pytorch lightn schedul save hyperparamet epoch"
    },
    {
        "Question_title":"Machine Learning",
        "Question_body":"<p>My company provides analytics and machine learning services to enterprises. Now that we are moving over to Azure (from AWS and on-prem), I would like to understand how our workflow might change. The main aspects I am uncertain about is how we ingest data from our customers and how we deploy models for real-time and batch inference with minimal technical overheads.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1595618344213,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/52106\/machine-learning",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"machin learn compani provid analyt machin learn servic enterpris move azur aw prem like understand workflow chang main aspect uncertain ingest data custom deploi model real time batch infer minim technic overhead",
        "Question_preprocessed_content":"machin learn compani provid analyt machin learn servic enterpris move azur like understand workflow chang main aspect uncertain ingest data custom deploi model batch infer minim technic overhead",
        "Question_gpt_summary_original":"The user's company is transitioning from AWS and on-prem to Azure and they are unsure about how their workflow will change, particularly in terms of data ingestion from customers and deploying models for real-time and batch inference with minimal technical overheads.",
        "Question_gpt_summary":"user compani transit aw prem azur unsur workflow chang particularli term data ingest custom deploi model real time batch infer minim technic overhead"
    },
    {
        "Question_title":"SageMaker Lifecycle configuration script - LR CF characters added when trying to copy into AWS on mac",
        "Question_body":"<p>So I have a lifecycle configuration file that I want to use. I have used tools to make sure the illegal CR CF are stripped out and that it is unix friendly. However, there is no way to upload the file &quot;as is&quot; in AWS Sagemaker lifecycle configurations - you have to directly copy in the text, and that adds the CR LF characters back on my Mac. How do I get around this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1627940239740,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1627944397470,
        "Question_score_count":0.0,
        "Question_view_count":160.0,
        "Poster_created_time":1421343783700,
        "Poster_location":null,
        "Poster_reputation_count":1387.0,
        "Poster_view_count":153.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68628185",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"lifecycl configur script charact ad try copi aw mac lifecycl configur file want us tool sure illeg strip unix friendli wai upload file lifecycl configur directli copi text add charact mac",
        "Question_preprocessed_content":"lifecycl configur script charact ad try copi aw mac lifecycl configur file want us tool sure illeg strip unix friendli wai upload file lifecycl configur directli copi text add charact mac",
        "Question_gpt_summary_original":"The user is facing challenges while trying to copy a lifecycle configuration file into AWS Sagemaker on their Mac. The file contains illegal CR CF characters that have been stripped out using tools to make it Unix-friendly. However, when the user tries to copy the file into AWS Sagemaker, the CR LF characters are added back. The user is seeking a solution to this problem.",
        "Question_gpt_summary":"user face challeng try copi lifecycl configur file mac file contain illeg charact strip tool unix friendli user tri copi file charact ad user seek solut problem"
    },
    {
        "Question_title":"Incremental ingestion into Feature Store",
        "Question_body":"Hello evryone,\n\nHow to set up a incremental ingestion for online serving using Feature Store?\n\nI would like to use this flow:\u00a0\n\nQuery into The BigQuery with delta time\n\nIngestion into FeatureStore\u00a0 witho only this incremental values\n\nUsing the online serving with Incremental + Previous data into FeatureStore\n\nHow Can I?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1674560700000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":45.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Incremental-ingestion-into-Feature-Store\/td-p\/513484\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"increment ingest featur store hello evryon set increment ingest onlin serv featur store like us flow queri bigqueri delta time ingest featurestor witho increment valu onlin serv increment previou data featurestor",
        "Question_preprocessed_content":"increment ingest featur store hello evryon set increment ingest onlin serv featur store like us flow queri bigqueri delta time ingest featurestor witho increment valu onlin serv increment previou data featurestor",
        "Question_gpt_summary_original":"The user is seeking guidance on how to set up incremental ingestion for online serving using Feature Store. They want to query BigQuery with delta time and ingest only incremental values into Feature Store, and then use online serving with both incremental and previous data in Feature Store. The user is asking for help on how to achieve this.",
        "Question_gpt_summary":"user seek guidanc set increment ingest onlin serv featur store want queri bigqueri delta time ingest increment valu featur store us onlin serv increment previou data featur store user ask help achiev"
    },
    {
        "Question_title":"How do I invoke a data enrichment function before model.predict while serving the model in Databricks",
        "Question_body":"<p>In Databricks, I have used mlflow and got my model served through REST API. It works fine when all model features are provided. But my use case is that only a single feature (the primary key) will be provided by the consumer application, and my code has to lookup the other features from a database based on that key and then use the model.predict to return the prediction. I tried researching but understood that the REST endpoints will simply invoke the model.predict function. How can I make it invoke a data massaging function before predicting?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1644826322870,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1644835966403,
        "Question_score_count":1.0,
        "Question_view_count":109.0,
        "Poster_created_time":1531142162080,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71108775",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"invok data enrich function model predict serv model databrick databrick got model serv rest api work fine model featur provid us case singl featur primari kei provid consum applic code lookup featur databas base kei us model predict return predict tri research understood rest endpoint simpli invok model predict function invok data massag function predict",
        "Question_preprocessed_content":"invok data enrich function serv model databrick databrick got model serv rest api work fine model featur provid us case singl featur provid consum applic code lookup featur databas base kei us return predict tri research understood rest endpoint simpli invok function invok data massag function predict",
        "Question_gpt_summary_original":"The user is facing a challenge while serving their model through REST API in Databricks. They want to invoke a data enrichment function before model.predict, but they are unable to do so as the REST endpoints only invoke the model.predict function. The user needs to find a way to lookup other features from a database based on a primary key provided by the consumer application before using the model.predict function to return the prediction.",
        "Question_gpt_summary":"user face challeng serv model rest api databrick want invok data enrich function model predict unabl rest endpoint invok model predict function user need wai lookup featur databas base primari kei provid consum applic model predict function return predict"
    },
    {
        "Question_title":"interpret statistical model metrics",
        "Question_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/7MiTV.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7MiTV.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Do you know how to intepret RAE and RSE values? I know a COD closer to 1 is a good sign. Does this indicate that boosted decision tree regression is best?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1437855669030,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":209.0,
        "Poster_created_time":1416689594160,
        "Poster_location":null,
        "Poster_reputation_count":270.0,
        "Poster_view_count":66.0,
        "Answer_body":"<p>RAE and RSE closer to 0 is a good sign...you want error to be as low as possible.  See <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-evaluate-model-performance\/\" rel=\"nofollow\">this article<\/a> for more information on evaluating your model.  From that page:<\/p>\n\n<blockquote>\n  <p>The term \"error\" here represents the difference between the predicted value and the true value. The absolute value or the square of this difference are usually computed to capture the total magnitude of error across all instances, as the difference between the predicted and true value could be negative in some cases. The error metrics measure the predictive performance of a regression model in terms of the mean deviation of its predictions from the true values. Lower error values mean the model is more accurate in making predictions. An overall error metric of 0 means that the model fits the data perfectly.<\/p>\n<\/blockquote>\n\n<p>Yes, with your current results, the boosted decision tree performs best.  I don't know the details of your work well enough to determine if that is good enough.  It honestly may be.  But if you determine it's not, you can also tweak the input parameters in your \"Boosted Decision Tree Regression\" module to try to get even better results.  The \"<a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/038d91b6-c2f2-42a1-9215-1f2c20ed1b40\/\" rel=\"nofollow\">ParameterSweep<\/a>\" module can help with that by trying many different input parameters for you and you specify the parameter that you want to optimize for (such as your RAE, RSE, or COD referenced in your question).  See <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-algorithm-parameters-optimize\/\" rel=\"nofollow\">this article<\/a> for a brief description.  Hope this helps.\n<br\/><br\/>\nP.S. I'm glad that you're looking into the black carbon levels in Westeros...I'm sure Cersei doesn't even care.  <\/p>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1438012963580,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1342628508448,
        "Answerer_location":"United States",
        "Answerer_reputation_count":5147.0,
        "Answerer_view_count":1739.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/31630745",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"interpret statist model metric know intepret rae rse valu know cod closer good sign indic boost decis tree regress best",
        "Question_preprocessed_content":"interpret statist model metric know intepret rae rse valu know cod closer good sign indic boost decis tree regress best",
        "Question_gpt_summary_original":"The user is seeking help in interpreting RAE and RSE values and is unsure if a COD closer to 1 indicates that boosted decision tree regression is the best option.",
        "Question_gpt_summary":"user seek help interpret rae rse valu unsur cod closer indic boost decis tree regress best option"
    },
    {
        "Question_title":"Training Metric logging on SageMaker experiment tracking: how to get time-series metrics with visualisation",
        "Question_body":"I am using the sagemaker python SDK to train a bespoke model. I have defined my `metric_definition` regexes and passed them to the estimator like:\n\n```python\nnum_re = \"([0-9\\\\.]+)(e-?[[01][0-9])?\"\nmetrics = [\n    {\"Name\": \"learning-rate\", \"Regex\": f\"lr: {num_re}\"},\n    {\"Name\": \"training:loss\", \"Regex\": f\"loss: {num_re}\"},\n    # ...\n]\nestimator = Estimator(\n    image_uri=training_image_uri,\n    # ...\n    metric_definitions=metrics,\n    enable_sagemaker_metrics=True,\n)\n```\n \nWhen I run training, these metrics are visible in my logs and I can also see them in SageMaker Studio in `Trial Components > Metrics (tab)` as a grid of numbers like:\n\n> Name | Minimum | Maximum | Standard Deviation | Average | Count | Final value \n\n> learning-rate | 8.889 | 8.907 | 0.010392304845413657 | 8.898 | 4 |8.907\n\n> ...\n\nWhich suggests that the regexes are correctly matching on the logs\n\nHowever, I am not able to visualise any graphs for my metrics. I have tried all of:\n- `Sagemaker Studio > Trial components > charts`. It is only possible to plot things like `learning-rate_min` (i.e. a point value not a time-series metric)\n- `SageMaker aws console > training > training jobs > <select job> > Scroll to Monitor section`. Here I can see metrics like CPUUtilization over time but for my metrics there is just an empty graph for each metric that I have defined that says 'No data available' \n- `SageMaker aws console > training > training jobs > <select job> > Scroll to Monitor section > View algorithm metrics (opens in CloudWatch) > Browse > select metric (e.g. learning-rate and 'Add to Graph' `. I filter by the correct time period and go the `Graphed metrics (1) tab`, even after updating the period to `1 second` I am not able to see anything on the graph. \n\nI'm not sure what the issue is here but any help would be much appreciated",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1648058261935,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668624556079,
        "Question_score_count":0.0,
        "Question_view_count":1598.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDNp9HXW9SCqdadORoXUX9g\/training-metric-logging-on-sagemaker-experiment-tracking-how-to-get-time-series-metrics-with-visualisation",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"train metric log experi track time seri metric visualis python sdk train bespok model defin metric definit regex pass estim like python num metric learn rate regex num train loss regex loss num estim estim imag uri train imag uri metric definit metric enabl metric true run train metric visibl log studio trial compon metric tab grid number like minimum maximum standard deviat averag count final valu learn rate suggest regex correctli match log abl visualis graph metric tri studio trial compon chart possibl plot thing like learn rate min point valu time seri metric aw consol train train job scroll monitor section metric like cpuutil time metric graph metric defin sai data avail aw consol train train job scroll monitor section view algorithm metric open cloudwatch brows select metric learn rate add graph filter correct time period graph metric tab updat period second abl graph sure issu help appreci",
        "Question_preprocessed_content":"train metric log experi track metric visualis python sdk train bespok model defin regex pass estim like run train metric visibl log studio grid number like minimum maximum standard deviat averag count final valu suggest regex correctli match log abl visualis graph metric tri possibl plot thing like metric like cpuutil time metric graph metric defin sai data avail filter correct time period updat period abl graph sure issu help appreci",
        "Question_gpt_summary_original":"The user is using the sagemaker python SDK to train a model and has defined metric definitions regexes. The metrics are visible in logs and in SageMaker Studio, but the user is unable to visualize any graphs for the metrics. The user has tried different methods to plot the graphs but has not been successful. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user python sdk train model defin metric definit regex metric visibl log studio user unabl visual graph metric user tri differ method plot graph success user seek help resolv issu"
    },
    {
        "Question_title":"About hyperparameters sweeping for DDP program",
        "Question_body":"<p>Hi, I have a program which needs multiple GPUs to run at the same time, currently I use DDP to launch the program. I wonder how can I do the sweeping , the program will still be launched  in DDP mode (using all GPUs) at each trial. Thanks!<\/p>\n<p>Best<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1652109385291,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":160.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/about-hyperparameters-sweeping-for-ddp-program\/2384",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"hyperparamet sweep ddp program program need multipl gpu run time current us ddp launch program wonder sweep program launch ddp mode gpu trial thank best",
        "Question_preprocessed_content":"hyperparamet sweep ddp program program need multipl gpu run time current us ddp launch program wonder sweep program launch ddp mode trial thank best",
        "Question_gpt_summary_original":"The user is seeking advice on how to perform hyperparameter sweeping for a program that requires multiple GPUs to run simultaneously using DDP mode. They want to ensure that the program is still launched using all GPUs during each trial.",
        "Question_gpt_summary":"user seek advic perform hyperparamet sweep program requir multipl gpu run simultan ddp mode want ensur program launch gpu trial"
    },
    {
        "Question_title":"Visual Bug in Documentation",
        "Question_body":"<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/fd0f258d7049524defde72d94701022480abd440.png\" data-download-href=\"\/uploads\/short-url\/A6FfBmGoIRNds6FR4YCdptOz7Fe.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_690x264.png\" alt=\"image\" data-base62-sha1=\"A6FfBmGoIRNds6FR4YCdptOz7Fe\" width=\"690\" height=\"264\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_690x264.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_1035x396.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_1380x528.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">2560\u00d7981 120 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>I think there is a missing ``` to finish the code block<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1654672339920,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":177.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/visual-bug-in-documentation\/2572",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"visual bug document imag think miss finish code block",
        "Question_preprocessed_content":"visual bug document imag think miss finish code block",
        "Question_gpt_summary_original":"The user has encountered a visual bug in the documentation where there seems to be a missing ``` to finish the code block.",
        "Question_gpt_summary":"user encount visual bug document miss finish code block"
    },
    {
        "Question_title":"Lost access to machine learning studio webservices. Cannot access already existing ones, nor depoy new ones.",
        "Question_body":"<p>Hello,  <\/p>\n<p>I've been working with machine learning studio webservices for over a year now, and have never had any trouble with the webservices. Suddenly me and a few colleagues have lost access to them. We can still create experiments, but when we try to deploy them we get the following error:  <\/p>\n<p>&quot;Web Service deployment failed. This account does not have sufficient access to the Azure subscription that contains the Workspace. In order to deploy a Web Service to Azure, the same account must be invited to the Workspace and be given access to the Azure subscription that contains the Workspace.&quot;  <\/p>\n<p>When trying to access an already existing one, we get this:  <\/p>\n<p>&quot;The Azure Subscription could not be accessed. This issue is most likely due the user account not having sufficient access claims to the Azure Subscription.&quot;  <\/p>\n<p>In regards to my access to the workspace\/subscription, I have a contributor role, and as far as I'm aware thats always been my role.  <\/p>\n<p>What possible other access shall I need to keep on working the the webservices?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1615306897887,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/306150\/lost-access-to-machine-learning-studio-webservices",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"lost access machin learn studio webservic access exist on depoi new on hello work machin learn studio webservic year troubl webservic suddenli colleagu lost access creat experi try deploi follow error web servic deploy fail account suffici access azur subscript contain workspac order deploi web servic azur account invit workspac given access azur subscript contain workspac try access exist azur subscript access issu like user account have suffici access claim azur subscript regard access workspac subscript contributor role far awar that role possibl access shall need work webservic",
        "Question_preprocessed_content":"lost access machin learn studio webservic access exist on depoi new on hello work machin learn studio webservic year troubl webservic suddenli colleagu lost access creat experi try deploi follow error web servic deploy fail account suffici access azur subscript contain workspac order deploi web servic azur account invit workspac given access azur subscript contain try access exist azur subscript access issu like user account have suffici access claim azur regard access contributor role far awar that role possibl access shall need work webservic",
        "Question_gpt_summary_original":"The user and their colleagues have lost access to machine learning studio webservices, preventing them from accessing existing ones or deploying new ones. They receive an error message stating that their account does not have sufficient access to the Azure subscription that contains the Workspace. The user has a contributor role but is unsure of what other access they may need to continue working with the webservices.",
        "Question_gpt_summary":"user colleagu lost access machin learn studio webservic prevent access exist on deploi new on receiv error messag state account suffici access azur subscript contain workspac user contributor role unsur access need continu work webservic"
    },
    {
        "Question_title":"Feature Request: Embed only a table\/report",
        "Question_body":"<p>I intend to do a series of blog posts which use W&amp;B reports like this one:<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/write.farook.org\/stable-diffusion-parameter-variations\/\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/31da22c8b3786e20cdf0326872a564cdf70efa6d.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/write.farook.org\/stable-diffusion-parameter-variations\/\" target=\"_blank\" rel=\"noopener nofollow ugc\">write.farook.org<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/199;\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e5563ba054b6d9637277a5f0369fed9909c580da_2_690x199.jpeg\" class=\"thumbnail\" width=\"690\" height=\"199\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e5563ba054b6d9637277a5f0369fed9909c580da_2_690x199.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e5563ba054b6d9637277a5f0369fed9909c580da_2_1035x298.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e5563ba054b6d9637277a5f0369fed9909c580da_2_1380x398.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e5563ba054b6d9637277a5f0369fed9909c580da_2_10x10.png\"><\/div>\n\n<h3><a href=\"https:\/\/write.farook.org\/stable-diffusion-parameter-variations\/\" target=\"_blank\" rel=\"noopener nofollow ugc\">Stable Diffusion Parameter Variations \u2013 Meandering Musings<\/a><\/h3>\n\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>As you\u2019ll notice, I have multiple reports in the blog post. The current embed with an IFRAME takes up way too much space because of the header and the footer where you can comment etc. I\u2019d prefer to have the ability to just embed a table (or preferably) the content section of a report.<\/p>\n<p>Does this ability currently exist? I checked the documentation but couldn\u2019t find anything like that.<\/p>\n<p>If the functionality does not exist, being able to do so in future would be a great help for me and probably for others too.<\/p>\n<p>I currently take screenshots of the relevant part of the report and then link to the full report on W&amp;B. But I hope you\u2019d agree that having the interactivity of the W&amp;B report in the post itself would be much more preferable <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_created_time":1661902937612,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":239.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/feature-request-embed-only-a-table-report\/3037",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"featur request emb tabl report intend seri blog post us report like write farook org stabl diffus paramet variat meander muse youll notic multipl report blog post current emb ifram take wai space header footer comment prefer abil emb tabl prefer content section report abil current exist check document like function exist abl futur great help probabl current screenshot relev report link report hope youd agre have interact report post prefer",
        "Question_preprocessed_content":"featur request emb intend seri blog post us report like stabl diffus paramet variat meander muse youll notic multipl report blog post current emb ifram take wai space header footer comment prefer abil emb tabl content section report abil current exist check document like function exist abl futur great help probabl current screenshot relev report link report hope youd agre have interact report post prefer",
        "Question_gpt_summary_original":"The user is requesting a feature to embed only a table or the content section of a report from W&B, as the current embed with an IFRAME takes up too much space due to the header and footer. The user currently takes screenshots of the relevant part of the report and links to the full report on W&B, but having the interactivity of the W&B report in the post itself would be much more preferable.",
        "Question_gpt_summary":"user request featur emb tabl content section report current emb ifram take space header footer user current take screenshot relev report link report have interact report post prefer"
    },
    {
        "Question_title":"timeSegments vs timeSegmentAnnotations",
        "Question_body":"timeSegments vs timeSegmentAnnotations\n\nCan anyone explain what's the difference between these 2 fields described here?\u00a0https:\/\/storage.cloud.google.com\/google-cloud-aiplatform\/schema\/dataset\/ioformat\/video_action_recogn...\n\nwhy would I want to tag timeSegments? what's the objective of this? associate a label to a time segment?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1668585720000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":60.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/timeSegments-vs-timeSegmentAnnotations\/td-p\/490092\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"timeseg timesegmentannot timeseg timesegmentannot explain differ field describ http storag cloud googl com googl cloud aiplatform schema dataset ioformat video action recogn want tag timeseg object associ label time segment",
        "Question_preprocessed_content":"timeseg timesegmentannot timeseg timesegmentannot explain differ field describ want tag timeseg object associ label time segment",
        "Question_gpt_summary_original":"The user is seeking clarification on the difference between \"timeSegments\" and \"timeSegmentAnnotations\" fields in a Google Cloud AI Platform schema for video action recognition. They are also asking about the purpose of tagging time segments and the objective of associating a label with them.",
        "Question_gpt_summary":"user seek clarif differ timeseg timesegmentannot field googl cloud platform schema video action recognit ask purpos tag time segment object associ label"
    },
    {
        "Question_title":"SageMaker Estimator fit job never ends",
        "Question_body":"<p>I have the following code<\/p>\n<pre><code>estimator = Estimator(                                                     \n    image_uri=ecr_image,                                                   \n    role=role,                                                             \n    instance_count=1,                                                      \n    instance_type=instance_type,                                           \n    hyperparameters=hyperparameters                                        \n)                                                                          \n\nestimator.fit({&quot;training&quot;: &quot;s3:\/\/&quot; + sess.default_bucket() + &quot;\/&quot; + prefix})\n<\/code><\/pre>\n<p>which seems to run smoothly until it is stuck at:<\/p>\n<pre><code>Finished Training\n2020-12-02 15:00:45,352 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n<\/code><\/pre>\n<p>and I see InProgress job in AWS SageMaker console. How can I fix this?<\/p>\n<p>I use <code>763104351884.dkr.ecr.us-west-2.amazonaws.com\/pytorch-inference-eia:1.3.1-cpu-py36-ubuntu16.04<\/code> Docker image with <code>pip install sagemaker-training<\/code> added.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_created_time":1606921724283,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1606922323816,
        "Question_score_count":1.0,
        "Question_view_count":252.0,
        "Poster_created_time":1382978984190,
        "Poster_location":null,
        "Poster_reputation_count":1311.0,
        "Poster_view_count":49.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65110736",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"estim fit job end follow code estim estim imag uri ecr imag role role instanc count instanc type instanc type hyperparamet hyperparamet estim fit train sess default bucket prefix run smoothli stuck finish train train toolkit info report train success inprogress job consol fix us dkr ecr west amazonaw com pytorch infer eia cpu ubuntu docker imag pip instal train ad",
        "Question_preprocessed_content":"estim fit job end follow code run smoothli stuck inprogress job consol fix us docker imag ad",
        "Question_gpt_summary_original":"The user is facing an issue where the SageMaker Estimator fit job never ends and gets stuck at \"Finished Training\" with an InProgress job in the AWS SageMaker console. The user is using a PyTorch Docker image with sagemaker-training installed and is seeking a solution to fix this issue.",
        "Question_gpt_summary":"user face issu estim fit job end get stuck finish train inprogress job consol user pytorch docker imag train instal seek solut fix issu"
    },
    {
        "Question_title":"Ensure Java is installed and PATH is set for `java` in Amazon SageMaker Jupyter Notebook",
        "Question_body":"<p>I am importing the tabula library and when I call this library's methods, I receive the following error in SageMaker.<\/p>\n<pre><code>JavaNotFoundError: `java` command is not found from this Python process.Please ensure Java is installed and PATH is set for `java`\n<\/code><\/pre>\n<p>How can I install Java to SageMaker?<\/p>\n<p>Below is my code:<\/p>\n<pre><code>import tabula\npdf_path = &quot;https:\/\/github.com\/chezou\/tabula-py\/raw\/master\/tests\/resources\/data.pdf&quot;\ndfs = tabula.read_pdf(pdf_path, stream=True)\n<\/code><\/pre>\n<p>Once I enter in this code, I receive the error message that SageMaker does not have Java.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1634184814133,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":183.0,
        "Poster_created_time":1615555459547,
        "Poster_location":"United States",
        "Poster_reputation_count":141.0,
        "Poster_view_count":8.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69564996",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"ensur java instal path set java jupyt notebook import tabula librari librari method receiv follow error javanotfounderror java command python process ensur java instal path set java instal java code import tabula pdf path http github com chezou tabula raw master test resourc data pdf df tabula read pdf pdf path stream true enter code receiv error messag java",
        "Question_preprocessed_content":"ensur java instal path set jupyt notebook import tabula librari librari method receiv follow error instal java code enter code receiv error messag java",
        "Question_gpt_summary_original":"The user is encountering a JavaNotFoundError while importing the tabula library in Amazon SageMaker Jupyter Notebook. The error message suggests that Java is not installed and PATH is not set for `java`. The user is seeking guidance on how to install Java in SageMaker.",
        "Question_gpt_summary":"user encount javanotfounderror import tabula librari jupyt notebook error messag suggest java instal path set java user seek guidanc instal java"
    },
    {
        "Question_title":"Emotional mobiles",
        "Question_body":"My idea is to create emotional mobiles. Were we cannot buy mobiles with only money,mobile must choose us for buy and unique emotional between specific person and his new mobile . An intimacy between mobile and human. Like a puppy or understanding couples mobile and human sinking using AI.",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1664233920000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":26.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Emotional-mobiles\/td-p\/471342\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"emot mobil idea creat emot mobil bui mobil monei mobil choos bui uniqu emot specif person new mobil intimaci mobil human like puppi understand coupl mobil human sink",
        "Question_preprocessed_content":"emot mobil idea creat emot mobil bui mobil monei mobil choos bui uniqu emot specif person new mobil intimaci mobil human like puppi understand coupl mobil human sink",
        "Question_gpt_summary_original":"The user's idea is to create emotional mobiles that cannot be bought with money, but instead, the mobiles must choose the person to buy them. The goal is to create a unique emotional connection between the person and their mobile device, similar to the bond between a pet and its owner or a couple that understands each other. The user plans to use AI to achieve this.",
        "Question_gpt_summary":"user idea creat emot mobil bought monei instead mobil choos person bui goal creat uniqu emot connect person mobil devic similar bond pet owner coupl understand user plan us achiev"
    },
    {
        "Question_title":"Run Sagemaker notebook instance and be able to close tab",
        "Question_body":"<p>I'm currently using Sagemaker notebook instance (not from Sagemaker Studio), and I want to run a notebook that is expected to take around 8 hours to finish. I want to leave it overnight, and see the output from each cell, the output is a combination of print statements and plots.<\/p>\n<p>Howevever, when I start running the notebook and make sure the initial cells run, I close the Jupyterlab tab in my browser, and some minutes after, I open it again to see how is it going, but the notebook is stopped.<\/p>\n<p>Is there any way where I can still use my notebook as it is, see the output from each cell (prints and plots) and do not have to keep the Jupyterlab tab open (turn my laptop off, etc)?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_created_time":1646922683383,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":1154.0,
        "Poster_created_time":1572208191120,
        "Poster_location":null,
        "Poster_reputation_count":139.0,
        "Poster_view_count":45.0,
        "Answer_body":"<p>Answering my own question.<\/p>\n<p>I ended up using Sagemaker Processing jobs for this. As initially suggested by the other answer. I found this library developed a few months ago: <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-run-notebook\" rel=\"nofollow noreferrer\">Sagemaker run notebook<\/a>, which helped still keep my notebook structure and cells as I had them, and be able to run it using Sagemaker run notebook using a bigger instance, and modifying the notebook in a smaller one.<\/p>\n<p>The output of each cell was saved, along the plots I had, in S3 as a jupyter notebook.<\/p>\n<p>I see that no constant support is given to the library, but you can fork it and make changes to it, and use it as per your requirements. For example, creating a docker container based on your needs.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1662537228663,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1572208191120,
        "Answerer_location":null,
        "Answerer_reputation_count":139.0,
        "Answerer_view_count":45.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71425842",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"run notebook instanc abl close tab current notebook instanc studio want run notebook expect hour finish want leav overnight output cell output combin print statement plot howevev start run notebook sure initi cell run close jupyterlab tab browser minut open go notebook stop wai us notebook output cell print plot jupyterlab tab open turn laptop",
        "Question_preprocessed_content":"run notebook instanc abl close tab current notebook instanc want run notebook expect hour finish want leav overnight output cell output combin print statement plot howevev start run notebook sure initi cell run close jupyterlab tab browser minut open go notebook stop wai us notebook output cell jupyterlab tab open",
        "Question_gpt_summary_original":"The user is facing an issue with running a Sagemaker notebook instance and being able to close the tab while still being able to see the output from each cell, which includes print statements and plots. The notebook stops running when the user closes the Jupyterlab tab in their browser, and they are looking for a solution to be able to leave the notebook running overnight without having to keep the tab open.",
        "Question_gpt_summary":"user face issu run notebook instanc abl close tab abl output cell includ print statement plot notebook stop run user close jupyterlab tab browser look solut abl leav notebook run overnight have tab open"
    },
    {
        "Question_title":"S3 remote permissions and integrity best practices",
        "Question_body":"<p>Does anyone have recommendations about best practices for S3 remote set-up?<\/p>\n<p>I\u2019m specifically interested if there are thoughts around permissions and integrity of data in S3. To enable S3, remotes users would be granted read &amp; write permissions. As long as users use the dvc tooling it appears the data should relatively save. However, granting permissions would allow direct access outside of dvc tooling. This seems to open the possibility of a scenario where the data history could be corrupted.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1553787278231,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":1795.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/s3-remote-permissions-and-integrity-best-practices\/165",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"remot permiss integr best practic recommend best practic remot set specif interest thought permiss integr data enabl remot user grant read write permiss long user us tool appear data rel save grant permiss allow direct access outsid tool open possibl scenario data histori corrupt",
        "Question_preprocessed_content":"remot permiss integr best practic recommend best practic remot specif interest thought permiss integr data enabl remot user grant read write permiss long user us tool appear data rel save grant permiss allow direct access outsid tool open possibl scenario data histori corrupt",
        "Question_gpt_summary_original":"The user is seeking recommendations for best practices in setting up S3 remote, particularly regarding permissions and data integrity. Granting read and write permissions to remote users could potentially lead to data corruption outside of the dvc tooling.",
        "Question_gpt_summary":"user seek recommend best practic set remot particularli permiss data integr grant read write permiss remot user potenti lead data corrupt outsid tool"
    },
    {
        "Question_title":"Trouble connecting to AMLS web service on AKS using Python requests",
        "Question_body":"<p>I am having trouble contacting an AMLS web service hosted on AKS in a vnet. I am able to successfully provision AKS and deploy the models, but I am not able to access the web service using the Python requests module:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>headers = {'Content-Type':'application\/json',\n           'Authorization': 'Bearer ' + &lt;AKS_KEY&gt;}\nresp = requests.post(&lt;AKS_URI&gt;, json={&quot;data&quot;:{&quot;x&quot;: &quot;1&quot;}}, headers=headers)\nprint(resp.text)\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<blockquote>\n<p>Error: HTTPConnectionPool(host='', port=80): Max retries exceeded with url: &lt;AKS_URL&gt; (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f33f6035a10&gt;: Failed to establish a new connection: [Errno 110] Connection timed out'))<\/p>\n<\/blockquote>\n<p>However, I am able to successfully connect to the web service using Postman:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>curl --location --request POST &lt;AKS_URI&gt; \\\n--header 'Authorization: Bearer &lt;AKS_KEY&gt;' \\\n--header 'Content-Type: application\/json' \\\n--data-raw '{&quot;data&quot;: {&quot;x&quot;: &quot;1&quot;}}'\n<\/code><\/pre>\n<p>If I load the AKS service in my AMLS workspace <code>aks_service.run()<\/code> also gives me the same error message. I don't have these problems when I deploy without vnet integration.<\/p>\n<p>What could be causing this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_created_time":1600383486567,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1600397592800,
        "Question_score_count":0.0,
        "Question_view_count":76.0,
        "Poster_created_time":1589738451347,
        "Poster_location":null,
        "Poster_reputation_count":179.0,
        "Poster_view_count":53.0,
        "Answer_body":"<p>I fixed this by adding an inbound security rule enabled for the scoring endpoint in the NSG group that controls the virtual network.<\/p>\n<p>This should be done so that the scoring endpoint can be called from outside the virtual network (see <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-inferencing-vnet\" rel=\"nofollow noreferrer\">documentation<\/a>), but apparently Postman can figure out how to access the endpoint without this security rule!<\/p>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1600397953580,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1589738451347,
        "Answerer_location":null,
        "Answerer_reputation_count":179.0,
        "Answerer_view_count":53.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63947132",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"troubl connect aml web servic ak python request have troubl contact aml web servic host ak vnet abl successfulli provis ak deploi model abl access web servic python request modul header content type applic json author bearer resp request post json data header header print resp text follow error error httpconnectionpool host port max retri exceed url caus newconnectionerror fail establish new connect errno connect time abl successfulli connect web servic postman curl locat request post header author bearer header content type applic json data raw data load ak servic aml workspac ak servic run give error messag problem deploi vnet integr caus",
        "Question_preprocessed_content":"troubl connect aml web servic ak python request have troubl contact aml web servic host ak vnet abl successfulli provis ak deploi model abl access web servic python request modul follow error error httpconnectionpool max retri exceed url abl successfulli connect web servic postman load ak servic aml workspac give error messag problem deploi vnet integr caus",
        "Question_gpt_summary_original":"The user is having trouble connecting to an AMLS web service hosted on AKS in a vnet using Python requests module. They are able to provision AKS and deploy the models, but not able to access the web service. The error message received is \"Max retries exceeded with url\" and \"Failed to establish a new connection\". However, they are able to connect to the web service using Postman. The same error message is received when loading the AKS service in AMLS workspace. The issue is not encountered when deploying without vnet integration.",
        "Question_gpt_summary":"user have troubl connect aml web servic host ak vnet python request modul abl provis ak deploi model abl access web servic error messag receiv max retri exceed url fail establish new connect abl connect web servic postman error messag receiv load ak servic aml workspac issu encount deploi vnet integr"
    },
    {
        "Question_title":"Averaging over runs with the same seed in a sweep",
        "Question_body":"<p>Hi everyone,<\/p>\n<p>I am currently running a sweep to do hyperparameter search on a set of parameters and a seed. What I would like to do is have the results averaged over the different seeds, so that for each group of runs sharing the same hyperparameters but different seeds I only have one value.<br>\nI already tried grouping them by all the parameters except the seed in the GUI, but I obtain a hierarchical grouping (split over each hyparaparameter) which is not what I would expect.<br>\nI think that I will have to go through the API and add a new group to the runs with the same hyperparameters and then group by that group. Is there anything else I can do?<\/p>\n<p>Thank you in advance for your help!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1647596975964,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":176.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/averaging-over-runs-with-the-same-seed-in-a-sweep\/2110",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"averag run seed sweep current run sweep hyperparamet search set paramet seed like result averag differ seed group run share hyperparamet differ seed valu tri group paramet seed gui obtain hierarch group split hyparaparamet expect think api add new group run hyperparamet group group thank advanc help",
        "Question_preprocessed_content":"averag run seed sweep current run sweep hyperparamet search set paramet seed like result averag differ seed group run share hyperparamet differ seed valu tri group paramet seed gui obtain hierarch group expect think api add new group run hyperparamet group group thank advanc help",
        "Question_gpt_summary_original":"The user is running a hyperparameter search on a set of parameters and a seed. They want to average the results over different seeds for each group of runs sharing the same hyperparameters. The user has tried grouping them by all the parameters except the seed in the GUI, but it resulted in a hierarchical grouping. They are considering using the API to add a new group to the runs with the same hyperparameters and then group by that group.",
        "Question_gpt_summary":"user run hyperparamet search set paramet seed want averag result differ seed group run share hyperparamet user tri group paramet seed gui result hierarch group consid api add new group run hyperparamet group group"
    },
    {
        "Question_title":"Disable Azure Datastore download message",
        "Question_body":"<p>Is there any way to disable the warning presented whenever a file is loaded for the first time in Azure ML using datastores?<\/p>\n<pre><code>Downloaded path: \/tmp\/tmp5dto5vq4\/path\/to\/file.txt is different from target path: \/tmp\/tmp5dto5vq4\/path\/to\/file.txt\n<\/code><\/pre>\n<p>I connect to the Datastore following the azure tutorial:<\/p>\n<pre><code>ws = Workspace.from_config()\nmounted_path = tempfile.mkdtemp()\ndataset = Dataset.get_by_name(workspace = ws, name = 'name')\nmount_context = dataset.mount(mounted_path)\n\nmount_context.start()\n\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1644884590780,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":106.0,
        "Poster_created_time":1490704811283,
        "Poster_location":"K\u00f8benhavn, Danmark",
        "Poster_reputation_count":94.0,
        "Poster_view_count":22.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71119807",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"disabl azur datastor download messag wai disabl warn present file load time datastor download path tmp tmpdtovq path file txt differ target path tmp tmpdtovq path file txt connect datastor follow azur tutori workspac config mount path tempfil mkdtemp dataset dataset workspac mount context dataset mount mount path mount context start",
        "Question_preprocessed_content":"disabl azur datastor download messag wai disabl warn present file load time datastor connect datastor follow azur tutori",
        "Question_gpt_summary_original":"The user is encountering a warning message when loading a file for the first time in Azure ML using datastores. They are looking for a way to disable this warning message. The user is following the Azure tutorial to connect to the Datastore.",
        "Question_gpt_summary":"user encount warn messag load file time datastor look wai disabl warn messag user follow azur tutori connect datastor"
    },
    {
        "Question_title":"sagemaker xgboost output to be JSON",
        "Question_body":"<p>I am new to AWS sagemaker and trying to do a simple test, where I am trying to call the xgboost model.<\/p>\n<pre><code>xgboost_container = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, 'us-east-1', &quot;1.2-1&quot;)\n<\/code><\/pre>\n<p>Creating the model and endpoint:<\/p>\n<pre><code>from sagemaker.serializers import JSONSerializer\nendp_name =&quot;myendpoint&quot;\nacc_model = sm_model.deploy(initial_instance_count=1, \n                instance_type='ml.m5.4xlarge',\n                endpoint_name=endp_name, \n                serializer=JSONSerializer(),\n                deserializer= sagemaker.deserializers.JSONDeserializer()\n                            \n               )\n<\/code><\/pre>\n<p>Creating the predictor instance:<\/p>\n<pre><code>from sagemaker.predictor import Predictor\nfrom sagemaker.serializers import CSVSerializer, JSONSerializer \n\nsess = sagemaker.Session()\n\npayload ={\n&quot;var1&quot;:1,\n&quot;var2&quot;:2,\n&quot;var3&quot;:3,\n&quot;var4&quot;:0,\n&quot;var5&quot;:4,\n&quot;var6&quot;:0,\n&quot;var7&quot;:5,\n&quot;var8&quot;:45,\n\n}\n\npredictor = Predictor(\n    endpoint_name=endp_name, sagemaker_session=sess, serializer=JSONSerializer(),\ndeserializer=JSONSerializer()  )\n<\/code><\/pre>\n<p>and then predicting:<\/p>\n<pre><code>predictor.predict(payload)\n<\/code><\/pre>\n<p>I want the output of the predictor.predict to be JSON format, however when I run this I get this.\n<a href=\"https:\/\/i.stack.imgur.com\/gnlxl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gnlxl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What need to be done so that I can see the output as JSON?<\/p>\n<p>PS: If I remove the deserializer I get the output as byte:\n<a href=\"https:\/\/i.stack.imgur.com\/0ncvK.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0ncvK.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>and if I change it to CSVDeserializer I get it out as:\n<a href=\"https:\/\/i.stack.imgur.com\/bv7L6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bv7L6.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1622515828260,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1622516341412,
        "Question_score_count":0.0,
        "Question_view_count":261.0,
        "Poster_created_time":1469929726907,
        "Poster_location":null,
        "Poster_reputation_count":101.0,
        "Poster_view_count":30.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67782131",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"xgboost output json new try simpl test try xgboost model xgboost contain imag uri retriev xgboost east creat model endpoint serial import jsonseri endp myendpoint acc model model deploi initi instanc count instanc type xlarg endpoint endp serial jsonseri deseri deseri jsondeseri creat predictor instanc predictor import predictor serial import csvserial jsonseri sess session payload var var var var var var var var predictor predictor endpoint endp session sess serial jsonseri deseri jsonseri predict predictor predict payload want output predictor predict json format run need output json remov deseri output byte chang csvdeseri",
        "Question_preprocessed_content":"xgboost output json new try simpl test try xgboost model creat model endpoint creat predictor instanc predict want output json format run need output json remov deseri output byte chang csvdeseri",
        "Question_gpt_summary_original":"The user is trying to call an xgboost model in AWS Sagemaker and wants the output of the predictor to be in JSON format. They have created a model and endpoint, and a predictor instance with a JSON serializer and deserializer. However, when they run the predictor, they get the output in byte or CSV format. They are seeking guidance on how to get the output in JSON format.",
        "Question_gpt_summary":"user try xgboost model want output predictor json format creat model endpoint predictor instanc json serial deseri run predictor output byte csv format seek guidanc output json format"
    },
    {
        "Question_title":"How to use MLfLow with private git repositories?",
        "Question_body":"<p>I tested <code>MLflow<\/code> experiment when the source code is stored in public a git repository. Example command looks like this<\/p>\n\n<pre><code>mlflow run  https:\/\/github.com\/amesar\/mlflow-fun.git#examples\/hello_world \\\n --experiment-id=2019 \\\n -Palpha=100 -Prun_origin=GitRun -Plog_artifact=True\n<\/code><\/pre>\n\n<p>However, when I provide an internal (private) git repository link instead of public- MLflow redirects to login url, and then execution fails like this.<\/p>\n\n<pre><code>git.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\ncmdline: git fetch -v origin\nstderr: 'fatal: unable to update url base from redirection:\nasked for: https:\/\/gitlab-master.companyname.com\/myusername\/project_name\n\/tree\/master\/models\/myclassifier\/info\/refs?service=git-upload-pack\nredirect: https:\/\/gitlab-master.company.com\/users\/sign_in'\n<\/code><\/pre>\n\n<p>Is there any way to commmunicate credentials of git account to MLflow?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1563170204117,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":1278.0,
        "Poster_created_time":1401104228227,
        "Poster_location":"Santa Clara, CA, USA",
        "Poster_reputation_count":4031.0,
        "Poster_view_count":244.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57033896",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"us privat git repositori test experi sourc code store public git repositori exampl command look like run http github com amesar fun git exampl hello world experi palpha prun origin gitrun plog artifact true provid intern privat git repositori link instead public redirect login url execut fail like git exc gitcommanderror cmd git fail exit code cmdline git fetch origin stderr fatal unabl updat url base redirect ask http gitlab master companynam com myusernam project tree master model myclassifi info ref servic git upload pack redirect http gitlab master compani com user sign wai counic credenti git account",
        "Question_preprocessed_content":"us privat git repositori test experi sourc code store public git repositori exampl command look like provid intern git repositori link instead public redirect login url execut fail like wai counic credenti git account",
        "Question_gpt_summary_original":"The user is facing challenges while using MLflow with private git repositories. When the user provides a private git repository link, MLflow redirects to a login URL, and the execution fails. The user is seeking a way to communicate the credentials of their git account to MLflow.",
        "Question_gpt_summary":"user face challeng privat git repositori user provid privat git repositori link redirect login url execut fail user seek wai commun credenti git account"
    },
    {
        "Question_title":"Batch pipeline support and multi I\/O",
        "Question_body":"<p>This tool looks very promising. However, I am wondering about the functionality of the pipeline feature. From the documentation it seems that I can chain an input file to an output file and each logged operation will be appended to the repro argument when I commit. In other words it is not clear to me <strong>how to create a batch process<\/strong> without first defining each step separately.<\/p>\n<p>It is also not clear to me <strong>how to handle multiple input and output files.<\/strong><\/p>\n<p>I am very curious if this would be possible.<\/p>\n<p>Here is an example of an offline analysis that I am performing on human EEG data and behavioral data (using python and java):<\/p>\n<pre><code>sample size \u00d7 \"ASCII_\" + n + \".txt\" --&gt; Pandas --&gt; SPSS Syntax --&gt; Tables + Figures\n\n    --&gt; First Level Summary: Individual Participants (Diagnostics)\n   \/\n--&lt;\n   \\\n    --&gt; Second Level Summary: Group Level\n\n    + --&gt; Behavioral CSV\n\n\nEEG Raw Binary Data --&gt; EEG Manual Preprocessing --&gt; MATLAB Scripts --&gt; Figures\n\n    + --&gt; EEG CSV\n\n\n Behavioral CSV --\n                  \\\n                   &gt;--&gt; Correlations (SPSS Syntax)\n                  \/\n        EEG CSV --<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1530180340930,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":854.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/batch-pipeline-support-and-multi-i-o\/42",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"batch pipelin support multi tool look promis wonder function pipelin featur document chain input file output file log oper append repro argument commit word clear creat batch process defin step separ clear handl multipl input output file curiou possibl exampl offlin analysi perform human eeg data behavior data python java sampl size ascii txt panda spss syntax tabl figur level summari individu particip diagnost second level summari group level behavior csv eeg raw binari data eeg manual preprocess matlab script figur eeg csv behavior csv correl spss syntax eeg csv",
        "Question_preprocessed_content":"batch pipelin support multi tool look promis wonder function pipelin featur document chain input file output file log oper append repro argument commit word clear creat batch process defin step separ clear handl multipl input output file curiou possibl exampl offlin analysi perform human eeg data behavior data",
        "Question_gpt_summary_original":"The user is interested in using a tool with pipeline feature but is unsure about how to create a batch process and handle multiple input and output files. They provide an example of an offline analysis they are performing on human EEG data and behavioral data using Python and Java.",
        "Question_gpt_summary":"user interest tool pipelin featur unsur creat batch process handl multipl input output file provid exampl offlin analysi perform human eeg data behavior data python java"
    },
    {
        "Question_title":"Is there a way to export experiment parameters and logged metrics in Azure ML to CSV?",
        "Question_body":"<p>I am running a bunch of ML experiments using AzureML, sometimes changing training parameters and sometimes aspects of the data preprocessing. In general, for a given experiment I will be able to get a table (aka &quot;view&quot;) like this:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/40466-image.png?platform=QnA\" alt=\"40466-image.png\" \/>    <\/p>\n<p>While the UI allows some minimum level of customization, sorting runs by e.g. desired columns (say the accuracy to identify the best runs) seems really problematic.     <\/p>\n<p>The only workaround I am aware of is to save the page to HTML (!) and extract the values from there.     <br \/>\nThe data in the cells can't by copied with a cursor either...    <\/p>\n<p>Is there an easy way to export the data collected during several runs, via the UI or programmatically, without the need to scrape the blob storage of the Azure ML workspace (I am asking the community here as <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-monitor-view-training-logs\">docs<\/a> don't seem particularly helpful)?    <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1605641332880,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/166057\/is-there-a-way-to-export-experiment-parameters-and",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"wai export experi paramet log metric csv run bunch experi chang train paramet aspect data preprocess gener given experi abl tabl aka view like allow minimum level custom sort run desir column accuraci identifi best run problemat workaround awar save page html extract valu data cell copi cursor easi wai export data collect run programmat need scrape blob storag workspac ask commun doc particularli help",
        "Question_preprocessed_content":"wai export experi paramet log metric csv run bunch experi chang train paramet aspect data preprocess gener given experi abl tabl like allow minimum level custom sort run desir column problemat workaround awar save page html extract valu data cell copi cursor easi wai export data collect run programmat need scrape blob storag workspac",
        "Question_gpt_summary_original":"The user is facing challenges in exporting experiment parameters and logged metrics in Azure ML to CSV. The UI allows minimum customization and sorting runs by desired columns is problematic. The only workaround is to save the page to HTML and extract values from there. The user is seeking an easy way to export data collected during several runs, either via the UI or programmatically, without the need to scrape the blob storage of the Azure ML workspace.",
        "Question_gpt_summary":"user face challeng export experi paramet log metric csv allow minimum custom sort run desir column problemat workaround save page html extract valu user seek easi wai export data collect run programmat need scrape blob storag workspac"
    },
    {
        "Question_title":"Vertax AI pipeline quota",
        "Question_body":"<p>I got a custom_model_training_cpus error when runing a submitted pipeline on Vertex AI. I could not find any documents. And I am using the n1-standard-4 for the deployment machine, I do not see any issue. Any commnents would be much appriciated.<\/p>\n<blockquote>\n<p>com.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits: aiplatform.googleapis.com\/custom_model_training_cpus, cause=null; Failed to create custom job for the task.<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TgCJD.png\" rel=\"nofollow noreferrer\">DAG flow and error message<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1660618735810,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":110.0,
        "Poster_created_time":1660618422463,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73368320",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"vertax pipelin quota got custom model train cpu error rune submit pipelin document standard deploy machin issu commnent apprici com googl cloud platform common error aiplatformexcept code resourc exhaust messag follow quota metric exce quota limit aiplatform googleapi com custom model train cpu caus null fail creat custom job task dag flow error messag",
        "Question_preprocessed_content":"vertax pipelin quota got error rune submit pipelin document deploy machin issu commnent apprici messag follow quota metric exce quota limit caus null fail creat custom job task dag flow error messag",
        "Question_gpt_summary_original":"The user encountered a custom_model_training_cpus error while running a submitted pipeline on Vertex AI. The error message indicates that the quota metrics exceed the quota limits, and the user is unable to find any relevant documents to resolve the issue. The user is using n1-standard-4 for the deployment machine and seeks comments to address the problem.",
        "Question_gpt_summary":"user encount custom model train cpu error run submit pipelin error messag indic quota metric exce quota limit user unabl relev document resolv issu user standard deploy machin seek comment address problem"
    },
    {
        "Question_title":"AWS web service to upload and analyse users video",
        "Question_body":"<p>I'm developing a prototype of a video analysis service on AWS.<\/p>\n<p>The question is: am thinking in the right direction or I will fail to implement this architecture?<\/p>\n<p>Architecture:<\/p>\n<ol>\n<li>Flask on EC2.<\/li>\n<li>User(authenticated) upload file via web view, I'm saving it to S3.<\/li>\n<li>Lambda triggers SageMaker.<\/li>\n<li>SageMaker taking a file from S3, making preparation and analysis then: 1) Saving the results to PostgreSQL DB. 2) Triggers lambda that sends a notification to Flask that analysis is Done<\/li>\n<li>User receives a notification from Flask that the analysis is done.<\/li>\n<li>Flask web page visualizes data from the analysis for the user.<\/li>\n<\/ol>\n<p>It has only a prototyping purpose, I'm trying to keep it as simple as possible.<\/p>\n<p>will appreciate any comments and recommendations.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1631895559793,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":39.0,
        "Poster_created_time":1441637659808,
        "Poster_location":null,
        "Poster_reputation_count":119.0,
        "Poster_view_count":27.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69226644",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"aw web servic upload analys user video develop prototyp video analysi servic aw question think right direct fail implement architectur architectur flask user authent upload file web view save lambda trigger take file make prepar analysi save result postgresql trigger lambda send notif flask analysi user receiv notif flask analysi flask web page visual data analysi user prototyp purpos try simpl possibl appreci comment recommend",
        "Question_preprocessed_content":"aw web servic upload analys user video develop prototyp video analysi servic aw question think right direct fail implement architectur architectur flask user upload file web view save lambda trigger take file make prepar analysi save result postgresql trigger lambda send notif flask analysi user receiv notif flask analysi flask web page visual data analysi user prototyp purpos try simpl possibl appreci comment recommend",
        "Question_gpt_summary_original":"The user is developing a prototype of a video analysis service on AWS and is seeking feedback on their proposed architecture. The architecture involves Flask on EC2, user-authenticated file uploads to S3, Lambda triggers SageMaker for analysis, and results saved to PostgreSQL DB. The user receives a notification from Flask when the analysis is done, and Flask web page visualizes data from the analysis for the user. The user is seeking comments and recommendations to keep the prototype as simple as possible.",
        "Question_gpt_summary":"user develop prototyp video analysi servic aw seek feedback propos architectur architectur involv flask user authent file upload lambda trigger analysi result save postgresql user receiv notif flask analysi flask web page visual data analysi user user seek comment recommend prototyp simpl possibl"
    },
    {
        "Question_title":"Procedure for changing cache type",
        "Question_body":"<p>Because I\u2019m using large files which are tens of gigabytes, I want to use hardlinks as my filesystem doesn\u2019t support reflinks.<br>\nI executed the following commands<\/p>\n<pre><code class=\"lang-auto\">dvc config cache.type = hardlink\ndvc config cache.protected = true\ndvc checkout\n<\/code><\/pre>\n<p>However when I checked using <code>ls -i<\/code> the inodes of the file in .dvc\/cache and the file in my working directory, they are still different.<br>\nAre there any further steps I need to take to get hardlinks to be used?<\/p>\n<p>Also I noticed that both the file in the working directory and cache were still writable, when I thought that they should have been made read-only by using <code>cache.protected = true<\/code><br>\nIs my understanding here mistaken?<\/p>\n<p>Update:<br>\nI mistyped the commands above. I actually had executed them correctly, i.e.:<\/p>\n<pre><code class=\"lang-auto\">dvc config cache.type hardlink\ndvc config cache.protected true\ndvc checkout\n<\/code><\/pre>",
        "Question_answer_count":11,
        "Question_comment_count":0,
        "Question_created_time":1566354562099,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":1005.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/procedure-for-changing-cache-type\/204",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"procedur chang cach type larg file ten gigabyt want us hardlink filesystem doesnt support reflink execut follow command config cach type hardlink config cach protect true checkout check inod file cach file work directori differ step need hardlink notic file work directori cach writabl thought read cach protect true understand mistaken updat mistyp command actual execut correctli config cach type hardlink config cach protect true checkout",
        "Question_preprocessed_content":"procedur chang cach type larg file ten gigabyt want us hardlink filesystem doesnt support reflink execut follow command check inod file file work directori differ step need hardlink notic file work directori cach writabl thought understand mistaken updat mistyp command actual execut correctli",
        "Question_gpt_summary_original":"The user wants to use hardlinks for large files but the inodes of the file in .dvc\/cache and the file in the working directory are still different after executing the necessary commands. The user also has a misunderstanding about the use of \"cache.protected = true\" as both the file in the working directory and cache were still writable.",
        "Question_gpt_summary":"user want us hardlink larg file inod file cach file work directori differ execut necessari command user misunderstand us cach protect true file work directori cach writabl"
    },
    {
        "Question_title":"Publishing AzureML Webservice from R requires external zip utility",
        "Question_body":"<p>I want to deploy a basic trained R model as a webservice to AzureML. Similar to what is done here:\n<a href=\"http:\/\/www.r-bloggers.com\/deploying-a-car-price-model-using-r-and-azureml\/\" rel=\"nofollow\">http:\/\/www.r-bloggers.com\/deploying-a-car-price-model-using-r-and-azureml\/<\/a><\/p>\n\n<p>Since that post the publishWebService function in the R AzureML package was has changed it now requires me to have a workspace object as first parameter thus my R code looks as follows:<\/p>\n\n<pre><code>    library(MASS)\n    library(AzureML)\n\n    PredictionModel = lm( medv ~ lstat , data = Boston )\n\n    PricePredFunktion = function(percent)\n    {return(predict(PredictionModel, data.frame(lstat =percent)))}\n\n    myWsID = \"&lt;my Workspace ID&gt;\"\n    myAuth = \"&lt;my Authorization code\"\n\n    ws = workspace(myWsID, myAuth, api_endpoint = \"https:\/\/studio.azureml.net\/\", .validate = TRUE)\n\n    # publish the R function to AzureML\n    PricePredService = publishWebService(\n      ws,\n\n      \"PricePredFunktion\",\n      \"PricePredOnline\",\n      list(\"lstat\" = \"float\"),\n      list(\"mdev\" = \"float\"),\n      myWsID,\n      myAuth\n    )\n<\/code><\/pre>\n\n<p>But every time I execute the code I get the following error:<\/p>\n\n<pre><code>    Error in publishWebService(ws, \"PricePredFunktion\", \"PricePredOnline\",  : \n    Requires external zip utility. Please install zip, ensure it's on your path and try again.\n<\/code><\/pre>\n\n<p>I tried installing programs that handle zip files (like 7zip) on my machine as well as calling the <code>utils<\/code> library in R which allows R to directly interact with zip files. But I couldn't get rid of the error.<\/p>\n\n<p>I also found the R package code that is throwing the error, it is on line 154 on this page:\n<a href=\"https:\/\/github.com\/RevolutionAnalytics\/AzureML\/blob\/master\/R\/internal.R\" rel=\"nofollow\">https:\/\/github.com\/RevolutionAnalytics\/AzureML\/blob\/master\/R\/internal.R<\/a> <\/p>\n\n<p>but it didn't help me in figuring out what to do.<\/p>\n\n<p>Thanks in advance for any Help!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1454506823477,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1456850085647,
        "Question_score_count":4.0,
        "Question_view_count":962.0,
        "Poster_created_time":1416588328840,
        "Poster_location":null,
        "Poster_reputation_count":149.0,
        "Poster_view_count":22.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35178688",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"publish webservic requir extern zip util want deploi basic train model webservic similar http blogger com deploi car price model post publishwebservic function packag chang requir workspac object paramet code look follow librari mass librari predictionmodel medv lstat data boston pricepredfunkt function percent return predict predictionmodel data frame lstat percent mywsid myauth author code workspac mywsid myauth api endpoint http studio net valid true publish function pricepredservic publishwebservic pricepredfunkt pricepredonlin list lstat float list mdev float mywsid myauth time execut code follow error error publishwebservic pricepredfunkt pricepredonlin requir extern zip util instal zip ensur path try tri instal program handl zip file like zip machin call util librari allow directli interact zip file couldn rid error packag code throw error line page http github com revolutionanalyt blob master intern help figur thank advanc help",
        "Question_preprocessed_content":"publish webservic requir extern zip util want deploi basic train model webservic similar post publishwebservic function packag chang requir workspac object paramet code look follow time execut code follow error tri instal program handl zip file machin call librari allow directli interact zip file couldn rid error packag code throw error line page help figur thank advanc help",
        "Question_gpt_summary_original":"The user is trying to deploy a trained R model as a webservice to AzureML using the publishWebService function in the R AzureML package. However, the function now requires a workspace object as the first parameter, and the user is encountering an error that says an external zip utility is required. The user has tried installing programs that handle zip files and calling the utils library in R, but the error persists. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user try deploi train model webservic publishwebservic function packag function requir workspac object paramet user encount error sai extern zip util requir user tri instal program handl zip file call util librari error persist user seek help resolv issu"
    },
    {
        "Question_title":"Page scrolls when moving panels in the web UI",
        "Question_body":"<p>Hello !<\/p>\n<p>When dragging panels within sections or from one section to another, the webpage scrolls back to the top. This makes it impossible to move panels around.<\/p>\n<p>Can this bug be fixed?<br>\nThank you.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1673361904230,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":130.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/page-scrolls-when-moving-panels-in-the-web-ui\/3672",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"page scroll move panel web hello drag panel section section webpag scroll make imposs panel bug fix thank",
        "Question_preprocessed_content":"page scroll move panel web hello drag panel section section webpag scroll make imposs panel bug fix thank",
        "Question_gpt_summary_original":"The user is facing an issue where the webpage scrolls back to the top when they try to move panels within sections or from one section to another in the web UI. This is making it impossible for them to move the panels around and they are seeking a solution to fix this bug.",
        "Question_gpt_summary":"user face issu webpag scroll try panel section section web make imposs panel seek solut fix bug"
    },
    {
        "Question_title":"Plotting the confuison matrix into wandb (pytorch)",
        "Question_body":"<p>I'm training a model and I'm trying to add a confusion matrix, which would be displayed in my <code>wandb<\/code>, but I got lost a bit. Basically, the matrix works; I can print it, but it's not loaded into <code>wandb<\/code>. Everything should be OK, except it's not. Can you please help me? I'm new to all this. Thanks a lot!<\/p>\n<p><strong>the code<\/strong><\/p>\n<pre><code>    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs))\n        print('-' * 10)\n\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  \n            else:\n                model.eval()   \n\n            running_loss = 0.0\n            running_corrects = 0\n\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n                from sklearn.metrics import f1_score\n                f1_score = f1_score(labels.cpu().data, preds.cpu(), average=None)\n                wandb.log({'F1 score' : f1_score})\n\n                nb_classes = 7\n\n                confusion_matrix = torch.zeros(nb_classes, nb_classes)\n                with torch.no_grad():\n                    for i, (inputs, classes) in enumerate(dataloaders['val']):\n                        inputs = inputs.to(device)\n                        classes = classes.to(device)\n                        outputs = model_ft(inputs)\n                        _, preds = torch.max(outputs, 1)\n                    \n                    for t, p in zip(classes.view(-1), preds.view(-1)):\n                        confusion_matrix[t.long(), p.long()] += 1\n              wandb.log({'matrix' : confusion_matrix})\n                           \n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n            wandb.log({'epoch loss': epoch_loss,\n                    'epoch acc': epoch_acc})\n            \n            data = [[i, random.random() + math.sin(i \/ 10)] for i in range(100)]\n            table = wandb.Table(data=data, columns=[&quot;step&quot;, &quot;height&quot;])\n            wandb.log({'line-plot1': wandb.plot.line(table, &quot;step&quot;, &quot;height&quot;)})\n\n        \n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc, f1_score))\n\n            if phase == 'val' and epoch_acc &gt; best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n        \n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n    print('f1_score: {}'.format(f1_score))\n   \n    model.load_state_dict(best_model_wts)\n    return model\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1660219911593,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1660228199172,
        "Question_score_count":0.0,
        "Question_view_count":69.0,
        "Poster_created_time":1659890757880,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73320449",
        "Tool":"Weights & Biases",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"plot confuison matrix pytorch train model try add confus matrix displai got lost bit basic matrix work print load help new thank lot code time time best model wt copi deepcopi model state dict best acc epoch rang num epoch print epoch format epoch num epoch print phase train val phase train model train model eval run loss run correct input label dataload phase input input devic label label devic optim zero grad torch set grad enabl phase train output model input pred torch max output loss criterion output label phase train loss backward optim step run loss loss item input size run correct torch sum pred label data sklearn metric import score score score label cpu data pred cpu averag log score score class confus matrix torch zero class class torch grad input class enumer dataload val input input devic class class devic output model input pred torch max output zip class view pred view confus matrix long long log matrix confus matrix phase train schedul step epoch loss run loss dataset size phase epoch acc run correct doubl dataset size phase log epoch loss epoch loss epoch acc epoch acc data random random math sin rang tabl tabl data data column step height log line plot plot line tabl step height print loss acc format phase epoch loss epoch acc score phase val epoch acc best acc best acc epoch acc best model wt copi deepcopi model state dict print time elaps time time print train complet format time elaps time elaps print best val acc format best acc print score format score model load state dict best model wt return model",
        "Question_preprocessed_content":"plot confuison matrix train model try add confus matrix displai got lost bit basic matrix work print load help new thank lot code",
        "Question_gpt_summary_original":"The user is trying to add a confusion matrix to their PyTorch model and display it in wandb, but is having trouble doing so. They are able to print the matrix but it is not being loaded into wandb. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user try add confus matrix pytorch model displai have troubl abl print matrix load user seek help resolv issu"
    },
    {
        "Question_title":"Wandb is doing the same possibility multiple times",
        "Question_body":"<p>hi, i am working on private data with keras. I use different activation functions and optimization algorithms. I\u2019m doing this wandb sweep, but it trains one possibility more than once. example relu-lr=0.001-adam-batch_size=4 has trained probability more than 6 times. what is the reason of this. and I\u2019m not sure of the correctness of my code<\/p>\n<p>my code :<\/p>\n<pre><code class=\"lang-auto\">import tensorflow as tf\nimport numpy as np\nbase_dir=\"\/content\/f1\"\n\ntrain_datagen=tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1.\/255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.1\n    )\n\ntest_datagen=tf.keras.preprocessing.image.ImageDataGenerator(\n     rescale=1.\/255,\n     validation_split=0.1\n)\n\ntrain_datagen=train_datagen.flow_from_directory(\n    base_dir,\n    target_size=(500,500),\n    subset='training',\n    batch_size=2\n)\n\ntest_datagen=test_datagen.flow_from_directory(\n    base_dir,\n    target_size=(500,500),\n    subset='validation',\n    batch_size=2\n)\nwandb.login()\nsweep_config = {\n    'method': 'random',\n    'metric': {\n      'name': 'accuracy',\n      'goal': 'maximize'   \n    },\n    'parameters': {\n        'epochs': {\n            'values': [2,4]\n        },\n        'learning_rate': {\n            'values': [0.01,0.001]\n        },\n        'optimizer': {\n            'values': ['adam','rmsprop']\n        },\n        'activation': {\n            'values': ['relu', 'elu', 'selu']\n        }\n    }\n}\nsweep_id = wandb.sweep(sweep_config, entity=\"sdad\", project=\"func\")\ndef train():\n    config_defaults = {\n        'epochs': 2,\n        'batch_size': 2,\n        'learning_rate': 0.001,\n        'activation': 'relu',\n        'optimizer': 'adam',\n        'seed': 42\n    }\n\n    wandb.init(config=config_defaults)\n    \n    config = wandb.config\n    \n    model= Sequential()\n\n    model.add(layers.Conv2D(filters=4,activation=config.activation,kernel_size=(5,5),input_shape=(500,500,3)))\n    model.add(layers.MaxPooling2D((2,2)))\n    model.add(layers.Conv2D(filters=8,activation=config.activation,kernel_size=(3,3)))\n    model.add(layers.MaxPooling2D((2,2)))\n    model.add(layers.Conv2D(filters=16,activation=config.activation,kernel_size=(2,2)))\n    model.add(layers.MaxPooling2D((2,2)))\n    model.add(layers.Conv2D(filters=32,activation=config.activation,kernel_size=(2,2)))\n\n    model.add(layers.Flatten())\n\n    model.add(Dense(50,activation=config.activation))\n    model.add(Dense(100,activation=config.activation))\n    model.add(Dense(100,activation=config.activation))\n    model.add(Dense(50,activation=config.activation))\n    model.add(Dense(4,activation=\"softmax\"))\n\n  \n    model.compile(loss = \"categorical_crossentropy\", optimizer = config.optimizer, metrics=['accuracy'])\n\n    model.fit(train_datagen, batch_size=config.batch_size,\n              epochs=config.epochs,\n              validation_data=test_datagen,\n              callbacks=[WandbCallback(data_type=\"image\", validation_data=test_datagen)])\n<\/code><\/pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/9dbbeab2a807227267a91e6519a401beaaa15592.jpeg\" data-download-href=\"\/uploads\/short-url\/mvnzvxYGjBl6IgsCgOkOZueYeoa.jpeg?dl=1\" title=\"swepl\u0131nt\u0131s\u0131\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/9dbbeab2a807227267a91e6519a401beaaa15592_2_690x36.jpeg\" alt=\"swepl\u0131nt\u0131s\u0131\" data-base62-sha1=\"mvnzvxYGjBl6IgsCgOkOZueYeoa\" width=\"690\" height=\"36\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/9dbbeab2a807227267a91e6519a401beaaa15592_2_690x36.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/9dbbeab2a807227267a91e6519a401beaaa15592_2_1035x54.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/9dbbeab2a807227267a91e6519a401beaaa15592_2_1380x72.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/9dbbeab2a807227267a91e6519a401beaaa15592_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">swepl\u0131nt\u0131s\u0131<\/span><span class=\"informations\">1780\u00d795 18.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1641159495641,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":223.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-is-doing-the-same-possibility-multiple-times\/1667",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"possibl multipl time work privat data kera us differ activ function optim algorithm sweep train possibl exampl relu adam batch size train probabl time reason sure correct code code import tensorflow import numpi base dir content train datagen kera preprocess imag imagedatagener rescal shear rang zoom rang horizont flip true valid split test datagen kera preprocess imag imagedatagener rescal valid split train datagen train datagen flow directori base dir target size subset train batch size test datagen test datagen flow directori base dir target size subset valid batch size login sweep config method random metric accuraci goal maxim paramet epoch valu learn rate valu optim valu adam rmsprop activ valu relu elu selu sweep sweep sweep config entiti sdad project func def train config default epoch batch size learn rate activ relu optim adam seed init config config default config config model sequenti model add layer convd filter activ config activ kernel size input shape model add layer maxpoolingd model add layer convd filter activ config activ kernel size model add layer maxpoolingd model add layer convd filter activ config activ kernel size model add layer maxpoolingd model add layer convd filter activ config activ kernel size model add layer flatten model add dens activ config activ model add dens activ config activ model add dens activ config activ model add dens activ config activ model add dens activ softmax model compil loss categor crossentropi optim config optim metric accuraci model fit train datagen batch size config batch size epoch config epoch valid data test datagen callback callback data type imag valid data test datagen sweplnt",
        "Question_preprocessed_content":"possibl multipl time work privat data kera us differ activ function optim algorithm sweep train possibl exampl train probabl time reason sure correct code code sweplnt",
        "Question_gpt_summary_original":"The user is encountering an issue with Wandb where it is training one possibility more than once during a sweep, even though different activation functions and optimization algorithms are being used. The user is unsure if their code is correct. They have provided their code and a screenshot of the Wandb sweep configuration.",
        "Question_gpt_summary":"user encount issu train possibl sweep differ activ function optim algorithm user unsur code correct provid code screenshot sweep configur"
    },
    {
        "Question_title":"Azure Machine Learning - Empty score results",
        "Question_body":"<p>I've trained a model, the test results on test-set are okay.\nNow I have saved the model as 'Trained model' and made a new experiment into a new dataset, for making predictions where I don't have the actual value's. <\/p>\n\n<p>Normally, the trained model gives me a scored label result per instance. \nBut now, the scored label results are empty. Also when I convert the score results to CSV the scored labels column is empty.<\/p>\n\n<p>Even stranger, when I take a look at the Statistics of the score Visualize tab, I DO see the statistics of the scored values. But no actual scored values... <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/R0cGb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/R0cGb.png\" alt=\"Screenshot of the empty score results\"><\/a><\/p>\n\n<p>Is this a bug? Or am I forgetting something important? Whats going on ;) ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_created_time":1458727749117,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1458736781830,
        "Question_score_count":4.0,
        "Question_view_count":1731.0,
        "Poster_created_time":1430303722392,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36175153",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"score result train model test result test set okai save model train model new experi new dataset make predict actual valu normal train model give score label result instanc score label result convert score result csv score label column stranger look statist score visual tab statist score valu actual score valu bug forget import what go",
        "Question_preprocessed_content":"score result train model test result okai save model train model new experi new dataset make predict actual valu normal train model give score label result instanc score label result convert score result csv score label column stranger look statist score visual tab statist score valu actual score bug forget import what go",
        "Question_gpt_summary_original":"The user has trained a model in Azure Machine Learning and saved it as 'Trained model'. However, when making predictions on a new dataset, the scored label results are empty, even though the statistics of the scored values are visible. The user is unsure if this is a bug or if they are missing something important.",
        "Question_gpt_summary":"user train model save train model make predict new dataset score label result statist score valu visibl user unsur bug miss import"
    },
    {
        "Question_title":"HyperOpt multi metric evalution",
        "Question_body":"<p>Does anyone know if it is possible to somehow calculate metrics other than accuracy in HyperOpt? I would also like it to display me F1, precision, recall. Is there any option to do it? If so could someone please explain it to me.<\/p>\n<pre><code>def objective(space):\n    pipe_params = {}\n    \n    for s in space:\n        pipe_params[f&quot;classifier__{s}&quot;] = space[s]\n        \n    pipe.set_params(**pipe_params)\n    score = cross_val_score(pipe, X_train, y_train, cv=10, scoring=&quot;accuracy&quot;,n_jobs=-1).mean()\n    # Is there an option to add other metrics to the return \n    return {'loss': 1- score, 'status': STATUS_OK, 'accuracy': score}\n<\/code><\/pre>\n<pre><code>\ntrials_df = []\n \nfor cl in classifiers:\n    cl_name = cl['class'].__class__.__name__\n    print(f&quot;\\n\\n{cl_name}&quot;)\n    \n    pipe = Pipeline(steps = [\n    ('data_processing_pipeline', data_processing_pipeline),\n    ('classifier', cl['class'])\n    ])\n    \n    space = {}\n    for k in cl['params']:\n        space[k] = cl['params'][k]\n    \n    max_evals = cl['max_evals']\n    \n    trials = Trials()\n    best = fmin(fn=objective,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=max_evals,\n                    trials=trials)\n     \n    best_params = space_eval(space, best)\n    print('\\nThe best params:')\n    print (&quot;{:&lt;30} {}&quot;.format('Parameter','Selected'))\n    for k, v in best_params.items():\n        print (&quot;{:&lt;30} {}&quot;.format(k, v))\n    \n    for trial in trials.trials:\n        trials_df.append({\n            'classifier': cl_name,\n            'loss': trial['result']['loss'],\n            'accuracy': trial['result']['accuracy'],\n            'params': trial['misc']['vals']\n            })\n<\/code><\/pre>\n<p>Here is my link to Github If anyone wants to see the whole code: <strong><a href=\"https:\/\/github.com\/mikolaj-halemba\/Water-Quality-\/blob\/main\/water_quality.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mikolaj-halemba\/Water-Quality-\/blob\/main\/water_quality.ipynb<\/a><\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1635271027090,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":186.0,
        "Poster_created_time":1608202968307,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69727854",
        "Tool":"Optuna",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"hyperopt multi metric evalut know possibl calcul metric accuraci hyperopt like displai precis recal option explain def object space pipe param space pipe param classifi space pipe set param pipe param score cross val score pipe train train score accuraci job mean option add metric return return loss score statu statu accuraci score trial classifi class class print pipe pipelin step data process pipelin data process pipelin classifi class space param space param max eval max eval trial trial best fmin object space space algo tpe suggest max eval max eval trial trial best param space eval space best print nthe best param print format paramet select best param item print format trial trial trial trial append classifi loss trial result loss accuraci trial result accuraci param trial misc val link github want code http github com mikolaj halemba water qualiti blob main water qualiti ipynb",
        "Question_preprocessed_content":"hyperopt multi metric evalut know possibl calcul metric accuraci hyperopt like displai precis recal option explain link github want code",
        "Question_gpt_summary_original":"The user is seeking information on how to calculate metrics other than accuracy, such as F1, precision, and recall, in HyperOpt. They have provided a code snippet and are asking if there is an option to add other metrics to the return. The user has also shared a link to their Github repository for reference.",
        "Question_gpt_summary":"user seek inform calcul metric accuraci precis recal hyperopt provid code snippet ask option add metric return user share link github repositori refer"
    },
    {
        "Question_title":"What are the recommended practices on how to use DDP with wandb?",
        "Question_body":"<p>I often use distributed data laoders in pytorch DDP and often just check the rank and have only rank 0 log. Is that the recommended way to use DDP and wandb?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1631289824937,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":374.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/what-are-the-recommended-practices-on-how-to-use-ddp-with-wandb\/502",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"recommend practic us ddp us distribut data laoder pytorch ddp check rank rank log recommend wai us ddp thank",
        "Question_preprocessed_content":"recommend practic us ddp us distribut data laoder pytorch ddp check rank rank log recommend wai us ddp thank",
        "Question_gpt_summary_original":"The user is seeking advice on the recommended practices for using Distributed Data Parallel (DDP) with wandb in PyTorch, specifically regarding logging and checking the rank.",
        "Question_gpt_summary":"user seek advic recommend practic distribut data parallel ddp pytorch specif log check rank"
    },
    {
        "Question_title":"Getting Conflict error for Amazon.RegionEndpoint with Sagemaker",
        "Question_body":"<p>I have created an applicataion in c#, where I need to put some data on S3 bucket, and to Invoke AWS sagemaker APIs.\nSince the same Amazon.RegionEndPoint class exists in both the references, it is giving below error.<\/p>\n\n<blockquote>\n  <p>The type 'RegionEndpoint' exists in both 'AWSSDK.Core,\n  Version=3.3.0.0, Culture=neutral, PublicKeyToken=885c28607f98e604' and\n  'AWSSDK, Version=2.3.55.2<\/p>\n<\/blockquote>\n\n<p>Basically I am trying to upload files on AWS S3, following code I have used.<\/p>\n\n<pre><code>AmazonS3Client s3Client = new AmazonS3Client(_AWS_ACCESS_KEY_ID, _AWS_SECRETE_ACCESS_KEY, Amazon.RegionEndpoint.USEast2);\n PutObjectRequest request = new PutObjectRequest\n  {\n    BucketName = _BucketName,\n    Key = i_sDestFileName,\n    FilePath = i_sSourceFilePath,\n    ContentType = \"text\/plain\"\n  };\n  s3Client.PutObject(request);\n<\/code><\/pre>\n\n<p>It is working fine on a single application, but when I integrated code with Sagemaker API invokation, the conflict occurs for Amazon.RegionEndpoint.USEast2.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_created_time":1517316966157,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1517391033467,
        "Question_score_count":3.0,
        "Question_view_count":2527.0,
        "Poster_created_time":1337759214688,
        "Poster_location":"Pune India",
        "Poster_reputation_count":1036.0,
        "Poster_view_count":124.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48522013",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"get conflict error amazon regionendpoint creat applicataion need data bucket invok api amazon regionendpoint class exist refer give error type regionendpoint exist awssdk core version cultur neutral publickeytoken cfe awssdk version basic try upload file aw follow code amazonscli sclient new amazonscli aw access kei aw secret access kei amazon regionendpoint useast putobjectrequest request new putobjectrequest bucketnam bucketnam kei sdestfilenam filepath ssourcefilepath contenttyp text plain sclient putobject request work fine singl applic integr code api invok conflict occur amazon regionendpoint useast",
        "Question_preprocessed_content":"get conflict error creat applicataion need data bucket invok api class exist refer give error type regionendpoint exist cultur neutral publickeytoken awssdk basic try upload file aw follow code work fine singl applic integr code api invok conflict occur",
        "Question_gpt_summary_original":"The user is encountering a conflict error with the Amazon.RegionEndpoint class while trying to upload files on AWS S3 and invoke AWS Sagemaker APIs in a C# application. The error is caused by the existence of the same class in both 'AWSSDK.Core' and 'AWSSDK' references. The code works fine on a single application but the conflict occurs when integrated with Sagemaker API invocation.",
        "Question_gpt_summary":"user encount conflict error amazon regionendpoint class try upload file aw invok api applic error caus exist class awssdk core awssdk refer code work fine singl applic conflict occur integr api invoc"
    },
    {
        "Question_title":"How to deploy a ML model to azure which requires an entire repository to run",
        "Question_body":"<p>Hi, I was following guide <a href=\"https:\/\/medium.com\/aiguys\/model-deployment-on-azure-as-a-web-app-b7bb5599bfbf\">https:\/\/medium.com\/aiguys\/model-deployment-on-azure-as-a-web-app-b7bb5599bfbf<\/a>  <br \/>\nthis worked, but in my actual use case I am deploying a model which is made up of 4 different ML models.<\/p>\n<p>this means that my code is rather complicated and not something I can do in a simple &quot;scoring.py&quot; file.<\/p>\n<p>I have an entire repository with a function that when called takes care of everything, my issue is how can I deploy this model as an online service which simply gets an image and then runs the code in the repository?<\/p>\n<p>I uploaded this repository to <a href=\"https:\/\/dev.azure.com\/\">https:\/\/dev.azure.com\/<\/a> and also git cloned it into my ML workspace notebook.<\/p>\n<p>the repository also already has the ML models to simplify things, I tried searching online but there is practically no use cases like this (which is quire surprising) so maybe I'm trying to use the wrong service.<\/p>\n<p>Would really appreciate some help, thx!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1675521234990,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1167665\/how-to-deploy-a-ml-model-to-azure-which-requires-a",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"deploi model azur requir entir repositori run follow guid http medium com aigui model deploy azur web app bbbbfbf work actual us case deploi model differ model mean code complic simpl score file entir repositori function call take care issu deploi model onlin servic simpli get imag run code repositori upload repositori http dev azur com git clone workspac notebook repositori model simplifi thing tri search onlin practic us case like quir surpris mayb try us wrong servic appreci help thx",
        "Question_preprocessed_content":"deploi model azur requir entir repositori run follow guid work actual us case deploi model differ model mean code complic simpl file entir repositori function call take care issu deploi model onlin servic simpli get imag run code repositori upload repositori git clone workspac notebook repositori model simplifi thing tri search onlin practic us case like mayb try us wrong servic appreci help thx",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a machine learning model to Azure which requires an entire repository to run. The model is made up of 4 different ML models, making the code complicated and not suitable for a simple \"scoring.py\" file. The user has uploaded the repository to dev.azure.com and git cloned it into their ML workspace notebook. However, they are struggling to find a way to deploy the model as an online service that can receive an image and run the code in the repository. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user face challeng deploi machin learn model azur requir entir repositori run model differ model make code complic suitabl simpl score file user upload repositori dev azur com git clone workspac notebook struggl wai deploi model onlin servic receiv imag run code repositori user seek help resolv issu"
    },
    {
        "Question_title":"Control tracked version of external dependency",
        "Question_body":"<p>I am trying to set up a DVC repository for machine learning data with different tagged versions of the dataset. I do this with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd \/raid\/ml_data  # folder on a data drive\n$ git init\n$ dvc init\n$ [add data]\n$ [commit to dvc, git]\n$ git tag -a 1.0.0\n$ [add or change data]\n$ [commit to dvc, git]\n$ git tag -a 1.1.0\n<\/code><\/pre>\n<p>I have multiple projects that each need to reference some version of this dataset. The problem is I can't figure out how to set up those projects to reference a specific version. I'm able to track the <code>HEAD<\/code> of the repo with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd ~\/my_proj  # different drive than the remote\n$ mkdir data\n$ git init\n$ dvc init\n$ dvc remote add -d local \/raid\/ml_data  # add the remote on my data drive\n$ dvc cache dir \/raid\/ml_data\/.dvc\/cache  # tell DVC to use the remote cache\n$ dvc checkout\n$ dvc run --external -d \/raid\/ml_data -o data\/ cp -r \/raid\/ml_data data\n<\/code><\/pre>\n<p>This gets me the latest version of the dataset, symlinked into my <code>data<\/code> folder, but what if I want some projects to use the <code>1.0.0<\/code> version and some to use the <code>1.1.0<\/code> version, or another version? Or for that matter, if I update the dataset to <code>2.0.0<\/code> but don't want my existing projects to necessarily track <code>HEAD<\/code> and instead keep the version with which they were set up?<\/p>\n<p>It's important to me to not create a ton of local copies of my dataset as the <code>\/home<\/code> drive is much smaller than the <code>\/raid<\/code> drive and some of these datasets are huge.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1604349754297,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1604361023336,
        "Question_score_count":2.0,
        "Question_view_count":139.0,
        "Poster_created_time":1370629593700,
        "Poster_location":"Colorado Springs, CO",
        "Poster_reputation_count":11685.0,
        "Poster_view_count":1329.0,
        "Answer_body":"<p>I think you are looking for the <a href=\"https:\/\/dvc.org\/doc\/start\/data-access\" rel=\"nofollow noreferrer\">data access<\/a> set of commands.<\/p>\n<p>In your particular case, <code>dvc import<\/code> makes sense:<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data\n<\/code><\/pre>\n<p>if you want to get the most recent version (HEAD). Then you will be able to update it with the <code>dvc update<\/code> command (if 2.0.0 is released, for example).<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data --rev 1.0.0\n<\/code><\/pre>\n<p>if you'd like to &quot;fix&quot; it to the specific version.<\/p>\n<h3>Avoiding copies<\/h3>\n<p>Make sure also, that <code>symlinks<\/code> are set for the second project, as described in the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"nofollow noreferrer\">Large Dataset Optimization<\/a>:<\/p>\n<pre><code>$ dvc config cache.type reflink,hardlink,symlink,copy\n<\/code><\/pre>\n<p>(there are config modifiers <code>--global<\/code>, <code>--local<\/code>, <code>--system<\/code> to set this setting for everyone at once, or just for one project, etc)<\/p>\n<p>Check the details instruction <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization#configuring-dvc-cache-file-link-type\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<hr \/>\n<p>Overall, it's a great setup, and looks like you got pretty much everything right. Please, don't hesitate to follow up and\/or create other questions here- we'll help you with this.<\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1604351561432,
        "Answer_last_edit_time":1604362563343,
        "Answer_score_count":1.0,
        "Answerer_created_time":1250347954880,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64653042",
        "Tool":"DVC",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"control track version extern depend try set repositori machin learn data differ tag version dataset like raid data folder data drive git init init add data commit git git tag add chang data commit git git tag multipl project need refer version dataset problem figur set project refer specif version abl track head repo like proj differ drive remot mkdir data git init init remot add local raid data add remot data drive cach dir raid data cach tell us remot cach checkout run extern raid data data raid data data get latest version dataset symlink data folder want project us version us version version matter updat dataset want exist project necessarili track head instead version set import creat ton local copi dataset home drive smaller raid drive dataset huge",
        "Question_preprocessed_content":"control track version extern depend try set repositori machin learn data differ tag version dataset like multipl project need refer version dataset problem figur set project refer specif version abl track repo like get latest version dataset symlink folder want project us version us version version matter updat dataset want exist project necessarili track instead version set import creat ton local copi dataset drive smaller drive dataset huge",
        "Question_gpt_summary_original":"The user is trying to set up a DVC repository for machine learning data with different tagged versions of the dataset. However, they are unable to figure out how to set up multiple projects to reference a specific version of the dataset. They are currently able to track the latest version of the dataset but want to be able to use different versions for different projects without creating multiple local copies of the dataset.",
        "Question_gpt_summary":"user try set repositori machin learn data differ tag version dataset unabl figur set multipl project refer specif version dataset current abl track latest version dataset want abl us differ version differ project creat multipl local copi dataset"
    },
    {
        "Question_title":"Deploying the sagemaker endpoint created as a service",
        "Question_body":"<p>I have trained a credit-fraud data set on AWS Sagemaker and created an endpoint of the model. Suppose I want to provide it as a service to my friend. He has some credit data and wanted to know whether the transaction is fraud or not. He wishes to use my endpoint. How do I share it?<\/p>\n\n<ol>\n<li>Should I share my ARN for endpoint? I don't think its the right way. without a common account he won't be able to use it.<\/li>\n<li>Or is there another way<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1573479066647,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":226.0,
        "Poster_created_time":1568318861627,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":486.0,
        "Poster_view_count":75.0,
        "Answer_body":"<p>To share your model as an endpoint, you should use lambda and API Gateway to create your API.<\/p>\n\n<ol>\n<li>Create an API gateway that triggers a Lambda with the HTTP POST method;<\/li>\n<li>your lambda should instantiate the SageMaker endpoint, get the requested parameter in the event, call the SageMaker endpoint and return the predicted value. you can also create a DynamoDB to store commonly requested parameters with their answers;<\/li>\n<li>Send the API Gateway Endpoint to your friend.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/qLss4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qLss4.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1573653626700,
        "Answer_last_edit_time":1573654268900,
        "Answer_score_count":6.0,
        "Answerer_created_time":1464391892936,
        "Answerer_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Answerer_reputation_count":2243.0,
        "Answerer_view_count":148.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58802366",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"deploi endpoint creat servic train credit fraud data set creat endpoint model suppos want provid servic friend credit data want know transact fraud wish us endpoint share share arn endpoint think right wai common account won abl us wai",
        "Question_preprocessed_content":"deploi endpoint creat servic train data set creat endpoint model suppos want provid servic friend credit data want know transact fraud wish us endpoint share share arn endpoint think right wai common account won abl us wai",
        "Question_gpt_summary_original":"The user has trained a credit-fraud dataset on AWS Sagemaker and created an endpoint of the model. They want to share it with a friend who has some credit data and wants to know whether the transaction is fraud or not. The user is unsure of the correct way to share the endpoint and is considering sharing their ARN, but believes it may not be the right way. They are seeking advice on alternative methods.",
        "Question_gpt_summary":"user train credit fraud dataset creat endpoint model want share friend credit data want know transact fraud user unsur correct wai share endpoint consid share arn believ right wai seek advic altern method"
    },
    {
        "Question_title":"How do S3 file download and estimator.fit() work in this blog post?",
        "Question_body":"<h1>Difficulty in understanding<\/h1>\n<p>Q2) How to download a file from S3?<\/p>\n<p><strong>From<\/strong>  <a href=\"https:\/\/medium.com\/akeneo-labs\/machine-learning-workflow-with-sagemaker-b83b293337ff\" rel=\"nofollow noreferrer\">The Machine Learning Workflow with SageMaker<\/a><\/p>\n<p>And also why are we using this piece of code?<\/p>\n<p><code>estimator.fit(train_data_location)<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1568926981497,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1592644375060,
        "Question_score_count":1.0,
        "Question_view_count":220.0,
        "Poster_created_time":1556451987416,
        "Poster_location":"India",
        "Poster_reputation_count":1309.0,
        "Poster_view_count":288.0,
        "Answer_body":"<h2>Downloading a file from S3:<\/h2>\n\n<p>This code block in the Q2 section defines the function that downloads a file from S3. The user instantiates an S3 client, and then passes the S3 URL along to the <code>s3.Bucket.download_file()<\/code> method.<\/p>\n\n<pre><code>def download_from_s3(url):\n    \"\"\"ex: url = s3:\/\/sagemakerbucketname\/data\/validation.tfrecords\"\"\"\n    url_parts = url.split(\"\/\")  # =&gt; ['s3:', '', 'sagemakerbucketname', 'data', ...\n    bucket_name = url_parts[2]\n    key = os.path.join(*url_parts[3:])\n    filename = url_parts[-1]\n    if not os.path.exists(filename):\n        try:\n            # Create an S3 client\n            s3 = boto3.resource('s3')\n            print('Downloading {} to {}'.format(url, filename))\n            s3.Bucket(bucket_name).download_file(key, filename)\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] == \"404\":\n                print('The object {} does not exist in bucket {}'.format(\n                    key, bucket_name))\n            else:\n                raise\n<\/code><\/pre>\n\n<h2>Estimator.fit() explanation:<\/h2>\n\n<p>The <code>estimator.fit(train_data_location)<\/code> line is what initiates the training process with SageMaker. When run, SageMaker will provision the necessary infrastructure, fetch the data from the location the user designated (here, <code>train_data_location<\/code> which is a path to Amazon S3) and distribute it amongst the training cluster, carry out the training process, return the resulting model, and tear down the training infrastructure. <\/p>\n\n<p>You can find the result of this training job in the SageMaker console.<\/p>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1568928522076,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1546969667040,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58018893",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"file download estim fit work blog post difficulti understand download file machin learn workflow piec code estim fit train data locat",
        "Question_preprocessed_content":"file download work blog post difficulti understand download file machin learn workflow piec code",
        "Question_gpt_summary_original":"The user is having difficulty understanding how to download a file from S3 and why the code \"estimator.fit(train_data_location)\" is being used in the blog post \"The Machine Learning Workflow with SageMaker\".",
        "Question_gpt_summary":"user have difficulti understand download file code estim fit train data locat blog post machin learn workflow"
    },
    {
        "Question_title":"Convert a datatable string from Azure ML WS to an actual Datatable C# Object?",
        "Question_body":"<p>Basically I'm receiving an output like this from my azure ws output:<\/p>\n\n<pre><code>{\n    'Results': {\n        'WSOutput': {\n            'type': 'table',\n            'value': {\n                'ColumnNames': ['ID', 'Start', 'Ask', 'Not', 'Passed', 'Suggest'],\n                'ColumnTypes': ['Int32', 'Int32', 'Int32', 'Double', 'Int64', 'Int32'],\n                'Values': [['13256025', '25000', '19000', '0.35', '1', '25000']]\n            }\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>The string, as you can see, has the info to create a datatable object. Now, I can't seem to find an easy way to cast it to an actual datatable POCO. I'm able to manually code a parser with Newtonsoft.Json.Linq but there has to be an easier way. <\/p>\n\n<p>Does anybody know how? I can't seem to find anything on the net.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":7,
        "Question_created_time":1519929782813,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1519930038023,
        "Question_score_count":2.0,
        "Question_view_count":83.0,
        "Poster_created_time":1324654920387,
        "Poster_location":"Waterloo, ON, Canada",
        "Poster_reputation_count":5211.0,
        "Poster_view_count":449.0,
        "Answer_body":"<p>Yes, there is a open source online gernator on the net (<a href=\"http:\/\/jsonutils.com\/\" rel=\"nofollow noreferrer\">http:\/\/jsonutils.com\/<\/a>). Copy paste your result will give you that:<\/p>\n\n<pre><code> public class Value\n    {\n        public IList&lt;string&gt; ColumnNames { get; set; }\n        public IList&lt;string&gt; ColumnTypes { get; set; }\n        public IList&lt;IList&lt;string&gt;&gt; Values { get; set; }\n    }\n\n    public class WSOutput\n    {\n        public string type { get; set; }\n        public Value value { get; set; }\n    }\n\n    public class Results\n    {\n        public WSOutput WSOutput { get; set; }\n    }\n\n    public class Example\n    {\n        public Results Results { get; set; }\n    }\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1519929928907,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1327234712912,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":53015.0,
        "Answerer_view_count":3262.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49056593",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"convert datat string actual datat object basic receiv output like azur output result wsoutput type tabl valu columnnam start ask pass suggest columntyp int int int doubl int int valu string info creat datat object easi wai cast actual datat poco abl manual code parser newtonsoft json linq easier wai anybodi know net",
        "Question_preprocessed_content":"convert datat string actual datat object basic receiv output like azur output string info creat datat object easi wai cast actual datat poco abl manual code parser easier wai anybodi know net",
        "Question_gpt_summary_original":"The user is having trouble converting a datatable string received from Azure ML WS into an actual Datatable C# object. The string contains the necessary information to create a datatable object, but the user is unable to find an easy way to cast it. The user has tried manually coding a parser with Newtonsoft.Json.Linq but is looking for an easier solution.",
        "Question_gpt_summary":"user have troubl convert datat string receiv actual datat object string contain necessari inform creat datat object user unabl easi wai cast user tri manual code parser newtonsoft json linq look easier solut"
    },
    {
        "Question_title":"botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not access model data",
        "Question_body":"<p>I want to deploy an MLflow image to an AWS Sagemaker endpoint that contains a machine learning model. I executed the following code, which I found in <a href=\"https:\/\/towardsdatascience.com\/deploying-models-to-production-with-mlflow-and-amazon-sagemaker-d21f67909198\" rel=\"nofollow noreferrer\">this blog post<\/a>.<\/p>\n<pre><code>import mlflow.sagemaker as mfs\n\nrun_id = run_id # the model you want to deploy - this run_id was saved when we trained our model\nregion = &quot;us-east-1&quot; # region of your account\naws_id = &quot;XXXXXXXXXXX&quot; # from the aws-cli output\narn = &quot;arn:aws:iam::XXXXXXXXXXX:role\/your-role&quot;\napp_name = &quot;iris-rf-1&quot;\nmodel_uri = &quot;mlruns\/%s\/%s\/artifacts\/random-forest-model&quot; % (experiment_id,run_id) # edit this path based on your working directory\nimage_url = aws_id + &quot;.dkr.ecr.&quot; + region + &quot;.amazonaws.com\/mlflow-pyfunc:1.2.0&quot; # change to your mlflow version\n\nmfs.deploy(app_name=app_name, \n           model_uri=model_uri, \n           region_name=region, \n           mode=&quot;create&quot;,\n           execution_role_arn=arn,\n           image_url=image_url)\n<\/code><\/pre>\n<p>But I got the following error. I checked all policies and permissions attached to the IAM role. They all comply with what the error message complains about. I don't know what to do next. I'd appreciate your help. Thanks.<\/p>\n<p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not access model data at <a href=\"https:\/\/s3.amazonaws.com\/mlflow-sagemaker-us-east-1-xxx\/mlflow-xgb-demo-model-eqktjeoit5mxhmjn-abpanw\/model.tar.gz\" rel=\"nofollow noreferrer\">https:\/\/s3.amazonaws.com\/mlflow-sagemaker-us-east-1-xxx\/mlflow-xgb-demo-model-eqktjeoit5mxhmjn-abpanw\/model.tar.gz<\/a>. Please ensure that the role &quot;arn:aws:iam::xxx:role\/mlflow-sagemaker-dev&quot; exists and that its trust relationship policy allows the action &quot;sts:AssumeRole&quot; for the service principal &quot;sagemaker.amazonaws.com&quot;. Also ensure that the role has &quot;s3:GetObject&quot; permissions and that the object is located in us-east-1.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1611848849710,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":1789.0,
        "Poster_created_time":1482418107070,
        "Poster_location":"Raleigh, NC, United States",
        "Poster_reputation_count":69.0,
        "Poster_view_count":9.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65940509",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"botocor except clienterror error occur validationexcept call createmodel oper access model data want deploi imag endpoint contain machin learn model execut follow code blog post import mf run run model want deploi run save train model region east region account aw aw cli output arn arn aw iam role role app iri model uri mlrun artifact random forest model experi run edit path base work directori imag url aw dkr ecr region amazonaw com pyfunc chang version mf deploi app app model uri model uri region region mode creat execut role arn arn imag url imag url got follow error check polici permiss attach iam role compli error messag complain know appreci help thank botocor except clienterror error occur validationexcept call createmodel oper access model data http amazonaw com east xgb demo model eqktjeoitmxhmjn abpanw model tar ensur role arn aw iam role dev exist trust relationship polici allow action st assumerol servic princip amazonaw com ensur role getobject permiss object locat east",
        "Question_preprocessed_content":"error occur call createmodel oper access model data want deploi imag endpoint contain machin learn model execut follow code blog post got follow error check polici permiss attach iam role compli error messag complain know appreci help thank error occur call createmodel oper access model data ensur role exist trust relationship polici allow action st assumerol servic princip ensur role getobject permiss object locat",
        "Question_gpt_summary_original":"The user encountered an error while trying to deploy an MLflow image to an AWS Sagemaker endpoint that contains a machine learning model. The error message indicates that the IAM role does not exist or its trust relationship policy does not allow the action \"sts:AssumeRole\" for the service principal \"sagemaker.amazonaws.com\". Additionally, the role needs to have \"s3:GetObject\" permissions and the object should be located in us-east-1. The user has checked all policies and permissions attached to the IAM role but is still unable to resolve the issue.",
        "Question_gpt_summary":"user encount error try deploi imag endpoint contain machin learn model error messag indic iam role exist trust relationship polici allow action st assumerol servic princip amazonaw com addition role need getobject permiss object locat east user check polici permiss attach iam role unabl resolv issu"
    },
    {
        "Question_title":"Semi-transparent smoothing stopped working",
        "Question_body":"<p>Smoothing suddenly stopped making the original graph semi-transparent.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e.png\" data-download-href=\"\/uploads\/short-url\/6HJfxA7ERNAiH7FwWeL8MzvFDIi.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e_2_512x500.png\" alt=\"image\" data-base62-sha1=\"6HJfxA7ERNAiH7FwWeL8MzvFDIi\" width=\"512\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e_2_512x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">517\u00d7504 48.9 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Any way your could rollback to the previous behavior? Some ETA for fixing this would be much appreciated, so our team could plan accordingly.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1639019539043,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":399.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/semi-transparent-smoothing-stopped-working\/1492",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"semi transpar smooth stop work smooth suddenli stop make origin graph semi transpar imag wai rollback previou behavior eta fix appreci team plan accordingli",
        "Question_preprocessed_content":"smooth stop work smooth suddenli stop make origin graph imag wai rollback previou behavior eta fix appreci team plan accordingli",
        "Question_gpt_summary_original":"The user is facing an issue where the semi-transparent smoothing feature has stopped working, resulting in the original graph not being semi-transparent anymore. They are requesting a rollback to the previous behavior and an ETA for fixing the issue.",
        "Question_gpt_summary":"user face issu semi transpar smooth featur stop work result origin graph semi transpar anymor request rollback previou behavior eta fix issu"
    },
    {
        "Question_title":"How to download an artifact from MLFlow using REST?",
        "Question_body":"<p>I see the Python API:\n<code>download_artifacts(run_id: str, path: str, dst_path: Optional[str] = None) \u2192 str<\/code> (<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.download_artifacts\" rel=\"nofollow noreferrer\">here<\/a>), but I can't find the equivalent in REST.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1636370032513,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1636454691216,
        "Question_score_count":3.0,
        "Question_view_count":243.0,
        "Poster_created_time":1244984040076,
        "Poster_location":"New York, NY",
        "Poster_reputation_count":13408.0,
        "Poster_view_count":687.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69882574",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"download artifact rest python api download artifact run str path str dst path option str str equival rest",
        "Question_preprocessed_content":"download artifact rest python api equival rest",
        "Question_gpt_summary_original":"The user is having difficulty finding the REST equivalent of the Python API \"download_artifacts\" in MLFlow to download an artifact.",
        "Question_gpt_summary":"user have difficulti find rest equival python api download artifact download artifact"
    },
    {
        "Question_title":"What's the training corpus of models behind GCP Natural Language APIs?",
        "Question_body":"Hi, where can I find some information about which datasets are used for training models that power the natural language APIs for sentiment analysis, entity extraction, etc.? Thanks!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1668695760000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":62.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-s-the-training-corpus-of-models-behind-GCP-Natural-Language\/td-p\/490614\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"train corpu model gcp natur languag api inform dataset train model power natur languag api sentiment analysi entiti extract thank",
        "Question_preprocessed_content":"train corpu model gcp natur languag api inform dataset train model power natur languag api sentiment analysi entiti extract thank",
        "Question_gpt_summary_original":"The user is seeking information about the datasets used to train the models that power the natural language APIs for sentiment analysis and entity extraction in GCP.",
        "Question_gpt_summary":"user seek inform dataset train model power natur languag api sentiment analysi entiti extract gcp"
    },
    {
        "Question_title":"Jupyter notebook in sagemaker kernel keeps dying when I try to load data from s3",
        "Question_body":"<p>In sagemaker jupyter notebook I run the following code to load data from an s3 bucket.<\/p>\n\n<pre><code>import boto3\nimport pandas as pd\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\nbucket='bucketname'\ndata_key = 'filename'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n\ndata=pd.read_csv(data_location)\n<\/code><\/pre>\n\n<p>Then the kernel dies and I get a pop up saying \"The kernel appears to have died. It will restart automatically.\"<\/p>\n\n<p>Is there an easy way to load the data from s3 in sagemaker?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":7,
        "Question_created_time":1572532514580,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":4.0,
        "Question_view_count":5852.0,
        "Poster_created_time":1568738384723,
        "Poster_location":null,
        "Poster_reputation_count":137.0,
        "Poster_view_count":5.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58645661",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"jupyt notebook kernel keep dy try load data jupyt notebook run follow code load data bucket import boto import panda import execut role role execut role bucket bucketnam data kei filenam data locat format bucket data kei data read csv data locat kernel di pop sai kernel appear di restart automat easi wai load data",
        "Question_preprocessed_content":"jupyt notebook kernel keep dy try load data jupyt notebook run follow code load data bucket kernel di pop sai kernel appear di restart easi wai load data",
        "Question_gpt_summary_original":"The user is facing an issue with the Jupyter notebook in Sagemaker kernel dying when attempting to load data from an S3 bucket. The user is seeking a solution to load data from S3 in Sagemaker without encountering this issue.",
        "Question_gpt_summary":"user face issu jupyt notebook kernel dy attempt load data bucket user seek solut load data encount issu"
    },
    {
        "Question_title":"Model explanation job failing",
        "Question_body":"<p>In Azure Automated ML, the Model explanation job keeps failing. I believe there is a timeout of 60 minutes that it is hitting.     <\/p>\n<p>Warning: AzureMLCompute job failed. MaxWallClockTimeExpired: Job was terminated since it's execution time exceeded the max_run_duration_seconds.    <\/p>\n<p>Is there a way to override the max run duration? Or any other kind of workaround?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1670280861333,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1117134\/model-explanation-job-failing",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"model explan job fail azur autom model explan job keep fail believ timeout minut hit warn comput job fail maxwallclocktimeexpir job termin execut time exceed max run durat second wai overrid max run durat kind workaround",
        "Question_preprocessed_content":"model explan job fail azur autom model explan job keep fail believ timeout minut hit warn comput job fail maxwallclocktimeexpir job termin execut time exceed wai overrid max run durat kind workaround",
        "Question_gpt_summary_original":"The user is facing issues with the Model explanation job in Azure Automated ML, which keeps failing due to hitting a timeout of 60 minutes. The job is terminated due to exceeding the maximum run duration, and the user is seeking a way to override this or find a workaround.",
        "Question_gpt_summary":"user face issu model explan job azur autom keep fail hit timeout minut job termin exceed maximum run durat user seek wai overrid workaround"
    },
    {
        "Question_title":"Azure ML dataset: label column of dataset is None",
        "Question_body":"<p>Hi,  <\/p>\n<p>For a project we need to load some data and its labels from an azure ML dataset. However when using the following sample code in python:  <\/p>\n<pre><code>    from azureml.core import Workspace, Dataset\n    subscription_id = 'sub-id...'\n    resource_group = 'res-grp...'\n    workspace_name = 'ws-name...'\n    ds_name = 'name-of-ds'\n\n    workspace = Workspace(subscription_id, resource_group, workspace_name)\n    dataset = Dataset.get_by_name(workspace, name=ds_name).to_pandas_dataframe()\n<\/code><\/pre>\n<p>the labels are None. The dataset was exported from a &quot;Image Classification Multi-label&quot; project. For a dataset exported from a &quot;Image Classification Multi-class&quot; the code works fine. Currently I work on a local PC in Pycharm with Python 3.5 (due to easier debugging). Can you maybe help me with that?  <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1605709509727,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/167244\/azure-ml-dataset-label-column-of-dataset-is-none",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"dataset label column dataset project need load data label dataset follow sampl code python core import workspac dataset subscript sub resourc group re grp workspac workspac workspac subscript resourc group workspac dataset dataset workspac panda datafram label dataset export imag classif multi label project dataset export imag classif multi class code work fine current work local pycharm python easier debug mayb help",
        "Question_preprocessed_content":"dataset label column dataset project need load data label dataset follow sampl code python label dataset export imag classif project dataset export imag classif code work fine current work local pycharm python mayb help",
        "Question_gpt_summary_original":"The user is facing an issue with loading data and its labels from an Azure ML dataset using Python code. The label column of the dataset is showing as None. The dataset was exported from an \"Image Classification Multi-label\" project, and the code works fine for datasets exported from an \"Image Classification Multi-class\" project. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user face issu load data label dataset python code label column dataset show dataset export imag classif multi label project code work fine dataset export imag classif multi class project user seek help resolv issu"
    },
    {
        "Question_title":"Azure Machine Learning Python Module failing to Execute Calls to Cognitive Services",
        "Question_body":"<p>I have created a Pipeline in Azure ML which makes calls to Azure Cognitive Services Text Analytics using its Python API. When I run the code I have written locally, it executes without error, but when run it in the pipeline it fails to perform the Sentiment Analysis and Key Phrase Extraction calls with a strange error message:<\/p>\n<blockquote>\n<p>Got exception when invoking script at line 243 in function azureml_main: 'ServiceRequestError: &lt;urllib3.connection.HTTPSConnection object at 0x7ff4dc727588&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'.<\/p>\n<\/blockquote>\n<p>Upon further testing, it appears that it is able to open the Text Analytics Client correctly (Or at least without throwing an error), but when it gets to the line that actually makes the call out using the Python API it throws the above error.<\/p>\n<p>I wondered if it was an Open SSL issue, but when I checked the version it had access to TLS 1.2: <code>OpenSSL 1.1.1k  25 Mar 2021<\/code><\/p>\n<p>It does not appear to be a temporary issue; I started seeing the issue last week, and I have seen it over a number of environments and with different input datasets.<\/p>\n<p>Has anyone seen a similar issue before? Any ideas on how it could be resolved?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1630894745580,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1630971784780,
        "Question_score_count":2.0,
        "Question_view_count":299.0,
        "Poster_created_time":1611181716003,
        "Poster_location":null,
        "Poster_reputation_count":119.0,
        "Poster_view_count":5.0,
        "Answer_body":"<p>After speaking with Microsoft Support, it turns out this error was a platform error introduced in a recent update of Azure ML. Their product team are currently investigating a solution.<\/p>\n<p>As a temporary fix, if you see this issue, you can try switching between using your personal endpoint and the generic regional endpoint; In this case, the error was only introduced for using personal endpoints. The endpoints in question have the following formats:<\/p>\n<ul>\n<li>Personal: <code>https:\/\/&lt;COGNITIVE-SERVICES-INSTANCE&gt;.cognitiveservices.azure.com\/<\/code><\/li>\n<li>Regional: <code>https:\/\/&lt;REGION&gt;.api.cognitive.microsoft.com\/<\/code><\/li>\n<\/ul>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1631580778540,
        "Answer_last_edit_time":null,
        "Answer_score_count":3.0,
        "Answerer_created_time":1611181716003,
        "Answerer_location":null,
        "Answerer_reputation_count":119.0,
        "Answerer_view_count":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69068520",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"python modul fail execut call cognit servic creat pipelin make call azur cognit servic text analyt python api run code written local execut error run pipelin fail perform sentiment analysi kei phrase extract call strang error messag got except invok script line function main servicerequesterror fail establish new connect errno temporari failur resolut test appear abl open text analyt client correctli throw error get line actual make python api throw error wonder open ssl issu check version access tl openssl mar appear temporari issu start see issu week seen number environ differ input dataset seen similar issu idea resolv",
        "Question_preprocessed_content":"python modul fail execut call cognit servic creat pipelin make call azur cognit servic text analyt python api run code written local execut error run pipelin fail perform sentiment analysi kei phrase extract call strang error messag got except invok script line function servicerequesterror fail establish new connect temporari failur resolut test appear abl open text analyt client correctli get line actual make python api throw error wonder open ssl issu check version access tl appear temporari issu start see issu week seen number environ differ input dataset seen similar issu idea resolv",
        "Question_gpt_summary_original":"The user is encountering an error when executing calls to Azure Cognitive Services Text Analytics using its Python API in an Azure ML pipeline. The error message indicates a failure to establish a new connection, and the issue persists across different environments and input datasets. The user is seeking advice on how to resolve the issue.",
        "Question_gpt_summary":"user encount error execut call azur cognit servic text analyt python api pipelin error messag indic failur establish new connect issu persist differ environ input dataset user seek advic resolv issu"
    },
    {
        "Question_title":"What is the role of feature type in AzureML?",
        "Question_body":"<p>I want to know what is the difference between <code>feature numeric<\/code> and <code>numeric<\/code> columns in Azure Machine Learning Studio.<\/p>\n\n<p>The <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/edit-metadata\" rel=\"nofollow noreferrer\">documentation site<\/a> states: <\/p>\n\n<blockquote>\n  <p>Because all columns are initially treated as features, for modules\n  that perform mathematical operations, you might need to use this\n  option to prevent numeric columns from being treated as variables.<\/p>\n<\/blockquote>\n\n<p>But nothing more. Not what a feature is, in which modules you need features. Nothing. <\/p>\n\n<p>I specifically would like to understand if the <code>clear feature<\/code> dropdown option in the <code>fields<\/code> in the <code>edit metadata<\/code>-module has any effect. Can somebody give me a szenario where this <code>clear feature<\/code>-operation changes the ML outcome? Thank you<\/p>\n\n<p>According to the documentation in ought to have an effect:<\/p>\n\n<blockquote>\n  <p>Use the Fields option if you want to change the way that Azure Machine\n  Learning uses the data in a model.<\/p>\n<\/blockquote>\n\n<p>But what can this effect be? Any example might help<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1538054010173,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1538054394440,
        "Question_score_count":1.0,
        "Question_view_count":70.0,
        "Poster_created_time":1368739128832,
        "Poster_location":"Riga, Latvia",
        "Poster_reputation_count":1763.0,
        "Poster_view_count":380.0,
        "Answer_body":"<p>As you suspect, setting a column as <code>feature<\/code> does have an effect, and it's actually quite important - when training a model, the algorithms will only take into account columns with the <code>feature<\/code> flag, effectively ignoring the others. <\/p>\n\n<p>For example, if you have a dataset with columns <code>Feature1<\/code>, <code>Feature2<\/code>, and <code>Label<\/code> and you want to try out just <code>Feature1<\/code>, you would apply <code>clear feature<\/code> to the <code>Feature2<\/code> column (while making sure that <code>Feature1<\/code> has the <code>feature<\/code> label set, of course).<\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1538116098500,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1250158552416,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52537861",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"role featur type want know differ featur numer numer column studio document site state column initi treat featur modul perform mathemat oper need us option prevent numer column treat variabl featur modul need featur specif like understand clear featur dropdown option field edit metadata modul effect somebodi szenario clear featur oper chang outcom thank accord document ought effect us field option want chang wai azur machin learn us data model effect exampl help",
        "Question_preprocessed_content":"role featur type want know differ column studio document site state column initi treat featur modul perform mathemat oper need us option prevent numer column treat variabl featur modul need featur specif like understand dropdown option modul effect somebodi szenario oper chang outcom thank accord document ought effect us field option want chang wai azur machin learn us data model effect exampl help",
        "Question_gpt_summary_original":"The user is seeking clarification on the difference between \"feature numeric\" and \"numeric\" columns in Azure Machine Learning Studio. They are also unsure about the purpose of the \"clear feature\" dropdown option in the \"edit metadata\" module and how it affects the ML outcome. The documentation provides limited information on these topics, and the user is looking for specific scenarios or examples.",
        "Question_gpt_summary":"user seek clarif differ featur numer numer column studio unsur purpos clear featur dropdown option edit metadata modul affect outcom document provid limit inform topic user look specif scenario exampl"
    },
    {
        "Question_title":"Unable to connect Azure DevOps and Azure ML",
        "Question_body":"<p>I have created an automated Service Principal from the service requests on Azure Devops with sufficient permissions. Now, when I am trying to create an artifact which is an ML model (registered) it is not auto populating the registered models and resulting in an error.<\/p>\n<p>I am using a free trial Azure account and attempting to implement CI CD for ML. I turned my firewall off and attempted as well but still the issue persists.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/imvGo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/imvGo.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1663480318187,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":40.0,
        "Poster_created_time":1501747110080,
        "Poster_location":null,
        "Poster_reputation_count":51.0,
        "Poster_view_count":47.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73760407",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unabl connect azur devop creat autom servic princip servic request azur devop suffici permiss try creat artifact model regist auto popul regist model result error free trial azur account attempt implement turn firewal attempt issu persist",
        "Question_preprocessed_content":"unabl connect azur devop creat autom servic princip servic request azur devop suffici permiss try creat artifact model auto popul regist model result error free trial azur account attempt implement turn firewal attempt issu persist",
        "Question_gpt_summary_original":"The user is facing challenges in connecting Azure DevOps and Azure ML. They have created an automated Service Principal with sufficient permissions, but when attempting to create an ML model artifact, it is not auto-populating the registered models and resulting in an error. The user has tried turning off their firewall and using a free trial Azure account, but the issue persists.",
        "Question_gpt_summary":"user face challeng connect azur devop creat autom servic princip suffici permiss attempt creat model artifact auto popul regist model result error user tri turn firewal free trial azur account issu persist"
    },
    {
        "Question_title":"Cache image in the wandb board?",
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m running a ML project where I need to log images every 10 epochs or so. Since I run for a large number of epochs, I often end up with a lot of images.<\/p>\n<p>I would like to get a sense of how the image quality improves overtime, however it is difficult to do so when the images have to load every time. Is there a way to cache the images in the board somehow?<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_created_time":1670429902330,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":341.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/cache-image-in-the-wandb-board\/3510",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"cach imag board run project need log imag epoch run larg number epoch end lot imag like sens imag qualiti improv overtim difficult imag load time wai cach imag board",
        "Question_preprocessed_content":"cach imag board run project need log imag epoch run larg number epoch end lot imag like sens imag qualiti improv overtim difficult imag load time wai cach imag board",
        "Question_gpt_summary_original":"The user is running a machine learning project and needs to log images every 10 epochs. Due to a large number of epochs, the user ends up with a lot of images. The user wants to know if there is a way to cache the images in the board to get a sense of how the image quality improves over time.",
        "Question_gpt_summary":"user run machin learn project need log imag epoch larg number epoch user end lot imag user want know wai cach imag board sens imag qualiti improv time"
    },
    {
        "Question_title":"How long does it take for AWS tech support team to respond to a \"system impaired\" issue?",
        "Question_body":"Hi all,\n\nI have raised a ticket for multiple issues we've been having with SageMaker lately, the ticket was created more than 36 hours  ago, and I have not had any response, in fact the ticket hasn't even been assigned yet.\n\nThe case ID is 10300240931.\n\nI thought AWS guarantee a response under 12 hours for \"system impaired\" issues, does anyone know what I can do to accelerate this?\n\nthank you!\nRuoy",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1656580942341,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1667931210104,
        "Question_score_count":0.0,
        "Question_view_count":125.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"Hi Ruoy!\nMy advice here is to scale this issue via your account team, they will have the mechanisms to scale this concern.\nIf you are on basic or developer support, you could look into upgrading to business support for a day and open a live chat with support!\nHope this helps",
        "Answer_comment_count":0.0,
        "Question_closed_time":1656585271688,
        "Answer_last_edit_time":1656585271688,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFgnjt9J3T0iXhE0axG10vQ\/how-long-does-it-take-for-aws-tech-support-team-to-respond-to-a-system-impaired-issue",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_original_content":"long aw tech support team respond impair issu rais ticket multipl issu have late ticket creat hour ago respons fact ticket hasn assign case thought aw guarante respons hour impair issu know acceler thank ruoi",
        "Question_preprocessed_content":"long aw tech support team respond impair issu rais ticket multipl issu have late ticket creat hour ago respons fact ticket hasn assign case thought aw guarante respons hour impair issu know acceler thank ruoi",
        "Question_gpt_summary_original":"The user has raised a ticket for multiple issues with SageMaker on AWS, including a \"system impaired\" issue. The ticket was created more than 36 hours ago, but the user has not received any response yet, and the ticket has not been assigned. The user is concerned as AWS guarantees a response under 12 hours for \"system impaired\" issues and is seeking advice on how to accelerate the process.",
        "Question_gpt_summary":"user rais ticket multipl issu aw includ impair issu ticket creat hour ago user receiv respons ticket assign user concern aw guarante respons hour impair issu seek advic acceler process"
    },
    {
        "Question_title":"what is the model(transformer) size limitation in sagemaker serverless endpoint deployment?",
        "Question_body":"is there a limitation on the size of the model that we can create a model and then eventually serverless endpoint? any documentation? I did some research and ran into something similar here. https:\/\/discuss.huggingface.co\/t\/sagemaker-serverless-inference-for-layoutlmv2-model\/14186\/3\ni\nas a solution , it is advised to set MMS_DEFAULT_WORKERS_PER_MODEL=1. I'm not sure what exactly does this do? is there any aws documentation around this?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1645726803899,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668481535712,
        "Question_score_count":0.0,
        "Question_view_count":366.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsU6_idiTQIWu2Poovfpfdw\/what-is-the-model-transformer-size-limitation-in-sagemaker-serverless-endpoint-deployment",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"model transform size limit serverless endpoint deploy limit size model creat model eventu serverless endpoint document research ran similar http discuss huggingfac serverless infer layoutlmv model solut advis set mm default worker model sure exactli aw document",
        "Question_preprocessed_content":"model size limit serverless endpoint deploy limit size model creat model eventu serverless endpoint document research ran similar solut advis set sure exactli aw document",
        "Question_gpt_summary_original":"The user is inquiring about the size limitation of a model that can be deployed on a serverless endpoint in Sagemaker. They are seeking documentation on this topic and have come across a similar issue on a discussion forum. As a solution, they were advised to set MMS_DEFAULT_WORKERS_PER_MODEL=1, but they are unsure of its purpose and are looking for AWS documentation on the matter.",
        "Question_gpt_summary":"user inquir size limit model deploi serverless endpoint seek document topic come similar issu discuss forum solut advis set mm default worker model unsur purpos look aw document matter"
    },
    {
        "Question_title":"Is it possible to use a Windows Account for configuring Machine Learning Server 9.4.7?",
        "Question_body":"<p>After configuring Machine Learning Server with a password, one connects to the server through the user name &quot;admin&quot; and the designated password. Is it possible configure Machine Learning Server to use a Windows Account instead of the default &quot;admin&quot;?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1610029811687,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/222486\/is-it-possible-to-use-a-windows-account-for-config",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"possibl us window account configur machin learn server configur machin learn server password connect server user admin design password possibl configur machin learn server us window account instead default admin",
        "Question_preprocessed_content":"possibl us window account configur machin learn server configur machin learn server password connect server user admin design password possibl configur machin learn server us window account instead default admin",
        "Question_gpt_summary_original":"The user wants to know if it is possible to use a Windows Account instead of the default \"admin\" username when configuring Machine Learning Server 9.4.7.",
        "Question_gpt_summary":"user want know possibl us window account instead default admin usernam configur machin learn server"
    },
    {
        "Question_title":"Statistical significant stage best practice",
        "Question_body":"<p>Hello,<\/p>\n<p>In machine learning, it is important to repeat multiple time the same training (often with a different seed) and then compute the average and standard deviation for a metric to evaluate the stability of the model and check the statistical significance of the results.<\/p>\n<p>Let\u2019s say we have a stage <code>trainmodel<\/code> which has a parameter <code>seed<\/code> and we want to run the training 10 times with 10 different seeds. Once all trainings have been completed, we have a script that takes as input the results (metric files) from the 10 runs and produces the average and standard deviation for each metric.<\/p>\n<p>The thing is that I don\u2019t want to store the 10 trained models (since it is very heavy) but only keep the best trained model.<br>\nHowever, I want to keep the metrics of all 10 runs in order to be able to add a stage that compute the average and std of all metrics.<br>\nI also want to launch all 10 trainings in parallel.<\/p>\n<p>What would be the best way to do this with dvc ?<\/p>",
        "Question_answer_count":9,
        "Question_comment_count":0,
        "Question_created_time":1623146955270,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":406.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/statistical-significant-stage-best-practice\/784",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"statist signific stage best practic hello machin learn import repeat multipl time train differ seed comput averag standard deviat metric evalu stabil model check statist signific result let stage trainmodel paramet seed want run train time differ seed train complet script take input result metric file run produc averag standard deviat metric thing dont want store train model heavi best train model want metric run order abl add stage comput averag std metric want launch train parallel best wai",
        "Question_preprocessed_content":"statist signific stage best practic hello machin learn import repeat multipl time train comput averag standard deviat metric evalu stabil model check statist signific result let stage paramet want run train time differ seed train complet script take input result run produc averag standard deviat metric thing dont want store train model best train model want metric run order abl add stage comput averag std metric want launch train parallel best wai",
        "Question_gpt_summary_original":"The user wants to run a stage in machine learning that trains a model 10 times with different seeds, but only wants to keep the best trained model and the metrics of all 10 runs. They also want to launch all 10 trainings in parallel and are seeking advice on the best way to do this with dvc.",
        "Question_gpt_summary":"user want run stage machin learn train model time differ seed want best train model metric run want launch train parallel seek advic best wai"
    },
    {
        "Question_title":"OAuth Rejection Notice",
        "Question_body":"After getting back an error message on running my javascript program in node.js with my VM instance, I was informed by a member of the Google Cloud Community:\u00a0\"If you need to set up your instance with Identity-Aware Proxy (IAP), you may refer to this document Setting up IAP for Compute Engine. But if you already set up your instance, you just need to enable IAP Enabling IAP for Compute Engine.\" Given that my VM instance had already been set up, I jumped (at the suggestion of the person advising me) to the part of the document dealing with \"Enabling IAP for Compute Engine\" without carrying out the other steps for persons who did not already have an up and running VM instance (i.e., load balancing, etc.) While following the steps listed in the documentation, I discovered I must activate the \"Configure consent screen\" button and create an OAuth consent screen. None of the things I saw after this point seemed to have anything to with my sort of project, that involves no one but myself to give consent to.\u00a0 But I decided this time not to \"think too hard\" and to simply follow the instructions. After several long, puzzling minutes trying to do what I thought I was supposed to do, I submitted my OAuth request. And after 48 hours or so of waiting for an answer from the Trust & Safety Security & Privacy Team, I just received a Request Denied from them, with no explanation of what was wrong with my submission (though I'm sure several things were), along with a warning that I might need to submit a video. But of what? Perhaps it's worth mentioning at this point that what I wish to do is \"simply\" use Google Cloud Translation (Advanced) with a glossary in .tmx format to translate a .docx file from one Google-supported human language to another Google-supported human language, outputting the result also in .docx format and storing both in different subdirectories of my bucket. (One directory for the .docx input file and another for the .docx output file, the result of the translation.) My main question is this: is it even necessary to create an OAuth consent screen in such a scenario,\u00a0 with only person (i.e. me) accessing my VM via the GC console? And if a consent screen turns out to be a necessity, can someone please suggest what I might do to get things right this time and not receive a second Request Denied from the members of the Trust & Safety Security & Privacy Team?",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1681215000000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":61.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/OAuth-Rejection-Notice\/td-p\/542724\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"oauth reject notic get error messag run javascript program node instanc inform member googl cloud commun need set instanc ident awar proxi iap refer document set iap comput engin set instanc need enabl iap enabl iap comput engin given instanc set jump suggest person advis document deal enabl iap comput engin carri step person run instanc load balanc follow step list document discov activ configur consent screen button creat oauth consent screen thing saw point sort project involv consent decid time think hard simpli follow instruct long puzzl minut try thought suppos submit oauth request hour wait answer trust safeti secur privaci team receiv request deni explan wrong submiss sure thing warn need submit video worth mention point wish simpli us googl cloud translat advanc glossari tmx format translat docx file googl support human languag googl support human languag output result docx format store differ subdirectori bucket directori docx input file docx output file result translat main question necessari creat oauth consent screen scenario person access consol consent screen turn necess suggest thing right time receiv second request deni member trust safeti secur privaci team",
        "Question_preprocessed_content":"oauth reject notic get error messag run javascript program instanc inform member googl cloud commun need set instanc proxi refer document set iap comput engin set instanc need enabl iap enabl iap comput given instanc set jump document deal enabl iap comput engin carri step person run instanc follow step list document discov activ configur consent screen button creat oauth consent screen thing saw point sort project involv consent decid time think hard simpli follow instruct long puzzl minut try thought suppos submit oauth request hour wait answer trust safeti secur privaci team receiv request deni explan wrong submiss warn need submit video worth mention point wish simpli us googl cloud translat glossari tmx format translat docx file human languag human languag output result docx format store differ subdirectori bucket main question necessari creat oauth consent screen scenario person access consol consent screen turn necess suggest thing right time receiv second request deni member trust safeti secur privaci team",
        "Question_gpt_summary_original":"The user encountered an issue with their JavaScript program in node.js on their VM instance. They were advised to enable Identity-Aware Proxy (IAP) and IAP for Compute Engine, but while following the steps, they were required to create an OAuth consent screen, which they found confusing and unnecessary for their project. After submitting their OAuth request, they received a Request Denied from the Trust & Safety Security & Privacy Team without any explanation. The user is unsure if creating an OAuth consent screen is necessary for their project and seeks guidance to avoid another rejection.",
        "Question_gpt_summary":"user encount issu javascript program node instanc advis enabl ident awar proxi iap iap comput engin follow step requir creat oauth consent screen confus unnecessari project submit oauth request receiv request deni trust safeti secur privaci team explan user unsur creat oauth consent screen necessari project seek guidanc avoid reject"
    },
    {
        "Question_title":"Azure POST request redirect using Azure services",
        "Question_body":"<p>Hello I am trying to deploy my Azure Machine Learning pipeline with a REST endpoint. My problem is that I was able to generate an endpoint but has some sensitive information in it (ex: subscription id, resource group, etc). How can I generate a URL that forwards the request body to my Azure ML REST endpoint?<\/p>\n<p>also, here is an approach I've done:<\/p>\n<ul>\n<li>Used Application Gateway Redirect (this approach didn't forward the request body. It instead turned my POST request into a GET request when it redirected to the correct URL.)<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1651686889963,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":58.0,
        "Poster_created_time":1588009934688,
        "Poster_location":"Los Angeles, CA, USA",
        "Poster_reputation_count":1.0,
        "Poster_view_count":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72117454",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"azur post request redirect azur servic hello try deploi pipelin rest endpoint problem abl gener endpoint sensit inform subscript resourc group gener url forward request bodi rest endpoint approach applic gatewai redirect approach forward request bodi instead turn post request request redirect correct url",
        "Question_preprocessed_content":"azur post request redirect azur servic hello try deploi pipelin rest endpoint problem abl gener endpoint sensit inform gener url forward request bodi rest endpoint approach applic gatewai redirect",
        "Question_gpt_summary_original":"The user is trying to deploy an Azure Machine Learning pipeline with a REST endpoint but is facing challenges in generating a URL that forwards the request body to the Azure ML REST endpoint. The user has tried using Application Gateway Redirect but it did not forward the request body and instead turned the POST request into a GET request.",
        "Question_gpt_summary":"user try deploi pipelin rest endpoint face challeng gener url forward request bodi rest endpoint user tri applic gatewai redirect forward request bodi instead turn post request request"
    },
    {
        "Question_title":"Handling a variable defined for a project",
        "Question_body":"<p>I am new to wandb so this might be a basic question,<\/p>\n<p>I want to assign, modify and keep track of a variable assigned to a project. The use case would be let\u2019s say I have a variable \u201ccount\u201d assigned to a project staging with several workers collaborating on this project. Then lets say user A performs some runs offline and now wants to sync them to the project, then A would want to first retrieve the value of the \u201ccount\u201d variable, assign it or tag it to all his runs and then sync them and finally increment \u201ccount\u201d for the project. Now if user B wants to start his experiments he  should be able to retrieve the current \u201ccount\u201d value assign it to his set runs before syncing  them, I would really appreciate any help on what would be the best way to achieve this.<br>\nThanks<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1679498320900,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":54.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/handling-a-variable-defined-for-a-project\/4099",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"handl variabl defin project new basic question want assign modifi track variabl assign project us case let variabl count assign project stage worker collabor project let user perform run offlin want sync project want retriev valu count variabl assign tag run sync final increment count project user want start experi abl retriev current count valu assign set run sync appreci help best wai achiev thank",
        "Question_preprocessed_content":"handl variabl defin project new basic question want assign modifi track variabl assign project us case let variabl count assign project stage worker collabor project let user perform run offlin want sync project want retriev valu count variabl assign tag run sync final increment count project user want start experi abl retriev current count valu assign set run sync appreci help best wai achiev thank",
        "Question_gpt_summary_original":"The user is new to wandb and wants to assign, modify, and keep track of a variable assigned to a project. The variable is called \"count\" and is used to track the progress of a project with multiple workers collaborating. The user wants to retrieve the current value of \"count,\" assign it to their runs, sync them, and increment \"count\" for the project. They are seeking advice on the best way to achieve this.",
        "Question_gpt_summary":"user new want assign modifi track variabl assign project variabl call count track progress project multipl worker collabor user want retriev current valu count assign run sync increment count project seek advic best wai achiev"
    },
    {
        "Question_title":"How to set learner type for Python Model in Azure Machine Learning Designer?",
        "Question_body":"<p>I am testing Azure Machine Learning Designer by having a custom Python Model (a simple kNN classification). I would like to tune the value of 'k' and get the best performing model but \"Tune Model Hyperparameters\" module gives following error when giving output from my \"Create Python Model\" as input.<\/p>\n\n<pre><code>ModuleExceptionMessage:LearnerTypesNotCompatible: Got incompatible learner type: \"None\". Expected learner types are: \"(&lt;TaskType.BinaryClassification: 1&gt;, &lt;TaskType.MultiClassification: 2&gt;, &lt;TaskType.Regression: 3&gt;)\".\n<\/code><\/pre>\n\n<p>How I can set the learner type of my own Python model? Is it even possible? Should I just code the parameter tuning myself with \"Execute Python Script\"-module?<\/p>\n\n<p>My \"Create Python model\"-module script:<\/p>\n\n<pre><code>import pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclass AzureMLModel:\n    def __init__(self, k = 3):\n        self.model = KNeighborsClassifier(n_neighbors = k)\n        self.feature_column_names = list()\n\n    def train(self, df_train, df_label):\n        self.feature_column_names = df_train.columns.tolist()\n        self.model.fit(df_train, df_label)\n\n    def predict(self, df):\n        return pd.DataFrame({'Scored Labels': self.model.predict(df[self.feature_column_names])})\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1592306247567,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1592306531683,
        "Question_score_count":1.0,
        "Question_view_count":108.0,
        "Poster_created_time":1592304949616,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62407200",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"set learner type python model design test design have custom python model simpl knn classif like tune valu best perform model tune model hyperparamet modul give follow error give output creat python model input moduleexceptionmessag learnertypesnotcompat got incompat learner type expect learner type set learner type python model possibl code paramet tune execut python script modul creat python model modul script import panda sklearn neighbor import kneighborsclassifi class model def init self self model kneighborsclassifi neighbor self featur column name list def train self train label self featur column name train column tolist self model fit train label def predict self return datafram score label self model predict self featur column name",
        "Question_preprocessed_content":"set learner type python model design test design have custom python model like tune valu best perform model tune model hyperparamet modul give follow error give output creat python model input set learner type python model possibl code paramet tune execut python creat python script",
        "Question_gpt_summary_original":"The user is encountering an error while trying to tune the value of 'k' for a custom Python Model in Azure Machine Learning Designer. The \"Tune Model Hyperparameters\" module is giving an error message stating that the learner type is incompatible. The user is seeking guidance on how to set the learner type of their own Python model and whether it is possible or not.",
        "Question_gpt_summary":"user encount error try tune valu custom python model design tune model hyperparamet modul give error messag state learner type incompat user seek guidanc set learner type python model possibl"
    },
    {
        "Question_title":"Fb-Prophet, Apache Spark in Colab and AWS SageMaker\/ Lambda",
        "Question_body":"<p>I am using <code>Google-Colab<\/code> for creating a model by using FbProphet and i am try to use Apache Spark in the <code>Google-Colab<\/code> itself. Now can i upload this <code>Google-colab<\/code> notebook in <code>aws Sagemaker\/Lambda<\/code> for free <code>(without charge for Apache Spark and only charge for AWS SageMaker)<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1658906440657,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1660220920907,
        "Question_score_count":1.0,
        "Question_view_count":51.0,
        "Poster_created_time":1658906023852,
        "Poster_location":null,
        "Poster_reputation_count":152.0,
        "Poster_view_count":11.0,
        "Answer_body":"<p>In short, You can upload the notebook without any issue into SageMaker. Few things to keep in mind<\/p>\n<ol>\n<li>If you are using the pyspark library in colab and running spark locally,  you should be able to do the same by installing necessary pyspark libs in Sagemaker studio kernels. Here you will only pay for the underlying compute for the notebook instance. If you are experimenting then I would recommend you to use <a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a> to create a free account and try things out.<\/li>\n<li>If you had a separate spark cluster setup then you may need a similar setup in AWS using EMR so that you can connect to the cluster to execute the job.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1658964743500,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1657058369727,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":9.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73133746",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"prophet apach spark colab lambda googl colab creat model fbprophet try us apach spark googl colab upload googl colab notebook lambda free charg apach spark charg",
        "Question_preprocessed_content":"apach spark colab lambda creat model fbprophet try us apach spark upload notebook free",
        "Question_gpt_summary_original":"The user is trying to create a model using FbProphet and Apache Spark in Google-Colab. They are wondering if they can upload the Google-Colab notebook to AWS SageMaker\/Lambda for free, without being charged for Apache Spark and only being charged for AWS SageMaker.",
        "Question_gpt_summary":"user try creat model fbprophet apach spark googl colab wonder upload googl colab notebook lambda free charg apach spark charg"
    },
    {
        "Question_title":"AWS sagemaker RandomCutForest (RCF) vs scikit lean RandomForest (RF)?",
        "Question_body":"<p>Is there a difference between the two, or are they different names for the same algorithm?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1561329917317,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":244.0,
        "Poster_created_time":1421343783700,
        "Poster_location":null,
        "Poster_reputation_count":1387.0,
        "Poster_view_count":153.0,
        "Answer_body":"<p>RandomCutForest (RCF) is an unsupervised method primarily used for anomaly detection, while RandomForest (RF) is a supervised method that can be used for regression or classification. <\/p>\n\n<p>For RCF, see documentation (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/randomcutforest.html\" rel=\"nofollow noreferrer\">here<\/a>) and notebook example (<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">here<\/a>)<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1561385097928,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1549488907923,
        "Answerer_location":"USA",
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56728230",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"randomcutforest rcf scikit lean randomforest differ differ name algorithm",
        "Question_preprocessed_content":"randomcutforest scikit lean randomforest differ differ name algorithm",
        "Question_gpt_summary_original":"The user is seeking clarification on whether AWS Sagemaker RandomCutForest (RCF) and scikit learn RandomForest (RF) are the same algorithm or if there are differences between the two.",
        "Question_gpt_summary":"user seek clarif randomcutforest rcf scikit learn randomforest algorithm differ"
    },
    {
        "Question_title":"Images are not saved\/logged",
        "Question_body":"<p>Hi all,<\/p>\n<p>I am using pytorch_lightning.loggers.WandbLogger to log some intermediate results of my preprocessing but nothing (i.e. plots and images) are logged, except the key name.<\/p>\n<p>Below is what I did:<\/p>\n<pre><code class=\"lang-auto\">wandb_logger = WandbLogger(..)\nrunner = Trainer(logger=wandb_logger,..)\ndata = VAEDataset(.., logger=runner.logger,..) # LightningDataModule\ndata.setup()\n\n# in Dataset (in some preprocessing steps)\nplt.figure(figsize=(10, 30))\nplt.subplot(131), plt.imshow(orig_spec.detach(), cmap='gray')\nplt.title('Input'), plt.xticks([]), plt.yticks([])\nplt.subplot(132), plt.imshow(filtered_spec.squeeze().detach(), cmap='gray')\nplt.title('FilteredSpec'), plt.xticks([]), plt.yticks([])\nplt.subplot(133), plt.imshow(magnitude_spec.detach(), cmap='gray')\nplt.title('MagnitudeSpec'), plt.xticks([]), plt.yticks([])\nself.logger.experiment.log(\n        {\"HPFilter\/Input_Filtered-_Magnitude-Spec\": plt}\n        )\n\n# alternatively, I used wandb directly\nwandb.log(\n            {\"HPFilter\/FilteredSpec\": wandb.Image(torch.flipud(filtered_spec).detach(), caption=\"FilteredSpec.png\")}\n        )\n<\/code><\/pre>\n<p>Both gave such empty panels:.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/d\/dd9cebdf46b068c21edbdbee92b5ec0fb5d8b9ac.png\" data-download-href=\"\/uploads\/short-url\/vCtIR0dL0KQ5J70c9oMVxBaaXq4.png?dl=1\" title=\"wandb\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/dd9cebdf46b068c21edbdbee92b5ec0fb5d8b9ac_2_690x229.png\" alt=\"wandb\" data-base62-sha1=\"vCtIR0dL0KQ5J70c9oMVxBaaXq4\" width=\"690\" height=\"229\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/dd9cebdf46b068c21edbdbee92b5ec0fb5d8b9ac_2_690x229.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/dd9cebdf46b068c21edbdbee92b5ec0fb5d8b9ac_2_1035x343.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/d\/dd9cebdf46b068c21edbdbee92b5ec0fb5d8b9ac.png 2x\" data-dominant-color=\"FDFDFD\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">wandb<\/span><span class=\"informations\">1171\u00d7389 13.8 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>There is no subdir called \u201cHPFilter\u201d under \u201cmedia\u201d but when I checked <code>wandb-summary.json<\/code>:<\/p>\n<pre><code class=\"lang-auto\">..., \"HPFilter\/FilteredSpec\": {\"_type\": \"image-file\", \"sha256\": \"92bd05772ad44d6be9ef9fed80df00ff649e378abbcbb34f73d4082aceacedb0\", \"size\": 86404, \"path\": \"media\/images\/HPFilter\/FilteredSpec_2_92bd05772ad44d6be9ef.png\", \"format\": \"png\", \"width\": 128, \"height\": 256, \"caption\": \"FilteredSpec_33150.png\"}, ...\n<\/code><\/pre>\n<p><strong>Added:<\/strong><br>\nTo eliminate that the problem lies in my actual data, I tried the below to replace my intended logging:<\/p>\n<pre><code class=\"lang-auto\">    arr = torch.rand(256, 128)\n    wandb.log(\n            {\"HPFilter\/FilteredSpec\": wandb.Image(torch.flipud(arr).detach().cpu().numpy(), caption=f\"FilteredSpec.png\")}\n        )\n<\/code><\/pre>\n<p>It still behaved the same.<\/p>\n<p>Many thanks in advance for any hints!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1673643065557,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":153.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/images-are-not-saved-logged\/3697",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"imag save log pytorch lightn logger logger log intermedi result preprocess plot imag log kei logger logger runner trainer logger logger data vaedataset logger runner logger lightningdatamodul data setup dataset preprocess step plt figur figsiz plt subplot plt imshow orig spec detach cmap grai plt titl input plt xtick plt ytick plt subplot plt imshow filter spec squeez detach cmap grai plt titl filteredspec plt xtick plt ytick plt subplot plt imshow magnitud spec detach cmap grai plt titl magnitudespec plt xtick plt ytick self logger experi log hpfilter input filter magnitud spec plt altern directli log hpfilter filteredspec imag torch flipud filter spec detach caption filteredspec png gave panel subdir call hpfilter media check summari json hpfilter filteredspec type imag file sha bdaddbeeffeddfffeabbcbbfdaceacedb size path media imag hpfilter filteredspec bdaddbeef png format png width height caption filteredspec png ad elimin problem li actual data tri replac intend log arr torch rand log hpfilter filteredspec imag torch flipud arr detach cpu numpi caption filteredspec png behav thank advanc hint",
        "Question_preprocessed_content":"imag log intermedi result preprocess log kei gave panel subdir call hpfilter media check ad elimin problem li actual data tri replac intend log behav thank advanc hint",
        "Question_gpt_summary_original":"The user is facing an issue where images and plots are not being saved or logged using pytorch_lightning.loggers.WandbLogger, despite providing the correct key name. The logger is not creating a subdirectory called \"HPFilter\" under \"media\", and the images are not being displayed in the logs. The user has also tried to replace the intended logging with a random array, but the issue persists.",
        "Question_gpt_summary":"user face issu imag plot save log pytorch lightn logger logger despit provid correct kei logger creat subdirectori call hpfilter media imag displai log user tri replac intend log random arrai issu persist"
    },
    {
        "Question_title":"Dependency missing when running AzureML Estimator in docker environment",
        "Question_body":"<h3>Scenario description<\/h3>\n\n<p>I'm trying to submit a training script to AzureML (want to use AmlCompute, but I'm starting\/testing locally first, for debugging purposes).<\/p>\n\n<p>The <code>train.py<\/code> script I have uses a custom package (<code>arcus.ml<\/code>) and I believe I have specified the right settings and dependencies, but still I get the error: <\/p>\n\n<p><code>User program failed with ModuleNotFoundError: No module named 'arcus.ml'<\/code><\/p>\n\n<h3>Code and reproduction<\/h3>\n\n<p>This the python code I have:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>name='test'\nscript_params = {\n    '--test-par': 0.2\n}\n\nest = Estimator(source_directory='.\/' + name,\n                   script_params=script_params,\n                   compute_target='local',\n                   entry_script='train.py',\n                   pip_requirements_file='requirements.txt',\n                   conda_packages=['scikit-learn','tensorflow', 'keras'])\n\nrun = exp.submit(est)\nprint(run.get_portal_url())\n<\/code><\/pre>\n\n<p>This is the (fully simplified) train.py script in the <code>test<\/code>directory:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from arcus.ml import dataframes as adf\nfrom azureml.core import Workspace, Dataset, Datastore, Experiment, Run\n\n# get hold of the current run\nrun = Run.get_context()\nws = run.get_environment()\n\nprint('training finished')\n<\/code><\/pre>\n\n<p>And this is my requirements.txt file<\/p>\n\n<pre><code>arcus-azureml\narcus-ml\nnumpy\npandas\nazureml-core\ntqdm\njoblib\nscikit-learn\nmatplotlib\ntensorflow\nkeras\n<\/code><\/pre>\n\n<h3>Logs<\/h3>\n\n<p>In the logs file of the run, I can see this section, sot it seems the external module is being installed anyhow.<\/p>\n\n<pre><code>Collecting arcus-azureml\n  Downloading arcus_azureml-1.0.3-py3-none-any.whl (3.1 kB)\nCollecting arcus-ml\n  Downloading arcus_ml-1.0.6-py3-none-any.whl (2.1 kB)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1591043054577,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":208.0,
        "Poster_created_time":1360655430743,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Answer_body":"<p>I think this error isn't necessarily about Azure ML. I think the error has to do w\/ the difference b\/w using a hyphen and a period in your package name. But I'm a python packaging newb. \nIn a new conda environment on my laptop, I ran the following<\/p>\n\n<pre><code>&gt; conda create -n arcus python=3.6 -y\n&gt; conda activate arcus\n&gt; pip install arcus-ml\n&gt; python\n&gt;&gt;&gt; from arcus.ml import dataframes as adf\nModuleNotFoundError: No module named 'arcus'\n<\/code><\/pre>\n\n<p>When I look in the env's site packages folder, I didn't see the <code>arcus\/ml<\/code> folder structure I was expecting. There's no arcus code there at all, only the <code>.dist-info<\/code> file<\/p>\n\n<h3><code>~\/opt\/anaconda3\/envs\/arcus\/lib\/python3.6\/site-packages<\/code><\/h3>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/caExn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/caExn.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1591043858447,
        "Answer_last_edit_time":1591044471110,
        "Answer_score_count":1.0,
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62140446",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"depend miss run estim docker environ scenario descript try submit train script want us amlcomput start test local debug purpos train script us custom packag arcu believ specifi right set depend error user program fail modulenotfounderror modul name arcu code reproduct python code test script param test par est estim sourc directori script param script param comput target local entri script train pip requir file requir txt conda packag scikit learn tensorflow kera run exp submit est print run portal url fulli simplifi train script testdirectori arcu import datafram adf core import workspac dataset datastor experi run hold current run run run context run environ print train finish requir txt file arcu arcu numpi panda core tqdm joblib scikit learn matplotlib tensorflow kera log log file run section sot extern modul instal collect arcu download arcu whl collect arcu download arcu whl",
        "Question_preprocessed_content":"depend miss run estim docker environ scenario descript try submit train script script us custom packag believ specifi right set depend error code reproduct python code script directori file log log file run section sot extern modul instal",
        "Question_gpt_summary_original":"The user is encountering an issue while running AzureML Estimator in a docker environment. The user is trying to submit a training script to AzureML, but is getting a \"ModuleNotFoundError: No module named 'arcus.ml'\" error. The user has specified the right settings and dependencies, but the error persists. The logs show that the external module is being installed.",
        "Question_gpt_summary":"user encount issu run estim docker environ user try submit train script get modulenotfounderror modul name arcu error user specifi right set depend error persist log extern modul instal"
    },
    {
        "Question_title":"How to alter shared memory for SageMaker Docker containers?",
        "Question_body":"<p>I have a Docker image in Elastic Container Registry (ECR). It was created via a simple Dockerfile which I have control over.<\/p>\n<p>The image itself is fine, but I have a problem where the shared memory is insufficient when working inside a container in SageMaker Studio. Therefore I need to raise the shared memory of these containers.<\/p>\n<p>To raise the shared memory of a container, I believe the usual method is to pass the <code>--shm-size<\/code> argument to the <code>docker run<\/code> command when starting the container. However, I do not have control over this command, as SageMaker is doing that bit for me. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">docs<\/a> say that SageMaker is running <code>docker run &lt;image&gt; train<\/code> when starting a container.<\/p>\n<p>Is it possible to work around this problem? Either via somehow providing additional arguments to the command, or specifying something when creating the Docker image (such as in the Dockerfile, deployment script to ECR).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1648550512340,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":456.0,
        "Poster_created_time":1352838205400,
        "Poster_location":null,
        "Poster_reputation_count":718.0,
        "Poster_view_count":43.0,
        "Answer_body":"<p>As pointed out by @rok (thank you!) it is not possible in this situation to pass arguments to <code>docker run<\/code>, although it would be if switching to ECS.<\/p>\n<p>It is however possible to pass the <code>--shm-size<\/code> argument to <code>docker build<\/code> when building the image to push to ECR. This seems to have fixed the problem, albeit it does require a new Docker image to be built and pushed whenever wanting to change this parameter.<\/p>",
        "Answer_comment_count":3.0,
        "Question_closed_time":1648732366736,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1352838205400,
        "Answerer_location":null,
        "Answerer_reputation_count":718.0,
        "Answerer_view_count":43.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71660619",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"alter share memori docker contain docker imag elast contain registri ecr creat simpl dockerfil control imag fine problem share memori insuffici work insid contain studio need rais share memori contain rais share memori contain believ usual method pass shm size argument docker run command start contain control command bit doc run docker run train start contain possibl work problem provid addit argument command specifi creat docker imag dockerfil deploy script ecr",
        "Question_preprocessed_content":"alter share memori docker contain docker imag elast contain registri creat simpl dockerfil control imag fine problem share memori insuffici work insid contain studio need rais share memori contain rais share memori contain believ usual method pass argument command start contain control command bit doc run start contain possibl work problem provid addit argument command specifi creat docker imag",
        "Question_gpt_summary_original":"The user has a Docker image in Elastic Container Registry (ECR) and needs to increase the shared memory of the containers in SageMaker Studio. However, they do not have control over the <code>docker run<\/code> command as it is done by SageMaker. The user is looking for a workaround to provide additional arguments to the command or specify something in the Docker image or deployment script to ECR.",
        "Question_gpt_summary":"user docker imag elast contain registri ecr need increas share memori contain studio control docker run command user look workaround provid addit argument command specifi docker imag deploy script ecr"
    },
    {
        "Question_title":"how to Create calculated column",
        "Question_body":"<p>Hi,  <\/p>\n<p>I want to create calculated column. i have text probability field and estimated revenue field. i want to multiple estimated value with probability field. probability field has value like 10, 20, 5 etc.  <\/p>\n<p>pls guide how to create calculated column to perform these calculation.  <\/p>\n<p>thx<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1598247159813,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/74771\/how-to-create-calculated-column",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"creat calcul column want creat calcul column text probabl field estim revenu field want multipl estim valu probabl field probabl field valu like pl guid creat calcul column perform calcul thx",
        "Question_preprocessed_content":"creat calcul column want creat calcul column text probabl field estim revenu field want multipl estim valu probabl field probabl field valu like pl guid creat calcul column perform calcul thx",
        "Question_gpt_summary_original":"The user wants to create a calculated column by multiplying the estimated revenue field with the probability field, which contains values like 10, 20, 5, etc. They are seeking guidance on how to perform this calculation.",
        "Question_gpt_summary":"user want creat calcul column multipli estim revenu field probabl field contain valu like seek guidanc perform calcul"
    },
    {
        "Question_title":"Undo 'dvc add' operation",
        "Question_body":"<p>I <code>dvc add<\/code>-ed a file I did not mean to add. I have not yet committed.<\/p>\n\n<p>How do I undo this operation? In Git, you would do <code>git rm --cached &lt;filename&gt;<\/code>.<\/p>\n\n<p>To be clear: I want to make DVC forget about the file, and I want the file to remain untouched in my working tree. This is the opposite of what <code>dvc remove<\/code> does.<\/p>\n\n<p>One <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1524\" rel=\"nofollow noreferrer\">issue<\/a> on the DVC issue tracker suggests that <code>dvc unprotect<\/code> is the right command. But reading the <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/unprotect\" rel=\"nofollow noreferrer\">manual page<\/a> suggests otherwise.<\/p>\n\n<p>Is this possible with DVC?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1568689927047,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":4.0,
        "Question_view_count":1304.0,
        "Poster_created_time":1383611307000,
        "Poster_location":"New York",
        "Poster_reputation_count":10846.0,
        "Poster_view_count":984.0,
        "Answer_body":"<p>As per mroutis on the DVC Discord server:<\/p>\n\n<ol>\n<li><code>dvc unprotect<\/code> the file; this won't be necessary if you don't use <code>symlink<\/code> or <code>hardlink<\/code> caching, but it can't hurt.<\/li>\n<li>Remove the .dvc file<\/li>\n<li>If you need to delete the cache entry itself, run <code>dvc gc<\/code>, or look up the MD5 in <code>data.dvc<\/code> and manually remove it from <code>.dvc\/cache<\/code>.<\/li>\n<\/ol>\n\n<p><em>Edit<\/em> -- there is now an issue on their Github page to add this to the manual: <a href=\"https:\/\/github.com\/iterative\/dvc.org\/issues\/625\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc.org\/issues\/625<\/a><\/p>",
        "Answer_comment_count":6.0,
        "Question_closed_time":1568693889196,
        "Answer_last_edit_time":1568725966083,
        "Answer_score_count":7.0,
        "Answerer_created_time":1383611307000,
        "Answerer_location":"New York",
        "Answerer_reputation_count":10846.0,
        "Answerer_view_count":984.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57966851",
        "Tool":"DVC",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"undo add oper add file mean add commit undo oper git git cach clear want forget file want file remain untouch work tree opposit remov issu issu tracker suggest unprotect right command read manual page suggest possibl",
        "Question_preprocessed_content":"undo add oper file mean add commit undo oper git clear want forget file want file remain untouch work tree opposit issu issu tracker suggest right command read manual page suggest possibl",
        "Question_gpt_summary_original":"The user accidentally added a file using the 'dvc add' command and wants to undo the operation without affecting the file in the working tree. They are seeking guidance on how to make DVC forget about the file, and are unsure if 'dvc unprotect' is the right command to use.",
        "Question_gpt_summary":"user accident ad file add command want undo oper affect file work tree seek guidanc forget file unsur unprotect right command us"
    },
    {
        "Question_title":"How can I save more metadata on an MLFlow model",
        "Question_body":"<p>I am trying to save a model to MLFlow, but as I have a custom prediction pipeline to retrieve data, I need to save extra metadata into the model.<\/p>\n<p>I tried using my custom signature class, which It does the job correctly and saves the model with the extra metadata in the MLModel file (YAML format). But when want to load the model from the MLFlow registry, the signature is not easy accesible.<\/p>\n<pre><code>mlflow.sklearn.log_model(model, &quot;model&quot;, signature = signature)\n<\/code><\/pre>\n<p>I've also tried to save an extra dictionary at the log_model function, but it saves it in the conda.yaml file:<\/p>\n<pre><code>mlflow.sklearn.log_model(model, &quot;model&quot;, {&quot;metadata1&quot;:&quot;value1&quot;, &quot;metadata2&quot;:&quot;value2&quot;})\n<\/code><\/pre>\n<p>Should I make my own flavour? Or my own Model inheritance? I've seen <a href=\"https:\/\/github1s.com\/mlflow\/mlflow\/blob\/HEAD\/mlflow\/pyfunc\/__init__.py\" rel=\"nofollow noreferrer\">here<\/a> that the PyFuncModel recieves some metadata class and an implementation to solve this, but I don't know where should I pass my own implementations to PyFuncModel on an experiment script. Here's a minimal example:<\/p>\n<pre><code>import mlflow\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\nmetadata_dic = {&quot;metadata1&quot;: &quot;value1&quot;, \n                &quot;metadata2&quot;: &quot;value2&quot;}\n\nX = np.array([[-2, -1, 0, 1, 2, 1],[-2, -1, 0, 1, 2, 1]]).T\ny = np.array([0, 0, 1, 1, 1, 0])\n\nX = pd.DataFrame(X, columns=[&quot;X1&quot;, &quot;X2&quot;])\ny = pd.DataFrame(y, columns=[&quot;y&quot;])\n\n\nmodel = LogisticRegression()\nmodel.fit(X, y)\n\nmlflow.sklearn.log_model(model, &quot;model&quot;)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1638276045390,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":323.0,
        "Poster_created_time":1550233102176,
        "Poster_location":null,
        "Poster_reputation_count":621.0,
        "Poster_view_count":103.0,
        "Answer_body":"<p>Finally, I made a class that contains every metadata and saved it as an model argument:<\/p>\n<pre><code>model = LogisticRegression()\nmodel.fit(X, y)\nmodel.metadata = ModelMetadata(**metadata_dic)\nmlflow.sklearn.log_model(model, &quot;model&quot;)\n<\/code><\/pre>\n<p>Here I lost the customizable <code>predict<\/code> process, but after reading the <code>MLFlow<\/code> documentation is not very clear how to proceed.<\/p>\n<p>If anyone finds a good approach It would be very appreciated.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1638361888372,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1550233102176,
        "Answerer_location":null,
        "Answerer_reputation_count":621.0,
        "Answerer_view_count":103.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70169519",
        "Tool":"MLflow",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"save metadata model try save model custom predict pipelin retriev data need save extra metadata model tri custom signatur class job correctli save model extra metadata mlmodel file yaml format want load model registri signatur easi acces sklearn log model model model signatur signatur tri save extra dictionari log model function save conda yaml file sklearn log model model model metadata valu metadata valu flavour model inherit seen pyfuncmodel reciev metadata class implement solv know pass implement pyfuncmodel experi script minim exampl import import numpi import panda sklearn linear model import logisticregress metadata dic metadata valu metadata valu arrai arrai datafram column datafram column model logisticregress model fit sklearn log model model model",
        "Question_preprocessed_content":"save metadata model try save model custom predict pipelin retriev data need save extra metadata model tri custom signatur class job correctli save model extra metadata mlmodel file want load model registri signatur easi acces tri save extra dictionari function save file flavour model inherit seen pyfuncmodel reciev metadata class implement solv know pass implement pyfuncmodel experi script minim exampl",
        "Question_gpt_summary_original":"The user is trying to save a model to MLFlow with extra metadata using a custom signature class or a dictionary, but is unable to access the signature when loading the model from the MLFlow registry. The user is considering creating their own flavor or model inheritance and is seeking guidance on how to pass their own implementations to PyFuncModel on an experiment script.",
        "Question_gpt_summary":"user try save model extra metadata custom signatur class dictionari unabl access signatur load model registri user consid creat flavor model inherit seek guidanc pass implement pyfuncmodel experi script"
    },
    {
        "Question_title":"I am exploring the Incremental Learning feature in Amazon Sagemaker",
        "Question_body":"<p>I have access to AWS Console and started exploring the Sagemaker Incremental Learning documentation. It is mentioned that only three built-in algorithms currently support incremental training: Object Detection Algorithm, Image Classification Algorithm, and Semantic Segmentation Algorithm.<\/p>\n<p>I want to know if we can successfully train the initial model using new incremental sample data for  other (non-built in algorithms) training images. For example, a classification or a regression.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1659355486950,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1659900635596,
        "Question_score_count":0.0,
        "Question_view_count":83.0,
        "Poster_created_time":1650874814367,
        "Poster_location":null,
        "Poster_reputation_count":49.0,
        "Poster_view_count":8.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73193381",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"explor increment learn featur access aw consol start explor increment learn document mention built algorithm current support increment train object detect algorithm imag classif algorithm semant segment algorithm want know successfulli train initi model new increment sampl data non built algorithm train imag exampl classif regress",
        "Question_preprocessed_content":"explor increment learn featur access aw consol start explor increment learn document mention algorithm current support increment train object detect algorithm imag classif algorithm semant segment algorithm want know successfulli train initi model new increment sampl data train imag exampl classif regress",
        "Question_gpt_summary_original":"The user is exploring the Incremental Learning feature in Amazon Sagemaker and has found that only three built-in algorithms currently support incremental training. They are unsure if they can train the initial model using new incremental sample data for other non-built-in algorithms such as classification or regression.",
        "Question_gpt_summary":"user explor increment learn featur built algorithm current support increment train unsur train initi model new increment sampl data non built algorithm classif regress"
    },
    {
        "Question_title":"Cannot install Python package on SageMaker",
        "Question_body":"I am trying to install CKG Python library on my SageMaker Studio, following this instruction, https:\/\/ckg.readthedocs.io\/en\/latest\/intro\/getting-started-with-build.html\n\nI changed 'python setup.py install' to 'pip install .'\n\nbut I still get an error\n\n\n```\ners\n            creating build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/__init__.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/acent.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/cheshire_tomography.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/circuits.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/image_processing.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/numpy_test.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/optimal_control.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/stock_tradeoff.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/test.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/xpress_example.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            creating build\/lib.linux-x86_64-cpython-37\/examples\/flows\n            copying examples\/flows\/__init__.py -> build\/lib.linux-x86_64-cpython-37\/examples\/flows\n            copying examples\/flows\/commodity_flow.py -> build\/lib.linux-x86_64-cpython-37\/examples\/flows\n            copying examples\/flows\/create_graph.py -> build\/lib.linux-x86_64-cpython-37\/examples\/flows\n            copying examples\/flows\/incidence_matrix.py -> build\/lib.linux-x86_64-cpython-37\/examples\/flows\n            copying examples\/flows\/leaky_edges.py -> build\/lib.linux-x86_64-cpython-37\/examples\/flows\n            copying examples\/flows\/max_flow.py -> build\/lib.linux-x86_64-cpython-37\/examples\/flows\n            running build_ext\n            building '_cvxcore' extension\n            creating build\/temp.linux-x86_64-cpython-37\n            creating build\/temp.linux-x86_64-cpython-37\/cvxpy\n            creating build\/temp.linux-x86_64-cpython-37\/cvxpy\/cvxcore\n            creating build\/temp.linux-x86_64-cpython-37\/cvxpy\/cvxcore\/python\n            creating build\/temp.linux-x86_64-cpython-37\/cvxpy\/cvxcore\/src\n            gcc -pthread -B \/opt\/conda\/envs\/BookEnv_01\/compiler_compat -Wl,--sysroot=\/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -Icvxpy\/cvxcore\/src\/ -Icvxpy\/cvxcore\/python\/ -Icvxpy\/cvxcore\/include\/ -I\/opt\/conda\/envs\/BookEnv_01\/include\/python3.7m -I\/tmp\/pip-build-env-21op54p3\/overlay\/lib\/python3.7\/site-packages\/numpy\/core\/include -c cvxpy\/cvxcore\/python\/cvxcore_wrap.cpp -o build\/temp.linux-x86_64-cpython-37\/cvxpy\/cvxcore\/python\/cvxcore_wrap.o -O3\n            error: command 'gcc' failed: No such file or directory: 'gcc'\n            [end of output]\n      \n        note: This error originates from a subprocess, and is likely not a problem with pip.\n        ERROR: Failed building wheel for cvxpy\n      Failed to build cvxpy\n      ERROR: Could not build wheels for cvxpy, which is required to install pyproject.toml-based projects\n      Traceback (most recent call last):\n        File \"<string>\", line 36, in <module>\n        File \"<pip-setuptools-caller>\", line 34, in <module>\n        File \"\/home\/sagemaker-user\/CKG\/setup.py\", line 52, in <module>\n          include_package_data=True,\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/__init__.py\", line 107, in setup\n          return distutils.core.setup(**attrs)\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/core.py\", line 185, in setup\n          return run_commands(dist)\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/core.py\", line 201, in run_commands\n          dist.run_commands()\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/dist.py\", line 969, in run_commands\n          self.run_command(cmd)\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 1243, in run_command\n          super().run_command(command)\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/dist.py\", line 988, in run_command\n          cmd_obj.run()\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/wheel\/bdist_wheel.py\", line 378, in run\n          self.run_command(\"install\")\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/cmd.py\", line 318, in run_command\n          self.distribution.run_command(command)\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 1243, in run_command\n          super().run_command(command)\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/dist.py\", line 988, in run_command\n          cmd_obj.run()\n        File \"\/home\/sagemaker-user\/CKG\/setup.py\", line 15, in run\n          check_call(\"pip install -r requirements.txt\".split())\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/subprocess.py\", line 363, in check_call\n          raise CalledProcessError(retcode, cmd)\n      subprocess.CalledProcessError: Command '['pip', 'install', '-r', 'requirements.txt']' returned non-zero exit status 1.\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for CKG\n  Running setup.py clean for CKG\nFailed to build CKG\nERROR: Could not build wheels for CKG, which is required to install pyproject.toml-based projects\n```\nPlease kindly help me figure this one out.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1682044905940,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1682390903136,
        "Question_score_count":0.0,
        "Question_view_count":58.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUh8krHZokSheAOAhOni-aYA\/cannot-install-python-package-on-sagemaker",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"instal python packag try instal ckg python librari studio follow instruct http ckg readthedoc latest intro get start build html chang python setup instal pip instal error er creat build lib linux cpython exampl advanc copi exampl advanc init build lib linux cpython exampl advanc copi exampl advanc acent build lib linux cpython exampl advanc copi exampl advanc cheshir tomographi build lib linux cpython exampl advanc copi exampl advanc circuit build lib linux cpython exampl advanc copi exampl advanc imag process build lib linux cpython exampl advanc copi exampl advanc numpi test build lib linux cpython exampl advanc copi exampl advanc optim control build lib linux cpython exampl advanc copi exampl advanc stock tradeoff build lib linux cpython exampl advanc copi exampl advanc test build lib linux cpython exampl advanc copi exampl advanc xpress exampl build lib linux cpython exampl advanc creat build lib linux cpython exampl flow copi exampl flow init build lib linux cpython exampl flow copi exampl flow commod flow build lib linux cpython exampl flow copi exampl flow creat graph build lib linux cpython exampl flow copi exampl flow incid matrix build lib linux cpython exampl flow copi exampl flow leaki edg build lib linux cpython exampl flow copi exampl flow max flow build lib linux cpython exampl flow run build ext build cvxcore extens creat build temp linux cpython creat build temp linux cpython cvxpy creat build temp linux cpython cvxpy cvxcore creat build temp linux cpython cvxpy cvxcore python creat build temp linux cpython cvxpy cvxcore src gcc pthread opt conda env bookenv compil compat sysroot wsign compar dndebug fwrapv wall wstrict prototyp fpic icvxpi cvxcore src icvxpi cvxcore python icvxpi cvxcore includ opt conda env bookenv includ python tmp pip build env opp overlai lib python site packag numpi core includ cvxpy cvxcore python cvxcore wrap cpp build temp linux cpython cvxpy cvxcore python cvxcore wrap error command gcc fail file directori gcc end output note error origin subprocess like problem pip error fail build wheel cvxpy fail build cvxpy error build wheel cvxpy requir instal pyproject toml base project traceback recent file line file line file home user ckg setup line includ packag data true file opt conda env bookenv lib python site packag setuptool init line setup return distutil core setup attr file opt conda env bookenv lib python site packag setuptool distutil core line setup return run command dist file opt conda env bookenv lib python site packag setuptool distutil core line run command dist run command file opt conda env bookenv lib python site packag setuptool distutil dist line run command self run command cmd file opt conda env bookenv lib python site packag setuptool dist line run command super run command command file opt conda env bookenv lib python site packag setuptool distutil dist line run command cmd obj run file opt conda env bookenv lib python site packag wheel bdist wheel line run self run command instal file opt conda env bookenv lib python site packag setuptool distutil cmd line run command self distribut run command command file opt conda env bookenv lib python site packag setuptool dist line run command super run command command file opt conda env bookenv lib python site packag setuptool distutil dist line run command cmd obj run file home user ckg setup line run check pip instal requir txt split file opt conda env bookenv lib python subprocess line check rais calledprocesserror retcod cmd subprocess calledprocesserror command pip instal requir txt return non zero exit statu end output note error origin subprocess like problem pip error fail build wheel ckg run setup clean ckg fail build ckg error build wheel ckg requir instal pyproject toml base project kindli help figur",
        "Question_preprocessed_content":"instal python packag try instal ckg python librari studio follow instruct chang python instal pip instal error kindli help figur",
        "Question_gpt_summary_original":"The user is facing an issue while trying to install the CKG Python library on SageMaker Studio. They followed the given instructions but still encountered an error related to the installation of cvxpy. The error message suggests that the 'gcc' command failed, which is likely not a problem with pip. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user face issu try instal ckg python librari studio follow given instruct encount error relat instal cvxpy error messag suggest gcc command fail like problem pip user seek help resolv issu"
    },
    {
        "Question_title":"Snowflake auth issues using AWS SageMaker notebooks",
        "Question_body":"<p>We're using Python 3.x locally to connect to Snowflake, using the &quot;externalbrowser&quot; authentication method. This works quite nicely running code locally, where it automatically opens another browser tab and finishes authenticating that way.<\/p>\n<pre><code>from sqlalchemy import create_engine\nfrom snowflake.sqlalchemy import URL\n\nengine = create_engine(URL(\naccount=&quot;MY_ACCOUNT&quot;,\nuser=&quot;MY_USERNAME&quot;,\nauthenticator=&quot;externalbrowser&quot;,\nrole=&quot;MY_ROLE&quot;,\nwarehouse=&quot;MY_WAREHOUSE&quot;\n))\nconnection = engine.connect()\n<\/code><\/pre>\n<p>However running the same code from within an AWS SageMaker (Juypter) notebooks throws an error since it has no browser tab to open to complete the authentication. The error being returned is,<\/p>\n<pre><code>DatabaseError: (snowflake.connector.errors.DatabaseError) 250008 (08001): Failed to connect to DB: &lt;MY_ACCOUNT&gt;.snowflakecomputing.com:443, Unable to open a browser in this environment.\n(Background on this error at: https:\/\/sqlalche.me\/e\/14\/4xp6)\n<\/code><\/pre>\n<p>We don't have any Okta authentication enabled or any other SAML (or OAuth) authentication options we can use instead. Is there any other way around this issue with SageMaker notebooks and externalbrowser authentication?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1642778045010,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":433.0,
        "Poster_created_time":1642777360656,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70803508",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"snowflak auth issu notebook python local connect snowflak externalbrows authent method work nice run code local automat open browser tab finish authent wai sqlalchemi import creat engin snowflak sqlalchemi import url engin creat engin url account account user usernam authent externalbrows role role warehous warehous connect engin connect run code juypter notebook throw error browser tab open complet authent error return databaseerror snowflak connector error databaseerror fail connect snowflakecomput com unabl open browser environ background error http sqlalch okta authent enabl saml oauth authent option us instead wai issu notebook externalbrows authent",
        "Question_preprocessed_content":"snowflak auth issu notebook python local connect snowflak externalbrows authent method work nice run code local automat open browser tab finish authent wai run code notebook throw error browser tab open complet authent error return okta authent enabl saml authent option us instead wai issu notebook externalbrows authent",
        "Question_gpt_summary_original":"The user is facing issues with Snowflake authentication using AWS SageMaker notebooks. The \"externalbrowser\" authentication method works fine locally, but throws an error in SageMaker notebooks as there is no browser tab to complete the authentication. The user does not have any other SAML or OAuth authentication options and is seeking a solution to this issue.",
        "Question_gpt_summary":"user face issu snowflak authent notebook externalbrows authent method work fine local throw error notebook browser tab complet authent user saml oauth authent option seek solut issu"
    },
    {
        "Question_title":"Specify guild home location on SSH remote",
        "Question_body":"<p>I have a remote SSH with a OS disk and a data disk. I want to put my guild runs on the data disk, so on my ssh machine I have set my <code>GUILD_HOME=\/mnt\/guild_runs<\/code>.<\/p>\n<p>When I want to run from my local machine like <code>guild runs --remote ssh-machine<\/code> it cannot find the runs on the remote. When I run with additional debug information I see that the ssh command guild issues overwrites my <code>GUILD_HOME<\/code> env variable.<\/p>\n<p>Is there a way to tell guild where my <code>GUILD_HOME<\/code> is on my remote? I know I can do <code>venv-path<\/code>, but since my venv path and <code>GUILD_HOME<\/code> directory aren\u2019t the same, I don\u2019t think this solves it.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1651083462913,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":143.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/specify-guild-home-location-on-ssh-remote\/866",
        "Tool":"Guild AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"specifi home locat ssh remot remot ssh disk data disk want run data disk ssh machin set home mnt run want run local machin like run remot ssh machin run remot run addit debug inform ssh command issu overwrit home env variabl wai tell home remot know venv path venv path home directori arent dont think solv",
        "Question_preprocessed_content":"specifi home locat ssh remot remot ssh disk data disk want run data disk ssh machin set want run local machin like run remot run addit debug inform ssh command issu overwrit env variabl wai tell remot know venv path directori arent dont think solv",
        "Question_gpt_summary_original":"The user is facing challenges in specifying the guild home location on their SSH remote. They have set the GUILD_HOME variable to \/mnt\/guild_runs on their SSH machine, but when running guild runs --remote ssh-machine from their local machine, the runs cannot be found on the remote. The user suspects that the SSH command issued by guild overwrites the GUILD_HOME env variable. They are looking for a way to tell guild where their GUILD_HOME is on the remote, but are unsure if venv-path is the solution since their venv path and GUILD_HOME directory are not the same.",
        "Question_gpt_summary":"user face challeng specifi home locat ssh remot set home variabl mnt run ssh machin run run remot ssh machin local machin run remot user suspect ssh command issu overwrit home env variabl look wai tell home remot unsur venv path solut venv path home directori"
    },
    {
        "Question_title":"Issue installing polyaxon python client urllib3.exceptions.ProtocolError: OSError",
        "Question_body":"Archived from slack discussion!\n\nI\u2019m receiving the following error recently, but it only occurs when I use a VPN, if I\u2019m in the office I don\u2019t get this issue.\nHas anyone received similar or have any clues on what might be the problem?\n\n File \"\/usr\/local\/opt\/python@3.8\/Frameworks\/Python.framework\/Versions\/3.8\/lib\/python3.8\/ssl.py\", line 1019, in _create\n    self.getpeername()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', OSError(22, 'Invalid argument'))",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1667914384000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"Sometimes the CA bundle is not up-to date and needs to be installed manually, especially if it's a new Python version. Please run the following commands:\n\ncd \/Applications\/Python\\ 3.8\/\n\nthen\n\n.\/Install\\ Certificates.command",
        "Answer_comment_count":0.0,
        "Question_closed_time":1667914455000,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1519",
        "Tool":"Polyaxon",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"issu instal python client urllib except protocolerror oserror archiv slack discuss receiv follow error recent occur us vpn offic dont issu receiv similar clue problem file usr local opt python framework python framework version lib python ssl line creat self getpeernam urllib except protocolerror connect abort oserror invalid argument",
        "Question_preprocessed_content":"issu instal python client oserror archiv slack discuss receiv follow error recent occur us vpn offic dont issu receiv similar clue problem file line",
        "Question_gpt_summary_original":"The user is encountering an error while installing the Polyaxon Python client. The error message is \"urllib3.exceptions.ProtocolError: ('Connection aborted.', OSError(22, 'Invalid argument'))\". The error only occurs when using a VPN and not in the office. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount error instal python client error messag urllib except protocolerror connect abort oserror invalid argument error occur vpn offic user seek help resolv issu"
    },
    {
        "Question_title":"ServicePrincipalAuthentication no longer working in Databricks",
        "Question_body":"<p>Hi all.<\/p>\n<p>I've had this problem for MONTHS now and, not having the option to give up, I'm getting desperate.<\/p>\n<p>I have a databricks set up where an azure file-share is mounted, and this is used to extract data and read it into a database. Up until Monday it was been working fine.<\/p>\n<p>Recently, although nothing has changed about the way the drive is mounted (via Azure ML libraries):<\/p>\n<p>sp = ServicePrincipalAuthentication(tenant_id=&quot;x&quot;, # tenantID service_principal_id=&quot;y&quot;, clientId service_principal_password=&quot;z&quot;)<\/p>\n<p>clientSecret ws = Workspace.get(name=&quot;wsname&quot;, auth=sp, subscription_id=&quot;a&quot;)<\/p>\n<p>Listing the contents of a directory suddenly takes an enormous amount of time to finish (50 minutes), before no longer being able to find the folder. Essentially, it repeatedly tries to switch back to interactive authentication before failing altogether, saying [Errno22]: Invalid Argument.<\/p>\n<p>import os<\/p>\n<p>folder = &quot;\/mnt\/tmp\/xx\/a\/b\/c&quot;<\/p>\n<p>patient_names = os.listdir(&quot;\/mnt\/tmp\/xx\/a\/b\/c&quot;) print(patient_names)<\/p>\n<p>I'm lost, is there anywhere I should be looking to try and find out what's wrong?<\/p>\n<p>It works fine using Interactive authentication and WAS working with SPA, but suddenly does not.  <br \/>\nI've tried:<\/p>\n<ul>\n<li>   Recreating the datastore and dataset in ML<\/li>\n<li>   Creating new databricks clusters on which to run the code.<\/li>\n<li>   Creating another service principal<\/li>\n<li>   Running the code on my windows work machine with Pycharm and Python 3.7<\/li>\n<li>   Creating an Ubuntu environment with Pycharm and Python 3.7<\/li>\n<li>   Creating a new machine learning environment<\/li>\n<li>   Running the code with the python logging module to see if anything useful has come back.<\/li>\n<li>   Trying several different versions of azureml-sdk[databricks]<\/li>\n<li>   Trying yet another service principal.    Nothing has worked. I don't understand how\/why service principals are suddenly being ignored, why there are no error messages or useful information of any kind, or how nobody else has come across this problem?<\/li>\n<\/ul>\n<p>Please can someone help?<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_created_time":1618246743070,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/354148\/serviceprincipalauthentication-no-longer-working-i",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"serviceprincipalauthent longer work databrick problem month have option get desper databrick set azur file share mount extract data read databas mondai work fine recent chang wai drive mount librari serviceprincipalauthent tenant tenantid servic princip clientid servic princip password clientsecret workspac wsname auth subscript list content directori suddenli take enorm time finish minut longer abl folder essenti repeatedli tri switch interact authent fail altogeth sai errno invalid argument import folder mnt tmp patient name listdir mnt tmp print patient name lost look try wrong work fine interact authent work spa suddenli tri recreat datastor dataset creat new databrick cluster run code creat servic princip run code window work machin pycharm python creat ubuntu environ pycharm python creat new machin learn environ run code python log modul us come try differ version sdk databrick try servic princip work understand servic princip suddenli ignor error messag us inform kind come problem help thank",
        "Question_preprocessed_content":"serviceprincipalauthent longer work databrick problem month have option get desper databrick set azur mount extract data read databas mondai work fine recent chang wai drive mount tenantid clientid clientsecret auth list content directori suddenli take enorm time finish longer abl folder essenti repeatedli tri switch interact authent fail altogeth sai invalid argument import folder lost look try wrong work fine interact authent work spa suddenli tri recreat datastor dataset creat new databrick cluster run code creat servic princip run code window work machin pycharm python creat ubuntu environ pycharm python creat new machin learn environ run code python log modul us come try differ version sdk try servic princip work understand servic princip suddenli ignor error messag us inform kind come problem help thank",
        "Question_gpt_summary_original":"The user is facing an issue with ServicePrincipalAuthentication in Databricks. The Azure file-share mounted for data extraction and reading into a database was working fine until recently. However, listing the contents of a directory takes an enormous amount of time to finish and then fails altogether, saying [Errno22]: Invalid Argument. The user has tried several solutions, including recreating the datastore and dataset in ML, creating new databricks clusters, and service principals, but nothing has worked. The user is seeking help to understand why service principals are suddenly being ignored and how to resolve the issue.",
        "Question_gpt_summary":"user face issu serviceprincipalauthent databrick azur file share mount data extract read databas work fine recent list content directori take enorm time finish fail altogeth sai errno invalid argument user tri solut includ recreat datastor dataset creat new databrick cluster servic princip work user seek help understand servic princip suddenli ignor resolv issu"
    },
    {
        "Question_title":"How to enable automated data labeling for custom labeling job?",
        "Question_body":"Hi,  \nI need to label large dataset which has around 200000 records and this doesn't fall into any of the prebuilt labeling job categories. So trying to create custom labeling job with automated data labeling enabled. But the configuration to enable the automated data labeling doesn't appear for custom labelling job. Any reference to proceed further asap would be of great help. Thank you.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1557318279000,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1667861880932,
        "Question_score_count":0.0,
        "Question_view_count":88.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-JeOVDZER4uGxiY0l_geKg\/how-to-enable-automated-data-labeling-for-custom-labeling-job",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"enabl autom data label custom label job need label larg dataset record fall prebuilt label job categori try creat custom label job autom data label enabl configur enabl autom data label appear custom label job refer proce asap great help thank",
        "Question_preprocessed_content":"enabl autom data label custom label job need label larg dataset record fall prebuilt label job categori try creat custom label job autom data label enabl configur enabl autom data label appear custom label job refer proce asap great help thank",
        "Question_gpt_summary_original":"The user needs to label a large dataset of 200,000 records that doesn't fit into any prebuilt labeling job categories. They are trying to create a custom labeling job with automated data labeling enabled, but are unable to find the configuration option for enabling automated data labeling for custom labeling jobs. They are seeking assistance to proceed further.",
        "Question_gpt_summary":"user need label larg dataset record fit prebuilt label job categori try creat custom label job autom data label enabl unabl configur option enabl autom data label custom label job seek assist proce"
    },
    {
        "Question_title":"How would one use databricks delta lake format with Kedro?",
        "Question_body":"<p>We are using <a href=\"http:\/\/kedro.readthedocs.io\/\" rel=\"nofollow noreferrer\">kedro<\/a> in our project. Normally, one can define datasets as such:<\/p>\n<pre><code>client_table:\n  type: spark.SparkDataSet\n  filepath: ${base_path_spark}\/${env}\/client_table\n  file_format: parquet\n  save_args:\n    mode: overwrite\n<\/code><\/pre>\n<p>Now we're running on databricks and they offer many <a href=\"https:\/\/databricks.com\/de\/blog\/2020\/10\/21\/faster-sql-adaptive-query-execution-in-databricks.html\" rel=\"nofollow noreferrer\">optimisations<\/a> such as <code>autoOptimizeShuffle<\/code>. We are considering to make use of this to handle our 15TB+ datasets.<\/p>\n<p>However, it's not clear to me how to use kedro with the databricks delta lake solution<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1604332595093,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":326.0,
        "Poster_created_time":1327570314367,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":2854.0,
        "Poster_view_count":324.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64648925",
        "Tool":"Kedro",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"us databrick delta lake format project normal defin dataset client tabl type spark sparkdataset filepath base path spark env client tabl file format parquet save arg mode overwrit run databrick offer optimis autooptimizeshuffl consid us handl dataset clear us databrick delta lake solut",
        "Question_preprocessed_content":"us databrick delta lake format project normal defin dataset run databrick offer optimis consid us handl dataset clear us databrick delta lake solut",
        "Question_gpt_summary_original":"The user is using Kedro in their project and wants to use Databricks Delta Lake format to handle their large datasets. They are unsure how to integrate Delta Lake with Kedro and are seeking guidance.",
        "Question_gpt_summary":"user project want us databrick delta lake format handl larg dataset unsur integr delta lake seek guidanc"
    },
    {
        "Question_title":"Vertex AI scheduled executor container updates",
        "Question_body":"<p>I have a Vertex AI scheduled executor which runs custom jobs. Runs are going fine and trains the model, but we often update helper classes in our project (our notebook instance has a GIT connection). These classes need to be updated in the Docker container as well. Is there a way to automatically point the schedule to the latest docker container in the artifact registry?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1663321616923,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":10.0,
        "Poster_created_time":1587457219480,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73742953",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"schedul executor contain updat schedul executor run custom job run go fine train model updat helper class project notebook instanc git connect class need updat docker contain wai automat point schedul latest docker contain artifact registri",
        "Question_preprocessed_content":"schedul executor contain updat schedul executor run custom job run go fine train model updat helper class project class need updat docker contain wai automat point schedul latest docker contain artifact registri",
        "Question_gpt_summary_original":"The user has a Vertex AI scheduled executor that runs custom jobs, but they face challenges when updating helper classes in their project. They need to update these classes in the Docker container as well and are looking for a way to automatically point the schedule to the latest Docker container in the artifact registry.",
        "Question_gpt_summary":"user schedul executor run custom job face challeng updat helper class project need updat class docker contain look wai automat point schedul latest docker contain artifact registri"
    },
    {
        "Question_title":"Error notification not working in background job Neptune Software",
        "Question_body":"<p>For a particular server script, we are adding a background job.\nIn that ,it has an option to add error notification emails ,which is not working.  There is error in my script, which I can see in job log but not getting any notifications on email.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_created_time":1639300561957,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1649766671956,
        "Question_score_count":0.0,
        "Question_view_count":36.0,
        "Poster_created_time":1583493572323,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":9.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70322291",
        "Tool":"Neptune",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error notif work background job softwar particular server script ad background job option add error notif email work error script job log get notif email",
        "Question_preprocessed_content":"error notif work background job softwar particular server script ad background job option add error notif email work error script job log get notif email",
        "Question_gpt_summary_original":"The user is facing an issue with error notification emails not working in a background job for a server script. Although there is an error in the script, the user is not receiving any notifications on email.",
        "Question_gpt_summary":"user face issu error notif email work background job server script error script user receiv notif email"
    },
    {
        "Question_title":"Meetup on machine translation for low-resource languages this Friday!",
        "Question_body":"The last machine translation meetup featured a PM for the Google Cloud Translation API in person.\n\nThe next machine translation meetup is all about low-resource machine translation\u00a0and it'll be online.\n\n\u00a0\nmachinetranslate.org\/meetup\n\u00a0\nThe 25-minute panel features guests from Meta AI,\u00a0NeuralSpace, LoResMT, and Masakhane!\n\nRegister to join us\u00a0this Friday at 8am PST",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1666181580000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":32.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Meetup-on-machine-translation-for-low-resource-languages-this\/td-p\/479955\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"meetup machin translat low resourc languag fridai machin translat meetup featur googl cloud translat api person machin translat meetup low resourc machin translationand onlin machinetransl org meetup minut panel featur guest meta neuralspac loresmt masakhan regist join usthi fridai pst",
        "Question_preprocessed_content":"meetup machin translat languag fridai machin translat meetup featur googl cloud translat api person machin translat meetup machin translationand onlin panel featur guest meta neuralspac loresmt masakhan regist join usthi fridai pst",
        "Question_gpt_summary_original":"The user is promoting an upcoming online machine translation meetup that will focus on low-resource languages. The event will feature a 25-minute panel with guests from Meta AI, NeuralSpace, LoResMT, and Masakhane. The meetup will take place on Friday at 8am PST and interested individuals can register through the provided link.",
        "Question_gpt_summary":"user promot upcom onlin machin translat meetup focu low resourc languag event featur minut panel guest meta neuralspac loresmt masakhan meetup place fridai pst interest individu regist provid link"
    },
    {
        "Question_title":"Is it possible to use MLFlow logo on our landing page?",
        "Question_body":"Hello,\u00a0\n\n\nAt NBT AG, we are currently developing\u00a0a\u00a0deployment and orchestration platform for MLOps. We are planning to make an integration of MLflow for deploying models on edge devices through our platform. Could we use MLflow logo on our landing page already until we actually make that integration?\n\n\nThank you!\n\n\nSimon Bernard",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1576476836000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":null,
        "Question_view_count":23.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/F1YiBxZa1dc",
        "Tool":"MLflow",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"possibl us logo land page hello nbt current developingadeploy orchestr platform mlop plan integr deploi model edg devic platform us logo land page actual integr thank simon bernard",
        "Question_preprocessed_content":"possibl us logo land page hello nbt current developingadeploy orchestr platform mlop plan integr deploi model edg devic platform us logo land page actual integr thank simon bernard",
        "Question_gpt_summary_original":"The user is asking if they can use the MLflow logo on their landing page while they are developing an integration of MLflow for deploying models on edge devices through their platform.",
        "Question_gpt_summary":"user ask us logo land page develop integr deploi model edg devic platform"
    },
    {
        "Question_title":"Redshift ML SageMaker Domain",
        "Question_body":"Which Sagemaker Domain is used when using Redshift ML CREATE MODEL",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1683292000016,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1683639542431,
        "Question_score_count":0.0,
        "Question_view_count":44.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"Hi @jkrice, Redshift ML does not use SageMaker Studio (domains). When you run a CREATE MODEL statement, it calls SageMaker to create an Autopilot job (https:\/\/aws.amazon.com\/sagemaker\/autopilot\/) to train a model. Autopilot usually does preprocessing, and trains on a variety of suitable models for your use case and finally returns the best model, that's then deployed for inference in Redshift. You can see more information here - https:\/\/docs.aws.amazon.com\/redshift\/latest\/dg\/machine_learning.html",
        "Answer_comment_count":0.0,
        "Question_closed_time":1683298611292,
        "Answer_last_edit_time":1683298611292,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHf0f_0SuS5KOSSctyn37xA\/redshift-ml-sagemaker-domain",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":1.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_original_content":"redshift domain domain redshift creat model",
        "Question_preprocessed_content":"redshift domain domain redshift creat model",
        "Question_gpt_summary_original":"The user is seeking information on which SageMaker domain to use when creating a model with Redshift ML.",
        "Question_gpt_summary":"user seek inform domain us creat model redshift"
    },
    {
        "Question_title":"where to find the equation for the line after making Azure ML linear regression model, 2 slopes and 1 y intercept",
        "Question_body":"<p>Hi, I've made a model and it's predicting prices of cars. hooray! I cannot find the the equation for Azure's Regression Linear model anywhere. I made this model using Designer GUI. For example, in R, the coefficients are returned by running summary(mymodel)  <br \/>\n= y-intercept + (slope * miles) + (slope * year)  <br \/>\n= 21022.96 + (-0.0249*98500) + (-6.5668*2016)  <br \/>\nsomething like this equation for a line is what I'm looking for in Azure.<\/p>\n<p>what I've tried:  <\/p>\n<ol>\n<li> If it was only 1 feature, I could solve for an equation using (y2-y1) \/ (miles2-miles1) to find slope and the solve to y intercept. But this model uses miles and year as variables.<\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1626236136657,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>@<a href=\"\/users\/na\/?userid=dfab4fce-bbe8-4d93-a6fe-317f22fb2756\">@MikeRichardson-3493  <\/a> Thanks, We currently do not have coefficients for regression models, but we will forward this with our data science team to check on this. We are working on an interface to surface models that compose ensembles, model weights and more. While not  as involved of an interface, some of this information is available today within the model details tags sections:.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/114674-a.png?platform=QnA\" alt=\"114674-a.png\" \/>    <\/p>\n",
        "Answer_comment_count":0.0,
        "Question_closed_time":1626277455030,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/474924\/where-to-find-the-equation-for-the-line-after-maki",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"equat line make linear regress model slope intercept model predict price car hoorai equat azur regress linear model model design gui exampl coeffici return run summari mymodel intercept slope mile slope year like equat line look azur tri featur solv equat mile mile slope solv intercept model us mile year variabl",
        "Question_preprocessed_content":"equat line make linear regress model slope intercept model predict price car hoorai equat azur regress linear model model design gui exampl coeffici return run summari like equat line look azur tri featur solv equat slope solv intercept model us mile year variabl",
        "Question_gpt_summary_original":"The user has created a linear regression model in Azure ML to predict car prices but is unable to find the equation for the line. They have tried to solve for the equation using the slope and y-intercept but the model uses two variables, miles and year.",
        "Question_gpt_summary":"user creat linear regress model predict car price unabl equat line tri solv equat slope intercept model us variabl mile year"
    },
    {
        "Question_title":"Python Code example to transcribe 2 audio inputs into speech at the same time",
        "Question_body":"I'm trying to create a piece of python code that can take 2 audio inputs,\u00a0\n\n1. from my microphone\n\n2. virtual input from zoom\n\nat the same time\n\nhowever, i am not sure how to transcribe them simultaneously.\n\nany help would be appreciated, thank you!",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1640045220000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":153.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Python-Code-example-to-transcribe-2-audio-inputs-into-speech-at\/td-p\/180446\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"python code exampl transcrib audio input speech time try creat piec python code audio input microphon virtual input zoom time sure transcrib simultan help appreci thank",
        "Question_preprocessed_content":"python code exampl transcrib audio input speech time try creat piec python code audio input microphon virtual input zoom time sure transcrib simultan help appreci thank",
        "Question_gpt_summary_original":"The user is seeking help to create a Python code that can transcribe two audio inputs, one from their microphone and the other from virtual input from Zoom, simultaneously. They are unsure how to achieve this and are seeking assistance.",
        "Question_gpt_summary":"user seek help creat python code transcrib audio input microphon virtual input zoom simultan unsur achiev seek assist"
    },
    {
        "Question_title":"AutoML Translation models response time",
        "Question_body":"Hi,\n\nWe have several AutoML Translation models and we are facing timeout issues when the first translation requests are sent. We have to retry a second time to get the translations back. After this first request, it seems the model is kept\u00a0 \"online\", and subsequent requests to the same model are performing well.\n\nWhat we don't really know is how long the models are kept online and ready for quick response times and how many models can be online simultaneously. We would like to have more information about this in order to handle the translation requests in a proper and controlled way.\n\nThank you,\n\nJulian",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1659592140000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":131.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-Translation-models-response-time\/td-p\/450442\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"automl translat model respons time automl translat model face timeout issu translat request sent retri second time translat request model kept onlin subsequ request model perform know long model kept onlin readi quick respons time model onlin simultan like inform order handl translat request proper control wai thank julian",
        "Question_preprocessed_content":"automl translat model respons time automl translat model face timeout issu translat request sent retri second time translat request model kept onlin subsequ request model perform know long model kept onlin readi quick respons time model onlin simultan like inform order handl translat request proper control wai thank julian",
        "Question_gpt_summary_original":"The user is facing timeout issues with their AutoML Translation models when the first translation requests are sent. They have to retry a second time to get the translations back. They are unsure how long the models are kept online and ready for quick response times and how many models can be online simultaneously. They are seeking more information to handle translation requests in a proper and controlled way.",
        "Question_gpt_summary":"user face timeout issu automl translat model translat request sent retri second time translat unsur long model kept onlin readi quick respons time model onlin simultan seek inform handl translat request proper control wai"
    },
    {
        "Question_title":"Documentation on Vertex AI Feature Store online serving architecture?",
        "Question_body":"<p>There's documentation on <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/serving-online\" rel=\"nofollow noreferrer\">Vertex AI online serving<\/a>, but no mention of the underlying system being used other than <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-featurestores\" rel=\"nofollow noreferrer\">&quot;online serving nodes&quot;<\/a>. Is it <a href=\"https:\/\/cloud.google.com\/datastore\" rel=\"nofollow noreferrer\">Datastore<\/a>? Something else?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1660103073257,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":21.0,
        "Poster_created_time":1386714193848,
        "Poster_location":null,
        "Poster_reputation_count":3767.0,
        "Poster_view_count":226.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73300381",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"document featur store onlin serv architectur document onlin serv mention underli onlin serv node datastor",
        "Question_preprocessed_content":"document featur store onlin serv architectur document onlin serv mention underli onlin serv node datastor",
        "Question_gpt_summary_original":"The user is looking for information on the underlying system being used for Vertex AI online serving architecture, as the documentation only mentions \"online serving nodes\" without specifying if it is Datastore or something else.",
        "Question_gpt_summary":"user look inform underli onlin serv architectur document mention onlin serv node specifi datastor"
    },
    {
        "Question_title":"Import error while Executing AWS Predictive Maintenance Using Machine Learning Sample",
        "Question_body":"<p>We are trying to execute and check what kind of output is provided by Predictive Maintenance Using Machine Learning on AWS sample data. We are referring <a href=\"https:\/\/aws.amazon.com\/solutions\/predictive-maintenance-using-machine-learning\/\" rel=\"nofollow noreferrer\">Predictive Maintenance Using Machine Learning<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/solutions\/latest\/predictive-maintenance-using-machine-learning\/welcome.html\" rel=\"nofollow noreferrer\">AWS Guide<\/a> to launch the sample template provided by the AWS. The template is executed properly and we can see the resources in account. Whenever we run the sagemaker notebook for the given example we are getting the error in CloudWatch logs as follows<\/p>\n\n<pre><code>ImportError: cannot import name 'replace_file' on line from mxnet.gluon.utils import download, check_sha1, _get_repo_file_url, replace_file.\n<\/code><\/pre>\n\n<p>This is the stage where the invoke the training job. We have tried following options to resolve the issue.<\/p>\n\n<ul>\n<li>Upgrading the mxnet module<\/li>\n<li>Upgrading the tensorflow module<\/li>\n<\/ul>\n\n<p>But no success.<\/p>\n\n<p>Thanks in advance.<\/p>\n\n<p>Error Traceback is as follows<\/p>\n\n<pre><code>  File \"\/usr\/lib\/python3.5\/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"\/usr\/lib\/python3.5\/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/ml\/code\/sagemaker_predictive_maintenance_entry_point.py\", line 10, in &lt;module&gt;\n    import gluonnlp\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/__init__.py\", line 25, in &lt;module&gt;\n    from . import data\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/data\/__init__.py\", line 23, in &lt;module&gt;\n    from . import (batchify, candidate_sampler, conll, corpora, dataloader,\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/data\/question_answering.py\", line 31, in &lt;module&gt;\n    from mxnet.gluon.utils import download, check_sha1, _get_repo_file_url, replace_file\n    ImportError: cannot import name 'replace_file'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1587652058893,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":398.0,
        "Poster_created_time":1473770138816,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":3.0,
        "Answer_body":"<p>A fix for this issue is being deployed to the official solution. In the meantime, you can make the changes described <a href=\"https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/pull\/7\/files\" rel=\"nofollow noreferrer\">here<\/a> in your SageMaker environment by following the instructions below:<\/p>\n\n<p>1) In the notebook, please change the <code>framework_version<\/code> to <code>1.6.0<\/code>.<\/p>\n\n<pre><code>MXNet(entry_point='sagemaker_predictive_maintenance_entry_point.py',\n          source_dir='sagemaker_predictive_maintenance_entry_point',\n          py_version='py3',\n          role=role, \n          train_instance_count=1, \n          train_instance_type=train_instance_type,\n          output_path=output_location,\n          hyperparameters={'num-datasets' : len(train_df),\n                           'num-gpus': 1,\n                           'epochs': 500,\n                           'optimizer': 'adam',\n                           'batch-size':1,\n                           'log-interval': 100},\n         input_mode='File',\n         train_max_run=7200,\n         framework_version='1.6.0')  &lt;- Change this to 1.6.0.\n<\/code><\/pre>\n\n<p>2) This will likely fix things, but just to be sure you don't have any stale packages, change the <code>requirements.txt<\/code> file as well.<\/p>\n\n<p>You'll need to open up a terminal in SageMaker.\n<a href=\"https:\/\/i.stack.imgur.com\/0Vn6l.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0Vn6l.png\" alt=\"enter image description here\"><\/a>\nimage taken from <a href=\"https:\/\/medium.com\/swlh\/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/swlh\/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439<\/a><\/p>\n\n<p>and run<\/p>\n\n<pre><code>cd SageMaker\/sagemaker_predictive_maintenance_entry_point\/\nsudo vim requirements.txt  # (or sudo nano requirements.txt)\n<\/code><\/pre>\n\n<p>Change the contents to:<\/p>\n\n<pre><code>gluonnlp==0.9.1\npandas==0.22\n<\/code><\/pre>\n\n<p>Save it, and then run the example again.<\/p>\n\n<p>Feel free to comment on the issue as well:\n<a href=\"https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/issues\/6\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/issues\/6<\/a> <\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1588063849632,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1499498135632,
        "Answerer_location":null,
        "Answerer_reputation_count":675.0,
        "Answerer_view_count":26.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61389632",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"import error execut aw predict mainten machin learn sampl try execut check kind output provid predict mainten machin learn aw sampl data refer predict mainten machin learn aw guid launch sampl templat provid aw templat execut properli resourc account run notebook given exampl get error cloudwatch log follow importerror import replac file line mxnet gluon util import download check sha repo file url replac file stage invok train job tri follow option resolv issu upgrad mxnet modul upgrad tensorflow modul success thank advanc error traceback follow file usr lib python runpi line run modul main main mod spec file usr lib python runpi line run code exec code run global file opt code predict mainten entri point line import gluonnlp file usr local lib python dist packag gluonnlp init line import data file usr local lib python dist packag gluonnlp data init line import batchifi candid sampler conll corpora dataload file usr local lib python dist packag gluonnlp data question answer line mxnet gluon util import download check sha repo file url replac file importerror import replac file",
        "Question_preprocessed_content":"import error execut aw predict mainten machin learn sampl try execut check kind output provid predict mainten machin learn aw sampl data refer predict mainten machin learn aw guid launch sampl templat provid aw templat execut properli resourc account run notebook given exampl get error cloudwatch log follow stage invok train job tri follow option resolv issu upgrad mxnet modul upgrad tensorflow modul success thank advanc error traceback follow",
        "Question_gpt_summary_original":"The user is encountering an import error while executing AWS Predictive Maintenance Using Machine Learning sample. The error occurs while invoking the training job and the error traceback indicates that the 'replace_file' module cannot be imported. The user has tried upgrading the mxnet and tensorflow modules but without success.",
        "Question_gpt_summary":"user encount import error execut aw predict mainten machin learn sampl error occur invok train job error traceback indic replac file modul import user tri upgrad mxnet tensorflow modul success"
    },
    {
        "Question_title":"Logging scalars when running ray[tune] tuning fails in a guild run",
        "Question_body":"<p>In my project I have a bit of automatic tuning of my pytorch-lightning models using ray and then I also automatically apply the model. The logging that ray[tune] uses is a SummaryWriter from tensorboardX package. I am also using tensorboardX SummaryWriter for logging other things in my project. For my own logging, there is no issue with this, but for some reason guild fails with the calls to <code>add_scalar()<\/code> when it\u2019s called from the tune library.<\/p>\n<p>The trace:<\/p>\n<pre><code>3\/8\/2021 5:40:52 PM\nTraceback (most recent call last):\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 594, in _process_trial\n3\/8\/2021 5:40:52 PM\ndecision = self._process_trial_result(trial, result)\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 666, in _process_trial_result\n3\/8\/2021 5:40:52 PM\nself._callbacks.on_trial_result(\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/callback.py\", line 192, in on_trial_result\n3\/8\/2021 5:40:52 PM\ncallback.on_trial_result(**info)\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/logger.py\", line 393, in on_trial_result\n3\/8\/2021 5:40:52 PM\nself.log_trial_result(iteration, trial, result)\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/logger.py\", line 631, in log_trial_result\n3\/8\/2021 5:40:52 PM\nself._trial_writer[trial].add_scalar(\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/guild\/python_util.py\", line 239, in wrapper\n3\/8\/2021 5:40:52 PM\ncb(wrapped_bound, *args, **kw)\n3\/8\/2021 5:40:52 PM\nTypeError: _handle_scalar() got an unexpected keyword argument 'global_step'\n<\/code><\/pre>\n<p>The line from tune in question in full is <code>self._trial_writer[trial].add_scalar(full_attr, value, global_step=step)<\/code>. This fails.<\/p>\n<p>In my own project I have the following line: <code> logger.add_scalar(f\"{prefix}\/{tag}\", scalar_value, global_step, walltime)<\/code> and this does not fail.<\/p>\n<p>So I went into the ray.tune library and I changed the call to <code>self._trial_writer[trial].add_scalar(full_attr, value, step)<\/code> and reran it. The failure went away.<\/p>\n<p>I dug into the <a href=\"https:\/\/github.com\/guildai\/guildai\/blob\/e9271824141583b96a6de7d1d5cebd44a04e43fe\/guild\/plugins\/summary_util.py#L181\" rel=\"noopener nofollow ugc\">github<\/a> source, and it looks like <code>_handle_scalar()<\/code> is expecting <code>step<\/code> and not <code>global_step<\/code>.<\/p>\n<p>I originally needed help with this but as I wrote this I ended up figuring out the answer. Looks like there\u2019s a potential bug here?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1615255994962,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":483.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/logging-scalars-when-running-ray-tune-tuning-fails-in-a-guild-run\/557",
        "Tool":"Guild AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"log scalar run rai tune tune fail run project bit automat tune pytorch lightn model rai automat appli model log rai tune us summarywrit tensorboardx packag tensorboardx summarywrit log thing project log issu reason fail call add scalar call tune librari trace traceback recent file home davina miniconda env lib python site packag rai tune trial runner line process trial decis self process trial result trial result file home davina miniconda env lib python site packag rai tune trial runner line process trial result self callback trial result file home davina miniconda env lib python site packag rai tune callback line trial result callback trial result info file home davina miniconda env lib python site packag rai tune logger line trial result self log trial result iter trial result file home davina miniconda env lib python site packag rai tune logger line log trial result self trial writer trial add scalar file home davina miniconda env lib python site packag python util line wrapper wrap bound arg typeerror handl scalar got unexpect keyword argument global step line tune question self trial writer trial add scalar attr valu global step step fail project follow line logger add scalar prefix tag scalar valu global step walltim fail went rai tune librari chang self trial writer trial add scalar attr valu step reran failur went awai dug github sourc look like handl scalar expect step global step origin need help wrote end figur answer look like there potenti bug",
        "Question_preprocessed_content":"log scalar run rai tune fail run project bit automat tune model rai automat appli model log rai us summarywrit tensorboardx packag tensorboardx summarywrit log thing project log issu reason fail call call tune librari trace line tune question fail project follow line fail went librari chang reran failur went awai dug github sourc look like expect origin need help wrote end figur answer look like there potenti bug",
        "Question_gpt_summary_original":"The user is encountering an issue with logging scalars when running ray[tune] tuning in a guild run. The logging that ray[tune] uses is a SummaryWriter from tensorboardX package, and the user is also using tensorboardX SummaryWriter for logging other things in their project. However, guild fails with the calls to add_scalar() when it\u2019s called from the tune library. The user found that the failure is due to the fact that _handle_scalar() is expecting \"step\" and not \"global_step\". The user was able to resolve the issue by changing the call to self._trial_writer[trial].add_scalar(full_attr, value, step) in the ray.tune library.",
        "Question_gpt_summary":"user encount issu log scalar run rai tune tune run log rai tune us summarywrit tensorboardx packag user tensorboardx summarywrit log thing project fail call add scalar call tune librari user failur fact handl scalar expect step global step user abl resolv issu chang self trial writer trial add scalar attr valu step rai tune librari"
    },
    {
        "Question_title":"Vertex AI Pipeline: The replica workerpool0-0 exited with a non-zero status of 13",
        "Question_body":"I'm trying to run a vertex AI pipeline (AutoML) using default setting and required service account permission but every time i run that pipeline, i will get failed with above error.\u00a0\n\n\n\u00a0\n\n\nsame i had run with custom will success\u00a0but when running with AutoML for Tabular Classification \/ Regression it will get failed after long process time.",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1678245300000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":222.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Pipeline-The-replica-workerpool0-0-exited-with-a-non\/td-p\/530067\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"pipelin replica workerpool exit non zero statu try run pipelin automl default set requir servic account permiss time run pipelin fail error run custom successbut run automl tabular classif regress fail long process time",
        "Question_preprocessed_content":"pipelin replica exit statu try run pipelin default set requir servic account permiss time run pipelin fail error run custom successbut run automl tabular classif regress fail long process time",
        "Question_gpt_summary_original":"The user is encountering an error while running a Vertex AI pipeline (AutoML) for Tabular Classification\/Regression. The error message indicates that the replica workerpool0-0 exited with a non-zero status of 13. The user has tried running the pipeline with default settings and required service account permission, but it fails every time. However, running the pipeline with custom settings is successful.",
        "Question_gpt_summary":"user encount error run pipelin automl tabular classif regress error messag indic replica workerpool exit non zero statu user tri run pipelin default set requir servic account permiss fail time run pipelin custom set success"
    },
    {
        "Question_title":"Forecast - how to handle missing values in the dataset",
        "Question_body":"I have a few questions regarding data preparation for Forecast.  \n\nI have a dataset with about 3,000 item_id's, the data is recorded on weekdays only (no row for weekends\/holidays), and the forecast horizon is 1 day.\nFor example:  \n[item_id | timestamp | target_value]  \nitem_A | 2022-01-19 (Wed) | 100  \nitem_A | 2022-01-20 (Thurs) | 101  \nitem_A | 2022-01-21 (Fri) | 99  \nitem_A | 2022-01-24 (Mon) | 98  \nitem_A | 2022-01-25 (Tues) | 102  \n\nQ1. Is it recommended that the weekends (1\/22, 1\/23) row is inserted to the dataset with NaN as the target_value?  \n\nQ2. If target_value for a timestamp is NaN, do the RTS attributes get ignored regardless of what value it is?  \n\nQ3. When the Forecast is training, does Forecast recognize that on Friday, the next value to predict is on Monday rather than on Saturday?  \n\nQ4. If an item_id has small time series data points (for instance, global start~end date ranges from 2018 to 2021, but a particular item_id only has data recorded for a couple months in 2020), should I front-fill & back-fill with NaN to match the global start~end range? (I only intend to use the data for training, not creating a forecast)",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1642832665715,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1667925884894,
        "Question_score_count":0.0,
        "Question_view_count":50.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEBKd6SqjSJuVY3xYuMgC5w\/forecast-how-to-handle-missing-values-in-the-dataset",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"forecast handl miss valu dataset question data prepar forecast dataset item data record weekdai row weekend holidai forecast horizon dai exampl item timestamp target valu item wed item thur item fri item mon item tue recommend weekend row insert dataset nan target valu target valu timestamp nan rt attribut ignor regardless valu forecast train forecast recogn fridai valu predict mondai saturdai item small time seri data point instanc global start end date rang particular item data record coupl month nan match global start end rang intend us data train creat forecast",
        "Question_preprocessed_content":"forecast handl miss valu dataset question data prepar forecast dataset data record weekdai forecast horizon dai exampl timestamp recommend weekend row insert dataset nan timestamp nan rt attribut ignor regardless valu forecast train forecast recogn fridai valu predict mondai saturdai small time seri data point nan match global start end rang",
        "Question_gpt_summary_original":"The user has questions regarding data preparation for Forecast. They have a dataset with 3,000 item_id's, recorded on weekdays only, and a forecast horizon of 1 day. The user is unsure whether to insert weekends with NaN as the target_value, whether RTS attributes get ignored if target_value is NaN, whether Forecast recognizes that the next value to predict is on Monday rather than Saturday, and whether to front-fill & back-fill with NaN to match the global start~end range for small time series data points.",
        "Question_gpt_summary":"user question data prepar forecast dataset item record weekdai forecast horizon dai user unsur insert weekend nan target valu rt attribut ignor target valu nan forecast recogn valu predict mondai saturdai nan match global start end rang small time seri data point"
    },
    {
        "Question_title":"How do I create a Dockerfile for BYOC training which allows me to pass in entry_point and source_dir arguments to SM Estimator?",
        "Question_body":"I am following this tutorial: https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/tensorflow_bring_your_own\/container\/Dockerfile\n\nby SM to create a docker image. According to this tutorial, the training scripts are copied into the docker container meaning that you have to rebuild the container if you make any changes.\n\nMy question is: How do I create a Dockerfile such that I don't have to copy my script when I am building the image? Instead, I want to pass in the entry_point and source_dir arguments to the SM Estimator which copies those files upon calling the .fit method.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1663592214344,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1667926452176,
        "Question_score_count":0.0,
        "Question_view_count":76.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUiVxzpWc9TMS0OkcqgqKPmQ\/how-do-i-create-a-dockerfile-for-byoc-training-which-allows-me-to-pass-in-entry-point-and-source-dir-arguments-to-sm-estimator",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"creat dockerfil byoc train allow pass entri point sourc dir argument estim follow tutori http github com aw amazon exampl blob main advanc function tensorflow bring contain dockerfil creat docker imag accord tutori train script copi docker contain mean rebuild contain chang question creat dockerfil copi script build imag instead want pass entri point sourc dir argument estim copi file call fit method",
        "Question_preprocessed_content":"creat dockerfil byoc train allow pass argument estim follow tutori creat docker imag accord tutori train script copi docker contain mean rebuild contain chang question creat dockerfil copi script build imag instead want pass argument estim copi file call fit method",
        "Question_gpt_summary_original":"The user is trying to create a Dockerfile for BYOC training using SM Estimator. They want to pass in entry_point and source_dir arguments to the Estimator instead of copying the training scripts into the docker container, which requires rebuilding the container for any changes made. The user is seeking guidance on how to achieve this.",
        "Question_gpt_summary":"user try creat dockerfil byoc train estim want pass entri point sourc dir argument estim instead copi train script docker contain requir rebuild contain chang user seek guidanc achiev"
    },
    {
        "Question_title":"MLflow 1.25.0 released!",
        "Question_body":"MLflow 1.25.0 has just been released. It includes several major features and improvements:\n\nFeatures:\n\n\n[Tracking] Introduce a new fluent API mlflow.last_active_run() that provides the most recent fluent active run (#5584, @MarkYHZhang)\n[Tracking] Add experiment_names argument to the mlflow.search_runs() API to support searching runs by experiment names (#5564, @r3stl355)\n[Tracking] Add a description parameter to mlflow.start_run() (#5534, @dogeplusplus)\n[Tracking] Add log_every_n_step parameter to mlflow.pytorch.autolog() to control metric logging frequency (#5516, @adamreeve)\n[Tracking] Log pyspark.ml.param.Params values as MLflow parameters during PySpark autologging (#5481, @serena-ruan)\n[Tracking] Add support for pyspark.ml.Transformers to PySpark autologging (#5466, @serena-ruan)\n[Tracking] Add input example and signature autologging for Keras models (#5461, @bali0019)\n[Models] Introduce mlflow.diviner flavor for large-scale\u00a0time series forecasting\u00a0(#5553, @BenWilson2)\n[Models] Add pyfunc.get_model_dependencies() API to retrieve reproducible environment specifications for MLflow Models with the pyfunc flavor (#5503, @WeichenXu123)\n[Models] Add code_paths argument to all model flavors to support packaging custom module code with MLflow Models (#5448, @stevenchen-db)\n[Models] Support creating custom artifacts when evaluating models with mlflow.evaluate() (#5405, #5476 @MarkYHZhang)\n[Models] Add mlflow_version field to MLModel specification (#5515, #5576, @r3stl355)\n[Models] Add support for logging models to preexisting destination directories (#5572, @akshaya-a)\n[Scoring \/ Projects] Introduce --env-manager configuration for specifying environment restoration tools (e.g. conda) and deprecate --no-conda (#5567, @harupy)\n[Scoring] Support restoring model dependencies in mlflow.pyfunc.spark_udf() to ensure accurate predictions (#5487, #5561, @WeichenXu123)\n[Scoring] Add support for numpy.ndarray type inputs to the TensorFlow pyfunc predict() function (#5545, @WeichenXu123)\n[Scoring] Support deployment of MLflow Models to Sagemaker Serverless (#5610, @matthewmayo)\n[UI] Add MLflow version to header beneath logo (#5504, @adamreeve)\n[Artifacts] Introduce a mlflow.artifacts.download_artifacts() API mirroring the functionality of the mlflow artifacts download CLI (#5585, @dbczumar)\n[Artifacts] Introduce environment variables for controlling GCS artifact upload\/download chunk size and timeouts (#5438, #5483, @mokrueger)\n\nBug fixes and documentation updates:\n\n\n[Tracking\/SQLAlchemy] Create an index on run_uuid for PostgreSQL to improve query performance (#5446, @harupy)\n[Tracking] Remove client-side validation of metric, param, tag, and experiment fields (#5593, @BenWilson2)\n[Projects] Support setting the name of the MLflow Run when executing an MLflow Project (#5187, @bramrodenburg)\n[Scoring] Use pandas split orientation for DataFrame inputs to SageMaker deployment predict() API to preserve column ordering (#5522, @dbczumar)\n[Server-Infra] Fix runs search compatibility bugs with PostgreSQL, MySQL, and MSSQL (#5540, @harupy)\n[CLI] Fix a bug in the mlflow-skinny client that caused mlflow --version to fail (#5573, @BenWilson2)\n[Docs] Update guidance and examples for model deployment to AzureML to recommend using the mlflow-azureml package (#5491, @santiagxf)\n\n\nNote: Version 1.25.0 of the MLflow R package has not yet been released. It will be available on CRAN within the next week.\nFor a comprehensive list of changes, see the\u00a0release change log, and check out the latest documentation on\u00a0mlflow.org.",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1649698298000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":null,
        "Question_view_count":10.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/qAKhi-1i0Z8",
        "Tool":"MLflow",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"releas releas includ major featur improv featur track introduc new fluent api activ run provid recent fluent activ run markyhzhang track add experi name argument search run api support search run experi name rstl track add descript paramet start run dogeplusplu track add log step paramet pytorch autolog control metric log frequenc adamreev track log pyspark param param valu paramet pyspark autolog serena ruan track add support pyspark transform pyspark autolog serena ruan track add input exampl signatur autolog kera model bali model introduc divin flavor larg scaletim seri forecast benwilson model add pyfunc model depend api retriev reproduc environ specif model pyfunc flavor weichenxu model add code path argument model flavor support packag custom modul code model stevenchen model support creat custom artifact evalu model evalu markyhzhang model add version field mlmodel specif rstl model add support log model preexist destin directori akshaya score project introduc env manag configur specifi environ restor tool conda deprec conda harupi score support restor model depend pyfunc spark udf ensur accur predict weichenxu score add support numpi ndarrai type input tensorflow pyfunc predict function weichenxu score support deploy model serverless matthewmayo add version header beneath logo adamreev artifact introduc artifact download artifact api mirror function artifact download cli dbczumar artifact introduc environ variabl control gc artifact upload download chunk size timeout mokrueg bug fix document updat track sqlalchemi creat index run uuid postgresql improv queri perform harupi track remov client valid metric param tag experi field benwilson project support set run execut project bramrodenburg score us panda split orient datafram input deploy predict api preserv column order dbczumar server infra fix run search compat bug postgresql mysql mssql harupi cli fix bug skinni client caus version fail benwilson doc updat guidanc exampl model deploy recommend packag santiagxf note version packag releas avail cran week comprehens list chang thereleas chang log check latest document org",
        "Question_preprocessed_content":"releas releas includ major featur improv featur track introduc new fluent api provid recent fluent activ run track add argument api support search run experi name track add descript paramet track add paramet control metric log frequenc track log valu paramet pyspark autolog track add support pyspark autolog track add input exampl signatur autolog kera model model introduc divin flavor seri forecast model add api retriev reproduc environ specif model pyfunc flavor model add argument model flavor support packag custom modul code model model support creat custom artifact evalu model evalu model add field mlmodel specif model add support log model preexist destin directori score project introduc configur specifi environ restor tool deprec score support restor model depend ensur accur predict score add support type input tensorflow pyfunc predict function score support deploy model serverless add version header beneath logo artifact introduc api mirror function artifact download cli artifact introduc environ variabl control gc artifact chunk size timeout bug fix document updat creat index postgresql improv queri perform track remov valid metric param tag experi field project support set run execut project score us panda split orient datafram input deploy predict api preserv column order fix run search compat bug postgresql mysql mssql cli fix bug skinni client caus fail doc updat guidanc exampl model deploy recommend packag note version packag releas avail cran week comprehens list chang thereleas chang log check latest document",
        "Question_gpt_summary_original":"MLflow 1.25.0 has been released with several major features and improvements. These include new APIs for tracking, models, scoring, artifacts, and bug fixes. The release also introduces a new fluent API, mlflow.last_active_run(), and a description parameter to mlflow.start_run(). The release also adds support for logging models to preexisting destination directories and introduces a mlflow.artifacts.download_artifacts() API. The release also includes bug fixes and documentation updates.",
        "Question_gpt_summary":"releas major featur improv includ new api track model score artifact bug fix releas introduc new fluent api activ run descript paramet start run releas add support log model preexist destin directori introduc artifact download artifact api releas includ bug fix document updat"
    },
    {
        "Question_title":"Migrating Runs to MLFlow 0.9",
        "Question_body":"<p>we have been using MLFlow 0.8.2 (with a local file store) for a while, and I was happy to see the release of MLFlow 0.9. After upgrading to the new version, I realized that pointing the MLFLow server to the old file store leads to a non-working web UI (I just see some image of a waterfall).<\/p>\n\n<p>Is there a recommendation for proper migration of data when upgrading?<\/p>\n\n<p>Thanks a lot in advance,<\/p>\n\n<p>Da<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1555442975623,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":49.0,
        "Poster_created_time":1403502510470,
        "Poster_location":"M\u00fcnchen, Deutschland",
        "Poster_reputation_count":41.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55715401",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"migrat run local file store happi releas upgrad new version realiz point server old file store lead non work web imag waterfal recommend proper migrat data upgrad thank lot advanc",
        "Question_preprocessed_content":"migrat run happi releas upgrad new version realiz point server old file store lead web recommend proper migrat data upgrad thank lot advanc",
        "Question_gpt_summary_original":"The user is facing challenges while migrating their runs from MLFlow 0.8.2 to MLFlow 0.9. They have noticed that pointing the MLFlow server to the old file store leads to a non-working web UI. The user is seeking recommendations for proper migration of data when upgrading.",
        "Question_gpt_summary":"user face challeng migrat run notic point server old file store lead non work web user seek recommend proper migrat data upgrad"
    },
    {
        "Question_title":"Why is the field \"compute target\" for data drift monitoring in ML studio still blank whereas I have a compute instance?",
        "Question_body":"<p>I have created a compute instance:    <\/p>\n<p><strong>Virtual machine size<\/strong>    <br \/>\nSTANDARD_DS3_V2 (4 Cores, 14 GB RAM, 28 GB Disk)    <\/p>\n<p><strong>Processing Unit<\/strong>    <br \/>\nCPU - General purpose    <\/p>\n<p>But, I'm not able to access it when trying to set it for data drift monitoring.    <br \/>\nThe dropdown list is empty. I can't understand why. Can you help me please?    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/28475-datadrift.png?platform=QnA\" alt=\"28475-datadrift.png\" \/>    <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1601275746390,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>I found the answer. You must give a <strong>cluster<\/strong> compute instance to do data drift in Azure Machine Learning Studio.  <\/p>\n",
        "Answer_comment_count":1.0,
        "Question_closed_time":1601279018463,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/109397\/why-is-the-field-compute-target-for-data-drift-mon",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"field comput target data drift monitor studio blank comput instanc creat comput instanc virtual machin size standard core ram disk process unit cpu gener purpos abl access try set data drift monitor dropdown list understand help",
        "Question_preprocessed_content":"field comput target data drift monitor studio blank comput instanc creat comput instanc virtual machin size process unit cpu gener purpos abl access try set data drift monitor dropdown list understand help",
        "Question_gpt_summary_original":"The user has created a compute instance in ML studio but is unable to access it when trying to set it for data drift monitoring as the \"compute target\" field is still blank and the dropdown list is empty. The user is seeking help to understand why this is happening.",
        "Question_gpt_summary":"user creat comput instanc studio unabl access try set data drift monitor comput target field blank dropdown list user seek help understand happen"
    },
    {
        "Question_title":"AzureML Model Register",
        "Question_body":"<p>I was trying to register a model using the <code>Run<\/code> Class like this:<\/p>\n<pre><code>model = run.register_model(\n    model_name=model_name,\n    model_path=model_path)\n<\/code><\/pre>\n<p>Errors with message: <code>Could not locate the provided model_path ... in the set of files uploaded to the run...<\/code><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1643643837027,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":319.0,
        "Poster_created_time":1589293508567,
        "Poster_location":null,
        "Poster_reputation_count":833.0,
        "Poster_view_count":55.0,
        "Answer_body":"<p>The only way I found to fix the issue was to use the <code>Model<\/code> Class instead:<\/p>\n<pre><code>        model = Model.register(\n            workspace=ws,\n            model_name=model_name,\n            model_path=model_path,\n            model_framework=Model.Framework.SCIKITLEARN,\n            model_framework_version=sklearn.__version__,\n            description='Model Deescription',\n            tags={'Name' : 'ModelName', 'Type' : 'Production'},\n            model_framework=Model.Framework.SCIKITLEARN,\n            model_framework_version='1.0'\n            )\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1643643837027,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1589293508567,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70928761",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"model regist try regist model run class like model run regist model model model model path model path error messag locat provid model path set file upload run",
        "Question_preprocessed_content":"model regist try regist model class like error messag",
        "Question_gpt_summary_original":"The user encountered an error while trying to register a model using the Run Class in AzureML. The error message stated that the provided model_path could not be located in the uploaded files.",
        "Question_gpt_summary":"user encount error try regist model run class error messag state provid model path locat upload file"
    },
    {
        "Question_title":"Properly return a label in post-annotation lambda for AWS SageMaker Ground Truth custom labeling job",
        "Question_body":"<p>I'm working on a SageMaker labeling job with custom datatypes. For some reason though, I'm not getting the correct label in the AWS web console. It should have the selected label which is &quot;Native&quot;, but instead, I'm getting the <a href=\"https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py#L65\" rel=\"nofollow noreferrer\"><code>&lt;labelattributename&gt;<\/code><\/a> which is &quot;new-test-14&quot;.<\/p>\n<p>After Ground Truth runs the post-annotation lambda, it seems to modify the metadata before returning a data object. The data object it returns doesn't contain a class-name key inside the metadata attribute, even when I hard-code the lambda to return an object that contains it.<\/p>\n<p>My manifest file looks like this:<\/p>\n<pre><code>{&quot;source-ref&quot; : &quot;s3:\/\/&lt;file-name&gt;&quot;, &quot;text&quot; : &quot;Hello world&quot;}\n{&quot;source-ref&quot; : &quot;s3:\/\/&quot;&lt;file-name&gt;&quot;, &quot;text&quot; : &quot;Hello world&quot;}\n<\/code><\/pre>\n<p>And the worker response looks like this:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{&quot;answers&quot;:[{&quot;acceptanceTime&quot;:&quot;2021-05-18T16:08:29.473Z&quot;,&quot;answerContent&quot;:{&quot;new-test-14&quot;:{&quot;label&quot;:&quot;Native&quot;}},&quot;submissionTime&quot;:&quot;2021-05-18T16:09:15.960Z&quot;,&quot;timeSpentInSeconds&quot;:46.487,&quot;workerId&quot;:&quot;private.us-east-1.ea05a03fcd679cbb&quot;,&quot;workerMetadata&quot;:{&quot;identityData&quot;:{&quot;identityProviderType&quot;:&quot;Cognito&quot;,&quot;issuer&quot;:&quot;https:\/\/cognito-idp.us-east-1.amazonaws.com\/us-east-1_XPxQ9txEq&quot;,&quot;sub&quot;:&quot;edc59ce1-e09d-4551-9e0d-a240465ea14a&quot;}}}]}\n<\/code><\/pre>\n<p>That worker response gets processed by my post-annotation lambda which is modeled after <a href=\"https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py\" rel=\"nofollow noreferrer\">this aws sample ground truth recipe<\/a>. Here's my code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import json\nimport sys\nimport boto3\nfrom datetime import datetime\n\n\n\ndef lambda_handler(event, context):\n\n\n    # Event received\n    print(&quot;Received event: &quot; + json.dumps(event, indent=2))\n\n    labeling_job_arn = event[&quot;labelingJobArn&quot;]\n    label_attribute_name = event[&quot;labelAttributeName&quot;]\n\n    label_categories = None\n    if &quot;label_categories&quot; in event:\n        label_categories = event[&quot;labelCategories&quot;]\n        print(&quot; Label Categories are : &quot; + label_categories)\n\n    payload = event[&quot;payload&quot;]\n    role_arn = event[&quot;roleArn&quot;]\n\n    output_config = None # Output s3 location. You can choose to write your annotation to this location\n    if &quot;outputConfig&quot; in event:\n        output_config = event[&quot;outputConfig&quot;]\n\n    # If you specified a KMS key in your labeling job, you can use the key to write\n    # consolidated_output to s3 location specified in outputConfig.\n    # kms_key_id = None\n    # if &quot;kmsKeyId&quot; in event:\n    #     kms_key_id = event[&quot;kmsKeyId&quot;]\n\n    # # Create s3 client object\n    # s3_client = S3Client(role_arn, kms_key_id)\n    s3_client = boto3.client('s3')\n\n    # Perform consolidation\n    return do_consolidation(labeling_job_arn, payload, label_attribute_name, s3_client)\n\n\ndef do_consolidation(labeling_job_arn, payload, label_attribute_name, s3_client):\n    &quot;&quot;&quot;\n        Core Logic for consolidation\n\n    :param labeling_job_arn: labeling job ARN\n    :param payload:  payload data for consolidation\n    :param label_attribute_name: identifier for labels in output JSON\n    :param s3_client: S3 helper class\n    :return: output JSON string\n    &quot;&quot;&quot;\n\n    # Extract payload data\n    if &quot;s3Uri&quot; in payload:\n        s3_ref = payload[&quot;s3Uri&quot;]\n        payload_bucket, payload_key = s3_ref.split('\/',2)[-1].split('\/',1)\n        payload = json.loads(s3_client.get_object(Bucket=payload_bucket, Key=payload_key)['Body'].read())\n#         print(payload)\n\n    # Payload data contains a list of data objects.\n    # Iterate over it to consolidate annotations for individual data object.\n    consolidated_output = []\n    success_count = 0  # Number of data objects that were successfully consolidated\n    failure_count = 0  # Number of data objects that failed in consolidation\n\n    for p in range(len(payload)):\n        response = None\n\n        dataset_object_id = payload[p]['datasetObjectId']\n        log_prefix = &quot;[{}] data object id [{}] :&quot;.format(labeling_job_arn, dataset_object_id)\n        print(&quot;{} Consolidating annotations BEGIN &quot;.format(log_prefix))\n\n        annotations = payload[p]['annotations']\n#             print(&quot;{} Received Annotations from all workers {}&quot;.format(log_prefix, annotations))\n\n        # Iterate over annotations. Log all annotation to your CloudWatch logs\n        annotationsFromAllWorkers = []\n        for i in range(len(annotations)):\n            worker_id = annotations[i][&quot;workerId&quot;]\n            anotation_data = annotations[i][&quot;annotationData&quot;]\n            annotation_content = anotation_data[&quot;content&quot;]\n            annotation_content_json = json.loads(annotation_content)\n            annotation_job = annotation_content_json[&quot;new_test&quot;]\n            annotation_label = annotation_job[&quot;label&quot;]\n            consolidated_annotation= {\n                &quot;workerId&quot;: worker_id,\n                &quot;annotationData&quot;: {\n                    &quot;content&quot;: {\n                        &quot;annotatedResult&quot;: {\n                            &quot;instances&quot;: [{&quot;label&quot;:annotation_label }]    \n                        }\n                    }\n                }\n            }\n            annotationsFromAllWorkers.append(consolidated_annotation)\n\n        consolidated_annotation = {&quot;annotationsFromAllWorkers&quot;: annotationsFromAllWorkers} # TODO : Add your consolidation logic\n\n        # Build consolidation response object for an individual data object\n        response = {\n            &quot;datasetObjectId&quot;: dataset_object_id,\n            &quot;consolidatedAnnotation&quot;: {\n                &quot;content&quot;: {\n                    label_attribute_name: consolidated_annotation,\n                    label_attribute_name+ &quot;-metadata&quot;: {\n                        &quot;class-name&quot;: &quot;Native&quot;,\n                        &quot;confidence&quot;: 0.00,\n                        &quot;human-annotated&quot;: &quot;yes&quot;,\n                        &quot;creation-date&quot;: datetime.strftime(datetime.now(), &quot;%Y-%m-%dT%H:%M:%S&quot;),\n                        &quot;type&quot;: &quot;groundtruth\/custom&quot;\n                    }\n \n                }\n            }\n        }\n\n        success_count += 1\n#             print(&quot;{} Consolidating annotations END &quot;.format(log_prefix))\n\n        # Append individual data object response to the list of responses.\n        if response is not None:\n            consolidated_output.append(response)\n\n\n        failure_count += 1\n        print(&quot; Consolidation failed for dataobject {}&quot;.format(p))\n        print(&quot; Unexpected error: Consolidation failed.&quot; + str(sys.exc_info()[0]))\n\n    print(&quot;Consolidation Complete. Success Count {}  Failure Count {}&quot;.format(success_count, failure_count))\n\n    print(&quot; -- Consolidated Output -- &quot;)\n    print(consolidated_output)\n    print(&quot; ------------------------- &quot;)\n    return consolidated_output\n<\/code><\/pre>\n<p>As you can see above, the <code>do_consolidation<\/code> method returns an object hard-coded to include a class-name of &quot;Native&quot;, and the <code>lambda_handler<\/code> method returns that same object. Here's the post-annotation function response:<\/p>\n<pre><code>[{\n    &quot;datasetObjectId&quot;: &quot;4&quot;,\n    &quot;consolidatedAnnotation&quot;: {\n        &quot;content&quot;: {\n            &quot;new-test-14&quot;: {\n                &quot;annotationsFromAllWorkers&quot;: [{\n                    &quot;workerId&quot;: &quot;private.us-east-1.ea05a03fcd679cbb&quot;,\n                    &quot;annotationData&quot;: {\n                        &quot;content&quot;: {\n                            &quot;annotatedResult&quot;: {\n                                &quot;instances&quot;: [{\n                                    &quot;label&quot;: &quot;Native&quot;\n                                }]\n                            }\n                        }\n                    }\n                }]\n            },\n            &quot;new-test-14-metadata&quot;: {\n                &quot;class-name&quot;: &quot;Native&quot;,\n                &quot;confidence&quot;: 0,\n                &quot;human-annotated&quot;: &quot;yes&quot;,\n                &quot;creation-date&quot;: &quot;2021-05-19T07:06:06&quot;,\n                &quot;type&quot;: &quot;groundtruth\/custom&quot;\n            }\n        }\n    }\n}]\n<\/code><\/pre>\n<p>As you can see, the post-annotation function return value has the class-name of &quot;Native&quot; in the metadata so I would expect the class-name to be present in the data object metadata, but it's not. And here's a screenshot of the data object summary:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wHpEl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wHpEl.png\" alt=\"Labeled object summary\" \/><\/a><\/p>\n<p>It seems like Ground Truth overwrote the metadata, and now the object doesn't contain the correct label. I think perhaps that's why my label is coming through as the label attribute name &quot;new-test-14&quot; instead of as the correct label &quot;Native&quot;. Here's a screenshot of the labeling job in the AWS web console:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/yOdfP.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/yOdfP.png\" alt=\"labeling job\" \/><\/a><\/p>\n<p>The web console is supposed to show the label <strong>&quot;Native&quot;<\/strong> inside the &quot;Label&quot; column but instead I'm getting the <a href=\"https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py#L65\" rel=\"nofollow noreferrer\"><code>&lt;labelattributename&gt;<\/code><\/a> <strong>&quot;new-test-14&quot;<\/strong> in the label column.<\/p>\n<p>Here is the output.manifest file generated by Ground Truth at the end:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;source-ref&quot;: &quot;s3:\/\/&lt;file-name&gt;&quot;,\n    &quot;text&quot;: &quot;Hello world&quot;,\n    &quot;new-test-14&quot;: {\n        &quot;annotationsFromAllWorkers&quot;: [{\n            &quot;workerId&quot;: &quot;private.us-east-1.ea05a03fcd679ert&quot;,\n            &quot;annotationData&quot;: {\n                &quot;content&quot;: {\n                    &quot;annotatedResult&quot;: {\n                        &quot;label&quot;: &quot;Native&quot;\n                    }\n                }\n            }\n        }]\n    },\n    &quot;new-test-14-metadata&quot;: {\n        &quot;type&quot;: &quot;groundtruth\/custom&quot;,\n        &quot;job-name&quot;: &quot;new-test-14&quot;,\n        &quot;human-annotated&quot;: &quot;yes&quot;,\n        &quot;creation-date&quot;: &quot;2021-05-18T12:34:17.400000&quot;\n    }\n}\n<\/code><\/pre>\n<p>What should I return from the Post-Annotation function? Am I missing something in my response? How do I get the proper label to appear in the AWS web console?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_created_time":1621410650207,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1622189648607,
        "Question_score_count":3.0,
        "Question_view_count":430.0,
        "Poster_created_time":1621405531012,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67599026",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"properli return label post annot lambda ground truth custom label job work label job custom datatyp reason get correct label aw web consol select label nativ instead get new test ground truth run post annot lambda modifi metadata return data object data object return contain class kei insid metadata attribut hard code lambda return object contain manifest file look like sourc ref text hello world sourc ref text hello world worker respons look like answer acceptancetim answercont new test label nativ submissiontim timespentinsecond workerid privat east eaafcdcbb workermetadata identitydata identityprovidertyp cognito issuer http cognito idp east amazonaw com east xpxqtxeq sub edcc aeaa worker respons get process post annot lambda model aw sampl ground truth recip code import json import sy import boto datetim import datetim def lambda handler event context event receiv print receiv event json dump event indent label job arn event labelingjobarn label attribut event labelattributenam label categori label categori event label categori event labelcategori print label categori label categori payload event payload role arn event rolearn output config output locat choos write annot locat outputconfig event output config event outputconfig specifi km kei label job us kei write consolid output locat specifi outputconfig km kei kmskeyid event km kei event kmskeyid creat client object client sclient role arn km kei client boto client perform consolid return consolid label job arn payload label attribut client def consolid label job arn payload label attribut client core logic consolid param label job arn label job arn param payload payload data consolid param label attribut identifi label output json param client helper class return output json string extract payload data suri payload ref payload suri payload bucket payload kei ref split split payload json load client object bucket payload bucket kei payload kei bodi read print payload payload data contain list data object iter consolid annot individu data object consolid output success count number data object successfulli consolid failur count number data object fail consolid rang len payload respons dataset object payload datasetobjectid log prefix data object format label job arn dataset object print consolid annot begin format log prefix annot payload annot print receiv annot worker format log prefix annot iter annot log annot cloudwatch log annotationsfromallwork rang len annot worker annot workerid anot data annot annotationdata annot content anot data content annot content json json load annot content annot job annot content json new test annot label annot job label consolid annot workerid worker annotationdata content annotatedresult instanc label annot label annotationsfromallwork append consolid annot consolid annot annotationsfromallwork annotationsfromallwork todo add consolid logic build consolid respons object individu data object respons datasetobjectid dataset object consolidatedannot content label attribut consolid annot label attribut metadata class nativ confid human annot ye creation date datetim strftime datetim type groundtruth custom success count print consolid annot end format log prefix append individu data object respons list respons respons consolid output append respons failur count print consolid fail dataobject format print unexpect error consolid fail str sy exc info print consolid complet success count failur count format success count failur count print consolid output print consolid output print return consolid output consolid method return object hard code includ class nativ lambda handler method return object post annot function respons datasetobjectid consolidatedannot content new test annotationsfromallwork workerid privat east eaafcdcbb annotationdata content annotatedresult instanc label nativ new test metadata class nativ confid human annot ye creation date type groundtruth custom post annot function return valu class nativ metadata expect class present data object metadata screenshot data object summari like ground truth overwrot metadata object contain correct label think label come label attribut new test instead correct label nativ screenshot label job aw web consol web consol suppos label nativ insid label column instead get new test label column output manifest file gener ground truth end sourc ref text hello world new test annotationsfromallwork workerid privat east eaafcdert annotationdata content annotatedresult label nativ new test metadata type groundtruth custom job new test human annot ye creation date return post annot function miss respons proper label appear aw web consol",
        "Question_preprocessed_content":"properli return label lambda ground truth custom label job work label job custom datatyp reason get correct label aw web consol select label nativ instead get ground truth run lambda modifi metadata return data object data object return contain kei insid metadata attribut lambda return object contain manifest file look like worker respons look like worker respons get process lambda model aw sampl ground truth recip code method return object includ nativ method return object function respons function return valu nativ metadata expect present data object metadata screenshot data object summari like ground truth overwrot metadata object contain correct label think label come label attribut instead correct label nativ screenshot label job aw web consol web consol suppos label nativ insid label column instead get label column file gener ground truth end return function miss respons proper label appear aw web consol",
        "Question_gpt_summary_original":"The user is encountering an issue with a SageMaker labeling job with custom datatypes. The post-annotation lambda is not returning the correct label in the AWS web console, instead returning the label attribute name. The metadata is being modified by Ground Truth, and the data object does not contain the correct label. The user is unsure of what to return from the Post-Annotation function and how to get the proper label to appear in the AWS web console.",
        "Question_gpt_summary":"user encount issu label job custom datatyp post annot lambda return correct label aw web consol instead return label attribut metadata modifi ground truth data object contain correct label user unsur return post annot function proper label appear aw web consol"
    },
    {
        "Question_title":"How to disable TLS 1.0 and 1.1 in dialogflow?",
        "Question_body":"Need to disable TLS 1.0 and 1.1 for oauth api and events api in the dialogflow. We get those apis while integrating with slack.",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1663317120000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":53.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-disable-TLS-1-0-and-1-1-in-dialogflow\/td-p\/467528\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"disabl tl dialogflow need disabl tl oauth api event api dialogflow api integr slack",
        "Question_preprocessed_content":"disabl tl dialogflow need disabl tl oauth api event api dialogflow api integr slack",
        "Question_gpt_summary_original":"The user needs to disable TLS 1.0 and 1.1 for the oauth and events APIs in Dialogflow, which are obtained while integrating with Slack.",
        "Question_gpt_summary":"user need disabl tl oauth event api dialogflow obtain integr slack"
    },
    {
        "Question_title":"Cannot create academic team",
        "Question_body":"<p>Unfortunately, I am not able to create an academic team, although my university email address is added in to the profile: b.khaertdinov[at]maastrichtuniversity.nl<\/p>\n<p>Any tips on how to solve this issue are very welcome!<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_created_time":1675870247160,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":318.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/cannot-create-academic-team\/3849",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"creat academ team unfortun abl creat academ team univers email address ad profil khaertdinov maastrichtunivers tip solv issu welcom",
        "Question_preprocessed_content":"creat academ team unfortun abl creat academ team univers email address ad profil tip solv issu welcom",
        "Question_gpt_summary_original":"The user is unable to create an academic team despite having added their university email address to their profile and is seeking tips to resolve the issue.",
        "Question_gpt_summary":"user unabl creat academ team despit have ad univers email address profil seek tip resolv issu"
    },
    {
        "Question_title":"Model Monitor Capture data - EndpointOutput Encoding is BASE64",
        "Question_body":"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-data-capture-endpoint.html\n\nI have followed the steps mentioned in this and it appears I cannot change the encoding for EndpointOutput in datacapture file. It's coming BASE64 for xgboost model. I am using latest version 1.2.3.\n\nFor monitor scheduler it required both EndpointOutput and EndpointInput to have the same encoding. My EndpointInput  is CSV but EndpointOutput is coming to be BASE64 and nothing can change it.\n\nThis is causing issue while run of analyzer. After baseline is generated and data is captured, when monitoring schedule runs the analyzer it throws error of Encoding mismatch. For it to run EndpointOutput and EndpointInput should have same encoding.\n\nI saw we cannot do anything to change the encoding of output. I used LightGBM, CatBoost algorithms also and found for these EndpointOuput encoding is JSON, which is readable but still not solving the purpose.\n\nIs there a way we can change EndpointOutput Encoding for DataCapture.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1673956972508,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1674302800224,
        "Question_score_count":0.0,
        "Question_view_count":69.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"Output encoding can be configured by using the [CaptureContentTypeHeader \nin EndpointConfig.DataCaptureConfig](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DataCaptureConfig.html#sagemaker-Type-DataCaptureConfig-CaptureContentTypeHeader). I believe since this is not being set, default encoding i.e. base64 is being used. \n\nPlease try once with this attribute set as below:\n```\n\"CaptureContentTypeHeader\": { \n         \"CsvContentTypes\": [ \"text\/csv\" ]\n      }\n```\n> Assuming that content_type\/accept is \"text_csv\" for the concerned model.",
        "Answer_comment_count":1.0,
        "Question_closed_time":1675066186694,
        "Answer_last_edit_time":1675066186694,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGSFVfrFJS_KsdrOMeepDPg\/model-monitor-capture-data-endpointoutput-encoding-is-base64",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_original_content":"model monitor captur data endpointoutput encod base http doc aw amazon com latest model monitor data captur endpoint html follow step mention appear chang encod endpointoutput datacaptur file come base xgboost model latest version monitor schedul requir endpointoutput endpointinput encod endpointinput csv endpointoutput come base chang caus issu run analyz baselin gener data captur monitor schedul run analyz throw error encod mismatch run endpointoutput endpointinput encod saw chang encod output lightgbm catboost algorithm endpointouput encod json readabl solv purpos wai chang endpointoutput encod datacaptur",
        "Question_preprocessed_content":"model monitor captur data endpointoutput encod base follow step mention appear chang encod endpointoutput datacaptur file come base xgboost model latest version monitor schedul requir endpointoutput endpointinput encod endpointinput csv endpointoutput come base chang caus issu run analyz baselin gener data captur monitor schedul run analyz throw error encod mismatch run endpointoutput endpointinput encod saw chang encod output lightgbm catboost algorithm endpointouput encod json readabl solv purpos wai chang endpointoutput encod datacaptur",
        "Question_gpt_summary_original":"The user is facing an issue with the EndpointOutput encoding in data capture files while using Amazon SageMaker. The EndpointOutput is coming as BASE64, which cannot be changed, causing an encoding mismatch error while running the analyzer. The user has tried using different algorithms, but the EndpointOutput encoding remains the same. The user is seeking a solution to change the EndpointOutput encoding for DataCapture.",
        "Question_gpt_summary":"user face issu endpointoutput encod data captur file endpointoutput come base chang caus encod mismatch error run analyz user tri differ algorithm endpointoutput encod remain user seek solut chang endpointoutput encod datacaptur"
    },
    {
        "Question_title":"How to return a Dataframe in Azure ML from a python script",
        "Question_body":"<p>I have the below Python script. And I'm trying to return the new Dataframe created. Unfortunately, I'm getting \"NotImplementedError:\"<\/p>\n\n<p>Please let me know, how to return a Dataframe in Azure ML from a python script<\/p>\n\n<p>Code:<\/p>\n\n<pre><code>def azureml_main(df) :\n\n    therm_patterns,therm_missed_patterns = thermostat_phase(df)\n    th_pat = json.loads(therm_patterns)\n    missed_th_pat = json.loads(therm_missed_patterns)\n\n    light_patterns, light_missed_patterns = light_phase(df)\n    lt_pat = json.loads(light_patterns)\n    missed_lt_pat = json.loads(light_missed_patterns)\n\n    for j in range (0,len(lt_pat)):\n        for i in range (0,len(lt_pat[0]['John'])):\n            th_pat[0]['John'].append(lt_pat[0]['John'][i])\n    for j in range (0,len(lt_pat)):\n        for i in range (0,len(lt_pat[0]['Jane'])):\n            th_pat[0]['Jane'].append(lt_pat[0]['Jane'][i])\n    for j in range (0,len(lt_pat)):\n        for i in range (0,len(lt_pat[0]['Jen'])):\n            th_pat[0]['Jen'].append(lt_pat[0]['Jen'][i])\n\n    for j in range (0,len(missed_lt_pat)):\n        for i in range (0,len(missed_lt_pat[0]['John'])):\n            missed_th_pat[0]['John'].append(missed_lt_pat[0]['John'][i])\n    for j in range (0,len(missed_lt_pat)):\n        for i in range (0,len(missed_lt_pat[0]['Jane'])):\n            missed_th_pat[0]['Jane'].append(missed_lt_pat[0]['Jane'][i])\n    for j in range (0,len(missed_lt_pat)):\n        for i in range (0,len(missed_lt_pat[0]['Jen'])):\n            missed_th_pat[0]['Jen'].append(missed_lt_pat[0]['Jen'][i])\n\n    output = json.dumps(th_pat)\n\n    df = pd.DataFrame.from_dict({k: v[0] for k, v in json.loads(output) \n    [0].items()}, 'index' ).rename_axis('User').reset_index()\n\n\n    return df         \n<\/code><\/pre>\n\n<p>Error:<\/p>\n\n<pre><code>  Error 0085: The following error occurred during script evaluation, please  \n  view the output log for more information:\n\n   ---------- Start of error message from Python interpreter ----------\n   Caught exception while executing function: Traceback (most recent call  \n   last):\n\n       File \"C:\\server\\invokepy.py\", line 211, in batch\n       xdrutils.XDRUtils.DataFrameToRFile(outlist[i], outfiles[i], True)\n\n       File \"C:\\server\\XDRReader\\xdrutils.py\", line 54, in DataFrameToRFile\n       xdrwriter.write_attribute_list(attributes)\n\n       File \"C:\\server\\XDRReader\\xdrwriter2.py\", line 155, in   \n       write_attribute_list\n       self.write_object(value)\n\n       File \"C:\\server\\XDRReader\\xdrwriter2.py\", line 215, in write_object\n       write_func(flags, converted, missingIndices)\n\n       File \"C:\\server\\XDRReader\\xdrwriter2.py\", line 185, in write_objects\n       self.write_object(value)\n\n       File \"C:\\server\\XDRReader\\xdrwriter2.py\", line 200, in write_object\n       raise NotImplementedError('Python Bridge conversion table not    \n       implemented for type [{0}]'.format(value.getType()))\n\n       NotImplementedError: Python Bridge conversion table not implemented  \n       for type [&lt;type 'list'&gt;]\n\n       Process returned with non-zero exit code 1\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1484124401607,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1484584933627,
        "Question_score_count":3.0,
        "Question_view_count":1101.0,
        "Poster_created_time":1479194627132,
        "Poster_location":null,
        "Poster_reputation_count":2713.0,
        "Poster_view_count":358.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41586275",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"return datafram python script python script try return new datafram creat unfortun get notimplementederror let know return datafram python script code def main therm pattern therm miss pattern thermostat phase pat json load therm pattern miss pat json load therm miss pattern light pattern light miss pattern light phase pat json load light pattern miss pat json load light miss pattern rang len pat rang len pat john pat john append pat john rang len pat rang len pat jane pat jane append pat jane rang len pat rang len pat jen pat jen append pat jen rang len miss pat rang len miss pat john miss pat john append miss pat john rang len miss pat rang len miss pat jane miss pat jane append miss pat jane rang len miss pat rang len miss pat jen miss pat jen append miss pat jen output json dump pat datafram dict json load output item index renam axi user reset index return error error follow error occur script evalu view output log inform start error messag python interpret caught except execut function traceback recent file server invokepi line batch xdrutil xdrutil dataframetorfil outlist outfil true file server xdrreader xdrutil line dataframetorfil xdrwriter write attribut list attribut file server xdrreader xdrwriter line write attribut list self write object valu file server xdrreader xdrwriter line write object write func flag convert missingindic file server xdrreader xdrwriter line write object self write object valu file server xdrreader xdrwriter line write object rais notimplementederror python bridg convers tabl implement type format valu gettyp notimplementederror python bridg convers tabl implement type process return non zero exit code",
        "Question_preprocessed_content":"return datafram python script python script try return new datafram creat unfortun get notimplementederror let know return datafram python script code error",
        "Question_gpt_summary_original":"The user is trying to return a new Dataframe created in a Python script in Azure ML, but is encountering a \"NotImplementedError\" and is unable to do so. The error message suggests that the Python Bridge conversion table is not implemented for the type \"list\".",
        "Question_gpt_summary":"user try return new datafram creat python script encount notimplementederror unabl error messag suggest python bridg convers tabl implement type list"
    },
    {
        "Question_title":"Custom computer vision for surface calculations on digital floor maps",
        "Question_body":"<p>I am working on a project to calculate surface area from digital floor maps. I am currently experimenting with azure cognitive services - Custom computer vision. However I don't know if this is the right track.   <\/p>\n<p>If possible I would like to use a existent tool instead of reinventing the wheel. Has anyone experience with this and can provide me with some guidance?  <\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1631108624923,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>Sure, thanks for clarifying. I agree, an out of box approach would be to use computer vision or custom vision to detect objects and then use the metadata to calculate the surface area. I haven't seen an existing solution using Azure Cognitive services at the moment, so you'd most likely have to use a heuristic approach.<\/p>\n",
        "Answer_comment_count":0.0,
        "Question_closed_time":1631217055323,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/544199\/custom-computer-vision-for-surface-calculations-on",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"custom vision surfac calcul digit floor map work project calcul surfac area digit floor map current experi azur cognit servic custom vision know right track possibl like us exist tool instead reinvent wheel experi provid guidanc",
        "Question_preprocessed_content":"custom vision surfac calcul digit floor map work project calcul surfac area digit floor map current experi azur cognit servic custom vision know right track possibl like us exist tool instead reinvent wheel experi provid guidanc",
        "Question_gpt_summary_original":"The user is working on a project to calculate surface area from digital floor maps and is experimenting with Azure Cognitive Services - Custom computer vision. They are unsure if this is the right approach and are seeking guidance on whether an existing tool can be used instead.",
        "Question_gpt_summary":"user work project calcul surfac area digit floor map experi azur cognit servic custom vision unsur right approach seek guidanc exist tool instead"
    },
    {
        "Question_title":"Compare different architectures for a same task",
        "Question_body":"<p>Hey there.<\/p>\n<p>I discovered W&amp;B recently and decided to use it for my current research project. The thing is that I have a specific task to solve and would like to evaluate a bunch of completely different model architectures, having different sets of hyper-parameters.<\/p>\n<p>Most of the online resources and tutorials I\u2019ve found only shows examples of W&amp;B usage to evaluate different experiments with different params selections (e.g. optimized using sweeps). However none of the examples I found explained how to optimally organize a W&amp;B project including different architectures to solve the same task, and thus being able to compare in a glimpse the different performances in a single view \/ report.<\/p>\n<p>My idea was to make use of the job_type flag and group every architecture instances together under a same job_type flag. But still seems like not the best solution and was wondering if there is some special feature or built-in tool that I\u2019ve not noticed yet (or even good practices?).<\/p>\n<p>(Other than that, W&amp;B looks really insane).<\/p>\n<p>Many thanks <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/smile.png?v=12\" title=\":smile:\" class=\"emoji\" alt=\":smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1657202004620,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":505.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/compare-different-architectures-for-a-same-task\/2724",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"compar differ architectur task hei discov recent decid us current research project thing specif task solv like evalu bunch complet differ model architectur have differ set hyper paramet onlin resourc tutori iv show exampl usag evalu differ experi differ param select optim sweep exampl explain optim organ project includ differ architectur solv task abl compar glimps differ perform singl view report idea us job type flag group architectur instanc job type flag like best solut wonder special featur built tool iv notic good practic look insan thank",
        "Question_preprocessed_content":"compar differ architectur task hei discov recent decid us current research project thing specif task solv like evalu bunch complet differ model architectur have differ set onlin resourc tutori iv show exampl usag evalu differ experi differ param select exampl explain optim organ project includ differ architectur solv task abl compar glimps differ perform singl view report idea us flag group architectur instanc flag like best solut wonder special featur tool iv notic look insan thank",
        "Question_gpt_summary_original":"The user is trying to use W&B for their research project and wants to evaluate different model architectures with different sets of hyper-parameters. They are looking for a way to optimally organize their W&B project to compare the different performances of the architectures in a single view or report. The user has tried using the job_type flag but is still looking for a better solution or built-in tool.",
        "Question_gpt_summary":"user try us research project want evalu differ model architectur differ set hyper paramet look wai optim organ project compar differ perform architectur singl view report user tri job type flag look better solut built tool"
    },
    {
        "Question_title":"Logging Artifacts from MlFlow on GCS Bucket",
        "Question_body":"<p>I have a running MlFlow server on GCS VM instance. I have created a bucket to log the artifacts.\nThis is the command I'm running to start the server and for specifying bucket path-<\/p>\n\n<pre><code>mlflow server --default-artifact-root gs:\/\/gcs_bucket\/artifacts --host x.x.x.x\n<\/code><\/pre>\n\n<p>But facing this error:<\/p>\n\n<pre><code>TypeError: stat: path should be string, bytes, os.PathLike or integer, not ElasticNet\n<\/code><\/pre>\n\n<p>Note- The mlflow server is running fine with the specified host alone. The problem is in the way when I'm specifying the storage bucket path.\nI have given permission of storage api by using these commands:<\/p>\n\n<pre><code>gcloud auth application-default login\ngcloud auth login\n<\/code><\/pre>\n\n<p>Also, on printing the artifact URI, this is what I'm getting:<\/p>\n\n<pre><code>mlflow.get_artifact_uri()\n<\/code><\/pre>\n\n<p>Output:<\/p>\n\n<pre><code>gs:\/\/gcs_bucket\/artifacts\/0\/122481bf990xxxxxxxxxxxxxxxxxxxxx\/artifacts\n<\/code><\/pre>\n\n<p>So in the above path from where this is coming <code>0\/122481bf990xxxxxxxxxxxxxxxxxxxxx\/artifacts<\/code> and why it's not getting auto-created at <code>gs:\/\/gcs_bucket\/artifacts<\/code><\/p>\n\n<p>After debugging more, why it's not able to get the local path from VM:\n<a href=\"https:\/\/i.stack.imgur.com\/ubDU0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ubDU0.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And this error I'm getting on VM:<\/p>\n\n<pre><code>ARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file '.\/mlruns\/mlruns\/meta.yaml' does not exist.\nTraceback (most recent call last):\n File \"\/usr\/local\/lib\/python3.6\/dist-packages\/mlflow\/store\/tracking\/file_store.py\", line 197, in list_experiments\n   experiment = self._get_experiment(exp_id, view_type)\n File \"\/usr\/local\/lib\/python3.6\/dist-packages\/mlflow\/store\/tracking\/file_store.py\", line 256, in _get_experiment\n   meta = read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n File \"\/usr\/local\/lib\/python3.6\/dist-packages\/mlflow\/utils\/file_utils.py\", line 160, in read_yaml\n   raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\nmlflow.exceptions.MissingConfigException: Yaml file '.\/mlruns\/mlruns\/meta.yaml' does not exist.\n<\/code><\/pre>\n\n<p>Can I get a solution to this and what I'm missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_created_time":1583840420677,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1583922371776,
        "Question_score_count":4.0,
        "Question_view_count":2153.0,
        "Poster_created_time":1451124057623,
        "Poster_location":"India",
        "Poster_reputation_count":736.0,
        "Poster_view_count":234.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60616879",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"log artifact gc bucket run server gc instanc creat bucket log artifact command run start server specifi bucket path server default artifact root gc bucket artifact host face error typeerror stat path string byte pathlik integ elasticnet note server run fine specifi host problem wai specifi storag bucket path given permiss storag api command gcloud auth applic default login gcloud auth login print artifact uri get artifact uri output gc bucket artifact artifact path come artifact get auto creat gc bucket artifact debug abl local path error get arn root malform experi mlrun detail error yaml file mlrun mlrun meta yaml exist traceback recent file usr local lib python dist packag store track file store line list experi experi self experi exp view type file usr local lib python dist packag store track file store line experi meta read yaml experi dir filestor meta data file file usr local lib python dist packag util file util line read yaml rais missingconfigexcept yaml file exist file path except missingconfigexcept yaml file mlrun mlrun meta yaml exist solut miss",
        "Question_preprocessed_content":"log artifact gc bucket run server gc instanc creat bucket log artifact command run start server specifi bucket path face error note server run fine specifi host problem wai specifi storag bucket path given permiss storag api command print artifact uri get output path come get debug abl local path error get solut miss",
        "Question_gpt_summary_original":"The user is encountering an error while trying to log artifacts from MlFlow on a GCS bucket. The error message states that the path should be a string, bytes, os.PathLike or integer, not ElasticNet. The user has given permission of storage API and is getting an output on printing the artifact URI. However, the path is not getting auto-created at the specified location. The user is also getting an error on the VM and is looking for a solution to the problem.",
        "Question_gpt_summary":"user encount error try log artifact gc bucket error messag state path string byte pathlik integ elasticnet user given permiss storag api get output print artifact uri path get auto creat specifi locat user get error look solut problem"
    },
    {
        "Question_title":"Problem with init() function for model deployment in Azure",
        "Question_body":"<p>I want to deploy model in Azure but I'm struggling with the following problem.<\/p>\n<p>I have my model registered in Azure. The file with extension .sav is located locally. The registration looks the following:<\/p>\n<pre><code>import urllib.request\nfrom azureml.core.model import Model\n\n# Register model\nmodel = Model.register(ws, model_name=&quot;my_model_name.sav&quot;, model_path=&quot;model\/&quot;) \n<\/code><\/pre>\n<p>I have my <code>score.py<\/code> file. The <code>init()<\/code> function in the file looks like this:<\/p>\n<pre><code>import json\nimport numpy as np\nimport pandas as pd\nimport os\nimport pickle\nfrom azureml.core.model import Model\n\n def init():\n    \n    global model\n    model_path = Model.get_model_path(model_name = 'my_model_name.sav', _workspace='workspace_name')\n    model = pickle(open(model_path, 'rb'))\n<\/code><\/pre>\n<p>But when I try to deploy I se the following error:<\/p>\n<pre><code>&quot;code&quot;: &quot;AciDeploymentFailed&quot;,\n  &quot;statusCode&quot;: 400,\n  &quot;message&quot;: &quot;Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\n    1. Please check the logs for your container instance: leak-tester-pm. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\n<\/code><\/pre>\n<p>And when I run <code>print(service.logs())<\/code> I have the following output (I have only one model registered in Azure):<\/p>\n<pre><code>None\n<\/code><\/pre>\n<p>Am I doing something wrong with loading  model in score.py file?<\/p>\n<p>P.S. The .yml file for the deployment:<\/p>\n<pre><code>name: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n  - scikit-learn==0.24.2\n  - azureml-defaults\n  - numpy\n  - pickle-mixin\n  - pandas\n  - xgboost\n  - azure-ml-api-sdk\nchannels:\n- anaconda\n- conda-forge\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1650013473623,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":271.0,
        "Poster_created_time":1564494174516,
        "Poster_location":"Wroc\u0142aw, \u041f\u043e\u043b\u044c\u0448\u0430",
        "Poster_reputation_count":73.0,
        "Poster_view_count":33.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71882074",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"problem init function model deploy azur want deploi model azur struggl follow problem model regist azur file extens sav locat local registr look follow import urllib request core model import model regist model model model regist model model sav model path model score file init function file look like import json import numpi import panda import import pickl core model import model def init global model model path model model path model model sav workspac workspac model pickl open model path try deploi follow error code acideploymentfail statuscod messag aci deploy fail except contain applic crash caus error score file init function check log contain instanc leak tester aml sdk run print servic log servic object fetch log run print servic log follow output model regist azur wrong load model score file yml file deploy project environ depend python interpret version current support later python pip scikit learn default numpi pickl mixin panda xgboost azur api sdk channel anaconda conda forg",
        "Question_preprocessed_content":"problem init function model deploy azur want deploi model azur struggl follow problem model regist azur file extens sav locat local registr look follow file function file look like try deploi follow error run follow output wrong load model file yml file deploy",
        "Question_gpt_summary_original":"The user is facing issues with the init() function while deploying a model in Azure. The user has registered the model in Azure and has a score.py file with the init() function. However, the deployment fails with an error message indicating that the container application crashed due to errors in the init() function. The user has checked the logs but found no output. The user suspects that there might be an issue with loading the model in the score.py file.",
        "Question_gpt_summary":"user face issu init function deploi model azur user regist model azur score file init function deploy fail error messag indic contain applic crash error init function user check log output user suspect issu load model score file"
    },
    {
        "Question_title":"Is it a good idea to store my dataset in my notebook instance in sagemaker?",
        "Question_body":"<p>I'm new to AWS and I am considering to use amazon sagemaker to train my deep learning model because I'm having memory issues due to the large dataset and neural network that I have to train. I'm confused whether to store my data in my notebook instance or in S3? If I store it in my s3 would I be able to access it to train on my notebook instance? I'm confused on the concepts. Can anyone explain the use of S3 in machine learning in AWS?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1619054238143,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":322.0,
        "Poster_created_time":1605938672327,
        "Poster_location":"Jakarta Selatan, South Jakarta City, Jakarta, Indonesia",
        "Poster_reputation_count":97.0,
        "Poster_view_count":34.0,
        "Answer_body":"<p>Yes you can use S3 as storage for your training datasets.<\/p>\n<p>Refer diagram in this link describing how everything works together: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html<\/a><\/p>\n<p>You may also want to checkout following blogs that details about File mode and Pipe mode, two mechanisms for transferring training data:<\/p>\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/<\/a><\/li>\n<\/ol>\n<blockquote>\n<p>In File mode, the training data is downloaded first to an encrypted EBS volume attached to the training instance prior to commencing the training. However, in Pipe mode the input data is streamed directly to the training algorithm while it is running.<\/p>\n<\/blockquote>\n<ol start=\"2\">\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/<\/a><\/li>\n<\/ol>\n<blockquote>\n<p>With Pipe input mode, your data is fed on-the-fly into the algorithm container without involving any disk I\/O. This approach shortens the lengthy download process and dramatically reduces startup time. It also offers generally better read throughput than File input mode. This is because your data is fetched from Amazon S3 by a highly optimized multi-threaded background process. It also allows you to train on datasets that are much larger than the 16 TB Amazon Elastic Block Store (EBS) volume size limit.<\/p>\n<\/blockquote>\n<p>The blog also contains python code snippets using Pipe input mode for reference.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1619062178832,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1336887489390,
        "Answerer_location":null,
        "Answerer_reputation_count":758.0,
        "Answerer_view_count":44.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67205469",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"good idea store dataset notebook instanc new aw consid us train deep learn model have memori issu larg dataset neural network train confus store data notebook instanc store abl access train notebook instanc confus concept explain us machin learn aw",
        "Question_preprocessed_content":"good idea store dataset notebook instanc new aw consid us train deep learn model have memori issu larg dataset neural network train confus store data notebook instanc store abl access train notebook instanc confus concept explain us machin learn aw",
        "Question_gpt_summary_original":"The user is new to AWS and is considering using Amazon SageMaker to train a deep learning model due to memory issues caused by a large dataset and neural network. They are unsure whether to store their data in their notebook instance or in S3 and are seeking clarification on the use of S3 in machine learning in AWS.",
        "Question_gpt_summary":"user new aw consid train deep learn model memori issu caus larg dataset neural network unsur store data notebook instanc seek clarif us machin learn aw"
    },
    {
        "Question_title":"BigQueryML Explainability Apparently Not Working",
        "Question_body":"I'm using BigQueryML to train an XGBoost model on some of my data. When I create the model, I set the ENABLE_GLOBAL_EXPLAIN flag to TRUE, the model then trains properly and I can evaluate it. However there is no Interpretability tab on the model's page, and when I try to query the model with the ML.GLOBAL_EXPLAIN command, I get an error that says:\n\n\u00a0\n\nInvalid table-valued function ML.GLOBAL_EXPLAIN In function GLOBAL_EXPLAIN, the input model was not explained when it was created. at [4:3]\n\nIs this a bug or am I doing something wrong?\n\nHere's my create model code:\n\n\u00a0\n\nCREATE OR REPLACE MODEL `apteo-gcp.2539775073233929517.gb_prod_recos5`\nOPTIONS(\n    INPUT_LABEL_COLS=['next_product_id'], -- label of future products purchased\n    MODEL_TYPE='BOOSTED_TREE_CLASSIFIER', -- gradient boosting using xgboost\n    CLASS_WEIGHTS=[STRUCT('gid:\/\/shopify\/Product\/6995522453704', 265),STRUCT('gid:\/\/shopify\/Product\/6995522355400', 265),STRUCT('gid:\/\/shopify\/Product\/6995522715848', 265),STRUCT('gid:\/\/shopify\/Product\/4448707969120', 100),STRUCT('gid:\/\/shopify\/Product\/6970658717896', 100),STRUCT('gid:\/\/shopify\/Product\/5688760467623', 100),STRUCT('gid:\/\/shopify\/Product\/4448711213152', 100),STRUCT('gid:\/\/shopify\/Product\/5430667804839', 100),STRUCT('gid:\/\/shopify\/Product\/5651893158055', 100),STRUCT('gid:\/\/shopify\/Product\/6995568918728', 125),STRUCT('gid:\/\/shopify\/Product\/4405707243616', 100),STRUCT('NO ORDER', 0.05)], \n    LEARN_RATE=0.15,\n    L2_REG=2.0,\n    L1_REG=1.5,\n    EARLY_STOP=TRUE,\n    MAX_ITERATIONS=75,\n    MIN_REL_PROGRESS=0.0001,\n    ENABLE_GLOBAL_EXPLAIN=TRUE,\n    DATA_SPLIT_METHOD=\"RANDOM\"\n)\n\nAS\n\n...",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1640165100000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":452.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/BigQueryML-Explainability-Apparently-Not-Working\/td-p\/181036\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"bigqueryml explain appar work bigqueryml train xgboost model data creat model set enabl global explain flag true model train properli evalu interpret tab model page try queri model global explain command error sai invalid tabl valu function global explain function global explain input model explain creat bug wrong creat model code creat replac model apteo gcp prod reco option input label col product label futur product purchas model type boost tree classifi gradient boost xgboost class weight struct gid shopifi product struct gid shopifi product struct gid shopifi product struct gid shopifi product struct gid shopifi product struct gid shopifi product struct gid shopifi product struct gid shopifi product struct gid shopifi product struct gid shopifi product struct gid shopifi product struct order learn rate reg reg earli stop true max iter min rel progress enabl global explain true data split method random",
        "Question_preprocessed_content":"bigqueryml explain appar work bigqueryml train xgboost model data creat model set flag true model train properli evalu interpret tab model page try queri model command error sai invalid function function input model explain creat bug wrong creat model code creat replac model option struct",
        "Question_gpt_summary_original":"The user is facing an issue with BigQueryML's explainability feature. Although the ENABLE_GLOBAL_EXPLAIN flag is set to TRUE while creating an XGBoost model, there is no Interpretability tab on the model's page, and querying the model with the ML.GLOBAL_EXPLAIN command results in an error. The error message suggests that the input model was not explained when it was created. The user is unsure if this is a bug or if they are doing something wrong.",
        "Question_gpt_summary":"user face issu bigqueryml explain featur enabl global explain flag set true creat xgboost model interpret tab model page queri model global explain command result error error messag suggest input model explain creat user unsur bug wrong"
    },
    {
        "Question_title":"unable to create datadrift alert in Azure ML",
        "Question_body":"<p>I am unable to create datadrift alert while trying to create a datadrift monitor. I am getting following error: Datadriftdetector with id: e83bd907-a268-4137. This may be because you do not have access to the AppInsights associated with this AzureML Workspace    <\/p>\n<p>I have even given User Access Admin role but not able to create alert<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_created_time":1663946292127,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1021112\/unable-to-create-datadrift-alert-in-azure-ml",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unabl creat datadrift alert unabl creat datadrift alert try creat datadrift monitor get follow error datadriftdetector ebd access appinsight associ workspac given user access admin role abl creat alert",
        "Question_preprocessed_content":"unabl creat datadrift alert unabl creat datadrift alert try creat datadrift monitor get follow error datadriftdetector access appinsight associ workspac given user access admin role abl creat alert",
        "Question_gpt_summary_original":"The user is unable to create a datadrift alert in Azure ML and is receiving an error message related to access to AppInsights associated with the workspace. The user has already tried giving User Access Admin role but is still unable to create the alert.",
        "Question_gpt_summary":"user unabl creat datadrift alert receiv error messag relat access appinsight associ workspac user tri give user access admin role unabl creat alert"
    },
    {
        "Question_title":"NOTEBOOK TERMINAL AZURE",
        "Question_body":"<p>I COPIED AND PASTED, pip install -r requirements.txt --user --upgrade UNDER MY NOTEBOOK TERMINAL IN AZURE. IT STARTED LAUNCHING BUT LAPTOP SHUTDOWN. I RESTARTED IT, TRIED COPYING AND PASTING THE ABOVE LINK, BUT NOW IT GIVES THE MESSAGE BELOW. HELP PLEASE.  <\/p>\n<p>Requirement already satisfied, skipping upgrade: oauthlib&gt;=3.0.0 in \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard~=2.4-&gt;tensorflow-&gt;-r requirements.txt (line 32)) (3.1.0)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_created_time":1618331988847,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/355752\/notebook-terminal-azure",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"notebook termin azur copi past pip instal requir txt user upgrad notebook termin azur start launch laptop shutdown restart tri copi past link give messag help requir satisfi skip upgrad oauthlib anaconda env lib python site packag request oauthlib googl auth oauthlib tensorboard tensorflow requir txt line",
        "Question_preprocessed_content":"notebook termin azur copi past pip instal notebook termin azur start launch laptop shutdown restart tri copi past link give messag help requir satisfi skip upgrad",
        "Question_gpt_summary_original":"The user encountered an issue while trying to install requirements in Azure Notebook Terminal. The laptop shut down during the installation process, and upon restarting, the user received a message indicating that the requirement was already satisfied. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount issu try instal requir azur notebook termin laptop shut instal process restart user receiv messag indic requir satisfi user seek help resolv issu"
    },
    {
        "Question_title":"Timing to create a dvc repo for a 60GB dataset?",
        "Question_body":"<p>Hello<\/p>\n<p>I am a new user, working in Linux, and I am creating a DVC remote repo on NFS. I have a dataset split across four sub-directories, each one with 10000 images, total size of all four sub-directories is 62GB.<\/p>\n<p>If I do a straight copy (not dvc) of the dataset to the repo location, the copy takes 50 min.<\/p>\n<p>If I do \u2018dvc add\u2019 to the remote repo, the command is still running after 3.5 days. I assume that this is not expected? What would be expected time for dvc add to a remote repo, versus a straight copy to the same location?<\/p>\n<p>Thanks, Paul<\/p>",
        "Question_answer_count":15,
        "Question_comment_count":0,
        "Question_created_time":1635739140181,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":567.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/timing-to-create-a-dvc-repo-for-a-60gb-dataset\/945",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"time creat repo dataset hello new user work linux creat remot repo nf dataset split sub directori imag total size sub directori straight copi dataset repo locat copi take min add remot repo command run dai assum expect expect time add remot repo versu straight copi locat thank paul",
        "Question_preprocessed_content":"time creat repo dataset hello new user work linux creat remot repo nf dataset split imag total size straight copi dataset repo locat copi take min add remot repo command run dai assum expect expect time add remot repo versu straight copi locat thank paul",
        "Question_gpt_summary_original":"The user is a new Linux user who is trying to create a DVC remote repo on NFS for a 60GB dataset split across four sub-directories. The user is facing challenges with the 'dvc add' command, which is taking more than 3.5 days to complete, whereas a straight copy of the dataset to the repo location takes only 50 minutes. The user is seeking advice on the expected time for 'dvc add' to a remote repo compared to a straight copy to the same location.",
        "Question_gpt_summary":"user new linux user try creat remot repo nf dataset split sub directori user face challeng add command take dai complet straight copi dataset repo locat take minut user seek advic expect time add remot repo compar straight copi locat"
    },
    {
        "Question_title":"Azure Get API",
        "Question_body":"<p>Hi,    <\/p>\n<p>We are trying to develop a python code which will return a json of all VMs for given Azure region along with its vCPU, Ram, pricing as per OS etc.     <\/p>\n<p>However after one hour it is getting expired. Here is the reference link for the API we are using.    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/rest\/api\/azureml\/virtual-machine-sizes\/list#code-try-0\">https:\/\/learn.microsoft.com\/en-us\/rest\/api\/azureml\/virtual-machine-sizes\/list#code-try-0<\/a>    <\/p>\n<p>How can we extend this one hour expiry to 1-3months?     <\/p>\n<p>Regards,     <br \/>\nShreeshail<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1642420332357,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/697928\/azure-get-api",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"azur api try develop python code return json vm given azur region vcpu ram price hour get expir refer link api http learn microsoft com rest api virtual machin size list code try extend hour expiri month regard shreeshail",
        "Question_preprocessed_content":"azur api try develop python code return json vm given azur region vcpu ram price hour get expir refer link api extend hour expiri regard shreeshail",
        "Question_gpt_summary_original":"The user is developing a python code to return a JSON of all VMs for a given Azure region with vCPU, RAM, and pricing as per OS. However, the API they are using is expiring after one hour, and they are seeking a way to extend the expiry to 1-3 months.",
        "Question_gpt_summary":"user develop python code return json vm given azur region vcpu ram price api expir hour seek wai extend expiri month"
    },
    {
        "Question_title":"AWS CLI: How to use variable to filter EFS",
        "Question_body":"<p>I want to use the value of the <code>DOMAIN_ID<\/code> variable to filter the EFS to get a FileSystemId. I used the commands below. The first command works and it stores the domain ID. The second one returns an empty list, even though the <code>DOMAIN_ID<\/code> variable is present.<\/p>\n<pre><code>DOMAIN_ID=$(aws sagemaker list-domains --query 'Domains[0].DomainId')\naws efs describe-file-systems --query 'FileSystems[?CreationToken==`$DOMAIN_ID`].FileSystemId'\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>[]\n<\/code><\/pre>\n<p>Expected output:<\/p>\n<pre><code>&lt;Some EFS identifier&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1658155362930,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1658167919760,
        "Question_score_count":0.0,
        "Question_view_count":64.0,
        "Poster_created_time":1503840831940,
        "Poster_location":"Rotterdam, Netherlands",
        "Poster_reputation_count":33.0,
        "Poster_view_count":9.0,
        "Answer_body":"<p>This works (escaping backticks) -<\/p>\n<pre><code>aws efs describe-file-systems --query &quot;FileSystems[?CreationToken==\\`$DOMAIN_ID\\`].FileSystemId&quot;\n<\/code><\/pre>\n<p>You can also use describe-domain command instead -<\/p>\n<pre><code>$ DOMAIN_ID=$(aws sagemaker list-domains --query 'Domains[0].DomainId' | tr -d '&quot;')\n$ aws sagemaker describe-domain --domain-id $DOMAIN_ID --query 'HomeEfsFileSystemId'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1658177973060,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1370505440848,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73024189",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"aw cli us variabl filter ef want us valu domain variabl filter ef filesystemid command command work store domain second return list domain variabl present domain list domain queri domain domainid aw ef file system queri filesystem creationtoken domain filesystemid output expect output",
        "Question_preprocessed_content":"aw cli us variabl filter ef want us valu variabl filter ef filesystemid command command work store domain second return list variabl present output expect output",
        "Question_gpt_summary_original":"The user is trying to use a variable to filter EFS and get a FileSystemId using AWS CLI. The first command to store the domain ID works, but the second command returns an empty list even though the DOMAIN_ID variable is present. The expected output is an EFS identifier.",
        "Question_gpt_summary":"user try us variabl filter ef filesystemid aw cli command store domain work second command return list domain variabl present expect output ef identifi"
    },
    {
        "Question_title":"How to assign two or more time series identifier columns in Vertex AI Tabular Forecasting",
        "Question_body":"<p>I was wondering if it is possible to have more than one time series identifier column in the model? Let's assume I'd like to create a forecast at a product and store level (which the documentation suggests should be possible).<\/p>\n<p>If I select product as the series identifier, the only options I have left for store is either a covariate or an attribute and neither is applicable in this scenario.<\/p>\n<p>Would concatenating product and store and using the individual product and store code values for that concatenated ID as attributes be a solution? It doesn't feel right, but I can't see any other option - am I missing something?<\/p>\n<p>Note: I understand that this feature of Vertex AI is currently in preview and that because of that the options may be limited.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1632228068913,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":269.0,
        "Poster_created_time":1519630645620,
        "Poster_location":"Northampton, UK",
        "Poster_reputation_count":333.0,
        "Poster_view_count":46.0,
        "Answer_body":"<p>There isn't an alternate way to assign 2 or more <strong>Time Series Identifiers<\/strong> in the <strong>Forecasting Model<\/strong> on <strong>Vertex AI<\/strong>. The &quot;<strong>Forecasting model<\/strong>&quot; is in the &quot;<strong>Preview<\/strong>&quot; <a href=\"https:\/\/cloud.google.com\/products#product-launch-stages\" rel=\"nofollow noreferrer\">Product launch stage<\/a>, as you are aware, with all consequences of that fact the options are limited. Please refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/bp-tabular#data_preparation_best_practices\" rel=\"nofollow noreferrer\">doc<\/a> for more information about the best practices for data preparation to train the forecasting model.<\/p>\n<p>As a workaround, the two columns can be concatenated and assigned a Time Series Identifier on that concatenated column, as you have mentioned in the question. This way, the concatenated column carries more contextual information into the training of the model.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1632376096768,
        "Answer_last_edit_time":1632482492492,
        "Answer_score_count":2.0,
        "Answerer_created_time":1619174589310,
        "Answerer_location":"Bangalore, Karnataka, India",
        "Answerer_reputation_count":1200.0,
        "Answerer_view_count":804.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69269073",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"assign time seri identifi column tabular forecast wonder possibl time seri identifi column model let assum like creat forecast product store level document suggest possibl select product seri identifi option left store covari attribut applic scenario concaten product store individu product store code valu concaten attribut solut feel right option miss note understand featur current preview option limit",
        "Question_preprocessed_content":"assign time seri identifi column tabular forecast wonder possibl time seri identifi column model let assum like creat forecast product store level select product seri identifi option left store covari attribut applic scenario concaten product store individu product store code valu concaten attribut solut feel right option miss note understand featur current preview option limit",
        "Question_gpt_summary_original":"The user is facing a challenge in assigning two or more time series identifier columns in Vertex AI Tabular Forecasting. They want to create a forecast at a product and store level, but the only options available for store are either a covariate or an attribute, which are not applicable in this scenario. The user is considering concatenating product and store and using the individual product and store code values for that concatenated ID as attributes, but they are unsure if this is the right solution. The user acknowledges that this feature is currently in preview and the options may be limited.",
        "Question_gpt_summary":"user face challeng assign time seri identifi column tabular forecast want creat forecast product store level option avail store covari attribut applic scenario user consid concaten product store individu product store code valu concaten attribut unsur right solut user acknowledg featur current preview option limit"
    },
    {
        "Question_title":"Table artifact to pandas dataframe",
        "Question_body":"<p>I logged a table artifact.<br>\nNow I want to analyze it using pd.DataFrame.<\/p>\n<p>I\u2019ve downloaded the artifact.<br>\nHowever, I don\u2019t mange to convert the json into dataframe.<br>\nIs there a method for doing so?<br>\nThanks<\/p>\n<pre><code class=\"lang-auto\">import wandb\nimport pandas as pd\nimport os.path as osp\n\nrun = wandb.init()\nartifact = run.use_artifact('PATH\/run-1n4emfxy-test_table:v19', type='run_table')\nartifact_dir = artifact.download()\npath_to_json = osp.join( next(iter(artifact._download_roots)), 'test_table.table.json')\npd.read_json(path_to_json)\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1646993727467,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":798.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/table-artifact-to-pandas-dataframe\/2059",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"tabl artifact panda datafram log tabl artifact want analyz datafram iv download artifact dont mang convert json datafram method thank import import panda import path osp run init artifact run us artifact path run nemfxi test tabl type run tabl artifact dir artifact download path json osp join iter artifact download root test tabl tabl json read json path json",
        "Question_preprocessed_content":"tabl artifact panda datafram log tabl artifact want analyz iv download artifact dont mang convert json datafram method thank",
        "Question_gpt_summary_original":"The user has encountered a challenge in converting a downloaded table artifact into a pandas dataframe using pd.DataFrame. The user has shared the code snippet used for downloading the artifact and attempting to convert it into a dataframe.",
        "Question_gpt_summary":"user encount challeng convert download tabl artifact panda datafram datafram user share code snippet download artifact attempt convert datafram"
    },
    {
        "Question_title":"Saving an Matlabplot as an MLFlow artifact",
        "Question_body":"<p>I am using DataBricks and Spark 7.4ML,<\/p>\n<p>The following code successfully logs the params and metrics, and I can see the ROCcurve.png in the MLFLOW gui (just the item in the tree below the model). But the actually plot is blank. Why?<\/p>\n<pre><code>with mlflow.start_run(run_name=&quot;logistic-regression&quot;) as run:\n  pipeModel = pipe.fit(trainDF)\n  mlflow.spark.log_model(pipeModel, &quot;model&quot;)\n  predTest = pipeModel.transform(testDF)\n  predTrain = pipeModel.transform(trainDF)\n  evaluator=BinaryClassificationEvaluator(labelCol=&quot;arrivedLate&quot;)\n  trainROC = evaluator.evaluate(predTrain)\n  testROC = evaluator.evaluate(predTest)\n  print(f&quot;Train ROC: {trainROC}&quot;)\n  print(f&quot;Test ROC: {testROC}&quot;)\n  mlflow.log_param(&quot;Dataset Name&quot;, &quot;Flights &quot; + datasetName)\n  mlflow.log_metric(key=&quot;Train ROC&quot;, value=trainROC)\n  mlflow.log_metric(key=&quot;Test ROC&quot;, value=testROC)\n\n  lrModel = pipeModel.stages[3]\n  trainingSummary = lrModel.summary\n  roc = trainingSummary.roc.toPandas()\n  plt.plot(roc['FPR'],roc['TPR'])\n  plt.ylabel('False Positive Rate')\n  plt.xlabel('True Positive Rate')\n  plt.title('ROC Curve')\n  plt.show()\n  plt.savefig(&quot;ROCcurve.png&quot;)\n  mlflow.log_artifact(&quot;ROCcurve.png&quot;)\n  plt.close()\n  \n  display(predTest.select(stringCols + [&quot;arrivedLate&quot;, &quot;prediction&quot;]))\n<\/code><\/pre>\n<p>What the notebook shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What the MLFlow shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1607094596640,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1607191847983,
        "Question_score_count":8.0,
        "Question_view_count":5219.0,
        "Poster_created_time":1316705139196,
        "Poster_location":"Boston, MA",
        "Poster_reputation_count":6711.0,
        "Poster_view_count":819.0,
        "Answer_body":"<p>Put <code>plt.show()<\/code> after <code>plt.savefig()<\/code> - <code>plt.show()<\/code> will remove your plot because it is shown already.<\/p>",
        "Answer_comment_count":3.0,
        "Question_closed_time":1607094854147,
        "Answer_last_edit_time":null,
        "Answer_score_count":7.0,
        "Answerer_created_time":1598380609848,
        "Answerer_location":"Oxford, UK",
        "Answerer_reputation_count":38531.0,
        "Answerer_view_count":4137.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65145994",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"save matlabplot artifact databrick spark follow code successfulli log param metric roccurv png gui item tree model actual plot blank start run run logist regress run pipemodel pipe fit traindf spark log model pipemodel model predtest pipemodel transform testdf predtrain pipemodel transform traindf evalu binaryclassificationevalu labelcol arrivedl trainroc evalu evalu predtrain testroc evalu evalu predtest print train roc trainroc print test roc testroc log param dataset flight datasetnam log metric kei train roc valu trainroc log metric kei test roc valu testroc lrmodel pipemodel stage trainingsummari lrmodel summari roc trainingsummari roc topanda plt plot roc fpr roc tpr plt ylabel fals posit rate plt xlabel true posit rate plt titl roc curv plt plt savefig roccurv png log artifact roccurv png plt close displai predtest select stringcol arrivedl predict notebook show show",
        "Question_preprocessed_content":"save matlabplot artifact databrick spark follow code successfulli log param metric gui actual plot blank notebook show show",
        "Question_gpt_summary_original":"The user is trying to save a Matplotlib plot as an MLFlow artifact in DataBricks and Spark 7.4ML. The code successfully logs the params and metrics, and the ROCcurve.png is visible in the MLFLOW gui, but the plot is blank. The user is seeking help to understand why this is happening.",
        "Question_gpt_summary":"user try save matplotlib plot artifact databrick spark code successfulli log param metric roccurv png visibl gui plot blank user seek help understand happen"
    },
    {
        "Question_title":"Wandb sweeps running on Kaggle GPU or Colab GPU are much slower than on my local CPU",
        "Question_body":"<p>Hi there,<\/p>\n<p>i have run a few sweeps on my local computer and the same sweeps on Kaggle and Colab<\/p>\n<p>i have an i7 (10th gen) CPU in my home computer but no GPU<br>\ni measured around 50secs for 100 epochs (1 run)<\/p>\n<p>on Kaggle and Colab the same 100 epochs took 2mins 30secs (Colab) and ~3mins  (Kaggle) <em>using GPU<\/em><\/p>\n<p>how is that possible? am i doing something wrong?<\/p>\n<p>i observed this extreme slowdown only when using W&amp;B Sweeps<br>\nno slowdown when running single experiments<\/p>\n<p>please help, any idea appreciated!<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_created_time":1632736952286,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":4.0,
        "Question_view_count":426.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-sweeps-running-on-kaggle-gpu-or-colab-gpu-are-much-slower-than-on-my-local-cpu\/794",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"sweep run kaggl gpu colab gpu slower local cpu run sweep local sweep kaggl colab gen cpu home gpu measur sec epoch run kaggl colab epoch took min sec colab min kaggl gpu possibl wrong observ extrem slowdown sweep slowdown run singl experi help idea appreci",
        "Question_preprocessed_content":"sweep run kaggl gpu colab gpu slower local cpu run sweep local sweep kaggl colab cpu home gpu measur sec epoch kaggl colab epoch took min sec min gpu possibl wrong observ extrem slowdown sweep slowdown run singl experi help idea appreci",
        "Question_gpt_summary_original":"The user is experiencing a significant slowdown when running W&B sweeps on Kaggle and Colab GPUs compared to their local CPU. They have observed that 100 epochs took 2.5-3 minutes on Kaggle and Colab using GPU, while it took only 50 seconds on their local computer with no GPU. The user is seeking help to understand why this is happening and if they are doing something wrong.",
        "Question_gpt_summary":"user experienc signific slowdown run sweep kaggl colab gpu compar local cpu observ epoch took minut kaggl colab gpu took second local gpu user seek help understand happen wrong"
    },
    {
        "Question_title":"Azureml dataset consumption does not work when whitespaces are in the file name",
        "Question_body":"<p>Hello,  <br \/>\nwhen i previously exported the data from a data labeling project to Azure ML dataset, i could consume them with the azureml.contrib.dataset  <\/p>\n<p>This seems to be not supported anymore, therefor i tried to download them with the azureml.core Dataset package.  <br \/>\nIt seems to works only for data which have no white space in their name.  <br \/>\nThe dataset is a Tabular dataset, i download them with:  <\/p>\n<pre><code>from azureml.core import Workspace, Dataset\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndataset = Dataset.get_by_name(workspace, name='my_dataset_name')\ndataset.download('image_url', target_path='.\/', overwrite=True)\n<\/code><\/pre>\n<p>Error:   <\/p>\n<blockquote>\n<p>AzureMLException:  <br \/>\n\tMessage: Some files have failed to download:('workspaceblobstore\/data\/quay_data_cra\/IMG_0082%20copy%207.jpg', 'Microsoft.DPrep.ErrorValues.DownloadFailed')  <br \/>\n('workspaceblobstore\/data\/quay_data_cra\/IMG_0543%20copy%205.jpg', 'Microsoft.DPrep.ErrorValues.DownloadFailed')  <\/p>\n<\/blockquote>\n<p>How can i download the dataset?  <\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_created_time":1645111306257,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/740186\/azureml-dataset-consumption-does-not-work-when-whi",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"dataset consumpt work whitespac file hello previous export data data label project dataset consum contrib dataset support anymor therefor tri download core dataset packag work data white space dataset tabular dataset download core import workspac dataset workspac workspac subscript resourc group workspac dataset dataset workspac dataset dataset download imag url target path overwrit true error except messag file fail download workspaceblobstor data quai data cra img copi jpg microsoft dprep errorvalu downloadfail workspaceblobstor data quai data cra img copi jpg microsoft dprep errorvalu downloadfail download dataset",
        "Question_preprocessed_content":"dataset consumpt work whitespac file hello previous export data data label project dataset consum support anymor therefor tri download core dataset packag work data white space dataset tabular dataset download error except messag file fail download dataset",
        "Question_gpt_summary_original":"The user is facing an issue while consuming Azure ML dataset with whitespaces in the file name. The user tried to download the dataset using the azureml.core Dataset package, but it only works for data without whitespaces in their name. The error message indicates that some files have failed to download due to the whitespaces in their name. The user is seeking a solution to download the dataset.",
        "Question_gpt_summary":"user face issu consum dataset whitespac file user tri download dataset core dataset packag work data whitespac error messag indic file fail download whitespac user seek solut download dataset"
    },
    {
        "Question_title":"Limit on the rate of inferences one can make for a SageMaker endpoint",
        "Question_body":"<p>Is there a limit on the rate of inferences one can make for a SageMaker endpoint?<\/p>\n\n<p>Is it determined somehow by the instance type behind the endpoint or the number of instances?<\/p>\n\n<p>I tried looking for this info as <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws_service_limits.html#limits_sagemaker\" rel=\"nofollow noreferrer\">AWS Service Quotas for SageMaker<\/a> but couldn't find it.<\/p>\n\n<p>I am invoking the endpoint from a Spark job abd wondered if the number of concurrent tasks is a factor I should be taking care of when running inference (assuming each task runs one inference at a time) <\/p>\n\n<p>Here's the throttling error I got:<\/p>\n\n<pre><code>com.amazonaws.services.sagemakerruntime.model.AmazonSageMakerRuntimeException: null (Service: AmazonSageMakerRuntime; Status Code: 400; Error Code: ThrottlingException; Request ID: b515121b-f3d5-4057-a8a4-6716f0708980)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.doInvoke(AmazonSageMakerRuntimeClient.java:236)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.invoke(AmazonSageMakerRuntimeClient.java:212)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.executeInvokeEndpoint(AmazonSageMakerRuntimeClient.java:176)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.invokeEndpoint(AmazonSageMakerRuntimeClient.java:151)\n    at lineefd06a2d143b4016906a6138a6ffec15194.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$a5cddfc4633c5dd8aa603ddc4f9aad5$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Predictor.predict(command-2334973:41)\n    at lineefd06a2d143b4016906a6138a6ffec15200.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$50a9225beeac265557e61f69d69d7d$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(command-2307906:11)\n    at lineefd06a2d143b4016906a6138a6ffec15200.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$50a9225beeac265557e61f69d69d7d$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(command-2307906:11)\n    at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n    at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:2000)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1220)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1220)\n    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n    at org.apache.spark.scheduler.Task.run(Task.scala:113)\n    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1579709239420,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1579710725728,
        "Question_score_count":3.0,
        "Question_view_count":1716.0,
        "Poster_created_time":1458550179920,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Answer_body":"<p>Amazon SageMaker is offering model hosting service (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-hosting.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-hosting.html<\/a>), which gives you a lot of flexibility based on your inference requirements. <\/p>\n\n<p>As you noted, first you can choose the instance type to use for your model hosting. The large set of options is important to tune to your models. You can host the model on a GPU based machines (P2\/P3\/P4) or CPU ones. You can have instances with faster CPU (C4, for example), or more RAM (R4, for example). You can also choose instances with more cores (16xl, for example) or less (medium, for example). Here is a list of the full range of instances that you can choose: <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/<\/a> . It is important to balance your performance and costs. The selection of the instance type and the type and size of your model will determine the invocations-per-second that you can expect from your model in this single-node configuration. It is important to measure this number to avoid hitting the throttle errors that you saw. <\/p>\n\n<p>The second important feature of the SageMaker hosting that you use is the ability to auto-scale your model to multiple instances. You can configure the endpoint of your model hosting to automatically add and remove instances based on the load on the endpoint. AWS is adding a load balancer in front of the multiple instances that are hosting your models and distributing the requests among them. Using the autoscaling functionality allows you to keep a smaller instance for low traffic hours, and to be able to scale up during peak traffic hours, and still keep your costs low and your throttle errors to the minimum. See here for documentation on the SageMaker autoscaling options: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling.html<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1579951518923,
        "Answer_last_edit_time":null,
        "Answer_score_count":4.0,
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59863842",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"limit rate infer endpoint limit rate infer endpoint determin instanc type endpoint number instanc tri look info aw servic quota couldn invok endpoint spark job abd wonder number concurr task factor take care run infer assum task run infer time throttl error got com amazonaw servic runtim model amazonruntimeexcept null servic amazonruntim statu code error code throttlingexcept request com amazonaw http amazonhttpcli requestexecutor handleerrorrespons amazonhttpcli java com amazonaw http amazonhttpcli requestexecutor executeonerequest amazonhttpcli java com amazonaw http amazonhttpcli requestexecutor executehelp amazonhttpcli java com amazonaw http amazonhttpcli requestexecutor doexecut amazonhttpcli java com amazonaw http amazonhttpcli requestexecutor executewithtim amazonhttpcli java com amazonaw http amazonhttpcli requestexecutor execut amazonhttpcli java com amazonaw http amazonhttpcli requestexecutor access amazonhttpcli java com amazonaw http amazonhttpcli requestexecutionbuilderimpl execut amazonhttpcli java com amazonaw http amazonhttpcli execut amazonhttpcli java com amazonaw http amazonhttpcli execut amazonhttpcli java com amazonaw servic runtim amazonruntimecli doinvok amazonruntimecli java com amazonaw servic runtim amazonruntimecli invok amazonruntimecli java com amazonaw servic runtim amazonruntimecli executeinvokeendpoint amazonruntimecli java com amazonaw servic runtim amazonruntimecli invokeendpoint amazonruntimecli java lineefdadbaaffec read acddfccddaaddcfaad predictor predict command lineefdadbaaffec read abeeacefddd anonfun appli command lineefdadbaaffec read abeeacefddd anonfun appli command scala collect iter anon iter scala org apach spark util util getiterators util scala org apach spark rdd rdd anonfun count appli rdd scala org apach spark rdd rdd anonfun count appli rdd scala org apach spark sparkcontext anonfun runjob appli sparkcontext scala org apach spark sparkcontext anonfun runjob appli sparkcontext scala org apach spark schedul resulttask runtask resulttask scala org apach spark schedul task doruntask task scala org apach spark schedul task run task scala org apach spark executor executor taskrunn anonfun appli executor scala org apach spark util util trywithsafefin util scala org apach spark executor executor taskrunn run executor scala java util concurr threadpoolexecutor runwork threadpoolexecutor java java util concurr threadpoolexecutor worker run threadpoolexecutor java java lang thread run thread java",
        "Question_preprocessed_content":"limit rate infer endpoint limit rate infer endpoint determin instanc type endpoint number instanc tri look info aw servic quota couldn invok endpoint spark job abd wonder number concurr task factor take care run infer throttl error got",
        "Question_gpt_summary_original":"The user is encountering a throttling error while invoking an endpoint from a Spark job in SageMaker. They are unsure if there is a limit on the rate of inferences one can make for a SageMaker endpoint and if it is determined by the instance type or number of instances. They have checked AWS Service Quotas for SageMaker but couldn't find the information. They are also wondering if the number of concurrent tasks is a factor they should be taking care of when running inference.",
        "Question_gpt_summary":"user encount throttl error invok endpoint spark job unsur limit rate infer endpoint determin instanc type number instanc check aw servic quota couldn inform wonder number concurr task factor take care run infer"
    },
    {
        "Question_title":"I am not able to create a feature store in vertexAI using labels",
        "Question_body":"<p>I am passing the values of lables as below to create a featurestore with labels. But after creation of the featurestore, I do not see the featurestore created with labels. Is it still not supported in VertexAI<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>    fs = aiplatform.Featurestore.create(\n        featurestore_id=featurestore_id,\n        labels=dict(project='retail', env='prod'),\n        online_store_fixed_node_count=online_store_fixed_node_count,\n        sync=sync\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/viOSu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/viOSu.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1651616413553,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1651623411107,
        "Question_score_count":0.0,
        "Question_view_count":83.0,
        "Poster_created_time":1530457174832,
        "Poster_location":null,
        "Poster_reputation_count":1043.0,
        "Poster_view_count":212.0,
        "Answer_body":"<p>As mentioned in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-featurestores\" rel=\"nofollow noreferrer\">featurestore documentation<\/a>:<\/p>\n<blockquote>\n<p>A <strong>featurestore<\/strong> is a top-level container for entity types, features,\nand feature values.<\/p>\n<\/blockquote>\n<p>With this, the GCP console UI &quot;labels&quot; are the &quot;labels&quot; at the <strong>Feature<\/strong> level.<\/p>\n<p>Once a <strong>featurestore<\/strong> is created, you will need to create an <strong>entity<\/strong> and then create a <strong>Feature<\/strong> that has the <em>labels<\/em> parameter as shown on the below sample python code.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from google.cloud import aiplatform\n\ntest_label = {'key1' : 'value1'}\n\ndef create_feature_sample(\n    project: str,\n    location: str,\n    feature_id: str,\n    value_type: str,\n    entity_type_id: str,\n    featurestore_id: str,\n):\n\n    aiplatform.init(project=project, location=location)\n\n    my_feature = aiplatform.Feature.create(\n        feature_id=feature_id,\n        value_type=value_type,\n        entity_type_name=entity_type_id,\n        featurestore_id=featurestore_id,\n        labels=test_label,\n    )\n\n    my_feature.wait()\n\n    return my_feature\n\ncreate_feature_sample('your-project','us-central1','test_feature3','STRING','test_entity3','test_fs3')\n<\/code><\/pre>\n<p>Below is the screenshot of the GCP console which shows that <em>labels<\/em> for <strong>test_feature3<\/strong> feature has the values defined in the above sample python code.\n<a href=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You may refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-features#create-feature\" rel=\"nofollow noreferrer\">creation of feature documentation<\/a> using python for more details.<\/p>\n<p>On the other hand, you may still view the <em>labels<\/em> you defined for your featurestore using the REST API as shown on the below sample.<\/p>\n<pre><code>curl -X GET \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\n&quot;https:\/\/&lt;your-location&gt;-aiplatform.googleapis.com\/v1\/projects\/&lt;your-project&gt;\/locations\/&lt;your-location&gt;\/featurestores&quot;\n<\/code><\/pre>\n<p>Below is the result of the REST API which also shows the value of the <em>labels<\/em> I defined for my &quot;test_fs3&quot; featurestore.\n<a href=\"https:\/\/i.stack.imgur.com\/gW45X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gW45X.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1651709813300,
        "Answer_last_edit_time":1651710305260,
        "Answer_score_count":0.0,
        "Answerer_created_time":1639972620503,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72106030",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"abl creat featur store vertexai label pass valu labl creat featurestor label creation featurestor featurestor creat label support vertexai aiplatform featurestor creat featurestor featurestor label dict project retail env prod onlin store fix node count onlin store fix node count sync sync",
        "Question_preprocessed_content":"abl creat featur store vertexai label pass valu labl creat featurestor label creation featurestor featurestor creat label support vertexai",
        "Question_gpt_summary_original":"The user is unable to create a feature store in VertexAI using labels. They have passed the values of labels to create a feature store, but after creation, they are unable to see the feature store created with labels. The user is unsure if this feature is supported in VertexAI.",
        "Question_gpt_summary":"user unabl creat featur store vertexai label pass valu label creat featur store creation unabl featur store creat label user unsur featur support vertexai"
    },
    {
        "Question_title":"SKLearn Processing Container - Error: \"WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager.\"",
        "Question_body":"Hey all,\n\nI am trying to run the script below in the writefile titled \"vw_aws_a_bijlageprofile.py\". This code has worked for me using other data sources, but now I am getting the following error message from the CloudWatch Logs:\n\n\"***2022-08-24T20:09:19.708-05:00\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https:\/\/pip.pypa.io\/warnings\/venv***\"\n\nAny idea how I get around this error?\n\nFull code below.\n\nThank you in advance!!!!\n\n```\n%%writefile vw_aws_a_bijlageprofile.py\n\nimport os\nimport sys\nimport subprocess\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall('awswrangler')\ninstall('tqdm')\ninstall('pandas')\ninstall('botocore')\ninstall('ruamel.yaml')\ninstall('pandas-profiling')\nimport awswrangler as wr\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom string import Template\nimport gc\nimport boto3\n\nfrom pandas_profiling import ProfileReport\n\nclient = boto3.client('s3')\nsession = boto3.Session(region_name=\"eu-west-2\")\n\n\ndef run_profile():\n\n\n\n    query = \"\"\"\n    SELECT  * FROM \"intl-euro-archmcc-database\".\"vw_aws_a_bijlage\"\n    ;\n    \"\"\"\n                                        #swich table name above\n        \n    tableforprofile = wr.athena.read_sql_query(query,\n                                            database=\"intl-euro-archmcc-database\",\n                                            boto3_session=session,\n                                            ctas_approach=False,\n                                            workgroup='DataScientists')\n    print(\"read in the table queried above\")\n\n    print(\"got rid of missing and added a new index\")\n\n    profile_tblforprofile = ProfileReport(tableforprofile, \n                                  title=\"Pandas Profiling Report\", \n                                  minimal=True)\n\n    print(\"Generated table profile\")\n                                      \n    return profile_tblforprofile\n\n\nif __name__ == '__main__':\n\n    profile_tblforprofile = run_profile()\n    \n    print(\"Generated outputs\")\n\n    output_path_tblforprofile = ('\/opt\/ml\/processing\/output\/profile_vw_aws_a_bijlage.html')\n                                    #switch profile name above\n    print(output_path_tblforprofile)\n    \n    profile_tblforprofile.to_file(output_path_tblforprofile)\n\n```\n\n```\nimport sagemaker\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsession = boto3.Session(region_name=\"eu-west-2\")\n\nbucket = 'intl-euro-uk-datascientist-prod'\n\nprefix = 'Mark'\n\nsm_session = sagemaker.Session(boto_session=session, default_bucket=bucket)\nsm_session.upload_data(path='vw_aws_a_bijlageprofile.py',\n                                bucket=bucket,\n                                key_prefix=f'{prefix}\/source')\n```\n\n\n```\nimport boto3\n#import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nregion = boto3.session.Session().region_name\n\n\nS3_ROOT_PATH = \"s3:\/\/{}\/{}\".format(bucket, prefix)\n\nrole = get_execution_role()\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     sagemaker_session=sm_session,\n                                     instance_type='ml.m5.24xlarge',\n                                     instance_count=1)\n```\n\n\n```\nsklearn_processor.run(code='s3:\/\/{}\/{}\/source\/vw_aws_a_bijlageprofile.py'.format(bucket, prefix),\n                      inputs=[],\n                      outputs=[ProcessingOutput(output_name='output',\n                                                source='\/opt\/ml\/processing\/output',\n                                                destination='s3:\/\/intl-euro-uk-datascientist-prod\/Mark\/IODataProfiles\/')])\n```",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1661439501840,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668527409096,
        "Question_score_count":0.0,
        "Question_view_count":81.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVtb8YdmWSG2wFr5aMI5MaA\/sklearn-processing-container-error-warning-running-pip-as-the-root-user-can-result-in-broken-permissions-and-conflicting-behaviour-with-the-system-package-manager",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"sklearn process contain error warn run pip root user result broken permiss conflict behaviour packag manag hei try run script writefil titl aw bijlageprofil code work data sourc get follow error messag cloudwatch log warn run pip root user result broken permiss conflict behaviour packag manag recommend us virtual environ instead http pip pypa warn venv idea error code thank advanc writefil aw bijlageprofil import import sy import subprocess def instal packag subprocess check sy execut pip instal packag instal awswrangl instal tqdm instal panda instal botocor instal ruamel yaml instal panda profil import awswrangl import panda import numpi import datetim dateutil relativedelta import relativedelta string import templat import import boto panda profil import profilereport client boto client session boto session region west def run profil queri select intl euro archmcc databas aw bijlag swich tabl tableforprofil athena read sql queri queri databas intl euro archmcc databas boto session session cta approach fals workgroup datascientist print read tabl queri print got rid miss ad new index profil tblforprofil profilereport tableforprofil titl panda profil report minim true print gener tabl profil return profil tblforprofil main profil tblforprofil run profil print gener output output path tblforprofil opt process output profil aw bijlag html switch profil print output path tblforprofil profil tblforprofil file output path tblforprofil import process import processinginput processingoutput session boto session region west bucket intl euro datascientist prod prefix mark session session boto session session default bucket bucket session upload data path aw bijlageprofil bucket bucket kei prefix prefix sourc import boto import import execut role sklearn process import sklearnprocessor region boto session session region root path format bucket prefix role execut role sklearn processor sklearnprocessor framework version role role session session instanc type xlarg instanc count sklearn processor run code sourc aw bijlageprofil format bucket prefix input output processingoutput output output sourc opt process output destin intl euro datascientist prod mark iodataprofil",
        "Question_preprocessed_content":"sklearn process contain error warn run pip root user result broken permiss conflict behaviour packag hei try run script writefil titl code work data sourc get follow error messag cloudwatch log warn run pip root user result broken permiss conflict behaviour packag manag recommend us virtual environ instead idea error code thank advanc",
        "Question_gpt_summary_original":"The user is encountering an error message while running a script in a writefile titled \"vw_aws_a_bijlageprofile.py\" using SKLearn Processing Container. The error message warns that running pip as the 'root' user can result in broken permissions and conflicting behavior with the system package manager. The user is seeking help to resolve this error.",
        "Question_gpt_summary":"user encount error messag run script writefil titl aw bijlageprofil sklearn process contain error messag warn run pip root user result broken permiss conflict behavior packag manag user seek help resolv error"
    },
    {
        "Question_title":"VSCode cannot see packages on a GCP VM",
        "Question_body":"<p>I have an issue where vscode when connected to a VM on GCP cannot see packages installed in <code>\/opt\/conda\/lib\/python3.7\/site-packages.<\/code> I have created the VM using Vertex AI. When I open the jupyter notebook through the UI in a the browser I can see all the packages via <code>pip3 list<\/code>. But when I am connected to the VM through SSH in vscode I cannot see the packages installed such as nltk, spacy etc. and when I try to load it gives me <code>ModuleNotFoundError<\/code>. This error does not show up when I use the jupyter notebook from the Vertex AI UI. The site-packages folder is in my system path and the python that I am using is <code>\/opt\/conda\/bin\/python3<\/code>.<\/p>\n<p>Any help is appreciated. Please do let me know if my question is clear.<\/p>\n<p>EDIT: I figured out that my packages are running on a container in the VM. Is there a way for me to access those packages via jupyter notebook in vscode?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1638473538253,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1638550660460,
        "Question_score_count":0.0,
        "Question_view_count":230.0,
        "Poster_created_time":1580840045043,
        "Poster_location":null,
        "Poster_reputation_count":85.0,
        "Poster_view_count":16.0,
        "Answer_body":"<p>Posting the answer as community wiki. As confirmed by @Abhishek, he was able to make it work by installing a docker extension on the VM then attach VS code to the container.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1638843453688,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1606724007903,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70205432",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"vscode packag gcp issu vscode connect gcp packag instal opt conda lib python site packag creat open jupyt notebook browser packag pip list connect ssh vscode packag instal nltk spaci try load give modulenotfounderror error us jupyt notebook site packag folder path python opt conda bin python help appreci let know question clear edit figur packag run contain wai access packag jupyt notebook vscode",
        "Question_preprocessed_content":"vscode packag gcp issu vscode connect gcp packag instal creat open jupyt notebook browser packag connect ssh vscode packag instal nltk spaci try load give error us jupyt notebook folder path python help appreci let know question clear edit figur packag run contain wai access packag jupyt notebook vscode",
        "Question_gpt_summary_original":"The user is facing an issue where VSCode, when connected to a VM on GCP, cannot see packages installed in \/opt\/conda\/lib\/python3.7\/site-packages. The packages are running on a container in the VM, and the user is looking for a way to access those packages via Jupyter Notebook in VSCode. The error message received is ModuleNotFoundError.",
        "Question_gpt_summary":"user face issu vscode connect gcp packag instal opt conda lib python site packag packag run contain user look wai access packag jupyt notebook vscode error messag receiv modulenotfounderror"
    },
    {
        "Question_title":"AciDeploymentFailed",
        "Question_body":"<p>Hello,    <\/p>\n<p>It's been a few days already since I've been struggling with this error which is not suggesting me anything.    <br \/>\nThis is the error I receive. Every time I'm trying to access the logs it displays me <em>&quot;None&quot;<\/em>. Also, the init() function is a very basic one. It's the one I've found in your tutorials and while I've followed your tutorials I didn't encounter this bug.    <\/p>\n<p><em><strong>score.py script:<\/strong><\/em>    <\/p>\n<pre><code>import pandas as pd  \nimport numpy as np  \nimport joblib  \nimport json  \nimport os  \n  \n# Called when the service is loaded  \ndef init():  \n    global model  \n    # Get the path to the deployed model file and load it  \n    model = joblib.load(Model.get_model_path(model_name='aml_live_model_end'))  \n  \n# Called when a request is received  \ndef run(raw_data):  \n    # Get the input data as a numpy array  \n    data = np.array(json.loads(raw_data)['data'])  \n    # Get a prediction from the model  \n    predictions = model.predict(data)  \n    # Get the corresponding classname for each prediction (0 or 1)  \n    classnames = ['De avizat', 'De analizat']  \n    predicted_classes = []  \n    for prediction in predictions:  \n        predicted_classes.append(classnames[prediction])  \n    # Return the predictions as JSON  \n    return json.dumps(predicted_classes)  \n<\/code><\/pre>\n<p><em><strong>.yaml file<\/strong><\/em>    <\/p>\n<pre><code>name: aml_live_env  \ndependencies:  \n- python=3.6.2  \n- scikit-learn  \n- ipykernel  \n- matplotlib  \n- pandas  \n- pip  \n- pip:  \n  - azureml-defaults  \n  - pyarrow  \n<\/code><\/pre>\n<p><em><strong>The error I receive<\/strong><\/em>    <\/p>\n<pre><code>Deploying model...  \nTips: You can try get_logs(): https:\/\/aka.ms\/debugimage#dockerlog or local deployment: https:\/\/aka.ms\/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.  \nRunning  \n2022-06-17 10:52:16+00:00 Creating Container Registry if not exists.  \n2022-06-17 10:52:16+00:00 Registering the environment.  \n2022-06-17 10:52:17+00:00 Use the existing image.  \n2022-06-17 10:52:17+00:00 Generating deployment configuration.  \n2022-06-17 10:52:18+00:00 Submitting deployment to compute.  \n2022-06-17 10:52:20+00:00 Checking the status of deployment aml-live-service-model..  \n2022-06-17 10:54:07+00:00 Checking the status of inference endpoint aml-live-service-model.  \nFailed  \nService deployment polling reached non-successful terminal state, current service state: Failed  \nOperation ID: 93d48e89-cb16-4d1c-bbb6-f453acaeaa7f  \nMore information can be found using '.get_logs()'  \nError:  \n{  \n  &quot;code&quot;: &quot;AciDeploymentFailed&quot;,  \n  &quot;statusCode&quot;: 400,  \n  &quot;message&quot;: &quot;Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.  \n\t1. Please check the logs for your container instance: aml-live-service-model. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.  \n\t2. You can interactively debug your scoring file locally. Please refer to https:\/\/learn.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.  \n\t3. You can also try to run image libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31 locally. Please refer to https:\/\/aka.ms\/debugimage#service-launch-fails for more information.&quot;,  \n  &quot;details&quot;: [  \n    {  \n      &quot;code&quot;: &quot;CrashLoopBackOff&quot;,  \n      &quot;message&quot;: &quot;Your container application crashed. This may be caused by errors in your scoring file's init() function.  \n\t1. Please check the logs for your container instance: aml-live-service-model. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.  \n\t2. You can interactively debug your scoring file locally. Please refer to https:\/\/learn.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.  \n\t3. You can also try to run image libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31 locally. Please refer to https:\/\/aka.ms\/debugimage#service-launch-fails for more information.&quot;  \n    },  \n    {  \n      &quot;code&quot;: &quot;AciDeploymentFailed&quot;,  \n      &quot;message&quot;: &quot;Your container application crashed. Please follow the steps to debug:  \n\t1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https:\/\/aka.ms\/debugimage#dockerlog for more information.  \n\t2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https:\/\/aka.ms\/debugimage#debug-locally for more information.  \n\t3. You can also interactively debug your scoring file locally. Please refer to https:\/\/learn.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.  \n\t4. View the diagnostic events to check status of container, it may help you to debug the issue.  \n&quot;RestartCount&quot;: 5  \n&quot;CurrentState&quot;: {&quot;state&quot;:&quot;Waiting&quot;,&quot;startTime&quot;:null,&quot;exitCode&quot;:null,&quot;finishTime&quot;:null,&quot;detailStatus&quot;:&quot;CrashLoopBackOff: Back-off restarting failed&quot;}  \n&quot;PreviousState&quot;: {&quot;state&quot;:&quot;Terminated&quot;,&quot;startTime&quot;:&quot;2022-06-17T10:56:57.554Z&quot;,&quot;exitCode&quot;:111,&quot;finishTime&quot;:&quot;2022-06-17T10:57:01.314Z&quot;,&quot;detailStatus&quot;:&quot;Error&quot;}  \n&quot;Events&quot;:  \n{&quot;count&quot;:1,&quot;firstTimestamp&quot;:&quot;2022-06-17T10:26:38Z&quot;,&quot;lastTimestamp&quot;:&quot;2022-06-17T10:26:38Z&quot;,&quot;name&quot;:&quot;Pulling&quot;,&quot;message&quot;:&quot;pulling image &quot;libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c&quot;&quot;,&quot;type&quot;:&quot;Normal&quot;}  \n{&quot;count&quot;:1,&quot;firstTimestamp&quot;:&quot;2022-06-17T10:27:42Z&quot;,&quot;lastTimestamp&quot;:&quot;2022-06-17T10:27:42Z&quot;,&quot;name&quot;:&quot;Pulled&quot;,&quot;message&quot;:&quot;Successfully pulled image &quot;libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c&quot;&quot;,&quot;type&quot;:&quot;Normal&quot;}  \n{&quot;count&quot;:10,&quot;firstTimestamp&quot;:&quot;2022-06-17T10:28:00Z&quot;,&quot;lastTimestamp&quot;:&quot;2022-06-17T10:47:11Z&quot;,&quot;name&quot;:&quot;Started&quot;,&quot;message&quot;:&quot;Started container&quot;,&quot;type&quot;:&quot;Normal&quot;}  \n{&quot;count&quot;:9,&quot;firstTimestamp&quot;:&quot;2022-06-17T10:28:03Z&quot;,&quot;lastTimestamp&quot;:&quot;2022-06-17T10:40:46Z&quot;,&quot;name&quot;:&quot;Killing&quot;,&quot;message&quot;:&quot;Killing container with id a7e717efa63259b36b19bc4951b3f3dcc5f1093177e729c589355a7371353ca3.&quot;,&quot;type&quot;:&quot;Normal&quot;}  \n{&quot;count&quot;:1,&quot;firstTimestamp&quot;:&quot;2022-06-17T10:31:33Z&quot;,&quot;lastTimestamp&quot;:&quot;2022-06-17T10:31:33Z&quot;,&quot;name&quot;:&quot;Pulling&quot;,&quot;message&quot;:&quot;pulling image &quot;libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c&quot;&quot;,&quot;type&quot;:&quot;Normal&quot;}  \n{&quot;count&quot;:1,&quot;firstTimestamp&quot;:&quot;2022-06-17T10:32:31Z&quot;,&quot;lastTimestamp&quot;:&quot;2022-06-17T10:32:31Z&quot;,&quot;name&quot;:&quot;Pulled&quot;,&quot;message&quot;:&quot;Successfully pulled image &quot;libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c&quot;&quot;,&quot;type&quot;:&quot;Normal&quot;}  \n{&quot;count&quot;:5,&quot;firstTimestamp&quot;:&quot;2022-06-17T10:47:52Z&quot;,&quot;lastTimestamp&quot;:&quot;2022-06-17T10:52:09Z&quot;,&quot;name&quot;:&quot;Started&quot;,&quot;message&quot;:&quot;Started container&quot;,&quot;type&quot;:&quot;Normal&quot;}  \n{&quot;count&quot;:6,&quot;firstTimestamp&quot;:&quot;2022-06-17T10:48:26Z&quot;,&quot;lastTimestamp&quot;:&quot;2022-06-17T10:52:37Z&quot;,&quot;name&quot;:&quot;Killing&quot;,&quot;message&quot;:&quot;Killing container with id 2bdec1005a6dd58312e10ab939d88ea08b312771de6573d9b86c5f571104277e.&quot;,&quot;type&quot;:&quot;Normal&quot;}  \n&quot;  \n    }  \n  ]  \n}  \n  \n---------------------------------------------------------------------------  \nWebserviceException                       Traceback (most recent call last)  \n&lt;ipython-input-17-315dbb5f83ec&gt; in &lt;module&gt;  \n     16 service_name = &quot;aml-live-service-model&quot;  \n     17 service = Model.deploy(ws, service_name, [model], inference_config, deployment_config, overwrite=True)  \n---&gt; 18 service.wait_for_deployment(True)  \n     19 print(service.state)  \n  \n\/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output, timeout_sec)  \n    916                     logs_response = 'Current sub-operation type not known, more logs unavailable.'  \n    917   \n--&gt; 918                 raise WebserviceException('Service deployment polling reached non-successful terminal state, current '  \n    919                                           'service state: {}\\n'  \n    920                                           'Operation ID: {}\\n'  \n  \nWebserviceException: WebserviceException:  \n\tMessage: Service deployment polling reached non-successful terminal state, current service state: Failed  \nOperation ID: 93d48e89-cb16-4d1c-bbb6-f453acaeaa7f  \nMore information can be found using '.get_logs()'  \nError:  \n{  \n  &quot;code&quot;: &quot;AciDeploymentFailed&quot;,  \n  &quot;statusCode&quot;: 400,  \n  &quot;message&quot;: &quot;Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.  \n\t1. Please check the logs for your container instance: aml-live-service-model. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.  \n\t2. You can interactively debug your scoring file locally. Please refer to https:\/\/learn.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.  \n\t3. You can also try to run image libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31 locally. Please refer to https:\/\/aka.ms\/debugimage#service-launch-fails for more information.&quot;,  \n  &quot;details&quot;: [  \n    {  \n      &quot;code&quot;: &quot;CrashLoopBackOff&quot;,  \n      &quot;message&quot;: &quot;Your container application crashed. This may be caused by errors in your scoring file's init() function.  \n\t1. Please check the logs for your container instance: aml-live-service-model. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.  \n\t2. You can interactively debug your scoring file locally. Please refer to https:\/\/learn.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.  \n\t3. You can also try to run image libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31 locally. Please refer to https:\/\/aka.ms\/debugimage#service-launch-fails for more information.&quot;  \n    },  \n    {  \n      &quot;code&quot;: &quot;AciDeploymentFailed&quot;,  \n      &quot;message&quot;: &quot;Your container application crashed. Please follow the steps to debug:  \n\t1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https:\/\/aka.ms\/debugimage#dockerlog for more information.  \n\t2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https:\/\/aka.ms\/debugimage#debug-locally for more information.  \n\t3. You can also interactively debug your scoring file locally. Please refer to https:\/\/learn.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.  \n\t4. View the diagnostic events to check status of container, it may help you to debug the issue.  \n&quot;RestartCount&quot;: 5  \n&quot;CurrentState&quot;: {&quot;state&quot;:&quot;Waiting&quot;,&quot;startTime&quot;:null,&quot;exitCode&quot;:null,&quot;finishTime&quot;:null,&quot;detailStatus&quot;:&quot;CrashLoopBackOff: Back-off restarting failed&quot;}  \n&quot;PreviousState&quot;: {&quot;state&quot;:&quot;Terminated&quot;,&quot;startTime&quot;:&quot;2022-06-17T10:56:57.554Z&quot;,&quot;exitCode&quot;:111,&quot;finishTime&quot;:&quot;2022-06-17T10:57:01.314Z&quot;,&quot;detailStatus&quot;:&quot;Error&quot;}  \n&quot;Events&quot;:  \n{&quot;count&quot;:1,&quot;firstTimestamp&quot;:&quot;2022-06-17T10:26:38Z&quot;,&quot;lastTimestamp&quot;:&quot;2022-06-17T10:26:38Z&quot;,&quot;name&quot;:&quot;Pulling&quot;,&quot;message&quot;:&quot;pulling image &quot;libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c&quot;&quot;,&quot;type&quot;:&quot;Normal&quot;}  \n{&quot;count&quot;:1,&quot;firstTimestamp&quot;:&quot;2022-06-17T10:27:42Z&quot;,&quot;lastTimestamp&quot;:&quot;2022-06-17T10:27:42Z&quot;,&quot;name&quot;:&quot;Pulled&quot;,&quot;message&quot;:&quot;Successfully pulled image &quot;libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c&quot;&quot;,&quot;type&quot;:&quot;Normal&quot;}  \n{&quot;count&quot;:10,&quot;firstTimestamp&quot;:&quot;2022-06-17T10:28:00Z&quot;,&quot;lastTimestamp&quot;:&quot;2022-06-17T10:47:11Z&quot;,&quot;name&quot;:&quot;Started&quot;,&quot;message&quot;:&quot;Started container&quot;,&quot;type&quot;:&quot;Normal&quot;}  \n{&quot;count&quot;:9,&quot;firstTimestamp&quot;:&quot;2022-06-17T10:28:03Z&quot;,&quot;lastTimestamp&quot;:&quot;2022-06-17T10:40:46Z&quot;,&quot;name&quot;:&quot;Killing&quot;,&quot;message&quot;:&quot;Killing container with id a7e717efa63259b36b19bc4951b3f3dcc5f1093177e729c589355a7371353ca3.&quot;,&quot;type&quot;:&quot;Normal&quot;}  \n{&quot;count&quot;:1,&quot;firstTimestamp&quot;:&quot;2022-06-17T10:31:33Z&quot;,&quot;lastTimestamp&quot;:&quot;2022-06-17T10:31:33Z&quot;,&quot;name&quot;:&quot;Pulling&quot;,&quot;message&quot;:&quot;pulling image &quot;libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c&quot;&quot;,&quot;type&quot;:&quot;Normal&quot;}  \n{&quot;count&quot;:1,&quot;firstTimestamp&quot;:&quot;2022-06-17T10:32:31Z&quot;,&quot;lastTimestamp&quot;:&quot;2022-06-17T10:32:31Z&quot;,&quot;name&quot;:&quot;Pulled&quot;,&quot;message&quot;:&quot;Successfully pulled image &quot;libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c&quot;&quot;,&quot;type&quot;:&quot;Normal&quot;}  \n{&quot;count&quot;:5,&quot;firstTimestamp&quot;:&quot;2022-06-17T10:47:52Z&quot;,&quot;lastTimestamp&quot;:&quot;2022-06-17T10:52:09Z&quot;,&quot;name&quot;:&quot;Started&quot;,&quot;message&quot;:&quot;Started container&quot;,&quot;type&quot;:&quot;Normal&quot;}  \n{&quot;count&quot;:6,&quot;firstTimestamp&quot;:&quot;2022-06-17T10:48:26Z&quot;,&quot;lastTimestamp&quot;:&quot;2022-06-17T10:52:37Z&quot;,&quot;name&quot;:&quot;Killing&quot;,&quot;message&quot;:&quot;Killing container with id 2bdec1005a6dd58312e10ab939d88ea08b312771de6573d9b86c5f571104277e.&quot;,&quot;type&quot;:&quot;Normal&quot;}  \n&quot;  \n    }  \n  ]  \n}  \n\tInnerException None  \n\tErrorResponse   \n{  \n    &quot;error&quot;: {  \n        &quot;message&quot;: &quot;Service deployment polling reached non-successful terminal state, current service state: Failed\\nOperation ID: 93d48e89-cb16-4d1c-bbb6-f453acaeaa7f\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\&quot;code\\&quot;: \\&quot;AciDeploymentFailed\\&quot;,\\n  \\&quot;statusCode\\&quot;: 400,\\n  \\&quot;message\\&quot;: \\&quot;Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\n\\t1. Please check the logs for your container instance: aml-live-service-model. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\\n\\t2. You can interactively debug your scoring file locally. Please refer to https:\/\/learn.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t3. You can also try to run image libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31 locally. Please refer to https:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\&quot;,\\n  \\&quot;details\\&quot;: [\\n    {\\n      \\&quot;code\\&quot;: \\&quot;CrashLoopBackOff\\&quot;,\\n      \\&quot;message\\&quot;: \\&quot;Your container application crashed. This may be caused by errors in your scoring file's init() function.\\n\\t1. Please check the logs for your container instance: aml-live-service-model. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\\n\\t2. You can interactively debug your scoring file locally. Please refer to https:\/\/learn.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t3. You can also try to run image libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31 locally. Please refer to https:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\&quot;\\n    },\\n    {\\n      \\&quot;code\\&quot;: \\&quot;AciDeploymentFailed\\&quot;,\\n      \\&quot;message\\&quot;: \\&quot;Your container application crashed. Please follow the steps to debug:\\n\\t1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https:\/\/aka.ms\/debugimage#dockerlog for more information.\\n\\t2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https:\/\/aka.ms\/debugimage#debug-locally for more information.\\n\\t3. You can also interactively debug your scoring file locally. Please refer to https:\/\/learn.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t4. View the diagnostic events to check status of container, it may help you to debug the issue.\\n\\&quot;RestartCount\\&quot;: 5\\n\\&quot;CurrentState\\&quot;: {\\&quot;state\\&quot;:\\&quot;Waiting\\&quot;,\\&quot;startTime\\&quot;:null,\\&quot;exitCode\\&quot;:null,\\&quot;finishTime\\&quot;:null,\\&quot;detailStatus\\&quot;:\\&quot;CrashLoopBackOff: Back-off restarting failed\\&quot;}\\n\\&quot;PreviousState\\&quot;: {\\&quot;state\\&quot;:\\&quot;Terminated\\&quot;,\\&quot;startTime\\&quot;:\\&quot;2022-06-17T10:56:57.554Z\\&quot;,\\&quot;exitCode\\&quot;:111,\\&quot;finishTime\\&quot;:\\&quot;2022-06-17T10:57:01.314Z\\&quot;,\\&quot;detailStatus\\&quot;:\\&quot;Error\\&quot;}\\n\\&quot;Events\\&quot;:\\n{\\&quot;count\\&quot;:1,\\&quot;firstTimestamp\\&quot;:\\&quot;2022-06-17T10:26:38Z\\&quot;,\\&quot;lastTimestamp\\&quot;:\\&quot;2022-06-17T10:26:38Z\\&quot;,\\&quot;name\\&quot;:\\&quot;Pulling\\&quot;,\\&quot;message\\&quot;:\\&quot;pulling image \\&quot;libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c\\&quot;\\&quot;,\\&quot;type\\&quot;:\\&quot;Normal\\&quot;}\\n{\\&quot;count\\&quot;:1,\\&quot;firstTimestamp\\&quot;:\\&quot;2022-06-17T10:27:42Z\\&quot;,\\&quot;lastTimestamp\\&quot;:\\&quot;2022-06-17T10:27:42Z\\&quot;,\\&quot;name\\&quot;:\\&quot;Pulled\\&quot;,\\&quot;message\\&quot;:\\&quot;Successfully pulled image \\&quot;libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c\\&quot;\\&quot;,\\&quot;type\\&quot;:\\&quot;Normal\\&quot;}\\n{\\&quot;count\\&quot;:10,\\&quot;firstTimestamp\\&quot;:\\&quot;2022-06-17T10:28:00Z\\&quot;,\\&quot;lastTimestamp\\&quot;:\\&quot;2022-06-17T10:47:11Z\\&quot;,\\&quot;name\\&quot;:\\&quot;Started\\&quot;,\\&quot;message\\&quot;:\\&quot;Started container\\&quot;,\\&quot;type\\&quot;:\\&quot;Normal\\&quot;}\\n{\\&quot;count\\&quot;:9,\\&quot;firstTimestamp\\&quot;:\\&quot;2022-06-17T10:28:03Z\\&quot;,\\&quot;lastTimestamp\\&quot;:\\&quot;2022-06-17T10:40:46Z\\&quot;,\\&quot;name\\&quot;:\\&quot;Killing\\&quot;,\\&quot;message\\&quot;:\\&quot;Killing container with id a7e717efa63259b36b19bc4951b3f3dcc5f1093177e729c589355a7371353ca3.\\&quot;,\\&quot;type\\&quot;:\\&quot;Normal\\&quot;}\\n{\\&quot;count\\&quot;:1,\\&quot;firstTimestamp\\&quot;:\\&quot;2022-06-17T10:31:33Z\\&quot;,\\&quot;lastTimestamp\\&quot;:\\&quot;2022-06-17T10:31:33Z\\&quot;,\\&quot;name\\&quot;:\\&quot;Pulling\\&quot;,\\&quot;message\\&quot;:\\&quot;pulling image \\&quot;libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c\\&quot;\\&quot;,\\&quot;type\\&quot;:\\&quot;Normal\\&quot;}\\n{\\&quot;count\\&quot;:1,\\&quot;firstTimestamp\\&quot;:\\&quot;2022-06-17T10:32:31Z\\&quot;,\\&quot;lastTimestamp\\&quot;:\\&quot;2022-06-17T10:32:31Z\\&quot;,\\&quot;name\\&quot;:\\&quot;Pulled\\&quot;,\\&quot;message\\&quot;:\\&quot;Successfully pulled image \\&quot;libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c\\&quot;\\&quot;,\\&quot;type\\&quot;:\\&quot;Normal\\&quot;}\\n{\\&quot;count\\&quot;:5,\\&quot;firstTimestamp\\&quot;:\\&quot;2022-06-17T10:47:52Z\\&quot;,\\&quot;lastTimestamp\\&quot;:\\&quot;2022-06-17T10:52:09Z\\&quot;,\\&quot;name\\&quot;:\\&quot;Started\\&quot;,\\&quot;message\\&quot;:\\&quot;Started container\\&quot;,\\&quot;type\\&quot;:\\&quot;Normal\\&quot;}\\n{\\&quot;count\\&quot;:6,\\&quot;firstTimestamp\\&quot;:\\&quot;2022-06-17T10:48:26Z\\&quot;,\\&quot;lastTimestamp\\&quot;:\\&quot;2022-06-17T10:52:37Z\\&quot;,\\&quot;name\\&quot;:\\&quot;Killing\\&quot;,\\&quot;message\\&quot;:\\&quot;Killing container with id 2bdec1005a6dd58312e10ab939d88ea08b312771de6573d9b86c5f571104277e.\\&quot;,\\&quot;type\\&quot;:\\&quot;Normal\\&quot;}\\n\\&quot;\\n    }\\n  ]\\n}&quot;  \n    }  \n}  \n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_created_time":1655464430947,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/893526\/acideploymentfailed",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"acideploymentfail hello dai struggl error suggest error receiv time try access log displai init function basic tutori follow tutori encount bug score script import panda import numpi import joblib import json import call servic load def init global model path deploi model file load model joblib load model model path model aml live model end call request receiv def run raw data input data numpi arrai data arrai json load raw data data predict model predict model predict data correspond classnam predict classnam avizat analizat predict class predict predict predict class append classnam predict return predict json return json dump predict class yaml file aml live env depend python scikit learn ipykernel matplotlib panda pip pip default pyarrow error receiv deploi model tip try log http aka debugimag dockerlog local deploy http aka debugimag debug local debug deploy take longer minut run creat contain registri exist regist environ us exist imag gener deploy configur submit deploy comput check statu deploy aml live servic model check statu infer endpoint aml live servic model fail servic deploy poll reach non success termin state current servic state fail oper facaeaaf inform log error code acideploymentfail statuscod messag aci deploy fail except contain applic crash caus error score file init function check log contain instanc aml live servic model aml sdk run print servic log servic object fetch log interact debug score file local refer http learn microsoft com azur machin learn debug visual studio code debug troubleshoot deploy inform try run imag libraaimlworcb azurecr fddeceebadfedcf local refer http aka debugimag servic launch fail inform detail code crashloopbackoff messag contain applic crash caus error score file init function check log contain instanc aml live servic model aml sdk run print servic log servic object fetch log interact debug score file local refer http learn microsoft com azur machin learn debug visual studio code debug troubleshoot deploy inform try run imag libraaimlworcb azurecr fddeceebadfedcf local refer http aka debugimag servic launch fail inform code acideploymentfail messag contain applic crash follow step debug aml sdk run print servic log servic object fetch log refer http aka debugimag dockerlog inform contain applic crash caus error score file init function try debug local refer http aka debugimag debug local inform interact debug score file local refer http learn microsoft com azur machin learn debug visual studio code debug troubleshoot deploy inform view diagnost event check statu contain help debug issu restartcount currentst state wait starttim null exitcod null finishtim null detailstatu crashloopbackoff restart fail previousst state termin starttim exitcod finishtim detailstatu error event count firsttimestamp lasttimestamp pull messag pull imag libraaimlworcb azurecr fddeceebadfedcf sha fcaebbbcbaaadebddbceaac type normal count firsttimestamp lasttimestamp pull messag successfulli pull imag libraaimlworcb azurecr fddeceebadfedcf sha fcaebbbcbaaadebddbceaac type normal count firsttimestamp lasttimestamp start messag start contain type normal count firsttimestamp lasttimestamp kill messag kill contain aeefabbbcbfdccfecaca type normal count firsttimestamp lasttimestamp pull messag pull imag libraaimlworcb azurecr fddeceebadfedcf sha fcaebbbcbaaadebddbceaac type normal count firsttimestamp lasttimestamp pull messag successfulli pull imag libraaimlworcb azurecr fddeceebadfedcf sha fcaebbbcbaaadebddbceaac type normal count firsttimestamp lasttimestamp start messag start contain type normal count firsttimestamp lasttimestamp kill messag kill contain bdecaddeabdeabdedbcf type normal webserviceexcept traceback recent servic aml live servic model servic model deploi servic model infer config deploy config overwrit true servic wait deploy true print servic state anaconda env lib python site packag core webservic webservic wait deploy self output timeout sec log respons current sub oper type known log unavail rais webserviceexcept servic deploy poll reach non success termin state current servic state oper webserviceexcept webserviceexcept messag servic deploy poll reach non success termin state current servic state fail oper facaeaaf inform log error code acideploymentfail statuscod messag aci deploy fail except contain applic crash caus error score file init function check log contain instanc aml live servic model aml sdk run print servic log servic object fetch log interact debug score file local refer http learn microsoft com azur machin learn debug visual studio code debug troubleshoot deploy inform try run imag libraaimlworcb azurecr fddeceebadfedcf local refer http aka debugimag servic launch fail inform detail code crashloopbackoff messag contain applic crash caus error score file init function check log contain instanc aml live servic model aml sdk run print servic log servic object fetch log interact debug score file local refer http learn microsoft com azur machin learn debug visual studio code debug troubleshoot deploy inform try run imag libraaimlworcb azurecr fddeceebadfedcf local refer http aka debugimag servic launch fail inform code acideploymentfail messag contain applic crash follow step debug aml sdk run print servic log servic object fetch log refer http aka debugimag dockerlog inform contain applic crash caus error score file init function try debug local refer http aka debugimag debug local inform interact debug score file local refer http learn microsoft com azur machin learn debug visual studio code debug troubleshoot deploy inform view diagnost event check statu contain help debug issu restartcount currentst state wait starttim null exitcod null finishtim null detailstatu crashloopbackoff restart fail previousst state termin starttim exitcod finishtim detailstatu error event count firsttimestamp lasttimestamp pull messag pull imag libraaimlworcb azurecr fddeceebadfedcf sha fcaebbbcbaaadebddbceaac type normal count firsttimestamp lasttimestamp pull messag successfulli pull imag libraaimlworcb azurecr fddeceebadfedcf sha fcaebbbcbaaadebddbceaac type normal count firsttimestamp lasttimestamp start messag start contain type normal count firsttimestamp lasttimestamp kill messag kill contain aeefabbbcbfdccfecaca type normal count firsttimestamp lasttimestamp pull messag pull imag libraaimlworcb azurecr fddeceebadfedcf sha fcaebbbcbaaadebddbceaac type normal count firsttimestamp lasttimestamp pull messag successfulli pull imag libraaimlworcb azurecr fddeceebadfedcf sha fcaebbbcbaaadebddbceaac type normal count firsttimestamp lasttimestamp start messag start contain type normal count firsttimestamp lasttimestamp kill messag kill contain bdecaddeabdeabdedbcf type normal innerexcept errorrespons error messag servic deploy poll reach non success termin state current servic state fail noper facaeaaf nmore inform log nerror code acideploymentfail statuscod messag aci deploy fail except contain applic crash caus error score file init function check log contain instanc aml live servic model aml sdk run print servic log servic object fetch log interact debug score file local refer http learn microsoft com azur machin learn debug visual studio code debug troubleshoot deploy inform try run imag libraaimlworcb azurecr fddeceebadfedcf local refer http aka debugimag servic launch fail inform detail code crashloopbackoff messag contain applic crash caus error score file init function check log contain instanc aml live servic model aml sdk run print servic log servic object fetch log interact debug score file local refer http learn microsoft com azur machin learn debug visual studio code debug troubleshoot deploy inform try run imag libraaimlworcb azurecr fddeceebadfedcf local refer http aka debugimag servic launch fail inform code acideploymentfail messag contain applic crash follow step debug aml sdk run print servic log servic object fetch log refer http aka debugimag dockerlog inform contain applic crash caus error score file init function try debug local refer http aka debugimag debug local inform interact debug score file local refer http learn microsoft com azur machin learn debug visual studio code debug troubleshoot deploy inform view diagnost event check statu contain help debug issu restartcount currentst state wait starttim null exitcod null finishtim null detailstatu crashloopbackoff restart fail previousst state termin starttim exitcod finishtim detailstatu error event count firsttimestamp lasttimestamp pull messag pull imag libraaimlworcb azurecr fddeceebadfedcf sha fcaebbbcbaaadebddbceaac type normal count firsttimestamp lasttimestamp pull messag successfulli pull imag libraaimlworcb azurecr fddeceebadfedcf sha fcaebbbcbaaadebddbceaac type normal count firsttimestamp lasttimestamp start messag start contain type normal count firsttimestamp lasttimestamp kill messag kill contain aeefabbbcbfdccfecaca type normal count firsttimestamp lasttimestamp pull messag pull imag libraaimlworcb azurecr fddeceebadfedcf sha fcaebbbcbaaadebddbceaac type normal count firsttimestamp lasttimestamp pull messag successfulli pull imag libraaimlworcb azurecr fddeceebadfedcf sha fcaebbbcbaaadebddbceaac type normal count firsttimestamp lasttimestamp start messag start contain type normal count firsttimestamp lasttimestamp kill messag kill contain bdecaddeabdeabdedbcf type normal",
        "Question_preprocessed_content":"acideploymentfail hello dai struggl error suggest error receiv time try access log displai init function basic tutori follow tutori encount bug script yaml file error receiv",
        "Question_gpt_summary_original":"The user is facing challenges in detecting 3D models through a mobile camera or 3D scanner and verifying if the scanned model is available in cloud storage. If the model is not available, the user needs a list of approximate models with a percentage match.",
        "Question_gpt_summary":"user face challeng detect model mobil camera scanner verifi scan model avail cloud storag model avail user need list approxim model percentag match"
    },
    {
        "Question_title":"KeyError: 'ETag' while trying to load data from S3 to Sagemaker",
        "Question_body":"<p>I Unload a file of 500 MB into S3 from Redshift, instead of saving into a single file in S3 it bifurcated into several chunks and now I am trying to access it from S3 to AWS Sagemaker. While trying to read the file using Pd.read_csv and dask.dataframe.read_csv I am getting Keyerror as 'ETag'<\/p>\n<p>I'm a newbie to AWS, please do help me.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9a78F.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9a78F.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_created_time":1650904478070,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":215.0,
        "Poster_created_time":1551957360467,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":327.0,
        "Poster_view_count":15.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72002863",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"keyerror etag try load data unload file redshift instead save singl file bifurc chunk try access try read file read csv dask datafram read csv get keyerror etag newbi aw help",
        "Question_preprocessed_content":"keyerror etag try load data unload file redshift instead save singl file bifurc chunk try access try read file get keyerror etag newbi aw help",
        "Question_gpt_summary_original":"The user is encountering a KeyError with the message 'ETag' while trying to load data from S3 to Sagemaker. The file was originally saved in S3 in several chunks instead of a single file, and the user is attempting to read it using Pd.read_csv and dask.dataframe.read_csv. The user is seeking assistance as a newcomer to AWS.",
        "Question_gpt_summary":"user encount keyerror messag etag try load data file origin save chunk instead singl file user attempt read read csv dask datafram read csv user seek assist newcom aw"
    },
    {
        "Question_title":"ML model deployment issue",
        "Question_body":"<p>I am trying to deploy an ML classification model on Azure using GUI.    <\/p>\n<p>After registering\/uploading the model inside the portal, I am deploying the model in the Azure container instance, with custom entry_script and the conda dependencies.    <\/p>\n<p><strong>Entry Script<\/strong>    <\/p>\n<pre><code># Importing Pacakges  \nimport pandas as pd  \nimport pickle  \nimport regex, json  \nimport numpy as np  \nimport sklearn  \nimport os  \n  \nfrom inference_schema.schema_decorators import input_schema, output_schema  \nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType  \n  \ndef init():  \n    global model  \n    global classes  \n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'randomForest50.pkl')  \n    model = pickle.load(open(model_path, &quot;rb&quot;))  \n    classes = lambda x : [&quot;F&quot;, &quot;M&quot;][x]  \n  \ninput_sample = np.array([['Thomas', 'Anna']])  \noutput_sample = np.array(['m', 'F'])  \n  \n  \n@input_schema('data', NumpyParameterType(input_sample))  \n@output_schema(NumpyParameterType(output_sample))  \ndef run(data):  \n    try:  \n        namesList = json.loads(data)[&quot;data&quot;][&quot;names&quot;]  \n        pred = list(map(classes, model.predict(preprocessing(namesList))))  \n        return str(pred[0])  \n    except Exception as e:  \n        error = str(e)  \n        return error  \n<\/code><\/pre>\n<p>Conda.yaml    <\/p>\n<pre><code>name: prediction  \ndependencies:  \n- python=3.7  \n- numpy  \n- scikit-learn  \n- pip:  \n    - azureml-defaults  \n    - pandas  \n    - pickle4  \n    - regex  \n    - inference-schema[numpy-support]     \n<\/code><\/pre>\n<p>After deployment, the endpoint deployment state goes to unhealthy. and the logs show that program is stuck in a loop. Check logs below:    <\/p>\n<pre><code>2021-04-26T08:14:55,433967500+00:00 - rsyslog\/run   \n2021-04-26T08:14:55,421414500+00:00 - iot-server\/run   \n2021-04-26T08:14:55,540534600+00:00 - gunicorn\/run   \n2021-04-26T08:14:55,646209100+00:00 - nginx\/run   \nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...  \n2021-04-26T08:14:58,234212800+00:00 - iot-server\/finish 1 0  \n2021-04-26T08:14:58,324505300+00:00 - Exit code 1 is normal. Not restarting iot-server.  \nStarting gunicorn 19.9.0  \nListening at: http:\/\/127.0.0.1:31311 (62)  \nUsing worker: sync  \nworker timeout is set to 300  \nBooting worker with pid: 89  \nSPARK_HOME not set. Skipping PySpark Initialization.  \nInitializing logger  \n2021-04-26 08:15:11,623 | root | INFO | Starting up app insights client  \n2021-04-26 08:15:11,624 | root | INFO | Starting up request id generator  \n2021-04-26 08:15:11,631 | root | INFO | Starting up app insight hooks  \n2021-04-26 08:15:11,632 | root | INFO | Invoking user's init function  \nworker timeout is set to 300  \nBooting worker with pid: 91  \nSPARK_HOME not set. Skipping PySpark Initialization.  \nInitializing logger  \n2021-04-26 08:15:29,014 | root | INFO | Starting up app insights client  \n2021-04-26 08:15:29,014 | root | INFO | Starting up request id generator  \n2021-04-26 08:15:29,014 | root | INFO | Starting up app insight hooks  \n2021-04-26 08:15:29,014 | root | INFO | Invoking user's init function  \nworker timeout is set to 300  \nBooting worker with pid: 98  \nSPARK_HOME not set. Skipping PySpark Initialization.  \n...  \n...  \n...  \n<\/code><\/pre>\n<p>I tried to deploy the model using python also. But it also failed with message:    <\/p>\n<pre><code>WebserviceException: WebserviceException:  \n Message: Service deployment polling reached non-successful terminal state, current service state: Failed  \nOperation ID: 98e464d4-5b15-4606-936f-a2625f7bd1fd  \nMore information can be found using '.get_logs()'  \nError:  \n{  \n  &quot;code&quot;: &quot;AciDeploymentFailed&quot;,  \n  &quot;statusCode&quot;: 400,  \n  &quot;message&quot;: &quot;Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\n\\t1. Please check the logs for your container instance: d16. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\\n\\t2. You can interactively debug your scoring file locally. Please refer to https:\/\/learn.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t3. You can also try to run image 20dd0f745f704eeb89ef4d52057871a0.azurecr.io\/azureml\/azureml_b9e8a2e66019f74c902eacced9684631 locally. Please refer to https:\/\/aka.ms\/debugimage#service-launch-fails for more information.&quot;,  \n  &quot;details&quot;: [  \n    {  \n      &quot;code&quot;: &quot;CrashLoopBackOff&quot;,  \n      &quot;message&quot;: &quot;Your container application crashed. This may be caused by errors in your scoring file's init() function.\\n\\t1. Please check the logs for your container instance: d16. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\\n\\t2. You can interactively debug your scoring file locally. Please refer to https:\/\/learn.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t3. You can also try to run image 20dd0f745f704eeb89ef4d52057871a0.azurecr.io\/azureml\/azureml_b9e8a2e66019f74c902eacced9684631 locally. Please refer to https:\/\/aka.ms\/debugimage#service-launch-fails for more information.&quot;  \n    },  \n    {  \n      &quot;code&quot;: &quot;AciDeploymentFailed&quot;,  \n      &quot;message&quot;: &quot;Your container application crashed. Please follow the steps to debug:\\n\\t1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https:\/\/aka.ms\/debugimage#dockerlog for more information.\\n\\t2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https:\/\/aka.ms\/debugimage#debug-locally for more information.\\n\\t3. You can also interactively debug your scoring file locally. Please refer to https:\/\/learn.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t4. View the diagnostic events to check status of container, it may help you to debug the issue.\\n\\&quot;RestartCount\\&quot;: 3\\n\\&quot;CurrentState\\&quot;: {\\&quot;state\\&quot;:\\&quot;Waiting\\&quot;,\\&quot;startTime\\&quot;:null,\\&quot;exitCode\\&quot;:null,\\&quot;finishTime\\&quot;:null,\\&quot;detailStatus\\&quot;:\\&quot;CrashLoopBackOff: Back-off restarting failed\\&quot;}\\n\\&quot;PreviousState\\&quot;: {\\&quot;state\\&quot;:\\&quot;Terminated\\&quot;,\\&quot;startTime\\&quot;:\\&quot;2021-04-27T10:46:03.903Z\\&quot;,\\&quot;exitCode\\&quot;:111,\\&quot;finishTime\\&quot;:\\&quot;2021-04-27T10:46:07.524Z\\&quot;,\\&quot;detailStatus\\&quot;:\\&quot;Error\\&quot;}\\n\\&quot;Events\\&quot;:\\n{\\&quot;count\\&quot;:1,\\&quot;firstTimestamp\\&quot;:\\&quot;2021-04-27T10:42:37Z\\&quot;,\\&quot;lastTimestamp\\&quot;:\\&quot;2021-04-27T10:42:37Z\\&quot;,\\&quot;name\\&quot;:\\&quot;Pulling\\&quot;,\\&quot;message\\&quot;:\\&quot;pulling image \\\\\\&quot;20dd0f745f704eeb89ef4d52057871a0.azurecr.io\/azureml\/azureml_b9e8a2e66019f74c902eacced9684631@sha256:322ebafbe88e98b0f57104fd0afad08a5caf57cc5e7f64b3b629c3ea50f54bb3\\\\\\&quot;\\&quot;,\\&quot;type\\&quot;:\\&quot;Normal\\&quot;}\\n{\\&quot;count\\&quot;:1,\\&quot;firstTimestamp\\&quot;:\\&quot;2021-04-27T10:44:15Z\\&quot;,\\&quot;lastTimestamp\\&quot;:\\&quot;2021-04-27T10:44:15Z\\&quot;,\\&quot;name\\&quot;:\\&quot;Pulled\\&quot;,\\&quot;message\\&quot;:\\&quot;Successfully pulled image \\\\\\&quot;20dd0f745f704eeb89ef4d52057871a0.azurecr.io\/azureml\/azureml_b9e8a2e66019f74c902eacced9684631@sha256:322ebafbe88e98b0f57104fd0afad08a5caf57cc5e7f64b3b629c3ea50f54bb3\\\\\\&quot;\\&quot;,\\&quot;type\\&quot;:\\&quot;Normal\\&quot;}\\n{\\&quot;count\\&quot;:4,\\&quot;firstTimestamp\\&quot;:\\&quot;2021-04-27T10:44:40Z\\&quot;,\\&quot;lastTimestamp\\&quot;:\\&quot;2021-04-27T10:46:03Z\\&quot;,\\&quot;name\\&quot;:\\&quot;Started\\&quot;,\\&quot;message\\&quot;:\\&quot;Started container\\&quot;,\\&quot;type\\&quot;:\\&quot;Normal\\&quot;}\\n{\\&quot;count\\&quot;:4,\\&quot;firstTimestamp\\&quot;:\\&quot;2021-04-27T10:44:43Z\\&quot;,\\&quot;lastTimestamp\\&quot;:\\&quot;2021-04-27T10:46:07Z\\&quot;,\\&quot;name\\&quot;:\\&quot;Killing\\&quot;,\\&quot;message\\&quot;:\\&quot;Killing container with id 5c5ddb266c4b38b1c306367712d9bec0687e5f6979e34afea7f6b943edf7db75.\\&quot;,\\&quot;type\\&quot;:\\&quot;Normal\\&quot;}\\n&quot;  \n    }  \n  ]  \n}  \n InnerException None  \n ErrorResponse   \n{  \n    &quot;error&quot;: {  \n        &quot;message&quot;: &quot;Service deployment polling reached non-successful terminal state, current service state: Failed\\nOperation ID: 98e464d4-5b15-4606-936f-a2625f7bd1fd\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\&quot;code\\&quot;: \\&quot;AciDeploymentFailed\\&quot;,\\n  \\&quot;statusCode\\&quot;: 400,\\n  \\&quot;message\\&quot;: \\&quot;Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\n\\\\t1. Please check the logs for your container instance: d16. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\\\\n\\\\t2. You can interactively debug your scoring file locally. Please refer to https:\/\/learn.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\\\n\\\\t3. You can also try to run image 20dd0f745f704eeb89ef4d52057871a0.azurecr.io\/azureml\/azureml_b9e8a2e66019f74c902eacced9684631 locally. Please refer to https:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\&quot;,\\n  \\&quot;details\\&quot;: [\\n    {\\n      \\&quot;code\\&quot;: \\&quot;CrashLoopBackOff\\&quot;,\\n      \\&quot;message\\&quot;: \\&quot;Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\n\\\\t1. Please check the logs for your container instance: d16. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\\\\n\\\\t2. You can interactively debug your scoring file locally. Please refer to https:\/\/learn.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\\\n\\\\t3. You can also try to run image 20dd0f745f704eeb89ef4d52057871a0.azurecr.io\/azureml\/azureml_b9e8a2e66019f74c902eacced9684631 locally. Please refer to https:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\&quot;\\n    },\\n    {\\n      \\&quot;code\\&quot;: \\&quot;AciDeploymentFailed\\&quot;,\\n      \\&quot;message\\&quot;: \\&quot;Your container application crashed. Please follow the steps to debug:\\\\n\\\\t1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https:\/\/aka.ms\/debugimage#dockerlog for more information.\\\\n\\\\t2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https:\/\/aka.ms\/debugimage#debug-locally for more information.\\\\n\\\\t3. You can also interactively debug your scoring file locally. Please refer to https:\/\/learn.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\\\n\\\\t4. View the diagnostic events to check status of container, it may help you to debug the issue.\\\\n\\\\\\&quot;RestartCount\\\\\\&quot;: 3\\\\n\\\\\\&quot;CurrentState\\\\\\&quot;: {\\\\\\&quot;state\\\\\\&quot;:\\\\\\&quot;Waiting\\\\\\&quot;,\\\\\\&quot;startTime\\\\\\&quot;:null,\\\\\\&quot;exitCode\\\\\\&quot;:null,\\\\\\&quot;finishTime\\\\\\&quot;:null,\\\\\\&quot;detailStatus\\\\\\&quot;:\\\\\\&quot;CrashLoopBackOff: Back-off restarting failed\\\\\\&quot;}\\\\n\\\\\\&quot;PreviousState\\\\\\&quot;: {\\\\\\&quot;state\\\\\\&quot;:\\\\\\&quot;Terminated\\\\\\&quot;,\\\\\\&quot;startTime\\\\\\&quot;:\\\\\\&quot;2021-04-27T10:46:03.903Z\\\\\\&quot;,\\\\\\&quot;exitCode\\\\\\&quot;:111,\\\\\\&quot;finishTime\\\\\\&quot;:\\\\\\&quot;2021-04-27T10:46:07.524Z\\\\\\&quot;,\\\\\\&quot;detailStatus\\\\\\&quot;:\\\\\\&quot;Error\\\\\\&quot;}\\\\n\\\\\\&quot;Events\\\\\\&quot;:\\\\n{\\\\\\&quot;count\\\\\\&quot;:1,\\\\\\&quot;firstTimestamp\\\\\\&quot;:\\\\\\&quot;2021-04-27T10:42:37Z\\\\\\&quot;,\\\\\\&quot;lastTimestamp\\\\\\&quot;:\\\\\\&quot;2021-04-27T10:42:37Z\\\\\\&quot;,\\\\\\&quot;name\\\\\\&quot;:\\\\\\&quot;Pulling\\\\\\&quot;,\\\\\\&quot;message\\\\\\&quot;:\\\\\\&quot;pulling image \\\\\\\\\\\\\\&quot;20dd0f745f704eeb89ef4d52057871a0.azurecr.io\/azureml\/azureml_b9e8a2e66019f74c902eacced9684631@sha256:322ebafbe88e98b0f57104fd0afad08a5caf57cc5e7f64b3b629c3ea50f54bb3\\\\\\\\\\\\\\&quot;\\\\\\&quot;,\\\\\\&quot;type\\\\\\&quot;:\\\\\\&quot;Normal\\\\\\&quot;}\\\\n{\\\\\\&quot;count\\\\\\&quot;:1,\\\\\\&quot;firstTimestamp\\\\\\&quot;:\\\\\\&quot;2021-04-27T10:44:15Z\\\\\\&quot;,\\\\\\&quot;lastTimestamp\\\\\\&quot;:\\\\\\&quot;2021-04-27T10:44:15Z\\\\\\&quot;,\\\\\\&quot;name\\\\\\&quot;:\\\\\\&quot;Pulled\\\\\\&quot;,\\\\\\&quot;message\\\\\\&quot;:\\\\\\&quot;Successfully pulled image \\\\\\\\\\\\\\&quot;20dd0f745f704eeb89ef4d52057871a0.azurecr.io\/azureml\/azureml_b9e8a2e66019f74c902eacced9684631@sha256:322ebafbe88e98b0f57104fd0afad08a5caf57cc5e7f64b3b629c3ea50f54bb3\\\\\\\\\\\\\\&quot;\\\\\\&quot;,\\\\\\&quot;type\\\\\\&quot;:\\\\\\&quot;Normal\\\\\\&quot;}\\\\n{\\\\\\&quot;count\\\\\\&quot;:4,\\\\\\&quot;firstTimestamp\\\\\\&quot;:\\\\\\&quot;2021-04-27T10:44:40Z\\\\\\&quot;,\\\\\\&quot;lastTimestamp\\\\\\&quot;:\\\\\\&quot;2021-04-27T10:46:03Z\\\\\\&quot;,\\\\\\&quot;name\\\\\\&quot;:\\\\\\&quot;Started\\\\\\&quot;,\\\\\\&quot;message\\\\\\&quot;:\\\\\\&quot;Started container\\\\\\&quot;,\\\\\\&quot;type\\\\\\&quot;:\\\\\\&quot;Normal\\\\\\&quot;}\\\\n{\\\\\\&quot;count\\\\\\&quot;:4,\\\\\\&quot;firstTimestamp\\\\\\&quot;:\\\\\\&quot;2021-04-27T10:44:43Z\\\\\\&quot;,\\\\\\&quot;lastTimestamp\\\\\\&quot;:\\\\\\&quot;2021-04-27T10:46:07Z\\\\\\&quot;,\\\\\\&quot;name\\\\\\&quot;:\\\\\\&quot;Killing\\\\\\&quot;,\\\\\\&quot;message\\\\\\&quot;:\\\\\\&quot;Killing container with id 5c5ddb266c4b38b1c306367712d9bec0687e5f6979e34afea7f6b943edf7db75.\\\\\\&quot;,\\\\\\&quot;type\\\\\\&quot;:\\\\\\&quot;Normal\\\\\\&quot;}\\\\n\\&quot;\\n    }\\n  ]\\n}&quot;  \n    }  \n}  \n<\/code><\/pre>\n<p><strong>I have deployed the same model with the same entryScript.py and the same conda.yaml previously, and it worked fine.<\/strong>    <\/p>\n<p>I cannot figure out what can be the issue here. Can anybody please suggest to me something for solving this?    <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1619524308143,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/373687\/ml-model-deployment-issue",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"model deploy issu try deploi classif model azur gui regist upload model insid portal deploi model azur contain instanc custom entri script conda depend entri script import pacakg import panda import pickl import regex json import numpi import sklearn import infer schema schema decor import input schema output schema infer schema paramet type numpi paramet type import numpyparametertyp def init global model global class model path path join getenv model dir randomforest pkl model pickl load open model path class lambda input sampl arrai thoma anna output sampl arrai input schema data numpyparametertyp input sampl output schema numpyparametertyp output sampl def run data try nameslist json load data data name pred list map class model predict preprocess nameslist return str pred except error str return error conda yaml predict depend python numpi scikit learn pip default panda pickl regex infer schema numpi support deploy endpoint deploy state goe unhealthi log program stuck loop check log rsyslog run iot server run gunicorn run nginx run edgehubconnectionstr iotedg iothubhostnam set exit iot server finish exit code normal restart iot server start gunicorn listen http worker sync worker timeout set boot worker pid spark home set skip pyspark initi initi logger root info start app insight client root info start request gener root info start app insight hook root info invok user init function worker timeout set boot worker pid spark home set skip pyspark initi initi logger root info start app insight client root info start request gener root info start app insight hook root info invok user init function worker timeout set boot worker pid spark home set skip pyspark initi tri deploi model python fail messag webserviceexcept webserviceexcept messag servic deploy poll reach non success termin state current servic state fail oper afbdfd inform log error code acideploymentfail statuscod messag aci deploy fail except contain applic crash caus error score file init function check log contain instanc aml sdk run print servic log servic object fetch log interact debug score file local refer http learn microsoft com azur machin learn debug visual studio code debug troubleshoot deploy inform try run imag ddffeebefda azurecr beaefceac local refer http aka debugimag servic launch fail inform detail code crashloopbackoff messag contain applic crash caus error score file init function check log contain instanc aml sdk run print servic log servic object fetch log interact debug score file local refer http learn microsoft com azur machin learn debug visual studio code debug troubleshoot deploy inform try run imag ddffeebefda azurecr beaefceac local refer http aka debugimag servic launch fail inform code acideploymentfail messag contain applic crash follow step debug aml sdk run print servic log servic object fetch log refer http aka debugimag dockerlog inform contain applic crash caus error score file init function try debug local refer http aka debugimag debug local inform interact debug score file local refer http learn microsoft com azur machin learn debug visual studio code debug troubleshoot deploy inform view diagnost event check statu contain help debug issu restartcount currentst state wait starttim null exitcod null finishtim null detailstatu crashloopbackoff restart fail previousst state termin starttim exitcod finishtim detailstatu error event count firsttimestamp lasttimestamp pull messag pull imag ddffeebefda azurecr beaefceac sha ebafbeebffdafadacafccefbbceafbb type normal count firsttimestamp lasttimestamp pull messag successfulli pull imag ddffeebefda azurecr beaefceac sha ebafbeebffdafadacafccefbbceafbb type normal count firsttimestamp lasttimestamp start messag start contain type normal count firsttimestamp lasttimestamp kill messag kill contain cddbcbbcdbecefeafeafbedfdb type normal innerexcept errorrespons error messag servic deploy poll reach non success termin state current servic state fail noper afbdfd nmore inform log nerror code acideploymentfail statuscod messag aci deploy fail except contain applic crash caus error score file init function check log contain instanc aml sdk run print servic log servic object fetch log interact debug score file local refer http learn microsoft com azur machin learn debug visual studio code debug troubleshoot deploy inform try run imag ddffeebefda azurecr beaefceac local refer http aka debugimag servic launch fail inform detail code crashloopbackoff messag contain applic crash caus error score file init function check log contain instanc aml sdk run print servic log servic object fetch log interact debug score file local refer http learn microsoft com azur machin learn debug visual studio code debug troubleshoot deploy inform try run imag ddffeebefda azurecr beaefceac local refer http aka debugimag servic launch fail inform code acideploymentfail messag contain applic crash follow step debug aml sdk run print servic log servic object fetch log refer http aka debugimag dockerlog inform contain applic crash caus error score file init function try debug local refer http aka debugimag debug local inform interact debug score file local refer http learn microsoft com azur machin learn debug visual studio code debug troubleshoot deploy inform view diagnost event check statu contain help debug issu restartcount currentst state wait starttim null exitcod null finishtim null detailstatu crashloopbackoff restart fail previousst state termin starttim exitcod finishtim detailstatu error event count firsttimestamp lasttimestamp pull messag pull imag ddffeebefda azurecr beaefceac sha ebafbeebffdafadacafccefbbceafbb type normal count firsttimestamp lasttimestamp pull messag successfulli pull imag ddffeebefda azurecr beaefceac sha ebafbeebffdafadacafccefbbceafbb type normal count firsttimestamp lasttimestamp start messag start contain type normal count firsttimestamp lasttimestamp kill messag kill contain cddbcbbcdbecefeafeafbedfdb type normal deploi model entryscript conda yaml previous work fine figur issu anybodi suggest solv",
        "Question_preprocessed_content":"model deploy issu try deploi classif model azur gui model insid portal deploi model azur contain instanc custom conda depend entri script deploy endpoint deploy state goe unhealthi log program stuck loop check log tri deploi model python fail messag deploi model previous work fine figur issu anybodi suggest solv",
        "Question_gpt_summary_original":"The user is facing challenges in deploying an ML classification model on Azure using GUI and Python. After registering and uploading the model, the endpoint deployment state goes to unhealthy and the logs show that the program is stuck in a loop. The user has tried deploying the model using Python, but it failed. The user has previously deployed the same model with the same entry script and conda.yaml, and it worked fine. The user is seeking suggestions to solve the issue.",
        "Question_gpt_summary":null
    },
    {
        "Question_title":"Wandb sync confusing personal project for team project",
        "Question_body":"<p>I have a project \u201cfoo\u201d on my personal wandb account (entity \u201cuser\u201d). However, I am also a member of a team (\u201cteam\u201d). When I try to sync an offline run using <code>wandb sync path\/to\/foo\/run<\/code>, I want it to be saved in project \u201cfoo\u201d on my personal account. However, wandb creates a new project \u201cfoo\u201d that is owned by \u201cteam\u201d.<\/p>\n<p>Is there any way I can fix this? Do I need to change the way I\u2019m logged in to my wandb account? wandb says that I am logged in as <code>user (team)<\/code>, but I\u2019m not sure how to change that.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1682957058371,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":75.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/adamoyoung\">@adamoyoung<\/a> thanks for reporting this issue. Could you please provide the <code>--entity<\/code> and <code>--project<\/code> arguments as follows:<br>\n<code>wandb sync -e personal -p foo path\/to\/foo\/run<\/code><\/p>\n<p>Would this work for you? There\u2019s a <code>Project Defaults<\/code> section in your <a href=\"https:\/\/wandb.ai\/settings\">personal settings page<\/a> where this in your case seems to be configured for your team entity. You may change that if you wanted the default to be your personal account.<\/p>",
        "Answer_comment_count":null,
        "Question_closed_time":1683034536819,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-sync-confusing-personal-project-for-team-project\/4315",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"sync confus person project team project project foo person account entiti user member team team try sync offlin run sync path foo run want save project foo person account creat new project foo own team wai fix need chang wai log account sai log user team sure chang",
        "Question_preprocessed_content":"sync confus person project team project project foo person account member team try sync offlin run want save project foo person account creat new project foo own team wai fix need chang wai log account sai log sure chang",
        "Question_gpt_summary_original":"The user has a personal project named \"foo\" on their Wandb account, but when they try to sync an offline run using \"wandb sync path\/to\/foo\/run\", it creates a new project named \"foo\" owned by their team instead of saving it in their personal account. The user is looking for a solution to fix this issue and is unsure if they need to change their login credentials.",
        "Question_gpt_summary":"user person project name foo account try sync offlin run sync path foo run creat new project name foo own team instead save person account user look solut fix issu unsur need chang login credenti"
    },
    {
        "Question_title":"How to configure comet (comet.ml) to log Tensorflow?",
        "Question_body":"<p>Im trying to set comet (<a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a>) to track my Tensorflow experiment, after I create an Experiment and log the data set i dont get the accuracy in my report.<\/p>\n\n<p>my code:<\/p>\n\n<pre><code>mnist = get_data()\ntrain_step, cross_entropy, accuracy, x, y, y_ = build_model_graph(hyper_params)\n\nexperiment = Experiment(api_key=\"XXXX\", log_code=True)\nexperiment.log_multiple_params(hyper_params)\nexperiment.log_dataset_hash(mnist)\n<\/code><\/pre>\n\n<p>in the example account : <a href=\"https:\/\/www.comet.ml\/view\/Jon-Snow\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml\/view\/Jon-Snow<\/a> I see that accuracy is reported<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1506094686057,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1514341154200,
        "Question_score_count":3.0,
        "Question_view_count":338.0,
        "Poster_created_time":1506066897167,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":4.0,
        "Answer_body":"<p>you can report accuracy using this method:<\/p>\n\n<ul>\n<li><code>experiment.log_accuracy(train_accuracy)<\/code><\/li>\n<\/ul>\n\n<p>take a look at the full Tensorflow example in our guide:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/tree\/master\/tensorflow\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/tree\/master\/tensorflow<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1506100257932,
        "Answer_last_edit_time":1513514205487,
        "Answer_score_count":3.0,
        "Answerer_created_time":1359884693920,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":9637.0,
        "Answerer_view_count":609.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46368389",
        "Tool":"Comet",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"configur log tensorflow try set http track tensorflow experi creat experi log data set dont accuraci report code mnist data train step cross entropi accuraci build model graph hyper param experi experi api kei log code true experi log multipl param hyper param experi log dataset hash mnist exampl account http view jon snow accuraci report",
        "Question_preprocessed_content":"configur log tensorflow try set track tensorflow experi creat experi log data set dont accuraci report code exampl account accuraci report",
        "Question_gpt_summary_original":"The user is trying to configure comet.ml to track their Tensorflow experiment, but after creating an experiment and logging the data set, they are not getting accuracy in their report. They have shared their code and noticed that accuracy is reported in an example account on comet.ml.",
        "Question_gpt_summary":"user try configur track tensorflow experi creat experi log data set get accuraci report share code notic accuraci report exampl account"
    },
    {
        "Question_title":"PowerBI and MLflow integration (through AzureML)",
        "Question_body":"<p>I'm currently trying to integrate an ML model currently deployed as a webservice on AzureML with PowerBI.<\/p>\n<p>I see that it can be <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#invoking-the-azure-ml-model-in-power-bi\" rel=\"nofollow noreferrer\">integrated<\/a> but the model requires the addition of a schema file when it is <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#schema-discovery-for-machine-learning-models\" rel=\"nofollow noreferrer\">being deployed as a webservice<\/a>. Without this, the model can't be viewed in PowerBI.<\/p>\n<p>The problem that I have come up against is that I use MLflow to log ML model performances and subsequently to deploy a selected model onto AzureML as a webservice using MLflow's AzureML integration - mlflow.azureml.deploy(). This unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in PowerBI as it lacks the required schema file.<\/p>\n<p>My options seem to be:<\/p>\n<ol>\n<li>Find a workaround, possibly using the working <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-serving\" rel=\"nofollow noreferrer\">REST api of the model in a power query<\/a>.<\/li>\n<li>Rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.<\/li>\n<\/ol>\n<p>I thought I would ask to see if I am maybe missing something as I can't find a workaround using my current code to define a schema file in MLflow when deploying with mlflow.azureml.deploy().<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1600261190477,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1600855880503,
        "Question_score_count":0.0,
        "Question_view_count":405.0,
        "Poster_created_time":1600260166047,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":4.0,
        "Answer_body":"<p>Point number 2 is the way we solved this issue. Instead of using MLflow to deploy to a scoring service on Azure, we wrote a custom code which load MLflow model when container is initialised.<\/p>\n<p>Scoring code is something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nfrom mlflow.pyfunc import load_model\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\ndef init():\n    global model\n    model = load_model(os.path.join(os.environ.get(&quot;AZUREML_MODEL_DIR&quot;), &quot;awesome_model&quot;))\n\n@input_schema('data', NumpyParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\n\ndef run(data):\n    return model.predict(data)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1600604920243,
        "Answer_last_edit_time":1600855957376,
        "Answer_score_count":0.0,
        "Answerer_created_time":1285219808283,
        "Answerer_location":"Perth WA, Australia",
        "Answerer_reputation_count":6770.0,
        "Answerer_view_count":1127.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63920599",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"powerbi integr current try integr model current deploi webservic powerbi integr model requir addit schema file deploi webservic model view powerbi problem come us log model perform subsequ deploi select model webservic integr deploi unfortun option defin schema file model deploi result model avail powerbi lack requir schema file option workaround possibl work rest api model power queri rewrit deploy code handl webservic deploy step azur instead thought ask mayb miss workaround current code defin schema file deploi deploi",
        "Question_preprocessed_content":"powerbi integr current try integr model current deploi webservic powerbi integr model requir addit schema file deploi webservic model view powerbi problem come us log model perform subsequ deploi select model webservic integr unfortun option defin schema file model deploi result model avail powerbi lack requir schema file option workaround possibl work rest api model power queri rewrit deploy code handl webservic deploy step azur instead thought ask mayb miss workaround current code defin schema file deploi",
        "Question_gpt_summary_original":"The user is trying to integrate an ML model deployed as a webservice on AzureML with PowerBI, but the model requires a schema file to be viewed in PowerBI. The user uses MLflow to deploy the model onto AzureML, but MLflow's AzureML integration does not have the option to define a schema file before deployment, resulting in no model being available in PowerBI. The user is considering finding a workaround using the REST API or rewriting the deployment code to handle the webservice deployment steps in Azure instead of MLflow.",
        "Question_gpt_summary":"user try integr model deploi webservic powerbi model requir schema file view powerbi user us deploi model integr option defin schema file deploy result model avail powerbi user consid find workaround rest api rewrit deploy code handl webservic deploy step azur instead"
    },
    {
        "Question_title":"Sagemaker Neo compiled model not using NVIDIA GPU",
        "Question_body":"I have a YOLOv5-trained model. Exported as Tensorflow, I'm trying to compile and deploy to my device as a Greengrass Noe edge component.\n\nDevice configs are,\ndevice: NVIDIA Jetson AGX Xavier\nJetpack:5.1\nCUDA: 11.4\nTensorRT:8.5\n\nI compiled the model with different configurations.\n\n**Compilation 1. **\nmodel:tensorflow 2.4\nPlatform: Linux\nArch: ARM64\n\nCan able to load the model and run inference. But its running on CPU and the inference time is very high.\n\n**Compilation 2. **\nmodel:tensorflow 2.4\nPlatform: Linux\nArch: ARM64\nAccelerator: NVIDIA\nConfig: {\"gpu-code\":\"sm_62\",\"trt-ver\": \"8.5\", \"cuda-ver\": \"11.4\"}\n\nUnable to load the model.\nGRPC error : \"UNKNOWN:Error received from peer unix:\/tmp\/aws.greengrass.SageMakerEdgeManager.sock {grpc_message:\"LoadModel request failed\", grpc_status:13\"\n\n**Compilation 3. **\nmodel:tensorflow 2.4\nDevice: jetson_xavier\nConfig: {\"trt-ver\": \"6.0.1\", \"cuda-ver\": \"10.0\"}\n\nUnable to load the model.\nGRPC error : \"UNKNOWN:Error received from peer unix:\/tmp\/aws.greengrass.SageMakerEdgeManager.sock {grpc_message:\"LoadModel request failed\", grpc_status:13\"\n\nEither the not running on GPU or the deployed model I was not able to load.\nCloud you please help me over here.",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1681127066445,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1681473607035,
        "Question_score_count":0.0,
        "Question_view_count":25.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUa7OaHGIQR96sEdJ-xKna_Q\/sagemaker-neo-compiled-model-not-using-nvidia-gpu",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"neo compil model nvidia gpu yolov train model export tensorflow try compil deploi devic greengrass noe edg compon devic config devic nvidia jetson agx xavier jetpack cuda tensorrt compil model differ configur compil model tensorflow platform linux arch arm abl load model run infer run cpu infer time high compil model tensorflow platform linux arch arm acceler nvidia config gpu code trt ver cuda ver unabl load model grpc error unknown error receiv peer unix tmp aw greengrass edgemanag sock grpc messag loadmodel request fail grpc statu compil model tensorflow devic jetson xavier config trt ver cuda ver unabl load model grpc error unknown error receiv peer unix tmp aw greengrass edgemanag sock grpc messag loadmodel request fail grpc statu run gpu deploi model abl load cloud help",
        "Question_preprocessed_content":"neo compil model nvidia gpu model export tensorflow try compil deploi devic greengrass noe edg compon devic config devic nvidia jetson agx xavier cuda compil model differ configur compil model tensorflow platform linux arch arm abl load model run infer run cpu infer time high compil model tensorflow platform linux arch arm acceler nvidia config unabl load model grpc error unknown error receiv peer unabl load model grpc error unknown error receiv peer request fail run gpu deploi model abl load cloud help",
        "Question_gpt_summary_original":"The user is trying to compile and deploy a YOLOv5-trained model to their NVIDIA Jetson AGX Xavier device using Greengrass Noe edge component. However, the model is running on CPU and the inference time is very high. The user tried to compile the model with different configurations, but they were unable to load the model and received GRPC errors. The user is seeking help to run the model on the GPU.",
        "Question_gpt_summary":"user try compil deploi yolov train model nvidia jetson agx xavier devic greengrass noe edg compon model run cpu infer time high user tri compil model differ configur unabl load model receiv grpc error user seek help run model gpu"
    },
    {
        "Question_title":"How to send own failure info in case of failed SageMaker Training Job?",
        "Question_body":"Good day! \n\n\n**My main purpose:**\n\nEasy way to collect information about different failure scenarios in SageMaker TrainingJob.\n\n**What do I use currently?**\n\nSagemaker SKLearn Estimator(TrainingJobs are inside)\n\n**Where will my model train?**\n\nDifferent datasets. So, I need control and collect all information about all training processes and their final statuses on different datasets.\n\n**Which failure scenarios do I have?**\n\nThere are plenty of them. I have create my own python Errors for them.  \nFor example:\n1. There are labels only for one class.\n2. Too small dataset(by my own criterions)\n3. Missing data for crucial columns\n4. e.t.c.\n\n\n**Where am I stuck?**\n\nAfter failed training I can't get own errors from training job response. All of them are \"ExecuteUserScriptError\"\nI can't pass my own info in FailureReason or ErrorMessage(always it's empty). I see which error was raised in CloudWatchLogs and TrainingJobTraceback(from SagemakerNotebook). So, bad solution is parse all CloudWatchLogs in case of failure. \n\n**Question:\nHow to provide my own ErrorMessage or FailureReason? **\n\nMay be I am digging in the wrong direction. Anyway, I need your advice.\nThank you so much for possibility to ask an advice here)",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1652720071234,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668501646379,
        "Question_score_count":0.0,
        "Question_view_count":465.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW-6fYh-kQy2bgc5PSkQG3Q\/how-to-send-own-failure-info-in-case-of-failed-sagemaker-training-job",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"send failur info case fail train job good dai main purpos easi wai collect inform differ failur scenario trainingjob us current sklearn estim trainingjob insid model train differ dataset need control collect inform train process final status differ dataset failur scenario plenti creat python error exampl label class small dataset criterion miss data crucial column stuck fail train error train job respons executeuserscripterror pass info failurereason errormessag error rais cloudwatchlog trainingjobtraceback notebook bad solut pars cloudwatchlog case failur question provid errormessag failurereason dig wrong direct need advic thank possibl ask advic",
        "Question_preprocessed_content":"send failur info case fail train job good dai main purpos easi wai collect inform differ failur scenario trainingjob us current sklearn estim model train differ dataset need control collect inform train process final status differ dataset failur scenario plenti creat python error exampl label class small dataset miss data crucial column stuck fail train error train job respons executeuserscripterror pass info failurereason errormessag error rais cloudwatchlog trainingjobtraceback bad solut pars cloudwatchlog case failur question provid errormessag failurereason dig wrong direct need advic thank possibl ask advic",
        "Question_gpt_summary_original":"The user is facing challenges in collecting information about different failure scenarios in SageMaker TrainingJob. They have created their own python errors for different failure scenarios, but after a failed training, they are unable to get their own errors from the training job response. The user is unable to pass their own info in FailureReason or ErrorMessage, and all errors are shown as \"ExecuteUserScriptError\". They are seeking advice on how to provide their own ErrorMessage or FailureReason.",
        "Question_gpt_summary":"user face challeng collect inform differ failur scenario trainingjob creat python error differ failur scenario fail train unabl error train job respons user unabl pass info failurereason errormessag error shown executeuserscripterror seek advic provid errormessag failurereason"
    },
    {
        "Question_title":"Is it possible to update automated machine learning model timeseries \"live\" as new events come in.",
        "Question_body":"<p>I have trained a simple time series ML model and deployed it.  <br \/>\nGetting predictions works well enough, but I would like to keep the model up to date as new events come along each hour.  <\/p>\n<p>For example I want to predict an event that happens in 10 minutes.   <br \/>\nAfter 10 minutes has gone by, and I learn the real value, i'd like to push that value at the end of the ML model data array without having to re-train\/deploy everything.  <br \/>\nIs that possible in Automated ML ?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_created_time":1628032373347,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/500499\/is-it-possible-to-update-automated-machine-learnin",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"possibl updat autom machin learn model timeseri live new event come train simpl time seri model deploi get predict work like model date new event come hour exampl want predict event happen minut minut gone learn real valu like push valu end model data arrai have train deploi possibl autom",
        "Question_preprocessed_content":"possibl updat autom machin learn model timeseri live new event come train simpl time seri model deploi get predict work like model date new event come hour exampl want predict event happen minut minut gone learn real valu like push valu end model data arrai have possibl autom",
        "Question_gpt_summary_original":"The user has trained a time series ML model and wants to update it as new events come in without having to re-train\/deploy everything. They are seeking to know if it is possible to update the automated machine learning model timeseries \"live\" as new events come in.",
        "Question_gpt_summary":"user train time seri model want updat new event come have train deploi seek know possibl updat autom machin learn model timeseri live new event come"
    },
    {
        "Question_title":"Using MLflow and Sagemaker with preprocessing steps",
        "Question_body":"I'm deploying my models to Sagemaker using MLflow integration. However, my ML pipeline includes some basic preprocessing steps, such as scalers, and I need it to be part of my inference endpoint. Is there a way to do that with MLflow? I looked in the\u00a0mlflow_pyfunc\u00a0is closer to what I want, but I'm not sure if it is compatible with Sagemaker.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1658304744000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":null,
        "Question_view_count":20.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/I7LdK_7KCIw",
        "Tool":"MLflow",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"preprocess step deploi model integr pipelin includ basic preprocess step scaler need infer endpoint wai look pyfunci closer want sure compat",
        "Question_preprocessed_content":"preprocess step deploi model integr pipelin includ basic preprocess step scaler need infer endpoint wai look closer want sure compat",
        "Question_gpt_summary_original":"The user is facing challenges in deploying ML models to Sagemaker using MLflow integration as their ML pipeline includes preprocessing steps that need to be part of the inference endpoint. They are unsure if the mlflow_pyfunc is compatible with Sagemaker.",
        "Question_gpt_summary":"user face challeng deploi model integr pipelin includ preprocess step need infer endpoint unsur pyfunc compat"
    },
    {
        "Question_title":"AzureMLCompute job failed with `FailedLoginToImageRegistry`",
        "Question_body":"<p>I've been trying to send a train job through azure ml python sdk with:<\/p>\n<pre><code>from azureml.core import Workspace, Experiment, ScriptRunConfig \n\nif __name__ == &quot;__main__&quot;:\n    ws = Workspace.from_config()\n    experiment = Experiment(workspace=ws, name='ConstructionTopicsModel')\n\n    config = ScriptRunConfig(source_directory='.\/',\n                         script='src\/azureml\/train.py',\n                         arguments=None,\n                         compute_target='ComputeTargetName',\n                         )\n\n    env = ws.environments['test-env']\n    config.run_config.environment = env\n    run = experiment.submit(config)\n    \n    run.wait_for_completion(show_output=True)\n\n    aml_url = run.get_portal_url()\n    print(aml_url)\n<\/code><\/pre>\n<p>But I was getting the <code>ServiceError<\/code> message:<\/p>\n<pre><code>AzureMLCompute job failed. FailedLoginToImageRegistry: Unable to login to docker image repo\nReason: Failed to login to the docker registry\nerror: WARNING! Using --password via the CLI is insecure. Use --password-stdin. Error saving credentials: error storing credentials - err: exit status 1, out: `Cannot autolaunch D-Bus without X11 $DISPLAY`\n\nserviceURL: 7ac86b04d6564d36aa80ae2ad090582c.azurecr.io\nReason: WARNING! Using --password via the CLI is insecure. Use --password-stdin. Error saving credentials: error storing credentials - err: exit status 1, out: `Cannot autolaunch D-Bus without X11 $DISPLAY`\n\nInfo: Failed to setup runtime for job execution: Job environment preparation failed on 10.0.0.5 with err exit status 1.\n<\/code><\/pre>\n<p>I also tried using the azure cli without success, same error message<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1643645330913,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":202.0,
        "Poster_created_time":1589293508567,
        "Poster_location":null,
        "Poster_reputation_count":833.0,
        "Poster_view_count":55.0,
        "Answer_body":"<p>The only way I've found so far to make this work, was to run it on a terminal of the compute-target itself. That's how the docker error goes away. Trying to run the experiment from a terminal of a different compute instance raises the exception.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1643645330912,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1589293508567,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70929123",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"comput job fail failedlogintoimageregistri try send train job python sdk core import workspac experi scriptrunconfig main workspac config experi experi workspac constructiontopicsmodel config scriptrunconfig sourc directori script src train argument comput target computetargetnam env environ test env config run config environ env run experi submit config run wait complet output true aml url run portal url print aml url get serviceerror messag comput job fail failedlogintoimageregistri unabl login docker imag repo reason fail login docker registri error warn password cli insecur us password stdin error save credenti error store credenti err exit statu autolaunch bu displai serviceurl acbddaaaeadc azurecr reason warn password cli insecur us password stdin error save credenti error store credenti err exit statu autolaunch bu displai info fail setup runtim job execut job environ prepar fail err exit statu tri azur cli success error messag",
        "Question_preprocessed_content":"comput job fail try send train job python sdk get messag tri azur cli success error messag",
        "Question_gpt_summary_original":"The user encountered an issue while trying to send a train job through Azure ML Python SDK. The job failed with a \"FailedLoginToImageRegistry\" error message, indicating that the user was unable to login to the docker image repo. The error message also suggested using \"--password-stdin\" instead of \"--password\" via the CLI. The user also tried using the Azure CLI but encountered the same error message.",
        "Question_gpt_summary":"user encount issu try send train job python sdk job fail failedlogintoimageregistri error messag indic user unabl login docker imag repo error messag suggest password stdin instead password cli user tri azur cli encount error messag"
    },
    {
        "Question_title":"AWS SageMaker Notebook instance cannot connect to internet",
        "Question_body":"<p>Trying to understand why my SageMaker notebook instance cannot connect to the internet.<\/p>\n\n<pre><code># jupyter notebook running conda_python3 kernel\n\nfrom sagemaker import get_execution_role\nrole = get_execution_role()\nprint(role)\n\n'Could not connect to the endpoint URL: \"https:\/\/api.sagemaker.us-east-1.amazonaws.com\/\"'\n<\/code><\/pre>\n\n<p><br><\/p>\n\n<pre><code># terminal\n\nwget tools.geekflare.com\n\n'unable to resolve host address \"tools.geekflare.com\"'\n<\/code><\/pre>\n\n<p>Any tips on how to debug the issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1576078707953,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":2613.0,
        "Poster_created_time":1426273558767,
        "Poster_location":null,
        "Poster_reputation_count":82.0,
        "Poster_view_count":17.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59289258",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"notebook instanc connect internet try understand notebook instanc connect internet jupyt notebook run conda python kernel import execut role role execut role print role connect endpoint url http api east amazonaw com termin wget tool geekflar com unabl resolv host address tool geekflar com tip debug issu",
        "Question_preprocessed_content":"notebook instanc connect internet try understand notebook instanc connect internet tip debug issu",
        "Question_gpt_summary_original":"The user is facing an issue with their AWS SageMaker Notebook instance as it is unable to connect to the internet. They have tried running a Jupyter notebook and a terminal command but both have failed to connect to the internet. The user is seeking tips on how to debug the issue.",
        "Question_gpt_summary":"user face issu notebook instanc unabl connect internet tri run jupyt notebook termin command fail connect internet user seek tip debug issu"
    },
    {
        "Question_title":"Metric data exceeds maximum size",
        "Question_body":"<p>When I use wandb.log online, the following error will be reported: \u201cMetric data exceeds maximum size of 10.4MB\u201d. Now if I don\u2019t want to run again, how can I fix this mistake?<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_created_time":1662622216472,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":659.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/metric-data-exceeds-maximum-size\/3082",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"metric data exce maximum size us log onlin follow error report metric data exce maximum size dont want run fix mistak",
        "Question_preprocessed_content":"metric data exce maximum size us log onlin follow error report metric data exce maximum size dont want run fix mistak",
        "Question_gpt_summary_original":"The user encountered an error while using wandb.log online, stating that the metric data exceeds the maximum size of 10.4MB. The user is seeking a solution to fix this error without having to rerun the process.",
        "Question_gpt_summary":"user encount error log onlin state metric data exce maximum size user seek solut fix error have rerun process"
    },
    {
        "Question_title":"Store Raw JSON file from azureml.core.Run",
        "Question_body":"<p>Hi,   <\/p>\n<p>in the details tab of a pipeline step (in an experiment) one of the last entries is &quot;See all properties&quot;. Below that the Raw JSON file is linked and can be opened. Is there a way to save or access these Raw JSON file within the Python SDK?   <\/p>\n<p>I want to store these file beside the model to guarantee traceability, if we deploy the registered model outside of Azure.  <\/p>\n<p>Thanks for your help <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1651092990657,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/829188\/store-raw-json-file-from-azureml-core-run",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"store raw json file core run detail tab pipelin step experi entri properti raw json file link open wai save access raw json file python sdk want store file model guarante traceabl deploi regist model outsid azur thank help",
        "Question_preprocessed_content":"store raw json file detail tab pipelin step entri properti raw json file link open wai save access raw json file python sdk want store file model guarante traceabl deploi regist model outsid azur thank help",
        "Question_gpt_summary_original":"The user is looking for a way to save or access the Raw JSON file from a pipeline step in an experiment within the Python SDK. They want to store the file beside the model for traceability purposes in case the registered model is deployed outside of Azure.",
        "Question_gpt_summary":"user look wai save access raw json file pipelin step experi python sdk want store file model traceabl purpos case regist model deploi outsid azur"
    },
    {
        "Question_title":"Deploying ml model using azureml and mlflow",
        "Question_body":"<p>I'm new to databricks and deploying models using mlflow and azureml, I'm trying to deploy my model but haven't found a lot of documentation or examples.<\/p>\n\n<p>I have my model which I save using:<\/p>\n\n<pre><code>mlflow.sklearn.save_model(model, model_path, \n                          conda_env=conda_env_file_name)\n<\/code><\/pre>\n\n<p>I created the workspace and the aci webservice, the next step is to create the image and the webservice:<\/p>\n\n<pre><code># image creation\nfrom azureml.core.image import ContainerImage\nmyimage_config = ContainerImage.image_configuration(execution_script = driver_file, \n                                                    runtime = \"python\", \n                                                    conda_file = conda_env_file_name)\n\n# Webservice creation\nmyservice = AciWebservice.deploy_from_model(\n  workspace=ws, \n  name=\"service\",\n  deployment_config = aciconfig,\n  models = [model_path],\n  image_config = myimage_config)\n\nmyservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n\n<p>However when I try to create the webservice I receive an error and looking at the log:<\/p>\n\n<pre><code>mlflow.exceptions.MlflowException: Could not find an \"MLmodel\" configuration file at \"mode_path\"\n<\/code><\/pre>\n\n<p>My score file init function is like this:<\/p>\n\n<pre><code>def init():\n    global model\n    # retreive the path to the model file using the model name\n    model_path = Model.get_model_path('model_path')\n    model = joblib.load(model_path)\n<\/code><\/pre>\n\n<p>It seems like it cannot find the path to the model. I'm not sure in the moment the image is saved, the model is not saved in it and thus it cannot be found by sklearn.load_model. I'm quite confused cause I've seen that a model can be deployed using mlflow or azureml. I think the problems is that mlflow.save_model does not register the model and then there's no path. Have someone been able to solve this? What is the best way to deploy a model?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_created_time":1559175361693,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1559831718176,
        "Question_score_count":3.0,
        "Question_view_count":933.0,
        "Poster_created_time":1461539594160,
        "Poster_location":null,
        "Poster_reputation_count":737.0,
        "Poster_view_count":116.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56370096",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"deploi model new databrick deploi model try deploi model haven lot document exampl model save sklearn save model model model path conda env conda env file creat workspac aci webservic step creat imag webservic imag creation core imag import containerimag myimag config containerimag imag configur execut script driver file runtim python conda file conda env file webservic creation myservic aciwebservic deploi model workspac servic deploy config aciconfig model model path imag config myimag config myservic wait deploy output true try creat webservic receiv error look log except except mlmodel configur file mode path score file init function like def init global model retreiv path model file model model path model model path model path model joblib load model path like path model sure moment imag save model save sklearn load model confus caus seen model deploi think problem save model regist model path abl solv best wai deploi model",
        "Question_preprocessed_content":"deploi model new databrick deploi model try deploi model haven lot document exampl model save creat workspac aci webservic step creat imag webservic try creat webservic receiv error look log score file init function like like path model sure moment imag save model save confus caus seen model deploi think problem regist model path abl solv best wai deploi model",
        "Question_gpt_summary_original":"The user is new to deploying models using mlflow and azureml and is encountering an error while creating a webservice. The error message suggests that the \"MLmodel\" configuration file cannot be found at the specified model path. The user suspects that the model is not saved in the image and thus cannot be found by sklearn.load_model. The user is seeking advice on the best way to deploy a model and if anyone has been able to solve this issue.",
        "Question_gpt_summary":"user new deploi model encount error creat webservic error messag suggest mlmodel configur file specifi model path user suspect model save imag sklearn load model user seek advic best wai deploi model abl solv issu"
    },
    {
        "Question_title":"AWS - SageMaker Serverless Inference with SageMaker Neo",
        "Question_body":"<p>I am planning to use SageMaker Serverless Inference in conjunction with SageMaker Neo to deploy my model for serverless, low latency inference. However, documentation is not clear whether it is possible to do so.<\/p>\n<p>According to Instance Types for Neo <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-supported-cloud.html\" rel=\"nofollow noreferrer\">here<\/a>, I can use e.g. <code>ml_m4<\/code> instance. However, OutputConfig for Neo Compiler <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_OutputConfig.html\" rel=\"nofollow noreferrer\">here<\/a> specifies that TargetDevice can be <code>lambda<\/code>. Serverless Inference <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html\" rel=\"nofollow noreferrer\">docs<\/a> specify that &quot;Serverless Inference integrates with AWS Lambda to offer you high availability...&quot;, so I assume that underlying instances are of the same (or compatible) type with AWS Lambda.<\/p>\n<ol>\n<li>Have you used such combination? Does it work?<\/li>\n<li>Which instance type should I compile for with Neo?<\/li>\n<\/ol>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1662626681740,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":23.0,
        "Poster_created_time":1520689858167,
        "Poster_location":null,
        "Poster_reputation_count":1040.0,
        "Poster_view_count":149.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73646137",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"aw serverless infer neo plan us serverless infer conjunct neo deploi model serverless low latenc infer document clear possibl accord instanc type neo us instanc outputconfig neo compil specifi targetdevic lambda serverless infer doc specifi serverless infer integr aw lambda offer high avail assum underli instanc compat type aw lambda combin work instanc type compil neo",
        "Question_preprocessed_content":"aw serverless infer neo plan us serverless infer conjunct neo deploi model serverless low latenc infer document clear possibl accord instanc type neo us instanc outputconfig neo compil specifi targetdevic serverless infer doc specifi serverless infer integr aw lambda offer high assum underli instanc type aw lambda combin work instanc type compil neo",
        "Question_gpt_summary_original":"The user is planning to use SageMaker Serverless Inference with SageMaker Neo to deploy their model for serverless, low latency inference. However, they are unsure if it is possible to do so as the documentation is not clear. They are seeking advice on whether this combination works and which instance type they should compile for with Neo.",
        "Question_gpt_summary":"user plan us serverless infer neo deploi model serverless low latenc infer unsur possibl document clear seek advic combin work instanc type compil neo"
    },
    {
        "Question_title":"AWS Sagemaker | region specific security credentials for endpoint",
        "Question_body":"<p>I am new to AWS infra and currently doing some POC\/Feasibility for new work.<\/p>\n\n<p>So I have created a S3 bucket in Ireland server, train and publish Sagemaker endpoint in Ireland server and its giving result in Jupyter notebook there. Now I want to use that endpoint in my browser javascript library to show some graphics. When I try to test my endpoint in Postman then its giving region specific error <\/p>\n\n<pre><code> {\n        \"message\": \"Credential should be scoped to a valid region, not 'us-east-1'. \nCredential should be scoped to correct service: 'sagemaker'. \"\n }\n<\/code><\/pre>\n\n<p>My AWS account is not yet enterprise managed so I am using as 'root user', Whenever I go to my profile>Security_Credential page and generate any security credential then it always create for 'us-east-1' region, As Sagemaker is region specific service, I am not able to find the way to create region specific security key for root user, can someone please help<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1526107432860,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":750.0,
        "Poster_created_time":1501403168107,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Answer_body":"<p>You should create an IAM role first that defines what should be permitted (mainly calling the invoke-endpoint API call for SageMaker runtime). Then you should create an IAM user, add the above role to that user, and then generate credentials that you can use in your Postman to call the service. Here you can find some details about the IAM role for SageMaker that you can use in this process: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/using-identity-based-policies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/using-identity-based-policies.html<\/a><\/p>\n\n<p>A popular option to achieve external access to a SageMaker endpoint, is to create an API Gateway that calls a Lambda Function that is then calling the invoke-endpoint API. This chain is giving you various options such as different authentication options for the users and API keys as part of API-GW, processing the user input and inference output using API-GW and Lambda code, and giving the permission to call the SageMaker endpoint to the Lambda function. This chain removes the need for the credentials creation, update and distribution.  <\/p>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1526179930343,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50303607",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"region specif secur credenti endpoint new aw infra current poc feasibl new work creat bucket ireland server train publish endpoint ireland server give result jupyt notebook want us endpoint browser javascript librari graphic try test endpoint postman give region specif error messag credenti scope valid region east credenti scope correct servic aw account enterpris manag root user profil secur credenti page gener secur credenti creat east region region specif servic abl wai creat region specif secur kei root user help",
        "Question_preprocessed_content":"region specif secur credenti endpoint new aw infra current new work creat bucket ireland server train publish endpoint ireland server give result jupyt notebook want us endpoint browser javascript librari graphic try test endpoint postman give region specif error aw account enterpris manag root user page gener secur credenti creat region region specif servic abl wai creat region specif secur kei root user help",
        "Question_gpt_summary_original":"The user is facing an issue with AWS Sagemaker where they are unable to create region-specific security credentials for their endpoint. They have created an S3 bucket and trained and published the Sagemaker endpoint in Ireland server, but when they try to test the endpoint in Postman, they receive a region-specific error. As the user is using a root user account, they are unable to generate region-specific security credentials for Sagemaker.",
        "Question_gpt_summary":"user face issu unabl creat region specif secur credenti endpoint creat bucket train publish endpoint ireland server try test endpoint postman receiv region specif error user root user account unabl gener region specif secur credenti"
    },
    {
        "Question_title":"Hypyerparameter optimization with k folds on each iteration",
        "Question_body":"<p>I am trying to perform hyperparameter optimization with wandb and for each iteration I would like to get the average performance across 3 different folds of my dataset.<\/p>\n<p>I have defined a function optimize that i pass to wandb.agent:<\/p>\n<pre><code class=\"lang-auto\">def optimize(config):\n    for fold in range(1, 4):    \n        dataset_artifact = f'fold-{fold}:latest'\n        config['dataset_artifact'] = dataset_artifact \n        with wandb.init(config=config, group=group_name, job_type=f'train-fold-{fold}', name=f'train-fold-{fold}', reinit=True) as run:   \n            train_and_log(config, run)  \n            run.finish()\n<\/code><\/pre>\n<p>I would expect this to creat a seperate run for each fold (since I have specified a different job type and run name as well as passing init=True) so that I would end up with:<\/p>\n<p>Group: param_combo_1<\/p>\n<p>\u00a0\u00a0\u00a0\u00a0&gt; Job Type: train-fold-1<\/p>\n<p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0&gt; train-fold-1<\/p>\n<p>\u00a0\u00a0\u00a0\u00a0&gt; Job Type: train-fold-2<\/p>\n<p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0&gt; train-fold-2<\/p>\n<p>\u00a0\u00a0\u00a0\u00a0&gt; Job Type: train-fold-3<\/p>\n<p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0&gt; train-fold-3<\/p>\n<p>However each run for a given hyperparameter iteration overwrites the previous fold so I in fact end up with<\/p>\n<p>Group: param_combo_1<\/p>\n<p>\u00a0\u00a0\u00a0\u00a0&gt; Job Type: train-fold-3<\/p>\n<p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0&gt; train-fold-3<\/p>\n<p>How can I resolve this issue?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_created_time":1668614113531,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":194.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/hypyerparameter-optimization-with-k-folds-on-each-iteration\/3429",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"hypyerparamet optim fold iter try perform hyperparamet optim iter like averag perform differ fold dataset defin function optim pass agent def optim config fold rang dataset artifact fold fold latest config dataset artifact dataset artifact init config config group group job type train fold fold train fold fold reinit true run train log config run run finish expect creat seper run fold specifi differ job type run pass init true end group param combo job type train fold train fold job type train fold train fold job type train fold train fold run given hyperparamet iter overwrit previou fold fact end group param combo job type train fold train fold resolv issu",
        "Question_preprocessed_content":"hypyerparamet optim fold iter try perform hyperparamet optim iter like averag perform differ fold dataset defin function optim pass agent expect creat seper run fold end group job type job type job type run given hyperparamet iter overwrit previou fold fact end group job type resolv issu",
        "Question_gpt_summary_original":"The user is trying to perform hyperparameter optimization with wandb and wants to get the average performance across 3 different folds of their dataset. They have defined a function to pass to wandb.agent and expected it to create a separate run for each fold, but each run for a given hyperparameter iteration overwrites the previous fold. The user is seeking a solution to this issue.",
        "Question_gpt_summary":"user try perform hyperparamet optim want averag perform differ fold dataset defin function pass agent expect creat separ run fold run given hyperparamet iter overwrit previou fold user seek solut issu"
    },
    {
        "Question_title":"Version 2 model in natural language API",
        "Question_body":"Hi, could anyone share the python code on how to get\u00a0 natural language API to use version 2 classify text\u00a0 categories?\n\nI can get it working well with the default (version 1) categories but can't figure out where to adapt the standard code (as here: https:\/\/cloud.google.com\/natural-language\/docs\/samples\/language-classify-text-tutorial-classify?hl=e...)\u00a0 to\u00a0 use model version 2.\n\nMany thanks",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1667301900000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":173.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"From the Classifying Content guide, you can include classification_model_options\u00a0within the request\u00a0dictionary argument to the classify_text()\u00a0function. In these options, you can define the model and version to use for content categories.\n\n\/\/ ...\ncontent_categories_version = (\n        language_v1.ClassificationModelOptions.V2Model.ContentCategoriesVersion.V2) \/\/ Assigning the v2 model type\n    response = client.classify_text(request = {\n        \"document\": document,\n        \"classification_model_options\": {\n            \"v2_model\": {\n                \"content_categories_version\": content_categories_version\n            }\n        }\n    })\n\/\/ ...\n\n\nYou can also check ClassificationModelOptions\u00a0reference for available options.\n\nView solution in original post",
        "Answer_comment_count":null,
        "Question_closed_time":1667406420000,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Version-2-model-in-natural-language-API\/td-p\/484641\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"version model natur languag api share python code natur languag api us version classifi text categori work default version categori figur adapt standard code http cloud googl com natur languag doc sampl languag classifi text tutori classifi us model version thank",
        "Question_preprocessed_content":"version model natur languag api share python code natur languag api us version classifi text categori work default categori figur adapt standard code us model version thank",
        "Question_gpt_summary_original":"The user is seeking help in modifying the standard python code for the natural language API to use version 2 model for classifying text categories. They are currently able to use the default version 1 categories but are struggling to adapt the code for version 2.",
        "Question_gpt_summary":"user seek help modifi standard python code natur languag api us version model classifi text categori current abl us default version categori struggl adapt code version"
    },
    {
        "Question_title":"How do I resolve \"KeyError: 'brand'\" when running an experiment using Azure Automated Machine Learning?",
        "Question_body":"<p>I am using the Azure Automated Machine Learning SDK to train a machine learning model on my dataset. However, after the experiment, all my training iterations fail with a <code>KeyError: 'brand'<\/code> error even if the model training itself succeeded.<\/p>\n<p>How can I resolve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1594314866923,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":977.0,
        "Poster_created_time":1341461304023,
        "Poster_location":null,
        "Poster_reputation_count":716.0,
        "Poster_view_count":50.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62820211",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"resolv keyerror brand run experi azur autom machin learn azur autom machin learn sdk train machin learn model dataset experi train iter fail keyerror brand error model train succeed resolv",
        "Question_preprocessed_content":"resolv keyerror brand run experi azur autom machin learn azur autom machin learn sdk train machin learn model dataset experi train iter fail error model train succeed resolv",
        "Question_gpt_summary_original":"The user is encountering a KeyError with the message 'brand' while running an experiment using Azure Automated Machine Learning SDK. All training iterations fail with this error even if the model training itself succeeded. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user encount keyerror messag brand run experi azur autom machin learn sdk train iter fail error model train succeed user seek help resolv issu"
    },
    {
        "Question_title":"Unable to parse the response from the Azure ML Web Service in Azure Stream Analytics",
        "Question_body":"<p>Hi, using Azure ML Studio I have created an endpoint for a model generated with automated ML. The model works fine in test (consume) - provides an expected outcome. Then I created a Stream Analytics query using the function to consume the same ML endpoint. However when I test the Stream Analytics query I receive the following error:    <\/p>\n<p>&quot;Callout failed within query runner. An error was encountered while calling the Azure ML web service. An error occurred when parsing the Azure ML web service response. Please check your Azure ML web service and data model. The content of the response from the ML web service should be a JSON array. The response received from the Web Service is: {&quot;Results&quot;: [&quot;none&quot;, &quot;none&quot;, &quot;none&quot;]} Parameter name: result&quot;    <\/p>\n<p>The result I am getting is fine - as expected - but the problem seems to be with parsing the result.    <br \/>\nSo by reading the docs I understand the desired output format from ML endpoint is JSON Array like this [&quot;none&quot;, &quot;none&quot;, &quot;none&quot;], while I am getting a JSON object. {&quot;Results&quot;: [&quot;none&quot;, &quot;none&quot;, &quot;none&quot;]}     <br \/>\nThe question is, can I (how I) modify the output format (swagger?) to have it return the json array? The model was calculated by automated ML (no-code)     <\/p>\n<p>For the record the Stream Analytics query is like this:    <\/p>\n<p>SELECT udf.pdmpredict(TRY_CAST(inputArray AS record))    <br \/>\nINTO [pdm-predict-data]    <br \/>\nFROM ModelInput    <br \/>\nWHERE inputArray is not null    <\/p>\n<p>udf.pdmpredict is my ASA ML function created in accordance to this article: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/stream-analytics\/machine-learning-udf\">https:\/\/learn.microsoft.com\/en-us\/azure\/stream-analytics\/machine-learning-udf<\/a>    <\/p>\n<p>Any ideas will be greatly appreciated!    <br \/>\nThanks.    <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":8,
        "Question_created_time":1664971317523,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1036188\/unable-to-parse-the-response-from-the-azure-ml-web",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unabl pars respons web servic azur stream analyt studio creat endpoint model gener autom model work fine test consum provid expect outcom creat stream analyt queri function consum endpoint test stream analyt queri receiv follow error callout fail queri runner error encount call web servic error occur pars web servic respons check web servic data model content respons web servic json arrai respons receiv web servic result paramet result result get fine expect problem pars result read doc understand desir output format endpoint json arrai like get json object result question modifi output format swagger return json arrai model calcul autom code record stream analyt queri like select udf pdmpredict try cast inputarrai record pdm predict data modelinput inputarrai null udf pdmpredict asa function creat accord articl http learn microsoft com azur stream analyt machin learn udf idea greatli appreci thank",
        "Question_preprocessed_content":"unabl pars respons web servic azur stream analyt studio creat endpoint model gener autom model work fine test provid expect outcom creat stream analyt queri function consum endpoint test stream analyt queri receiv follow error callout fail queri runner error encount call web servic error occur pars web servic respons check web servic data model content respons web servic json arrai respons receiv web servic paramet result result get fine expect problem pars result read doc understand desir output format endpoint json arrai like get json object question modifi output format return json arrai model calcul autom record stream analyt queri like select record modelinput inputarrai null asa function creat accord articl idea greatli appreci thank",
        "Question_gpt_summary_original":"The user is facing an issue while parsing the response from the Azure ML Web Service in Azure Stream Analytics. The error message indicates that the response received from the ML web service is a JSON object, whereas the desired output format is a JSON array. The user is seeking guidance on how to modify the output format to return the JSON array. The model was generated using automated ML, and the Stream Analytics query is using a custom ML function.",
        "Question_gpt_summary":"user face issu pars respons web servic azur stream analyt error messag indic respons receiv web servic json object desir output format json arrai user seek guidanc modifi output format return json arrai model gener autom stream analyt queri custom function"
    },
    {
        "Question_title":"How to determin which GCP VM do I need for ML",
        "Question_body":"Hi to all\n\nIm trying to run a procedure looking to reduce the number of features for a model.\n\nThe first try was with google Colab pro+ but it keep crashing and nver run the entire process, then I got a VM\u00a0n1-highmem-8 that has:\u00a0\n\nGPUs1 x NVIDIA Tesla V100\u00a0 +\u00a0\u00a0n1-highmem-8 (vCPUs: 8, RAM: 52GB)\n\nand still not getting the process done.\n\nThe question is how to determin which type of machine should I use? Can I get any metric from the cell that is runing in colab and be able to determin the Type of VM that I need?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1658842140000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":100.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"There are a few things to take in consideration:\n\nHave you installed all the necessary drivers for the GPU? Here is a complete guide that you can follow.\nI do not see any Python wrapper for CUDA in your code. The way you specify when to use the GPU for specific tasks is through this wrapper, it seems to me that you are using the CPU instead and that is why the task keeps crashing. Now, converting your code to a CUDA version is not a trivial task, and it involves a deeper knowledge on how a GPU works. If you are in a hurry, you could try the Py2CUDA github project, but I would strongly recommend taking a look at the Getting Started Blogs.\u00a0\u00a0\n\nView solution in original post",
        "Answer_comment_count":null,
        "Question_closed_time":1659349860000,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-determin-which-GCP-VM-do-I-need-for-ML\/td-p\/447075\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"determin gcp need try run procedur look reduc number featur model try googl colab pro crash nver run entir process got vmn highmem gpu nvidia tesla highmem vcpu ram get process question determin type machin us metric cell rune colab abl determin type need",
        "Question_preprocessed_content":"determin gcp need try run procedur look reduc number featur model try googl colab pro crash nver run entir process got gpu nvidia tesla get process question determin type machin us metric cell rune colab abl determin type need",
        "Question_gpt_summary_original":"The user is trying to reduce the number of features for a model using Google Colab pro+ but it keeps crashing. They have now tried using a VM n1-highmem-8 with 1 NVIDIA Tesla V100 GPU and 8 vCPUs with 52GB RAM, but the process is still not completing. The user is seeking advice on how to determine which type of machine they should use and if there are any metrics from the cell running in Colab that can help them determine the appropriate VM.",
        "Question_gpt_summary":"user try reduc number featur model googl colab pro keep crash tri highmem nvidia tesla gpu vcpu ram process complet user seek advic determin type machin us metric cell run colab help determin appropri"
    },
    {
        "Question_title":"Connect locally to Jupyter in Sagemaker",
        "Question_body":"<p>When I run <code>%connect_info<\/code> in Jupyterlab on Sagemaker I get session info. and<\/p>\n<pre><code>{&quot;shell_port&quot;: ,\n  &quot;iopub_port&quot;: ,\n  &quot;stdin_port&quot;: ,\n  &quot;control_port&quot;: ,\n  &quot;hb_port&quot;: ,\n  &quot;ip&quot;: &quot;&quot;,\n  &quot;key&quot;: &quot;&quot;,\n  &quot;transport&quot;: &quot;&quot;,\n  &quot;signature_scheme&quot;: &quot;&quot;,\n  &quot;kernel_name&quot;: &quot;&quot;}\n\nPaste the above JSON into a file, and connect with:\n    $&gt; jupyter &lt;app&gt; --existing &lt;file&gt;\nor, if you are local, you can connect with just:\n    $&gt; jupyter &lt;app&gt; --existing kernel-052ed888-e682-4786-aa4c-cdb19c6145bf.json\nor even just:\n    $&gt; jupyter &lt;app&gt; --existing\nif this is the most recent Jupyter kernel you have started.\n<\/code><\/pre>\n<p>But when I run the Jupiter statement against the saved text file, it doesn't connect.<\/p>\n<p>Sometimes, it would be convenient to connect my local VS code app to my notebooks instead of developing in the browser.<\/p>\n<p>Any suggestions are appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1644617317783,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":385.0,
        "Poster_created_time":1337798831903,
        "Poster_location":null,
        "Poster_reputation_count":971.0,
        "Poster_view_count":104.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71087045",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"connect local jupyt run connect info jupyterlab session info shell port iopub port stdin port control port port kei transport signatur scheme kernel past json file connect jupyt exist local connect jupyt exist kernel aac cdbcbf json jupyt exist recent jupyt kernel start run jupit statement save text file connect conveni connect local code app notebook instead develop browser suggest appreci",
        "Question_preprocessed_content":"connect local jupyt run jupyterlab session info run jupit statement save text file connect conveni connect local code app notebook instead develop browser suggest appreci",
        "Question_gpt_summary_original":"The user is facing challenges in connecting locally to Jupyter in Sagemaker. They have tried running the %connect_info command in Jupyterlab and received session information in JSON format, but when they tried to connect using the saved text file, it did not work. The user is seeking suggestions to connect their local VS code app to their notebooks instead of developing in the browser.",
        "Question_gpt_summary":"user face challeng connect local jupyt tri run connect info command jupyterlab receiv session inform json format tri connect save text file work user seek suggest connect local code app notebook instead develop browser"
    },
    {
        "Question_title":"How to pass values for a \"shap_baseline\" if we have categorical values (string values) as features in classsagemaker.clarify.SHAPConfig method.",
        "Question_body":"using [this documentation](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.clarify.SHAPConfig) i  passing a single row as to shap_baseline parameter to implement explainability monitoring , a similar implementation of what is done in [in this github repo implementation](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_model_monitor\/fairness_and_explainability\/SageMaker-Model-Monitor-Fairness-and-Explainability.ipynb). if I am passing a single row as input to shap_baseline parameter, the schedule is failing by concatenating 2 rows. If i ignore the shap_baseline (as it is optional), the schedule is taking forever to run. Help of any kind is really appreciated.\n\nthanks for your time and effort :)",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1663938111276,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1667926207272,
        "Question_score_count":0.0,
        "Question_view_count":78.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVdzi-7h5RAapCYJUehzuZw\/how-to-pass-values-for-a-shap-baseline-if-we-have-categorical-values-string-values-as-features-in-classsagemaker-clarify-shapconfig-method",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"pass valu shap baselin categor valu string valu featur class clarifi shapconfig method document http readthedoc stabl api train process html clarifi shapconfig pass singl row shap baselin paramet implement explain monitor similar implement github repo implement http github com aw amazon exampl blob main model monitor fair explain model monitor fair explain ipynb pass singl row input shap baselin paramet schedul fail concaten row ignor shap baselin option schedul take forev run help kind appreci thank time effort",
        "Question_preprocessed_content":"pass valu categor valu featur method pass singl row paramet implement explain monitor similar implement pass singl row input paramet schedul fail concaten row ignor schedul take forev run help kind appreci thank time effort",
        "Question_gpt_summary_original":"The user is facing challenges while passing values for a \"shap_baseline\" in the classsagemaker.clarify.SHAPConfig method. They are trying to implement explainability monitoring using a single row as input to the shap_baseline parameter, but the schedule is failing by concatenating 2 rows. If they ignore the shap_baseline, the schedule takes forever to run. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user face challeng pass valu shap baselin class clarifi shapconfig method try implement explain monitor singl row input shap baselin paramet schedul fail concaten row ignor shap baselin schedul take forev run user seek help resolv issu"
    },
    {
        "Question_title":"lambda endpoint prediction failed with error can't convert string to float",
        "Question_body":"<p>I am facing the issue in getting the prediction from sagemaker xgboost endpoint. When I feed the transformed data in endpoint like: <\/p>\n\n<pre><code>response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,ContentType='text\/csv',Body = csv_buffer.getvalue())\n<\/code><\/pre>\n\n<p>throwing the error : <\/p>\n\n<blockquote>\n  <p>can not convert string to float. <\/p>\n<\/blockquote>\n\n<p>Could you please help me out what I am missing. <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1542792950100,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1542794653796,
        "Question_score_count":1.0,
        "Question_view_count":93.0,
        "Poster_created_time":1542788486503,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53409038",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"lambda endpoint predict fail error convert string float face issu get predict xgboost endpoint feed transform data endpoint like respons runtim invok endpoint endpointnam endpoint contenttyp text csv bodi csv buffer getvalu throw error convert string float help miss",
        "Question_preprocessed_content":"lambda endpoint predict fail error convert string float face issu get predict xgboost endpoint feed transform data endpoint like throw error convert string float help miss",
        "Question_gpt_summary_original":"The user is encountering an issue with getting predictions from a Sagemaker XGBoost endpoint. When feeding transformed data into the endpoint, the error \"can not convert string to float\" is thrown. The user is seeking assistance in resolving the issue.",
        "Question_gpt_summary":"user encount issu get predict xgboost endpoint feed transform data endpoint error convert string float thrown user seek assist resolv issu"
    },
    {
        "Question_title":"delete compute instance by python sdk",
        "Question_body":"<p>hello    <\/p>\n<p>I request your help to configure the deletion of the azure machine learning compute instance after the job is finished     <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1672862465810,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1151897\/delete-compute-instance-by-python-sdk",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"delet comput instanc python sdk hello request help configur delet comput instanc job finish",
        "Question_preprocessed_content":"delet comput instanc python sdk hello request help configur delet comput instanc job finish",
        "Question_gpt_summary_original":"The user is seeking assistance in configuring the deletion of an Azure machine learning compute instance using Python SDK after a job is completed.",
        "Question_gpt_summary":"user seek assist configur delet comput instanc python sdk job complet"
    },
    {
        "Question_title":"VertexAI Batch Inference Failing for Custom Container Model",
        "Question_body":"<p>I'm having trouble executing VertexAI's batch inference, despite endpoint deployment and inference working perfectly. My TensorFlow model has been trained in a custom Docker container with the following arguments:<\/p>\n<pre><code>aiplatform.CustomContainerTrainingJob(\n        display_name=display_name,\n        command=[&quot;python3&quot;, &quot;train.py&quot;],\n        container_uri=container_uri,\n        model_serving_container_image_uri=container_uri,\n        model_serving_container_environment_variables=env_vars,\n        model_serving_container_predict_route='\/predict',\n        model_serving_container_health_route='\/health',\n        model_serving_container_command=[\n            &quot;gunicorn&quot;,\n            &quot;src.inference:app&quot;,\n            &quot;--bind&quot;,\n            &quot;0.0.0.0:5000&quot;,\n            &quot;-k&quot;,\n            &quot;uvicorn.workers.UvicornWorker&quot;,\n            &quot;-t&quot;,\n            &quot;6000&quot;,\n        ],\n        model_serving_container_ports=[5000],\n)\n<\/code><\/pre>\n<p>I have a Flask endpoint defined for predict and health essentially defined below:<\/p>\n<pre><code>@app.get(f&quot;\/health&quot;)\ndef health_check_batch():\n    return 200\n\n@app.post(f&quot;\/predict&quot;)\ndef predict_batch(request_body: dict):\n    pred_df = pd.DataFrame(request_body['instances'],\n                           columns = request_body['parameters']['columns'])\n    # do some model inference things\n    return {&quot;predictions&quot;: predictions.tolist()}\n<\/code><\/pre>\n<p>As described, when training a model and deploying to an endpoint, I can successfully hit the API with JSON schema like:<\/p>\n<pre><code>{&quot;instances&quot;:[[1,2], [1,3]], &quot;parameters&quot;:{&quot;columns&quot;:[&quot;first&quot;, &quot;second&quot;]}}\n<\/code><\/pre>\n<p>This also works when using the endpoint Python SDK and feeding in instances\/parameters as functional arguments.<\/p>\n<p>However, I've tried performing batch inference with a CSV file and a JSONL file, and every time it fails with an Error Code 3. I can't find logs on why it failed in Logs Explorer either. I've read through all the documentation I could find and have seen other's successfully invoke batch inference, but haven't been able to find a guide. Does anyone have recommendations on batch file structure or the structure of my APIs? Thank you!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_created_time":1650785345693,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":111.0,
        "Poster_created_time":1467136510360,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":7.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71986344",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"vertexai batch infer fail custom contain model have troubl execut vertexai batch infer despit endpoint deploy infer work perfectli tensorflow model train custom docker contain follow argument aiplatform customcontainertrainingjob displai displai command python train contain uri contain uri model serv contain imag uri contain uri model serv contain environ variabl env var model serv contain predict rout predict model serv contain health rout health model serv contain command gunicorn src infer app bind uvicorn worker uvicornwork model serv contain port flask endpoint defin predict health essenti defin app health def health check batch return app post predict def predict batch request bodi dict pred datafram request bodi instanc column request bodi paramet column model infer thing return predict predict tolist describ train model deploi endpoint successfulli hit api json schema like instanc paramet column second work endpoint python sdk feed instanc paramet function argument tri perform batch infer csv file jsonl file time fail error code log fail log explor read document seen successfulli invok batch infer haven abl guid recommend batch file structur structur api thank",
        "Question_preprocessed_content":"vertexai batch infer fail custom contain model have troubl execut vertexai batch infer despit endpoint deploy infer work perfectli tensorflow model train custom docker contain follow argument flask endpoint defin predict health essenti defin describ train model deploi endpoint successfulli hit api json schema like work endpoint python sdk feed function argument tri perform batch infer csv file jsonl file time fail error code log fail log explor read document seen successfulli invok batch infer haven abl guid recommend batch file structur structur api thank",
        "Question_gpt_summary_original":"The user is facing issues while performing batch inference using VertexAI despite successful endpoint deployment and inference. The TensorFlow model has been trained in a custom Docker container, and the Flask endpoint is defined for predict and health. The user has tried performing batch inference with a CSV file and a JSONL file, but it fails every time with an Error Code 3, and the logs do not provide any information on why it failed. The user is seeking recommendations on batch file structure or API structure to resolve the issue.",
        "Question_gpt_summary":"user face issu perform batch infer vertexai despit success endpoint deploy infer tensorflow model train custom docker contain flask endpoint defin predict health user tri perform batch infer csv file jsonl file fail time error code log provid inform fail user seek recommend batch file structur api structur resolv issu"
    },
    {
        "Question_title":"Which GPU instances are supported by the sagemaker algorithm forecasting-deepar?",
        "Question_body":"I previously ran a hyperparameter tuning job for SageMaker DeepAR with the instance type ml.c5.18xlarge but it seems insufficient to complete the tuning job within the max_run time specified in my account. Now, having tried to use the accelerated GPU instance ml.g4dn.16xlarge, I am prompted with an error - \"Instance type ml.g4dn.16xlarge is not supported by algorithm forecasting-deepar.\" \n\nI cannot find any documentation that indicates the list of instance types supported by deepar. What GPU\/CPU instances have more compute capacity than ml.c5.18xlarge which I could leverage for my tuning job? \n\nIf there isn't, I would appreciate any recommendations as to how I could hasten the run time of the job. I require the tuning job to complete within the max run time of 432000 seconds. Thank you in advance!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1653437621164,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1667925624792,
        "Question_score_count":0.0,
        "Question_view_count":75.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0TwRR6KzRzuS5Xme3uMdEw\/which-gpu-instances-are-supported-by-the-sagemaker-algorithm-forecasting-deepar",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"gpu instanc support algorithm forecast deepar previous ran hyperparamet tune job deepar instanc type xlarg insuffici complet tune job max run time specifi account have tri us acceler gpu instanc gdn xlarg prompt error instanc type gdn xlarg support algorithm forecast deepar document indic list instanc type support deepar gpu cpu instanc comput capac xlarg leverag tune job isn appreci recommend hasten run time job requir tune job complet max run time second thank advanc",
        "Question_preprocessed_content":"gpu instanc support algorithm previous ran hyperparamet tune job deepar instanc type insuffici complet tune job time specifi account have tri us acceler gpu instanc prompt error instanc type support algorithm document indic list instanc type support deepar instanc comput capac leverag tune job isn appreci recommend hasten run time job requir tune job complet max run time second thank advanc",
        "Question_gpt_summary_original":"The user is trying to run a hyperparameter tuning job for SageMaker DeepAR using the ml.g4dn.16xlarge instance type, but is receiving an error message stating that this instance type is not supported by the forecasting-deepar algorithm. The user is seeking information on which GPU\/CPU instances have more compute capacity than ml.c5.18xlarge and are supported by deepar, or recommendations on how to hasten the run time of the job to complete within the max run time of 432000 seconds.",
        "Question_gpt_summary":"user try run hyperparamet tune job deepar gdn xlarg instanc type receiv error messag state instanc type support forecast deepar algorithm user seek inform gpu cpu instanc comput capac xlarg support deepar recommend hasten run time job complet max run time second"
    },
    {
        "Question_title":"Azure ML Enpoint deployment failed EAST US region",
        "Question_body":"<p>I have an Azure ML Real-time inference endpoint deployed ran for a month till yesterday. Today it is in the state of &quot;Failed&quot;.  <\/p>\n<p>I did create a new compute and did a new deployment in the same region EAST US and it failed again.  <\/p>\n<p>What's going? Is this just a problem for me or a general issue?  <\/p>\n<p>Thanks  <br \/>\n-Dali<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1594945530060,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>Hi, thanks for reaching out. I successfully deployed in the east us region. Please review the following <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-pipelines\">troubleshooting guidelines<\/a>. Also check for any <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/service-health\/service-health-overview\">service\/resource health issues<\/a> that could be impacting your service. Let me know if you're still experiencing issues afterwards and please share the logs so we can investigate further. Thanks.    <\/p>\n",
        "Answer_comment_count":1.0,
        "Question_closed_time":1594953029752,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/48609\/azure-ml-enpoint-deployment-failed-east-us-region",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"enpoint deploy fail east region real time infer endpoint deploi ran month till yesterdai todai state fail creat new comput new deploy region east fail go problem gener issu thank dali",
        "Question_preprocessed_content":"enpoint deploy fail east region infer endpoint deploi ran month till yesterdai todai state fail creat new comput new deploy region east fail go problem gener issu thank dali",
        "Question_gpt_summary_original":"The user's Azure ML Real-time inference endpoint deployment in the EAST US region has failed, and a new deployment in the same region has also failed. The user is unsure if this is a general issue or specific to their account.",
        "Question_gpt_summary":"user real time infer endpoint deploy east region fail new deploy region fail user unsur gener issu specif account"
    },
    {
        "Question_title":"How to you implement Retail recommendation model?",
        "Question_body":"We are trying to implement a Retail recommendation model, in particular, the\u00a0Recommended for you model.\n\nI share here the docs for reference:\u00a0https:\/\/cloud.google.com\/retail\/docs\/reference\/rest\/v2beta\/projects.locations.catalogs.placements\/pr...\n\nAll the catalog is uploaded correctly, the model is trained and inside GCP in google Retail we get a prediction if we provide a cookie.\n\nOur problem is that we are trying to get the list of predictions\u00a0 of users with the new url:\n\n\"https:\/\/retail.googleapis.com\/v2beta\/YOUR_MODEL_LOCATION\"\n\nBut we have to provide the API Key and also the user has to be authenticated (OAuth).\n\nThis is an issue because we users can't authentica themselfs in order to get the predictions.\n\nA prior version that worked had a different url:\n\nhttps:\/\/recommendationengine.googleapis.com\/v1beta1\/\n\nThis one worked and no OAuth was needed, it returned a list\/array with the predictions for each user correctly. Sadly this website is going to be deprecated and we have to use the new one.\n\nHas somebody encountered the same issue or implemented the Retail models correctly in a website?\n\nThank you in advance!",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1673251260000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":48.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-you-implement-Retail-recommendation-model\/td-p\/508252\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"implement retail recommend model try implement retail recommend model particular therecommend model share doc refer http cloud googl com retail doc refer rest vbeta project locat catalog placement catalog upload correctli model train insid gcp googl retail predict provid cooki problem try list predict user new url http retail googleapi com vbeta model locat provid api kei user authent oauth issu user authentica themself order predict prior version work differ url http recommendationengin googleapi com vbeta work oauth need return list arrai predict user correctli sadli websit go deprec us new somebodi encount issu implement retail model correctli websit thank advanc",
        "Question_preprocessed_content":"implement retail recommend model try implement retail recommend model particular therecommend model share doc catalog upload correctli model train insid gcp googl retail predict provid cooki problem try list predict user new url provid api kei user authent issu user authentica themself order predict prior version work differ url work oauth need return predict user correctli sadli websit go deprec us new somebodi encount issu implement retail model correctli websit thank advanc",
        "Question_gpt_summary_original":"The user is trying to implement a Retail recommendation model, specifically the \"Recommended for you\" model. The catalog is uploaded correctly, and the model is trained. However, the user is facing an issue while trying to get the list of predictions of users with the new URL, as it requires an API key and authentication. The prior version worked without OAuth, but it is going to be deprecated. The user is seeking help to implement the Retail models correctly on their website.",
        "Question_gpt_summary":"user try implement retail recommend model specif recommend model catalog upload correctli model train user face issu try list predict user new url requir api kei authent prior version work oauth go deprec user seek help implement retail model correctli websit"
    },
    {
        "Question_title":"Clean Up Azure Machine Learning Blob Storage",
        "Question_body":"<p>I manage a frequently used Azure Machine Learning workspace. With several Experiments and active pipelines. Everything is working good so far. My problem is to get rid of old data from runs, experiments and pipelines. Over the last year the blob storage grew to enourmus size, because every pipeline data is stored.<\/p>\n<p>I have deleted older runs from experimnents by using the gui, but the actual pipeline data on the blob store is not deleted. Is there a smart way to clean up data on the blob store from runs which have been deleted ?<\/p>\n<p>On one of the countless Microsoft support pages, I found the following not very helpfull post:<\/p>\n<p>*Azure does not automatically delete intermediate data written with OutputFileDatasetConfig. To avoid storage charges for large amounts of unneeded data, you should either:<\/p>\n<ol>\n<li>Programmatically delete intermediate data at the end of a pipeline\nrun, when it is no longer needed<\/li>\n<li>Use blob storage with a short-term storage policy for intermediate data (see Optimize costs by automating Azure Blob Storage access tiers)<\/li>\n<li>Regularly review and delete no-longer-needed data*<\/li>\n<\/ol>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-data-in-out-of-pipelines#delete-outputfiledatasetconfig-contents-when-no-longer-needed\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-data-in-out-of-pipelines#delete-outputfiledatasetconfig-contents-when-no-longer-needed<\/a><\/p>\n<p>Any idea is welcome.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1642771426213,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":4.0,
        "Question_view_count":368.0,
        "Poster_created_time":1635428968927,
        "Poster_location":"Germany",
        "Poster_reputation_count":61.0,
        "Poster_view_count":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70802006",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"clean blob storag manag frequent workspac experi activ pipelin work good far problem rid old data run experi pipelin year blob storag grew enourmu size pipelin data store delet older run experimn gui actual pipelin data blob store delet smart wai clean data blob store run delet countless microsoft support page follow helpful post azur automat delet intermedi data written outputfiledatasetconfig avoid storag charg larg amount unneed data programmat delet intermedi data end pipelin run longer need us blob storag short term storag polici intermedi data optim cost autom azur blob storag access tier regularli review delet longer need data http doc microsoft com azur machin learn data pipelin delet outputfiledatasetconfig content longer need idea welcom",
        "Question_preprocessed_content":"clean blob storag manag frequent workspac experi activ pipelin work good far problem rid old data run experi pipelin year blob storag grew enourmu size pipelin data store delet older run experimn gui actual pipelin data blob store delet smart wai clean data blob store run delet countless microsoft support page follow helpful post azur automat delet intermedi data written outputfiledatasetconfig avoid storag charg larg amount unneed data programmat delet intermedi data end pipelin run longer need us blob storag storag polici intermedi data regularli review delet data idea welcom",
        "Question_gpt_summary_original":"The user manages an Azure Machine Learning workspace with several experiments and active pipelines. The blob storage has grown to an enormous size due to the storage of pipeline data. The user has deleted older runs from experiments using the GUI, but the actual pipeline data on the blob store is not deleted. The user is looking for a smart way to clean up data on the blob store from deleted runs. The Microsoft support page suggests programmatically deleting intermediate data, using blob storage with a short-term storage policy, or regularly reviewing and deleting no-longer-needed data. The user is seeking any ideas to resolve the issue.",
        "Question_gpt_summary":"user manag workspac experi activ pipelin blob storag grown enorm size storag pipelin data user delet older run experi gui actual pipelin data blob store delet user look smart wai clean data blob store delet run microsoft support page suggest programmat delet intermedi data blob storag short term storag polici regularli review delet longer need data user seek idea resolv issu"
    },
    {
        "Question_title":"How do I remove files which are no longer tracked by DVC from DVC remote SSH storage",
        "Question_body":"<p>Hi,<\/p>\n<p>I had added a folder inputs to remote ssh dvc storage.<br>\nAfter some time, i decided to remove this folder<br>\nso i did<\/p>\n<ul>\n<li><code>dvc remove inputs.dvc<\/code><\/li>\n<li>did a git commit and push of the dvc changes<br>\nBut if i use an older commit where the inputs folder was tracked by dvc i am still able to pull inputs folder from remote dvc storage<\/li>\n<\/ul>\n<p>How can i remove the file from the remote ssh dvc storage?<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_created_time":1548846841252,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":3122.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-do-i-remove-files-which-are-no-longer-tracked-by-dvc-from-dvc-remote-ssh-storage\/113",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"remov file longer track remot ssh storag ad folder input remot ssh storag time decid remov folder remov input git commit push chang us older commit input folder track abl pull input folder remot storag remov file remot ssh storag",
        "Question_preprocessed_content":"remov file longer track remot ssh storag ad folder input remot ssh storag time decid remov folder git commit push chang us older commit input folder track abl pull input folder remot storag remov file remot ssh storag",
        "Question_gpt_summary_original":"The user added a folder to remote SSH DVC storage and later decided to remove it using \"dvc remove\" command. However, even after committing and pushing the changes, the user is still able to pull the folder from the remote DVC storage using an older commit. The user is seeking guidance on how to remove the file from the remote SSH DVC storage.",
        "Question_gpt_summary":"user ad folder remot ssh storag later decid remov remov command commit push chang user abl pull folder remot storag older commit user seek guidanc remov file remot ssh storag"
    },
    {
        "Question_title":"AzureML Sending and parsing datarequests to update AZURE Blob Storage.",
        "Question_body":"<p>Hello dear,  <\/p>\n<ol>\n<li> I need to be able to send data (HTTP request)s from different systems (SAP, for example) to update data in Azure Blob Storage.   <\/li>\n<li> And correspondingly, I need to be able to accept coming requests with data from Azure Blob Storage, so that I can train ML models based on that.   <\/li>\n<\/ol>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_created_time":1595285072103,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/49944\/azureml-sending-and-parsing-datarequests-to-update",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"send pars datarequest updat azur blob storag hello dear need abl send data http request differ system sap exampl updat data azur blob storag correspondingli need abl accept come request data azur blob storag train model base",
        "Question_preprocessed_content":"send pars datarequest updat azur blob storag hello dear need abl send data differ system updat data azur blob storag correspondingli need abl accept come request data azur blob storag train model base",
        "Question_gpt_summary_original":"The user needs to send data requests from different systems to update data in Azure Blob Storage and accept incoming requests with data from Azure Blob Storage to train ML models.",
        "Question_gpt_summary":"user need send data request differ system updat data azur blob storag accept incom request data azur blob storag train model"
    },
    {
        "Question_title":"Jupyter Notebook: Git Files",
        "Question_body":"<p>I\u2019m trying to populate my Sagemaker EC2 instance with scripts\/modules from my git repo for running Jupyter notebooks.<\/p>\n<p>When I clone the git repo using access token, I get a repo.git file in my Jupyter directory where notebooks live.<\/p>\n<p>What do I do with this file and how do I Import the py files from the repo? My goal is to actually have the files in my local instance and navigate through them(vs a single flat .git file)<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_created_time":1615845518817,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":23.0,
        "Poster_created_time":1610051240692,
        "Poster_location":null,
        "Poster_reputation_count":21.0,
        "Poster_view_count":24.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66646237",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"jupyt notebook git file try popul instanc script modul git repo run jupyt notebook clone git repo access token repo git file jupyt directori notebook live file import file repo goal actual file local instanc navig singl flat git file thank advanc",
        "Question_preprocessed_content":"jupyt notebook git file try popul instanc git repo run jupyt notebook clone git repo access token file jupyt directori notebook live file import file repo goal actual file local instanc navig thank advanc",
        "Question_gpt_summary_original":"The user is trying to populate their Sagemaker EC2 instance with scripts\/modules from their git repo for running Jupyter notebooks. However, when they clone the git repo using an access token, they get a repo.git file in their Jupyter directory where notebooks live. The user is unsure of what to do with this file and how to import the py files from the repo. Their goal is to have the files in their local instance and navigate through them.",
        "Question_gpt_summary":"user try popul instanc script modul git repo run jupyt notebook clone git repo access token repo git file jupyt directori notebook live user unsur file import file repo goal file local instanc navig"
    },
    {
        "Question_title":"How do I get embeddings from Huggingface(in sagemaker) instead of features?",
        "Question_body":"<p>I have a text classifier model that depends on embeddings from a certain huggingface model<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('T-Systems-onsite\/cross-en-de-roberta-sentence-transformer')\nencodings = model.encode(&quot;guckst du bundesliga&quot;)\n<\/code><\/pre>\n<p>this has a shape of (768,)<\/p>\n<p>tldr: is there a clean simple way to do this on sagemaker (hopefully using the images it provides) ?<\/p>\n<p>context:\nlooking at docs of this <a href=\"https:\/\/huggingface.co\/T-Systems-onsite\/cross-en-de-roberta-sentence-transformer?text=guckst%20du%20bundesliga\" rel=\"nofollow noreferrer\">huggingface model<\/a> the only sagemaker option I see is feature extraction<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.huggingface import HuggingFaceModel\nimport sagemaker\n\nrole = sagemaker.get_execution_role()\n# Hub Model configuration. https:\/\/huggingface.co\/models\nhub = {\n    'HF_MODEL_ID':'T-Systems-onsite\/cross-en-de-roberta-sentence-transformer',\n    'HF_TASK':'feature-extraction'\n}\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n    transformers_version='4.6.1',\n    pytorch_version='1.7.1',\n    py_version='py36',\n    env=hub,\n    role=role, \n)\n\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n    initial_instance_count=1, # number of instances\n    instance_type='ml.m5.xlarge' # ec2 instance type\n)\n\npredictor.predict({\n    'inputs': &quot;Today is a sunny day and I'll get some ice cream.&quot;\n})\n<\/code><\/pre>\n<p>this gives my the features which has a shape (9, 768)<\/p>\n<p>there is a connection between these two values, which is seen from a another code sample<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef embeddings(feature_envelope, attention_mask):\n    features = feature_envelope[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(features.size()).float()\n    sum_embeddings = torch.sum(features * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    return sum_embeddings \/ sum_mask\n\n#Sentences we want sentence embeddings for\nsentences = ['guckst du bundesliga']\n\n#Load AutoModel from huggingface model repository\ntokenizer = AutoTokenizer.from_pretrained('T-Systems-onsite\/cross-en-de-roberta-sentence-transformer')\nmodel = AutoModel.from_pretrained('T-Systems-onsite\/cross-en-de-roberta-sentence-transformer')\n\n#Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n\n#Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n#     print(model_output)\n\n#Perform pooling. In this case, mean pooling\nsentence_embeddings = embeddings(model_output, encoded_input['attention_mask'])\nsentence_embeddings.shape, sentence_embeddings\n<\/code><\/pre>\n<p>But as you can see cant derive embedding given only features<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1645212748743,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":415.0,
        "Poster_created_time":1479754482240,
        "Poster_location":null,
        "Poster_reputation_count":349.0,
        "Poster_view_count":65.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71178934",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"embed huggingfac instead featur text classifi model depend embed certain huggingfac model sentenc transform import sentencetransform model sentencetransform system onsit cross roberta sentenc transform encod model encod guckst bundesliga shape tldr clean simpl wai hopefulli imag provid context look doc huggingfac model option featur extract huggingfac import huggingfacemodel import role execut role hub model configur http huggingfac model hub model system onsit cross roberta sentenc transform task featur extract creat hug face model class huggingfac model huggingfacemodel transform version pytorch version version env hub role role deploi model infer predictor huggingfac model deploi initi instanc count number instanc instanc type xlarg instanc type predictor predict input todai sunni dai ic cream give featur shape connect valu seen code sampl transform import autotoken automodel import torch mean pool attent mask account correct averag def embed featur envelop attent mask featur featur envelop element model output contain token embed input mask expand attent mask unsqueez expand featur size float sum embed torch sum featur input mask expand sum mask torch clamp input mask expand sum min return sum embed sum mask sentenc want sentenc embed sentenc guckst bundesliga load automodel huggingfac model repositori token autotoken pretrain system onsit cross roberta sentenc transform model automodel pretrain system onsit cross roberta sentenc transform token sentenc encod input token sentenc pad true truncat true max length return tensor comput token embed torch grad model output model encod input print model output perform pool case mean pool sentenc embed embed model output encod input attent mask sentenc embed shape sentenc embed deriv embed given featur",
        "Question_preprocessed_content":"embed huggingfac instead featur text classifi model depend embed certain huggingfac model shape tldr clean simpl wai context look doc huggingfac model option featur extract give featur shape connect valu seen code sampl deriv embed given featur",
        "Question_gpt_summary_original":"The user is trying to obtain embeddings from a Huggingface model in SageMaker, but the only option available is feature extraction. The user has tried using the provided images but is only able to obtain features with a shape of (9, 768), which cannot be used to derive embeddings. The user is looking for a clean and simple way to obtain embeddings in SageMaker.",
        "Question_gpt_summary":"user try obtain embed huggingfac model option avail featur extract user tri provid imag abl obtain featur shape deriv embed user look clean simpl wai obtain embed"
    },
    {
        "Question_title":"SageMaker failed to extract model data archive tar.gz for container when deploying",
        "Question_body":"<p>I am trying in Amazon Sagemaker to deploy an existing Scikit-Learn model. So a model that wasn't trained on SageMaker, but locally on my machine.<\/p>\n<p>On my local (windows) machine I've saved my model as model.joblib and tarred the model to model.tar.gz.<\/p>\n<p>Next, I've uploaded this model to my S3 bucket ('my_bucket') in the following path s3:\/\/my_bucket\/models\/model.tar.gz. I can see the tar file in S3.<\/p>\n<p>But when I'm trying to deploy the model, it keeps giving the error message &quot;Failed to extract model data archive&quot;.<\/p>\n<p>The .tar.gz is generated on my local machine by running 'tar -czf model.tar.gz model.joblib' in a powershell command window.<\/p>\n<p>The code for uploading to S3<\/p>\n<pre><code>import boto3\ns3 = boto3.client(&quot;s3&quot;, \n              region_name='eu-central-1', \n              aws_access_key_id=AWS_KEY_ID, \n              aws_secret_access_key=AWS_SECRET)\ns3.upload_file(Filename='model.tar.gz', Bucket=my_bucket, Key='models\/model.tar.gz')\n<\/code><\/pre>\n<p>The code for creating the estimator and deploying:<\/p>\n<pre><code>import boto3\nfrom sagemaker.sklearn.estimator import SKLearnModel\n\n...\n\nmodel_data = 's3:\/\/my_bucket\/models\/model.tar.gz'\nsklearn_model = SKLearnModel(model_data=model_data,\n                             role=role,\n                             entry_point=&quot;my-script.py&quot;,\n                             framework_version=&quot;0.23-1&quot;)\npredictor = sklearn_model.deploy(instance_type=&quot;ml.t2.medium&quot;, initial_instance_count=1)                             \n<\/code><\/pre>\n<p>The error message:<\/p>\n<blockquote>\n<p>error message: UnexpectedStatusException: Error hosting endpoint\nsagemaker-scikit-learn-2021-01-24-17-24-42-204: Failed. Reason: Failed\nto extract model data archive for container &quot;container_1&quot; from URL\n&quot;s3:\/\/my_bucket\/models\/model.tar.gz&quot;. Please ensure that the object\nlocated at the URL is a valid tar.gz archive<\/p>\n<\/blockquote>\n<p>Is there a way to see why the archive is invalid?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_created_time":1611565411923,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1611823707607,
        "Question_score_count":4.0,
        "Question_view_count":1859.0,
        "Poster_created_time":1472970520888,
        "Poster_location":"Amersfoort, Nederland",
        "Poster_reputation_count":424.0,
        "Poster_view_count":21.0,
        "Answer_body":"<p>I had a similar issue as well, along with a similar fix to Bas (per comment above).<\/p>\n<p>I was finding I wasn't necessarily having issues with the .tar.gz step, this command does work fine:<\/p>\n<p><code>tar -czf &lt;filename&gt; .\/&lt;directory-with-files&gt;<\/code><\/p>\n<p>but rather with the uploading step.<\/p>\n<p>Manually uploading to S3 should take care of this, however, if you're doing this step programmatically, you might need to double check the steps taken. Bas appears to have had filename issues, mine were around using boto properly. Here's some code that works (Python only here, but watch for similar issues with other libraries):<\/p>\n<pre><code>bucket = 'bucket-name'\nkey = 'directory-inside-bucket'\nfile = 'the file name of the .tar.gz'\n\ns3_client = boto3.client('s3')\ns3_client.upload_file(file, bucket, key)\n<\/code><\/pre>\n<p>Docs: <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1611852069143,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1363541433296,
        "Answerer_location":"Twin Cities, MN, USA",
        "Answerer_reputation_count":348.0,
        "Answerer_view_count":24.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65881699",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"fail extract model data archiv tar contain deploi try deploi exist scikit learn model model wasn train local machin local window machin save model model joblib tar model model tar upload model bucket bucket follow path bucket model model tar tar file try deploi model keep give error messag fail extract model data archiv tar gener local machin run tar czf model tar model joblib powershel command window code upload import boto boto client region central aw access kei aw kei aw secret access kei aw secret upload file filenam model tar bucket bucket kei model model tar code creat estim deploi import boto sklearn estim import sklearnmodel model data bucket model model tar sklearn model sklearnmodel model data model data role role entri point script framework version predictor sklearn model deploi instanc type medium initi instanc count error messag error messag unexpectedstatusexcept error host endpoint scikit learn fail reason fail extract model data archiv contain contain url bucket model model tar ensur object locat url valid tar archiv wai archiv invalid",
        "Question_preprocessed_content":"fail extract model data archiv contain deploi try deploi exist model model wasn train local machin local machin save model tar model upload model bucket follow path tar file try deploi model keep give error messag fail extract model data archiv gener local machin run tar czf powershel command window code upload code creat estim deploi error messag error messag unexpectedstatusexcept error host endpoint fail reason fail extract model data archiv contain url ensur object locat url valid archiv wai archiv invalid",
        "Question_gpt_summary_original":"The user is trying to deploy an existing Scikit-Learn model on Amazon SageMaker that was not trained on SageMaker but locally on their machine. They saved the model as model.joblib and tarred it to model.tar.gz. The user uploaded the model to their S3 bucket but encountered an error message \"Failed to extract model data archive\" when trying to deploy the model. The user is seeking a way to see why the archive is invalid.",
        "Question_gpt_summary":"user try deploi exist scikit learn model train local machin save model model joblib tar model tar user upload model bucket encount error messag fail extract model data archiv try deploi model user seek wai archiv invalid"
    },
    {
        "Question_title":"capturing Parameter advanced speech settings from pages through pagesClient",
        "Question_body":"hello everyone,\n\nI was trying to capture parameter DTMF settings in pages from backend through pagesClient , but that client has limitation , it is unable to capture the DTMF settings. Is there any other way to capture DTMF settings from backend?\n\nreference :\u00a0@https:\/\/cloud.google.com\/dialogflow\/cx\/docs\/reference\/rest\/v3\/projects.locations.agents.flows.pages#...",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1677029760000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":80.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/capturing-Parameter-advanced-speech-settings-from-pages-through\/td-p\/524933\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"captur paramet advanc speech set page pagescli hello try captur paramet dtmf set page backend pagescli client limit unabl captur dtmf set wai captur dtmf set backend refer http cloud googl com dialogflow doc refer rest project locat agent flow page",
        "Question_preprocessed_content":"captur paramet advanc speech set page pagescli hello try captur paramet dtmf set page backend pagescli client limit unabl captur dtmf set wai captur dtmf set backend refer",
        "Question_gpt_summary_original":"The user is facing challenges in capturing DTMF settings from pages through pagesClient due to limitations in the client. They are seeking alternative ways to capture these settings from the backend.",
        "Question_gpt_summary":"user face challeng captur dtmf set page pagescli limit client seek altern wai captur set backend"
    },
    {
        "Question_title":"Detailed Vision API usage report",
        "Question_body":"Hello,\n\nWe would need help to get list of Vision API hits with source IPs as there are no standard reports available for the same GCP console\n\nIf some one can share details or ways to extract those data, it will be great help",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1675997640000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":41.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Detailed-Vision-API-usage-report\/td-p\/520704\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"detail vision api usag report hello need help list vision api hit sourc ip standard report avail gcp consol share detail wai extract data great help",
        "Question_preprocessed_content":"detail vision api usag report hello need help list vision api hit sourc ip standard report avail gcp consol share detail wai extract data great help",
        "Question_gpt_summary_original":"The user is seeking assistance in obtaining a list of Vision API hits with source IPs as there are no standard reports available for this information on the GCP console. They are requesting guidance on how to extract this data.",
        "Question_gpt_summary":"user seek assist obtain list vision api hit sourc ip standard report avail inform gcp consol request guidanc extract data"
    },
    {
        "Question_title":"how to upload images from csv file that have column with labels and urls of images in Azure machine learning by code",
        "Question_body":"<p>I have a dataset(CSV) file that looks like this one column with an identifier and another with the URL of the image, and I need to download images to my storage in azure machine learning, maybe someone could help what is the core should be?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1607106159790,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1607459547320,
        "Question_score_count":1.0,
        "Question_view_count":127.0,
        "Poster_created_time":1547213373932,
        "Poster_location":null,
        "Poster_reputation_count":47.0,
        "Poster_view_count":59.0,
        "Answer_body":"<p>Please refer to the sample code provided in this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-bring-data\" rel=\"nofollow noreferrer\">tutorial<\/a>. Specifically, where it explains how to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-bring-data#upload\" rel=\"nofollow noreferrer\">upload data to Azure<\/a>.<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\ndatastore.upload(src_dir='.\/data',\n                 target_path='datasets\/cifar10',\n                 overwrite=True)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1607110542867,
        "Answer_last_edit_time":1607110967612,
        "Answer_score_count":0.0,
        "Answerer_created_time":1566583092316,
        "Answerer_location":null,
        "Answerer_reputation_count":479.0,
        "Answerer_view_count":51.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65148768",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"upload imag csv file column label url imag code dataset csv file look like column identifi url imag need download imag storag mayb help core",
        "Question_preprocessed_content":"upload imag csv file column label url imag code dataset file look like column identifi url imag need download imag storag mayb help core",
        "Question_gpt_summary_original":"The user needs assistance in uploading images from a CSV file that contains columns with labels and URLs of images in Azure machine learning. They are seeking guidance on the necessary code to download the images to their storage.",
        "Question_gpt_summary":"user need assist upload imag csv file contain column label url imag seek guidanc necessari code download imag storag"
    },
    {
        "Question_title":"Merge distinct sklearn models into a single one",
        "Question_body":"<p>I have a dataset where, after exploring data, I detect some patron:<\/p>\n<ul>\n<li>The entire dataset have, imagine, 9 numerical variables, 1 dichotomous variable (take 'A' or 'B' value) and 1 numerical output<\/li>\n<li>The output is a cost (in \u20ac)<\/li>\n<li>I find a sklearn regression model that, when 'A', using 4 of 9 variables I can predict output with good performance.<\/li>\n<li>I find another sklearn regression model that, when 'B', using the last 5 variables, I can predict output with good performance.<\/li>\n<li>If I try to find a model which predict output with all the variables as input, encoding the dichotomous one with One-Hot-Encoder, the model has a bad performance.<\/li>\n<\/ul>\n<p>My goal is to implement a unique model in Azure Machine Learning, using a .joblib\/.pkl, but with this approach, I have two separated models with the same output (a cost) but different inputs, depending of dichotomous variable.<\/p>\n<p>Is there any way to merge the two models into a single one? So that with the 10 inputs, estimate a single output (internally discriminate options 'A' and 'B' to select the correct model and its inputs).<\/p>\n<p>Notice that using something like Voting Ensemble it's not valid because there are different inputs on each category (or I think it so)<\/p>\n<p>I accept another approach as a solution. Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1633074956947,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":119.0,
        "Poster_created_time":1611139028547,
        "Poster_location":"Vigo, Espa\u00f1a",
        "Poster_reputation_count":39.0,
        "Poster_view_count":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69402391",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"merg distinct sklearn model singl dataset explor data detect patron entir dataset imagin numer variabl dichotom variabl valu numer output output cost sklearn regress model variabl predict output good perform sklearn regress model variabl predict output good perform try model predict output variabl input encod dichotom hot encod model bad perform goal implement uniqu model joblib pkl approach separ model output cost differ input depend dichotom variabl wai merg model singl input estim singl output intern discrimin option select correct model input notic like vote ensembl valid differ input categori think accept approach solut thank",
        "Question_preprocessed_content":"merg distinct sklearn model singl dataset explor data detect patron entir dataset imagin numer variabl dichotom variabl numer output output cost sklearn regress model variabl predict output good perform sklearn regress model variabl predict output good perform try model predict output variabl input encod dichotom model bad perform goal implement uniqu model approach separ model output differ input depend dichotom variabl wai merg model singl input estim singl output notic like vote ensembl valid differ input categori accept approach solut thank",
        "Question_gpt_summary_original":"The user has two distinct sklearn regression models that predict output with good performance, but with different inputs depending on a dichotomous variable. The user wants to merge the two models into a single one that can estimate a single output using all 10 inputs, while internally discriminating between the options 'A' and 'B' to select the correct model and its inputs. The user is seeking a solution to implement a unique model in Azure Machine Learning.",
        "Question_gpt_summary":"user distinct sklearn regress model predict output good perform differ input depend dichotom variabl user want merg model singl estim singl output input intern discrimin option select correct model input user seek solut implement uniqu model"
    },
    {
        "Question_title":"Experiments stuck as queued",
        "Question_body":"<p>I have been trying to run experiments through Azure Machine Learning and today they have all been stuck as Queued and nothing is progressing.  I have tried cancelling them but whenever I try to run an experiment is just shows as 'Queued'.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_created_time":1635394156133,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/606918\/experiments-stuck-as-queued",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"experi stuck queu try run experi todai stuck queu progress tri cancel try run experi show queu",
        "Question_preprocessed_content":"experi stuck queu try run experi todai stuck queu progress tri cancel try run experi show queu",
        "Question_gpt_summary_original":"The user is facing an issue with Azure Machine Learning as all their experiments are stuck in a queued state and are not progressing. The user has attempted to cancel the experiments but they continue to remain in a queued state.",
        "Question_gpt_summary":"user face issu experi stuck queu state progress user attempt cancel experi continu remain queu state"
    },
    {
        "Question_title":"Dvc push - ERROR: failed to transfer",
        "Question_body":"<p>hi, i just  created a new repo with dvc and now I added 1 data file, but I can\u2019t seem to manage to do <code>dvc push<\/code>, what could be the reason?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/4cfdbb5212656025eadc3a4b339a3892fc93d39d.png\" data-download-href=\"\/uploads\/short-url\/aZ5U8AjYSacHl6fq6MwI4Pk4quN.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/4cfdbb5212656025eadc3a4b339a3892fc93d39d_2_690x113.png\" alt=\"image\" data-base62-sha1=\"aZ5U8AjYSacHl6fq6MwI4Pk4quN\" width=\"690\" height=\"113\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/4cfdbb5212656025eadc3a4b339a3892fc93d39d_2_690x113.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/4cfdbb5212656025eadc3a4b339a3892fc93d39d_2_1035x169.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/4cfdbb5212656025eadc3a4b339a3892fc93d39d.png 2x\" data-dominant-color=\"3F4349\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1333\u00d7220 26.5 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1676995646296,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":98.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-push-error-failed-to-transfer\/1533",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"push error fail transfer creat new repo ad data file manag push reason imag",
        "Question_preprocessed_content":"push error fail transfer creat new repo ad data file manag reason imag",
        "Question_gpt_summary_original":"The user has created a new repo with DVC and added one data file, but is unable to do \"dvc push\". The user is seeking help to identify the reason for this error.",
        "Question_gpt_summary":"user creat new repo ad data file unabl push user seek help identifi reason error"
    },
    {
        "Question_title":"How to use scikit learn model from inside sagemaker 'model.tar.gz' file?",
        "Question_body":"<p>New to Sagemaker..<\/p>\n<p>Trained a &quot;linear-learner&quot; classification model using the Sagemaker API, and it saved a &quot;model.tar.gz&quot; file in my s3 path. From what I understand SM just used an image of a scikit logreg model.<\/p>\n<p>Finally, I'd like to gain access to the model object itself, so I unpacked the &quot;model.tar.gz&quot; file only to find another file called &quot;model_algo-1&quot; with no extension.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DPUMO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DPUMO.png\" alt=\"contents of unknown file\" \/><\/a><\/p>\n<p>Can anyone tell me how I can find the &quot;real&quot; modeling object without using the inference\/Endpoint delpoy API provided by Sagemaker? There are some things I want to look at manually.<\/p>\n<p>Thanks,\nCraig<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1658258900593,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":42.0,
        "Poster_created_time":1474555414616,
        "Poster_location":null,
        "Poster_reputation_count":129.0,
        "Poster_view_count":6.0,
        "Answer_body":"<p>Linear-Learner is a built in algorithm written using MX-net and the binary is also MXNET compatible. You can't use this model outside of SageMaker as there is no open source implementation for this.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1658447826900,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1657058369727,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":9.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73042521",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"us scikit learn model insid model tar file new train linear learner classif model api save model tar file path understand imag scikit logreg model final like gain access model object unpack model tar file file call model algo extens tell real model object infer endpoint delpoi api provid thing want look manual thank craig",
        "Question_preprocessed_content":"us scikit learn model insid file new train classif model api save file path understand imag scikit logreg model final like gain access model object unpack file file call extens tell real model object delpoi api provid thing want look manual thank craig",
        "Question_gpt_summary_original":"The user has trained a linear-learner classification model using the Sagemaker API, which saved a \"model.tar.gz\" file in their s3 path. However, upon unpacking the file, they found another file called \"model_algo-1\" with no extension and are unsure how to access the actual modeling object without using the inference\/Endpoint deploy API provided by Sagemaker.",
        "Question_gpt_summary":"user train linear learner classif model api save model tar file path unpack file file call model algo extens unsur access actual model object infer endpoint deploi api provid"
    },
    {
        "Question_title":"Cannot debug code with Azure ML workspace + VS Code",
        "Question_body":"<p>I am trying to remotely debug Python code in an Azure ML workspace using VS Code 1.64.2. I have Azure ML extension installed in VS Code.<\/p>\n<p>I can connect to Azure ML workspace and most of the features work ok. I'd like to start remote debugging following a tutorial on youtube. However I cannot do it because when I right-click a python file, there is no <code>Azure ML: Run as Experiment in Azure<\/code> menu which I can see in the video.<\/p>\n<p>What am I doing wrong?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BYfcQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BYfcQ.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1647014651957,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1647072440036,
        "Question_score_count":0.0,
        "Question_view_count":142.0,
        "Poster_created_time":1380793161492,
        "Poster_location":null,
        "Poster_reputation_count":2522.0,
        "Poster_view_count":274.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71441335",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"debug code workspac code try remot debug python code workspac code extens instal code connect workspac featur work like start remot debug follow tutori youtub right click python file run experi azur menu video wrong",
        "Question_preprocessed_content":"debug code workspac code try remot debug python code workspac code extens instal code connect workspac featur work like start remot debug follow tutori youtub python file menu video wrong",
        "Question_gpt_summary_original":"The user is unable to remotely debug Python code in an Azure ML workspace using VS Code due to the absence of the \"Azure ML: Run as Experiment in Azure\" menu when right-clicking a Python file. The user has the Azure ML extension installed in VS Code and is seeking assistance to resolve the issue.",
        "Question_gpt_summary":"user unabl remot debug python code workspac code absenc run experi azur menu right click python file user extens instal code seek assist resolv issu"
    },
    {
        "Question_title":"How to create wandb.Table with image previews for a big dataset with most efficiency?",
        "Question_body":"<p>I have a dataset that has ~40k images. I want to upload them all using arifacts, i used run.add_dir(\u201cpath_to_dir_with_images\u201d).<br>\nI also have a csv that contains the labels of these images as well as the name of the image. Is there a way to create a wandb.Table so that i can explore this csv but also to have a column called images that has a reference to the images from the artifact via the file name. So that when i do EDA i can look at the images that are in the artifact instead of adding a column \u201cimages\u201d to the csv and populating it with wandb.Image objects and then uploading it as a wandb.Table?<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_created_time":1675938416479,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":211.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-create-wandb-table-with-image-previews-for-a-big-dataset-with-most-efficiency\/3855",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"creat tabl imag preview big dataset effici dataset imag want upload arifact run add dir path dir imag csv contain label imag imag wai creat tabl explor csv column call imag refer imag artifact file eda look imag artifact instead ad column imag csv popul imag object upload tabl",
        "Question_preprocessed_content":"creat tabl imag preview big dataset effici dataset imag want upload arifact csv contain label imag imag wai creat tabl explor csv column call imag refer imag artifact file eda look imag artifact instead ad column imag csv popul imag object upload tabl",
        "Question_gpt_summary_original":"The user has a dataset of around 40k images and a CSV file containing image labels and names. They want to create a wandb.Table that includes a column for image previews, referencing the images from the artifact via the file name, to explore the CSV during EDA without having to add a column of wandb.Image objects to the CSV and upload it as a wandb.Table.",
        "Question_gpt_summary":"user dataset imag csv file contain imag label name want creat tabl includ column imag preview referenc imag artifact file explor csv eda have add column imag object csv upload tabl"
    },
    {
        "Question_title":"Sagemaker Semantic Segmentation: iou and pixel accuracy per class",
        "Question_body":"<p>I am doing some semantic segemnetation work on AWS sagemaker. My output is the mean iou and pixel accuracy of all my classes but i need more granularity so require the iou and pixel accuracy for each class. This is to see what classes need to be improved.<\/p>\n\n<p>I imagine its possible but how? <\/p>\n\n<p>I have had a look at these similar questions but no solution as yet.<\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/54837529\/tensorflow-iou-per-class\">Tensorflow : IOU per class<\/a><\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/44041096\/iou-for-semantic-segmentation-implementation-in-python-caffe-per-class\">IoU for semantic segmentation implementation in python\/caffe per class<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1560854402057,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":393.0,
        "Poster_created_time":1558518328152,
        "Poster_location":"London, UK",
        "Poster_reputation_count":79.0,
        "Poster_view_count":16.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56647277",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"semant segment iou pixel accuraci class semant segemnet work output mean iou pixel accuraci class need granular requir iou pixel accuraci class class need improv imagin possibl look similar question solut tensorflow iou class iou semant segment implement python caff class",
        "Question_preprocessed_content":"semant segment iou pixel accuraci class semant segemnet work output mean iou pixel accuraci class need granular requir iou pixel accuraci class class need improv imagin possibl look similar question solut tensorflow iou class iou semant segment implement class",
        "Question_gpt_summary_original":"The user is working on semantic segmentation on AWS Sagemaker and is currently getting mean iou and pixel accuracy for all classes. However, they need more granularity and want to get iou and pixel accuracy for each class to identify which classes need improvement. The user is seeking a solution to obtain iou and pixel accuracy per class.",
        "Question_gpt_summary":"user work semant segment current get mean iou pixel accuraci class need granular want iou pixel accuraci class identifi class need improv user seek solut obtain iou pixel accuraci class"
    },
    {
        "Question_title":"Track train script version along with hyperaparams (ideally automated)",
        "Question_body":"<p>I\u2019m just getting started with experiments, my goal is to ablation-study some potentially new methods for finetuning NLP models. I use HuggingFace trainer and the W&amp;B integration works flawlessly, so the hyperparams are super easy to track. I\u2019m going to be doing some experimentation within the training code though - is there an easy way to track the training script version\/content along with the experiments, ideally integrated with Github?  Thanks! Darek<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1638831742465,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":229.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/track-train-script-version-along-with-hyperaparams-ideally-automated\/1479",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"track train script version hyperaparam ideal autom get start experi goal ablat studi potenti new method finetun nlp model us huggingfac trainer integr work flawlessli hyperparam super easi track go experiment train code easi wai track train script version content experi ideal integr github thank darek",
        "Question_preprocessed_content":"track train script version hyperaparam get start experi goal potenti new method finetun nlp model us huggingfac trainer integr work flawlessli hyperparam super easi track go experiment train code easi wai track train script experi ideal integr github thank darek",
        "Question_gpt_summary_original":"The user is looking for a way to track the training script version and content along with experiments, ideally integrated with Github, while using HuggingFace trainer and W&B integration for ablation-study of potentially new methods for finetuning NLP models.",
        "Question_gpt_summary":"user look wai track train script version content experi ideal integr github huggingfac trainer integr ablat studi potenti new method finetun nlp model"
    },
    {
        "Question_title":"Adding experiments. Long time",
        "Question_body":"<p>Hi, eveyone.<\/p>\n<p>I am using the solution for tuning hyperparams in my project as described in this video <a href=\"https:\/\/www.youtube.com\/watch?v=W48Tvx2p-xE\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Tuning Hyperparameters Using Grid Search and Random Search with DVC - YouTube<\/a>.<\/p>\n<p>But when start an adding experiments usually it takes a long time. It takes the same time as a grid searh in my small project. Is it a common situation or I have something like a bug?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1683870266696,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":19.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/adding-experiments-long-time\/1662",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"ad experi long time eveyon solut tune hyperparam project describ video tune hyperparamet grid search random search youtub start ad experi usual take long time take time grid searh small project common situat like bug",
        "Question_preprocessed_content":"ad experi long time eveyon solut tune hyperparam project describ video tune hyperparamet grid search random search youtub start ad experi usual take long time take time grid searh small project common situat like bug",
        "Question_gpt_summary_original":"The user is facing challenges with adding experiments to their project using a solution for tuning hyperparameters, as it takes a long time and is as slow as a grid search in their small project. They are unsure if this is a common issue or if there is a bug in their system.",
        "Question_gpt_summary":"user face challeng ad experi project solut tune hyperparamet take long time slow grid search small project unsur common issu bug"
    },
    {
        "Question_title":"How to save and load ML model with Azure Data Factory",
        "Question_body":"<p>I have an Azure Data factory that receives data from a service bus and then I want to classify my data with an ML model.  <\/p>\n<p>Is there any solution to save and load the ML model on the Azure Data Factory pipeline?  <\/p>\n<p>For your information, I want to use cloud base solution. I don't use the PICKLE library. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1623657811393,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>Hello <a href=\"\/users\/na\/?userid=b5c4f434-7ffe-0003-0000-000000000000\">@Mohsen Akhavan  <\/a>,    <br \/>\nThanks for the ask and using the Microsoft Q&amp;A platform  .    <\/p>\n<p>I think you can use the machine learning activity . Read and watch the video <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/transform-data-machine-learning-service\">here<\/a> .    <\/p>\n<p>The challenge in your case is the data is in EH and at this time ADF cannot read EH data . I suggest you to use a Azure stream analytics jobs and read the data from EH and write it to SQL or blob . Once the data is in any of these two sources ADF can be used to read the data .    <\/p>\n<p>Please do let me know how it goes .    <br \/>\nThanks     <br \/>\nHimanshu    <br \/>\nPlease do consider clicking on <strong>&quot;Accept Answer&quot;<\/strong> and <strong>&quot;Up-vote&quot;<\/strong> on the post that helps you, as it can be beneficial to other community members    <\/p>\n",
        "Answer_comment_count":2.0,
        "Question_closed_time":1623860467256,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/434449\/how-to-save-and-load-ml-model-with-azure-data-fact",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"save load model azur data factori azur data factori receiv data servic bu want classifi data model solut save load model azur data factori pipelin inform want us cloud base solut us pickl librari",
        "Question_preprocessed_content":"save load model azur data factori azur data factori receiv data servic bu want classifi data model solut save load model azur data factori pipelin inform want us cloud base solut us pickl librari",
        "Question_gpt_summary_original":"The user is seeking a cloud-based solution to save and load an ML model on an Azure Data Factory pipeline without using the PICKLE library. The ML model is intended to classify data received from a service bus.",
        "Question_gpt_summary":"user seek cloud base solut save load model azur data factori pipelin pickl librari model intend classifi data receiv servic bu"
    },
    {
        "Question_title":"How connect Azure Function with my own model? It is possible to use Azure Storage?",
        "Question_body":"<h3>Intro<\/h3>\n<p>I created my own model locally and then register it and deploy it to azure and it works.<\/p>\n<h3>deployed model output:<\/h3>\n<p><a href=\"https:\/\/i.stack.imgur.com\/gaLCH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gaLCH.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h3>my approach<\/h3>\n<p>I used <a href=\"https:\/\/medium.com\/microsoftazure\/deploying-azure-machine-learning-containers-41bcb02a4e1b\" rel=\"nofollow noreferrer\">this tutorial<\/a>, and I want use my model in Azure Function and I can do it:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def main(req: func.HttpRequest, msg: func.Out[func.QueueMessage]) -&gt; str:\n    name = req.params.get('name')\n    scoring_uri = 'http:\/\/1f72b1bf-5ca9-42d9-bedd-f41773591a4f.francecentral.azurecontainer.io\/score'\n    headers = {'Content-Type':'application\/json'}\n    test_data = json.dumps({'text': 'Today is a great day!'})\n    response = requests.post(scoring_uri, data=test_data, headers=headers)\n\nif not name:\n    try:\n        req_body = req.get_json()\n    except ValueError:\n        pass\n    else:\n        name = req_body.get('name')\n\nif name:\n    msg.set(name)\n    return func.HttpResponse(f&quot;Hello {name}! Najlepszy wynik: {response.json()}&quot;)\nelse:\n    return func.HttpResponse(\n        &quot;Please pass a name on the query string or in the request body&quot;,\n        status_code=400\n    )\n<\/code><\/pre>\n<h3>questions<\/h3>\n<ol>\n<li>Is my usage correct?<\/li>\n<li>Is it possible to use azure storage for model storage and how to do it?<\/li>\n<li>Is there any other way to use the model in Azure Function?<\/li>\n<\/ol>\n<p>I am wondering because I had specified in the requirements that I should use azure functions and azure storage. I don't understand why.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1609881766290,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1609945294496,
        "Question_score_count":0.0,
        "Question_view_count":210.0,
        "Poster_created_time":1605834001336,
        "Poster_location":null,
        "Poster_reputation_count":73.0,
        "Poster_view_count":24.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65586756",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"connect azur function model possibl us azur storag intro creat model local regist deploi azur work deploi model output approach tutori want us model azur function def main req func httprequest msg func func queuemessag str req param score uri http fbbf bedd faf francecentr azurecontain score header content type applic json test data json dump text todai great dai respons request post score uri data test data header header try req bodi req json valueerror pass req bodi msg set return func httprespons hello najlepszi wynik respons json return func httprespons pass queri string request bodi statu code question usag correct possibl us azur storag model storag wai us model azur function wonder specifi requir us azur function azur storag understand",
        "Question_preprocessed_content":"connect azur function model possibl us azur storag intro creat model local regist deploi azur work deploi model output approach tutori want us model azur function question usag correct possibl us azur storag model storag wai us model azur function wonder specifi requir us azur function azur storag understand",
        "Question_gpt_summary_original":"The user has successfully deployed their own model on Azure using a tutorial and wants to use it in Azure Function. They have some questions regarding the correct usage, the possibility of using Azure Storage for model storage, and alternative ways to use the model in Azure Function. They are also unsure why they were asked to use Azure Functions and Azure Storage in the requirements.",
        "Question_gpt_summary":"user successfulli deploi model azur tutori want us azur function question correct usag possibl azur storag model storag altern wai us model azur function unsur ask us azur function azur storag requir"
    },
    {
        "Question_title":"Python Notebook Invoke Endpoint Sagemaker from Local",
        "Question_body":"<p>I am trying to invoke an Amazon Sagemaker Endpoint from a local python notebook. This is the code I am using.  <\/p>\n\n<pre><code>import boto3\n\naws_access_key_id = '...............'\naws_secret_access_key = '................'\ntkn = '..........'\nregion_name = '............'\n\namz = boto3.client('sagemaker-runtime',\n                   aws_access_key_id=aws_access_key_id,\n                   aws_secret_access_key=aws_secret_access_key,\n                   aws_session_token=tkn,\n                   region_name=region_name)\n\n\nresponse = amz.invoke_endpoint(\n    EndpointName='mymodel',\n    Body=b'bytes'\n)               \n<\/code><\/pre>\n\n<p>However, this doesn't work. Do I have to specify something else in <em>Body<\/em> ?   <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1529050767617,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":3972.0,
        "Poster_created_time":1432680790120,
        "Poster_location":null,
        "Poster_reputation_count":455.0,
        "Poster_view_count":58.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50871651",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"python notebook invok endpoint local try invok endpoint local python notebook code import boto aw access kei aw secret access kei tkn region amz boto client runtim aw access kei aw access kei aw secret access kei aw secret access kei aw session token tkn region region respons amz invok endpoint endpointnam mymodel bodi byte work specifi bodi",
        "Question_preprocessed_content":"python notebook invok endpoint local try invok endpoint local python notebook code work specifi bodi",
        "Question_gpt_summary_original":"The user is attempting to invoke an Amazon Sagemaker Endpoint from a local Python notebook using a specific code, but it is not working. They are unsure if they need to specify something else in the \"Body\" parameter.",
        "Question_gpt_summary":"user attempt invok endpoint local python notebook specif code work unsur need specifi bodi paramet"
    },
    {
        "Question_title":"sagemaker giving UnicodeDecodeError while deserializing",
        "Question_body":"<p>In sagemaker, was able to load and deploy model from s3. While deserializing the data for prediction, I am getting \"UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd7 in position 2: invalid continuation byte\" on line \n\"results = predictor.predict(test_X)\"<\/p>\n\n<p>I tried the following sagemaker example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/linear_time_series_forecast\/linear_time_series_forecast.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/linear_time_series_forecast\/linear_time_series_forecast.ipynb<\/a> . I was able to create train, validate and deploy model and store model in s3. <\/p>\n\n<p>After this I wanted to import model from s3 into sagemaker and test using the imported model. Was able to load and deploy the model, but when predicting for test values, I am getting UnicodeDecodeError<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.predictor import csv_serializer, json_deserializer\n\nrole = get_execution_role()\nsagemaker_session = sagemaker.Session()\nmodel_data = sagemaker.session.s3_input( model_file_location_in_s3, distribution='FullyReplicated', content_type='application\/x-sagemaker-model', s3_data_type='S3Prefix')\nsagemaker_model = sagemaker.LinearLearnerModel(model_data=model_file,\n                                       role=role, \n                                       sagemaker_session=sagemaker_session)\npredictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n\n#loading test data\ngas = pd.read_csv('gasoline.csv', header=None, names=['thousands_barrels'],encoding='utf-8')\ngas['thousands_barrels_lag1'] = gas['thousands_barrels'].shift(1)\ngas['thousands_barrels_lag2'] = gas['thousands_barrels'].shift(2)\ngas['thousands_barrels_lag3'] = gas['thousands_barrels'].shift(3)\ngas['thousands_barrels_lag4'] = gas['thousands_barrels'].shift(4)\ngas['trend'] = np.arange(len(gas))\ngas['log_trend'] = np.log1p(np.arange(len(gas)))\ngas['sq_trend'] = np.arange(len(gas)) ** 2\nweeks = pd.get_dummies(np.array(list(range(52)) * 15)[:len(gas)], prefix='week')\ngas = pd.concat([gas, weeks], axis=1)\ngas = gas.iloc[4:, ]\nsplit_train = int(len(gas) * 0.6)\nsplit_test = int(len(gas) * 0.3)\ntest_y = gas['thousands_barrels'][split_test:]\ntest_X = gas.drop('thousands_barrels', axis=1).iloc[split_test:, ].as_matrix()\n\npredictor.content_type = 'text\/csv'\npredictor.serializer = csv_serializer\npredictor.deserializer = json_deserializer\n\nresults = predictor.predict(test_X)\none_step = np.array([r['score'] for r in results['predictions']])\n<\/code><\/pre>\n\n<p>the program works fine when model is trained and deployed(as in example) but when loading from s3, it throws this error.<\/p>\n\n<p>The test data is numpy ndarray.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1566987140893,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":451.0,
        "Poster_created_time":1367922558556,
        "Poster_location":"Noida, Uttar Pradesh, India",
        "Poster_reputation_count":91.0,
        "Poster_view_count":7.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57690003",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"give unicodedecodeerror deseri abl load deploi model deseri data predict get unicodedecodeerror utf codec decod byte posit invalid continu byte line result predictor predict test tri follow exampl http github com awslab amazon exampl blob master introduct appli machin learn linear time seri forecast linear time seri forecast ipynb abl creat train valid deploi model store model want import model test import model abl load deploi model predict test valu get unicodedecodeerror predictor import csv serial json deseri role execut role session session model data session input model file locat distribut fullyrepl content type applic model data type sprefix model linearlearnermodel model data model file role role session session predictor model deploi initi instanc count instanc type medium load test data ga read csv gasolin csv header name thousand barrel encod utf ga thousand barrel lag ga thousand barrel shift ga thousand barrel lag ga thousand barrel shift ga thousand barrel lag ga thousand barrel shift ga thousand barrel lag ga thousand barrel shift ga trend arang len ga ga log trend logp arang len ga ga trend arang len ga week dummi arrai list rang len ga prefix week ga concat ga week axi ga ga iloc split train int len ga split test int len ga test ga thousand barrel split test test ga drop thousand barrel axi iloc split test matrix predictor content type text csv predictor serial csv serial predictor deseri json deseri result predictor predict test step arrai score result predict program work fine model train deploi exampl load throw error test data numpi ndarrai",
        "Question_preprocessed_content":"give unicodedecodeerror deseri abl load deploi model deseri data predict get unicodedecodeerror codec decod byte posit invalid continu byte line result tri follow exampl abl creat train valid deploi model store model want import model test import model abl load deploi model predict test valu get unicodedecodeerror program work fine model train deploi load throw error test data numpi ndarrai",
        "Question_gpt_summary_original":"The user is encountering a UnicodeDecodeError while deserializing data for prediction in Sagemaker. They were able to load and deploy the model from S3, but when predicting for test values, they are getting the error. The user tried an example from Sagemaker and was able to create, train, validate, and deploy the model and store it in S3. However, when importing the model from S3 into Sagemaker and testing it, they encountered the error. The test data is in numpy ndarray format.",
        "Question_gpt_summary":"user encount unicodedecodeerror deseri data predict abl load deploi model predict test valu get error user tri exampl abl creat train valid deploi model store import model test encount error test data numpi ndarrai format"
    },
    {
        "Question_title":"How to save models trained locally in Amazon SageMaker?",
        "Question_body":"<p>I'm trying to use a local training job in SageMaker.<\/p>\n<p>Following this AWS notebook (<a href=\"http:\/\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb<\/a>) I was able to train and predict locally.<\/p>\n<p>There is any way to train locally and save the trained model in the Amazon SageMaker Training Job section?\nOtherwise, how can I properly save trained models I trained using local mode?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1595954217667,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1595959142728,
        "Question_score_count":7.0,
        "Question_view_count":3195.0,
        "Poster_created_time":1464391892936,
        "Poster_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Poster_reputation_count":2243.0,
        "Poster_view_count":148.0,
        "Answer_body":"<p>There is no way to have your local mode training jobs appear in the AWS console. The intent of local mode is to allow for faster iteration\/debugging before using SageMaker for training your model.<\/p>\n<p>You can create SageMaker Models from local model artifacts. Compress your model artifacts into a <code>.tar.gz<\/code> file, upload that file to S3, and then create the Model (with the SDK or in the console).<\/p>\n<p>Documentation:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1596039571220,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1512770138847,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63138835",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"save model train local try us local train job follow aw notebook http github com awslab amazon exampl blob master python sdk mxnet gluon mnist mxnet mnist gluon local mode ipynb abl train predict local wai train local save train model train job section properli save train model train local mode",
        "Question_preprocessed_content":"save model train local try us local train job follow aw notebook abl train predict local wai train local save train model train job section properli save train model train local mode",
        "Question_gpt_summary_original":"The user is trying to use a local training job in Amazon SageMaker and has successfully trained and predicted locally using an AWS notebook. However, the user is facing challenges in saving the trained model in the Amazon SageMaker Training Job section and is seeking guidance on how to properly save trained models trained using local mode.",
        "Question_gpt_summary":"user try us local train job successfulli train predict local aw notebook user face challeng save train model train job section seek guidanc properli save train model train local mode"
    },
    {
        "Question_title":"ModuleNotFoundError: No module named 'ruamel' when excuting from azureml.core",
        "Question_body":"<p>I am trying to execute the Azure ml sdk from the local system using the Jupyter notebook. When I run the below code i am getting an error.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore, Dataset\n\nModuleNotFoundError: No module named 'ruamel' \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1622646368700,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":332.0,
        "Poster_created_time":1599816833352,
        "Poster_location":"New Delhi, Delhi, India",
        "Poster_reputation_count":329.0,
        "Poster_view_count":58.0,
        "Answer_body":"<p>You have to add pip 20.1.1<\/p>\n<p>Conda ruamel needs higher version of pip<\/p>\n<pre><code>conda install pip=20.1.1\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1626743937907,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1536318818623,
        "Answerer_location":"\u0130zmit, Kocaeli, T\u00fcrkiye",
        "Answerer_reputation_count":1033.0,
        "Answerer_view_count":55.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67807756",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"modulenotfounderror modul name ruamel excut core try execut sdk local jupyt notebook run code get error core import workspac datastor dataset modulenotfounderror modul name ruamel",
        "Question_preprocessed_content":"modulenotfounderror modul name ruamel excut core try execut sdk local jupyt notebook run code get error",
        "Question_gpt_summary_original":"The user is encountering a \"ModuleNotFoundError\" when trying to execute the Azure ml sdk from their local system using Jupyter notebook. The error message specifically states that there is no module named 'ruamel'.",
        "Question_gpt_summary":"user encount modulenotfounderror try execut sdk local jupyt notebook error messag specif state modul name ruamel"
    },
    {
        "Question_title":"Underlying pipeline differences between Batch Endpoints & direct pipeline runs",
        "Question_body":"<p>I am looking at executing some inference jobs based on a published model, but I'm not completely clear on the differences between using a pipeline directly (python v1 sdk), or using batch endpoints (python v2 sdk or http rest). I do not have a requirement for an endpoint to be online at all times, as all jobs will execute and shutdown immediately. Most inference jobs are executing off of a single dataset at a time.<\/p>\n<p>I've investigated batch inference jobs and the underlying pipeline logs\/execution appears to match pipelines.<\/p>\n<p>Are there any other runtime differences? Cost? Are direct pipelines being phased out?<\/p>\n<p>Edit: I have the ability to interact with AML via Python &amp; HTTP REST, so both options are in play. I've found both <code>pipeline endpoints<\/code> and <code>batch endpoints<\/code> produce HTTP endpoints that can be consumed.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1660572647103,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1660573191972,
        "Question_score_count":0.0,
        "Question_view_count":75.0,
        "Poster_created_time":1261591543563,
        "Poster_location":"Milwaukee, WI",
        "Poster_reputation_count":447.0,
        "Poster_view_count":108.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73362189",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"underli pipelin differ batch endpoint direct pipelin run look execut infer job base publish model complet clear differ pipelin directli python sdk batch endpoint python sdk http rest requir endpoint onlin time job execut shutdown immedi infer job execut singl dataset time investig batch infer job underli pipelin log execut appear match pipelin runtim differ cost direct pipelin phase edit abil interact aml python http rest option plai pipelin endpoint batch endpoint produc http endpoint consum",
        "Question_preprocessed_content":"underli pipelin differ batch endpoint direct pipelin run look execut infer job base publish model complet clear differ pipelin directli batch endpoint requir endpoint onlin time job execut shutdown immedi infer job execut singl dataset time investig batch infer job underli pipelin appear match pipelin runtim differ cost direct pipelin phase edit abil interact aml python http rest option plai produc http endpoint consum",
        "Question_gpt_summary_original":"The user is unsure about the differences between using a pipeline directly or using batch endpoints for executing inference jobs based on a published model. They do not require an endpoint to be online at all times and most jobs will execute and shutdown immediately. The user has investigated batch inference jobs and found that the underlying pipeline logs\/execution appears to match pipelines. They are asking about any other runtime differences, cost, and whether direct pipelines are being phased out. The user has the ability to interact with AML via Python & HTTP REST, so both options are in play.",
        "Question_gpt_summary":"user unsur differ pipelin directli batch endpoint execut infer job base publish model requir endpoint onlin time job execut shutdown immedi user investig batch infer job underli pipelin log execut appear match pipelin ask runtim differ cost direct pipelin phase user abil interact aml python http rest option plai"
    },
    {
        "Question_title":"Cost of autoscaling endpoint Amazon SageMaker endpoint to zero",
        "Question_body":"I want to use an Amazon Sagemaker endpoint for a custom classification model. The endpoint should only handle sporadic input (say a few times a week). \nFor this purpose I want to employ autoscaling that scales the number of instances down to 0 when the endpoint is not used. \n\nAre there any costs associated with having an endpoint with 0 instances? \n\nThanks!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1666360814590,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1668547705502,
        "Question_score_count":1.0,
        "Question_view_count":468.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"You dont pay any compute costs for the duration when the endpoint size scales down to 0. But i think you can design it better. There are few other options for you to use in SageMaker Endpoint(assuming you are using realtime endpoint)\n\n1. Try using [SageMaker Serverless Inference](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html) instead. Its purely serverless in nature so you pay only when the endpoint is serving inference. i think that would fit your requirement better.\n2. You can think of using Lambda as well which will reduce your hosting costs. but you have to do more work in setting up the inference stack all by yourself.\n3. There is also an option of [SageMaker asynchronous inference](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/async-inference.html) but its mostly useful for inference which require longer time to process each request. The reason i mention this is it also support scale to 0 when no traffic is coming.",
        "Answer_comment_count":1.0,
        "Question_closed_time":1666371029704,
        "Answer_last_edit_time":1666796875366,
        "Answer_score_count":2.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0VGYdZe8TRivmtGHoiDDHw\/cost-of-autoscaling-endpoint-amazon-sagemaker-endpoint-to-zero",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_original_content":"cost autosc endpoint endpoint zero want us endpoint custom classif model endpoint handl sporad input time week purpos want emploi autosc scale number instanc endpoint cost associ have endpoint instanc thank",
        "Question_preprocessed_content":"cost autosc endpoint endpoint zero want us endpoint custom classif model endpoint handl sporad input purpos want emploi autosc scale number instanc endpoint cost associ have endpoint instanc thank",
        "Question_gpt_summary_original":"The user wants to use an Amazon SageMaker endpoint for a custom classification model that only handles sporadic input. They want to use autoscaling to scale the number of instances down to 0 when the endpoint is not used. The user is asking if there are any costs associated with having an endpoint with 0 instances.",
        "Question_gpt_summary":"user want us endpoint custom classif model handl sporad input want us autosc scale number instanc endpoint user ask cost associ have endpoint instanc"
    },
    {
        "Question_title":"How to strip HTML from a text column in Azure ML Execute Python Script step",
        "Question_body":"<p>If I have a column of data of type string in an incoming Azure ML dataset that contains HTML tags screwing up my results, how can I remove those tags?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1484610622880,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":325.0,
        "Poster_created_time":1265234764768,
        "Poster_location":"Denver, CO",
        "Poster_reputation_count":30577.0,
        "Poster_view_count":6460.0,
        "Answer_body":"<p>Like this:<\/p>\n\n<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):\n  dataframe1[1] = dataframe1['text'].str.replace('&lt;[^&lt;]+?&gt;', ' ', case=False)\n  return dataframe1,\n<\/code><\/pre>\n\n<p>Remember to precede the <code>Execute Python Script<\/code> step with <code>Clean Missing Data<\/code> step and change the action to remove the entire row (if appropriate). This is important because the <code>Execute Python Script<\/code> step cannot return an empty <code>dataframe<\/code>. Only you know your data, in this case.<\/p>\n\n<p>Let me also point out that the <code>Preprocessing Text<\/code> step allows you to apply a Regular Expression. That is another alternative that might be right for your situation.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1484610622880,
        "Answer_last_edit_time":1484679092912,
        "Answer_score_count":1.0,
        "Answerer_created_time":1265234764768,
        "Answerer_location":"Denver, CO",
        "Answerer_reputation_count":30577.0,
        "Answerer_view_count":6460.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41686871",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"strip html text column execut python script step column data type string incom dataset contain html tag screw result remov tag",
        "Question_preprocessed_content":"strip html text column execut python script step column data type string incom dataset contain html tag screw result remov tag",
        "Question_gpt_summary_original":"The user is facing a challenge in removing HTML tags from a string column in an incoming Azure ML dataset to avoid interference with the results.",
        "Question_gpt_summary":"user face challeng remov html tag string column incom dataset avoid interfer result"
    },
    {
        "Question_title":"Azure ML Deploy cannot connect to Kafka Server",
        "Question_body":"<p>In the run function of the entry script to deploy a model in azure ml, I included a producer function from kafka-python. However, using the deployed service, I can't seem to connect to the bootstrap_servers\/topics in Kafka.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1599861372500,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/92960\/azure-ml-deploy-cannot-connect-to-kafka-server",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"deploi connect kafka server run function entri script deploi model includ produc function kafka python deploi servic connect bootstrap server topic kafka",
        "Question_preprocessed_content":"deploi connect kafka server run function entri script deploi model includ produc function deploi servic connect kafka",
        "Question_gpt_summary_original":"The user is facing challenges in connecting Azure ML Deploy to Kafka Server while using the producer function from kafka-python in the entry script of the deployed model.",
        "Question_gpt_summary":"user face challeng connect deploi kafka server produc function kafka python entri script deploi model"
    },
    {
        "Question_title":"Advice for versioning many many small files?",
        "Question_body":"<p>Hello! I\u2019m new to DVC <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><br>\nI was working on versioning a large dataset (totaling ~140gb) which consists of many many very small files. They are mp3 audio files (imagine for training a speech model), each for example 100-200kb (they\u2019re only 5-15 seconds each on average). This means I\u2019ve got a lot of files of course<\/p>\n<p>I ran <code>dvc push<\/code> and let it run for ~16 hours before realizing this probably isn\u2019t a great idea, since it seemed like it was nowhere near completion, and cancelled the run. I found a couple links online telling me dvc isn\u2019t ideal for many small files, but I also found a comment from a dvc maintainer on this forum saying recent changes (that post was ~apr 2020) should improve performance for dvc push on many small files \u2013 Would anyone be able to advise me? Did I possibly do something wrong, or should I be rolling these files into a tar, zip, etc and versioning that? Wouldn\u2019t it make more sense to version individual data files? (I\u2019m new to working with data too <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/upside_down_face.png?v=9\" title=\":upside_down_face:\" class=\"emoji\" alt=\":upside_down_face:\">)<\/p>\n<p>Help would be really appreciated! Thanks<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_created_time":1609777134249,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":2505.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/advice-for-versioning-many-many-small-files\/609",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"advic version small file hello new work version larg dataset total consist small file audio file imagin train speech model exampl theyr second averag mean iv got lot file cours ran push let run hour realiz probabl isnt great idea like near complet cancel run coupl link onlin tell isnt ideal small file comment maintain forum sai recent chang post apr improv perform push small file abl advis possibl wrong roll file tar zip version wouldnt sens version individu data file new work data help appreci thank",
        "Question_preprocessed_content":"advic version small file hello new work version larg dataset consist small file audio file exampl mean iv got lot file cours ran let run hour realiz probabl isnt great idea like near complet cancel run coupl link onlin tell isnt ideal small file comment maintain forum sai recent chang improv perform push small file abl advis possibl wrong roll file tar zip version wouldnt sens version individu data file help appreci thank",
        "Question_gpt_summary_original":"The user is new to DVC and is trying to version a large dataset consisting of many small mp3 audio files. They ran \"dvc push\" for 16 hours but cancelled it as it seemed to be nowhere near completion. The user is seeking advice on whether to roll the files into a tar, zip, etc and version that or to version individual data files. They are also seeking advice on whether recent changes in DVC have improved performance for versioning many small files.",
        "Question_gpt_summary":"user new try version larg dataset consist small audio file ran push hour cancel near complet user seek advic roll file tar zip version version individu data file seek advic recent chang improv perform version small file"
    },
    {
        "Question_title":"Invoke the SageMaker model endpoint directly from PostMan without Lambda and API gateway setup",
        "Question_body":"Hi All,\n\nGood day!!\n\nHow to invoke the SageMaker model endpoint directly from Postman without Lambda and API gateway setup?\n\nWe have deployed the real-time endpoint on SM and data scientist (has AWS account) would like to invoke that endpoint via Postman from their local machine and test?\n\nMethod: Post\n\nURL: https:\/\/runtime.sagemaker.ap-southeast-1.amazonaws.com\/endpoints\/my-endpoint-name\/invocations\n\nWe know that we can invoke SM endpoint using SM boto3 SDK like below, but we would like to invoke the endpoint directly via Postman.\n\nimport boto3 smr = boto3.client('sagemaker-runtime')\n\nresp = smr.invoke_endpoint(EndpointName=endpoint_name, Body=b'.345,0.224414,.131102,0.042329,.279923,-0.110329,-0.099358,0.0', ContentType='text\/csv')\n\nThanks, Vinayak",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1645172179513,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668588217236,
        "Question_score_count":0.0,
        "Question_view_count":1746.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2IGM0IeOSb-6elPnkRpd-Q\/invoke-the-sagemaker-model-endpoint-directly-from-postman-without-lambda-and-api-gateway-setup",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"invok model endpoint directli postman lambda api gatewai setup good dai invok model endpoint directli postman lambda api gatewai setup deploi real time endpoint data scientist aw account like invok endpoint postman local machin test method post url http runtim southeast amazonaw com endpoint endpoint invoc know invok endpoint boto sdk like like invok endpoint directli postman import boto smr boto client runtim resp smr invok endpoint endpointnam endpoint bodi contenttyp text csv thank vinayak",
        "Question_preprocessed_content":"invok model endpoint directli postman lambda api gatewai setup good dai invok model endpoint directli postman lambda api gatewai setup deploi endpoint data scientist like invok endpoint postman local machin test method post url know invok endpoint boto sdk like like invok endpoint directli postman import boto smr resp thank vinayak",
        "Question_gpt_summary_original":"The user is seeking guidance on how to directly invoke the SageMaker model endpoint from Postman without setting up Lambda and API gateway. They have already deployed the real-time endpoint on SM and want to test it by invoking the endpoint via Postman from their local machine. The user is aware of the method to invoke the endpoint using SM boto3 SDK but wants to know how to do it directly via Postman.",
        "Question_gpt_summary":"user seek guidanc directli invok model endpoint postman set lambda api gatewai deploi real time endpoint want test invok endpoint postman local machin user awar method invok endpoint boto sdk want know directli postman"
    },
    {
        "Question_title":"How to disable MLFlow temporarily?",
        "Question_body":"<p>Is it possible to disable MLFlow temporarily for debugging code or adding new features? If it is not disabled it saves a bunch of executions not actually usefull or unfinished executions.<\/p>\n\n<p>Or is the best strategy to use similar code that doesn't call <code>mlflow.start_run()<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1586290455120,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":653.0,
        "Poster_created_time":1472251053727,
        "Poster_location":null,
        "Poster_reputation_count":548.0,
        "Poster_view_count":81.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61088651",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"disabl temporarili possibl disabl temporarili debug code ad new featur disabl save bunch execut actual useful unfinish execut best strategi us similar code start run",
        "Question_preprocessed_content":"disabl temporarili possibl disabl temporarili debug code ad new featur disabl save bunch execut actual useful unfinish execut best strategi us similar code",
        "Question_gpt_summary_original":"The user is seeking information on how to temporarily disable MLFlow for debugging or adding new features, as it saves unfinished or unnecessary executions. They are also asking if using similar code without calling mlflow.start_run() is a better strategy.",
        "Question_gpt_summary":"user seek inform temporarili disabl debug ad new featur save unfinish unnecessari execut ask similar code call start run better strategi"
    },
    {
        "Question_title":"Cannot connect PlainText (JSON) to Dataset at Azure Machine Learning",
        "Question_body":"<p>I uploaded a PlainText file in a JSON format to the new Azure Machine Learning Studio (studio.azureml.net), but I cannot connect the PlainText object with any module. I get all the time the error message \"Cannot connect PlainText to Dataset...\". <\/p>\n\n<p>At the documentation (<a href=\"http:\/\/help.azureml.net\/Content\/html\/e8219c57-e8dd-4989-9559-bbd73ba5bcea.htm\" rel=\"nofollow\">here<\/a>) is written that \"Plain text can be read and then split up into columns with the help of downstream preprocessing modules.\", but I can't find any downstream preprocessing modules.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1418490465200,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1446192454607,
        "Question_score_count":5.0,
        "Question_view_count":2157.0,
        "Poster_created_time":1408374893790,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":11.0,
        "Answer_body":"<p>Actually Azure ML can't process JSON data. It will probably be added in a future update, but the easiest way (in my opinion) to consume that data is to convert it into CSV format. This can be done quickly with Power Query. Then you upload the CSV file as a new dataset.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1418919495800,
        "Answer_last_edit_time":null,
        "Answer_score_count":6.0,
        "Answerer_created_time":1345114008840,
        "Answerer_location":"Lyon, France",
        "Answerer_reputation_count":4233.0,
        "Answerer_view_count":151.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/27461432",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"connect plaintext json dataset upload plaintext file json format new studio studio net connect plaintext object modul time error messag connect plaintext dataset document written plain text read split column help downstream preprocess modul downstream preprocess modul",
        "Question_preprocessed_content":"connect plaintext dataset upload plaintext file json format new studio connect plaintext object modul time error messag connect plaintext document written plain text read split column help downstream preprocess downstream preprocess modul",
        "Question_gpt_summary_original":"The user is unable to connect a PlainText file in JSON format to any module in Azure Machine Learning Studio and is receiving an error message stating \"Cannot connect PlainText to Dataset\". The user is also unable to find any downstream preprocessing modules as mentioned in the documentation.",
        "Question_gpt_summary":"user unabl connect plaintext file json format modul studio receiv error messag state connect plaintext dataset user unabl downstream preprocess modul mention document"
    },
    {
        "Question_title":"Unable to deploy model to endpoint",
        "Question_body":"Hi,\n\nI am trying to deploy a simple model on an endpoint in order to start making predictions.\n\nI followed these steps:\n\n1. Create the model, create a docker image, push it into Artifact Registry\n\n2. Upload the model to Vertex AI:\n\n\u00a0\n\n! gcloud ai models upload --container-image-uri=<REGION>-docker.pkg.dev\/<PROJECT>\/<REPOS>\/<MODEL>:latest --region=<REGION> --display-name=my-model\n\n\u00a0\n\n3. Create an endpoint on Vertex AI:\n\n\u00a0\n\n! gcloud ai endpoints create --display-name=my-model-endpoint --region=<REGION>\n\n\u00a0\n\n4. Deploy the model to the endpoint:\n\n\u00a0\n\n! gcloud ai endpoints deploy-model <ENDPOINT_ID> --model <MODEL_ID> -display-name my-model --traffic-split=0=100 --region <REGION> --machine-type=n1-standard-8 --enable-access-logging \n\n\u00a0\n\nHere's the message:\n\nModel server terminated: model server container terminated: exit_code: 0 reason: \"Completed\" started_at { seconds: 1684309447 } finished_at { seconds: 1684309449 } . Model server logs can be found at xxxxx\n\nWhen i check the logs:\u00a0\n\nAs you can see i have no errors.\n\nWhat i tried:\n\n- upsized the machine-type\u00a0\n\n- deployed with a python script\u00a0\n\n- deployed manually using the portal\u00a0\u00a0\n\n- changed my model to a simple linear regression to test",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1684285800000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":159.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unable-to-deploy-model-to-endpoint\/td-p\/554155\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unabl deploi model endpoint try deploi simpl model endpoint order start make predict follow step creat model creat docker imag push artifact registri upload model gcloud model upload contain imag uri docker pkg dev latest region displai model creat endpoint gcloud endpoint creat displai model endpoint region deploi model endpoint gcloud endpoint deploi model model displai model traffic split region machin type standard enabl access log messag model server termin model server contain termin exit code reason complet start second finish second model server log check log error tri upsiz machin type deploi python script deploi manual portal chang model simpl linear regress test",
        "Question_preprocessed_content":"unabl deploi model endpoint try deploi simpl model endpoint order start make predict follow step creat model creat docker imag push artifact registri upload model gcloud model upload creat endpoint gcloud endpoint creat deploi model endpoint gcloud endpoint messag model server termin model server contain termin reason complet model server log check log error tri upsiz deploi python script deploi manual portal chang model simpl linear regress test",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a simple model to an endpoint for making predictions. The user has followed the necessary steps, including creating a model, creating a docker image, pushing it into Artifact Registry, uploading the model to Vertex AI, creating an endpoint, and deploying the model to the endpoint. However, the model server terminated with no errors, and the user has tried various solutions, including upsizing the machine-type, deploying with a python script, deploying manually using the portal, and changing the model to a simple linear regression to test.",
        "Question_gpt_summary":"user face challeng deploi simpl model endpoint make predict user follow necessari step includ creat model creat docker imag push artifact registri upload model creat endpoint deploi model endpoint model server termin error user tri solut includ upsiz machin type deploi python script deploi manual portal chang model simpl linear regress test"
    },
    {
        "Question_title":"How to fix ModuleNotFoundError in azureml-sdk when installed inside conda environment",
        "Question_body":"<p>I'm setting up a conda environment on Windows 10 Pro x64 using Miniconda 4.5.12 and have done a pip install of azureml-sdk inside the environment but get a ModuleNotFoundError when attempting to execute the following code:<\/p>\n\n<pre><code>import azureml.core\nazureml.core.VERSION\n<\/code><\/pre>\n\n<p>This is the output:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"azureml.py\", line 1, in &lt;module&gt;\n    import azureml.core\n  File \"D:\\Projects\\style-transfer\\azureml.py\", line 1, in &lt;module&gt;\n    import azureml.core\nModuleNotFoundError: No module named 'azureml.core'; 'azureml' is not a package\n<\/code><\/pre>\n\n<p>The code above has been run from the conda prompt, with the test environment active as well as in vscode with the same environment selected.<\/p>\n\n<p>I setup the conda environment as per the following:<\/p>\n\n<ol>\n<li>Created the conda environment <code>conda create -n test<\/code>.<\/li>\n<li>Activated the environment <code>activate test<\/code>.<\/li>\n<li>Installed pip <code>conda install pip<\/code>.<\/li>\n<li>Installed azureml-sdk <code>pip install azureml-sdk<\/code>.<\/li>\n<\/ol>\n\n<p>This results in the following packages being installed in the environment as per <code>conda list<\/code>:<\/p>\n\n<pre><code>adal                      1.2.0                     &lt;pip&gt;\nantlr4-python3-runtime    4.7.2                     &lt;pip&gt;\napplicationinsights       0.11.7                    &lt;pip&gt;\nargcomplete               1.9.4                     &lt;pip&gt;\nasn1crypto                0.24.0                    &lt;pip&gt;\nazure-cli-command-modules-nspkg 2.0.2                     &lt;pip&gt;\nazure-cli-core            2.0.54                    &lt;pip&gt;\nazure-cli-nspkg           3.0.3                     &lt;pip&gt;\nazure-cli-profile         2.1.2                     &lt;pip&gt;\nazure-cli-telemetry       1.0.0                     &lt;pip&gt;\nazure-common              1.1.16                    &lt;pip&gt;\nazure-graphrbac           0.53.0                    &lt;pip&gt;\nazure-mgmt-authorization  0.51.1                    &lt;pip&gt;\nazure-mgmt-containerregistry 2.5.0                     &lt;pip&gt;\nazure-mgmt-keyvault       1.1.0                     &lt;pip&gt;\nazure-mgmt-nspkg          3.0.2                     &lt;pip&gt;\nazure-mgmt-resource       2.0.0                     &lt;pip&gt;\nazure-mgmt-storage        3.1.0                     &lt;pip&gt;\nazure-nspkg               3.0.2                     &lt;pip&gt;\nazure-storage-blob        1.4.0                     &lt;pip&gt;\nazure-storage-common      1.4.0                     &lt;pip&gt;\nazure-storage-nspkg       3.1.0                     &lt;pip&gt;\nazureml-core              1.0.6                     &lt;pip&gt;\nazureml-pipeline          1.0.6                     &lt;pip&gt;\nazureml-pipeline-core     1.0.6                     &lt;pip&gt;\nazureml-pipeline-steps    1.0.6                     &lt;pip&gt;\nazureml-sdk               1.0.6                     &lt;pip&gt;\nazureml-telemetry         1.0.6                     &lt;pip&gt;\nazureml-train             1.0.6                     &lt;pip&gt;\nazureml-train-core        1.0.6                     &lt;pip&gt;\nazureml-train-restclients-hyperdrive 1.0.6                     &lt;pip&gt;\nbackports.tempfile        1.0                       &lt;pip&gt;\nbackports.weakref         1.0.post1                 &lt;pip&gt;\nbcrypt                    3.1.5                     &lt;pip&gt;\nca-certificates           2018.03.07                    0\ncertifi                   2018.11.29               py37_0\ncffi                      1.11.5                    &lt;pip&gt;\nchardet                   3.0.4                     &lt;pip&gt;\ncolorama                  0.4.1                     &lt;pip&gt;\ncontextlib2               0.5.5                     &lt;pip&gt;\ncryptography              2.4.2                     &lt;pip&gt;\ndocker                    3.6.0                     &lt;pip&gt;\ndocker-pycreds            0.4.0                     &lt;pip&gt;\nfutures                   3.1.1                     &lt;pip&gt;\nhumanfriendly             4.17                      &lt;pip&gt;\nidna                      2.8                       &lt;pip&gt;\nisodate                   0.6.0                     &lt;pip&gt;\njmespath                  0.9.3                     &lt;pip&gt;\njsonpickle                1.0                       &lt;pip&gt;\nknack                     0.5.1                     &lt;pip&gt;\nmsrest                    0.6.2                     &lt;pip&gt;\nmsrestazure               0.6.0                     &lt;pip&gt;\nndg-httpsclient           0.5.1                     &lt;pip&gt;\noauthlib                  2.1.0                     &lt;pip&gt;\nopenssl                   1.1.1a               he774522_0\nparamiko                  2.4.2                     &lt;pip&gt;\npathspec                  0.5.9                     &lt;pip&gt;\npip                       18.1                     py37_0\nportalocker               1.2.1                     &lt;pip&gt;\npyasn1                    0.4.4                     &lt;pip&gt;\npycparser                 2.19                      &lt;pip&gt;\nPygments                  2.3.1                     &lt;pip&gt;\nPyJWT                     1.7.1                     &lt;pip&gt;\nPyNaCl                    1.3.0                     &lt;pip&gt;\npyOpenSSL                 18.0.0                    &lt;pip&gt;\npypiwin32                 223                       &lt;pip&gt;\npyreadline                2.1                       &lt;pip&gt;\npython                    3.7.1                h8c8aaf0_6\npython-dateutil           2.7.5                     &lt;pip&gt;\npytz                      2018.7                    &lt;pip&gt;\npywin32                   224                       &lt;pip&gt;\nPyYAML                    3.13                      &lt;pip&gt;\nrequests                  2.21.0                    &lt;pip&gt;\nrequests-oauthlib         1.0.0                     &lt;pip&gt;\nruamel.yaml               0.15.51                   &lt;pip&gt;\nSecretStorage             2.3.1                     &lt;pip&gt;\nsetuptools                40.6.3                   py37_0\nsix                       1.12.0                    &lt;pip&gt;\nsqlite                    3.26.0               he774522_0\ntabulate                  0.8.2                     &lt;pip&gt;\nurllib3                   1.23                      &lt;pip&gt;\nvc                        14.1                 h0510ff6_4\nvs2015_runtime            14.15.26706          h3a45250_0\nwebsocket-client          0.54.0                    &lt;pip&gt;\nwheel                     0.32.3                   py37_0\nwheel                     0.30.0                    &lt;pip&gt;\nwincertstore              0.2                      py37_0\n<\/code><\/pre>\n\n<p>If I run <code>which pip<\/code>, I get the following output, which confirms that I used the pip inside the environment to install azureml-sdk, I think:<\/p>\n\n<pre><code>\/c\/Users\/allan\/Miniconda3\/envs\/test\/Scripts\/pip\n<\/code><\/pre>\n\n<p>I can also see that the azureml packages do in fact exist within the environment folder structure.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1545617082117,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":6343.0,
        "Poster_created_time":1460456204196,
        "Poster_location":"Australia",
        "Poster_reputation_count":140.0,
        "Poster_view_count":10.0,
        "Answer_body":"<p>It's probably because the name if your python file is the same as a module name you are trying import. In this case, rename the file to something other than <code>azureml.py<\/code>.<\/p>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1545633498167,
        "Answer_last_edit_time":null,
        "Answer_score_count":3.0,
        "Answerer_created_time":1401187183867,
        "Answerer_location":"Sweden",
        "Answerer_reputation_count":1709.0,
        "Answerer_view_count":112.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53908529",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"fix modulenotfounderror sdk instal insid conda environ set conda environ window pro miniconda pip instal sdk insid environ modulenotfounderror attempt execut follow code import core core version output traceback recent file line import core file project style transfer line import core modulenotfounderror modul name core packag code run conda prompt test environ activ vscode environ select setup conda environ follow creat conda environ conda creat test activ environ activ test instal pip conda instal pip instal sdk pip instal sdk result follow packag instal environ conda list adal antlr python runtim applicationinsight argcomplet asncrypto azur cli command modul nspkg azur cli core azur cli nspkg azur cli profil azur cli telemetri azur common azur graphrbac azur mgmt author azur mgmt containerregistri azur mgmt keyvault azur mgmt nspkg azur mgmt resourc azur mgmt storag azur nspkg azur storag blob azur storag common azur storag nspkg core pipelin pipelin core pipelin step sdk telemetri train train core train restclient hyperdr backport tempfil backport weakref post bcrypt certif certifi cffi chardet colorama contextlib cryptographi docker docker pycr futur humanfriendli idna isod jmespath jsonpickl knack msrest msrestazur ndg httpsclient oauthlib openssl paramiko pathspec pip portalock pyasn pycpars pygment pyjwt pynacl pyopenssl pypiwin pyreadlin python hcaaf python dateutil pytz pywin pyyaml request request oauthlib ruamel yaml secretstorag setuptool sqlite tabul urllib hff runtim websocket client wheel wheel wincertstor run pip follow output confirm pip insid environ instal sdk think user allan miniconda env test script pip packag fact exist environ folder structur",
        "Question_preprocessed_content":"fix modulenotfounderror sdk instal insid conda environ set conda environ window pro miniconda pip instal sdk insid environ modulenotfounderror attempt execut follow code output code run conda prompt test environ activ vscode environ select setup conda environ follow creat conda environ activ environ instal pip instal sdk result follow packag instal environ run follow output confirm pip insid environ instal sdk think packag fact exist environ folder structur",
        "Question_gpt_summary_original":"The user has installed azureml-sdk inside a conda environment on Windows 10 Pro x64 using Miniconda 4.5.12. However, when attempting to execute the code, the user gets a ModuleNotFoundError. The user has installed pip, azureml-sdk, and other packages inside the environment, and the azureml packages do exist within the environment folder structure.",
        "Question_gpt_summary":"user instal sdk insid conda environ window pro miniconda attempt execut code user get modulenotfounderror user instal pip sdk packag insid environ packag exist environ folder structur"
    },
    {
        "Question_title":"How to install python ta-lib library in Azure Machine Learning Juypyter Notebooks",
        "Question_body":"<p>SO I need to use this python library explained here:<\/p>\n<p><a href=\"https:\/\/medium.com\/analytics-vidhya\/recognizing-over-50-candlestick-patterns-with-python-4f02a1822cb5\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/analytics-vidhya\/recognizing-over-50-candlestick-patterns-with-python-4f02a1822cb5<\/a><\/p>\n<p>However when I try to execute:<\/p>\n<p>!pip install talib\nimport talib<\/p>\n<p>I get lots of errors:<\/p>\n<pre><code>Collecting talib\n  Using cached talib-0.1.1.tar.gz (1.3 kB)\nBuilding wheels for collected packages: talib\n  Building wheel for talib (setup.py) ... error\n  ERROR: Command errored out with exit status 1:\n   command: \/anaconda\/envs\/azureml_py36\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-lof3_wuc\/talib\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-lof3_wuc\/talib\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' bdist_wheel -d \/tmp\/pip-wheel-1dvr6fd9\n       cwd: \/tmp\/pip-install-lof3_wuc\/talib\/\n  Complete output (29 lines):\n  running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build\/lib\n  creating build\/lib\/talib\n  copying talib\/__init__.py -&gt; build\/lib\/talib\n  copying talib\/cli.py -&gt; build\/lib\/talib\n  installing to build\/bdist.linux-x86_64\/wheel\n  running install\n  Traceback (most recent call last):\n    File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n    File &quot;\/tmp\/pip-install-lof3_wuc\/talib\/setup.py&quot;, line 47, in &lt;module&gt;\n      'talib = talib.cli:cli',\n    File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/distutils\/core.py&quot;, line 148, in setup\n      dist.run_commands()\n    File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/distutils\/dist.py&quot;, line 955, in run_commands\n      self.run_command(cmd)\n    File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/distutils\/dist.py&quot;, line 974, in run_command\n      cmd_obj.run()\n    File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/wheel\/bdist_wheel.py&quot;, line 326, in run\n      self.run_command('install')\n    File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/distutils\/cmd.py&quot;, line 313, in run_command\n      self.distribution.run_command(command)\n    File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/distutils\/dist.py&quot;, line 974, in run_command\n      cmd_obj.run()\n    File &quot;\/tmp\/pip-install-lof3_wuc\/talib\/setup.py&quot;, line 20, in run\n      raise Exception(&quot;You probably meant to install and run ta-lib&quot;)\n  Exception: You probably meant to install and run ta-lib\n  ----------------------------------------\n  ERROR: Failed building wheel for talib\n  Running setup.py clean for talib\nFailed to build talib\nInstalling collected packages: talib\n    Running setup.py install for talib ... error\n    ERROR: Command errored out with exit status 1:\n     command: \/anaconda\/envs\/azureml_py36\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-lof3_wuc\/talib\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-lof3_wuc\/talib\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-7fqjayvz\/install-record.txt --single-version-externally-managed --compile --install-headers \/anaconda\/envs\/azureml_py36\/include\/python3.6m\/talib\n         cwd: \/tmp\/pip-install-lof3_wuc\/talib\/\n    Complete output (14 lines):\n    running install\n    Traceback (most recent call last):\n      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n      File &quot;\/tmp\/pip-install-lof3_wuc\/talib\/setup.py&quot;, line 47, in &lt;module&gt;\n        'talib = talib.cli:cli',\n      File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/distutils\/core.py&quot;, line 148, in setup\n        dist.run_commands()\n      File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/distutils\/dist.py&quot;, line 955, in run_commands\n        self.run_command(cmd)\n      File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/distutils\/dist.py&quot;, line 974, in run_command\n        cmd_obj.run()\n      File &quot;\/tmp\/pip-install-lof3_wuc\/talib\/setup.py&quot;, line 20, in run\n        raise Exception(&quot;You probably meant to install and run ta-lib&quot;)\n    Exception: You probably meant to install and run ta-lib\n    ----------------------------------------\nERROR: Command errored out with exit status 1: \/anaconda\/envs\/azureml_py36\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-lof3_wuc\/talib\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-lof3_wuc\/talib\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-7fqjayvz\/install-record.txt --single-version-externally-managed --compile --install-headers \/anaconda\/envs\/azureml_py36\/include\/python3.6m\/talib Check the logs for full command output.\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-56-ef2dd36041a5&gt; in &lt;module&gt;\n      1 get_ipython().system('pip install talib')\n----&gt; 2 import talib\n\nModuleNotFoundError: No module named 'talib'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1604608935850,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":770.0,
        "Poster_created_time":1302030303092,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64704974",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"instal python lib librari juypyt notebook need us python librari explain http medium com analyt vidhya recogn candlestick pattern python facb try execut pip instal talib import talib lot error collect talib cach talib tar build wheel collect packag talib build wheel talib setup error error command error exit statu command anaconda env bin python import sy setuptool token sy argv tmp pip instal lof wuc talib setup file tmp pip instal lof wuc talib setup getattr token open open file code read replac close exec compil code file exec bdist wheel tmp pip wheel dvrfd cwd tmp pip instal lof wuc talib complet output line run bdist wheel run build run build creat build creat build lib creat build lib talib copi talib init build lib talib copi talib cli build lib talib instal build bdist linux wheel run instal traceback recent file line file tmp pip instal lof wuc talib setup line talib talib cli cli file anaconda env lib python distutil core line setup dist run command file anaconda env lib python distutil dist line run command self run command cmd file anaconda env lib python distutil dist line run command cmd obj run file anaconda env lib python site packag wheel bdist wheel line run self run command instal file anaconda env lib python distutil cmd line run command self distribut run command command file anaconda env lib python distutil dist line run command cmd obj run file tmp pip instal lof wuc talib setup line run rais except probabl meant instal run lib except probabl meant instal run lib error fail build wheel talib run setup clean talib fail build talib instal collect packag talib run setup instal talib error error command error exit statu command anaconda env bin python import sy setuptool token sy argv tmp pip instal lof wuc talib setup file tmp pip instal lof wuc talib setup getattr token open open file code read replac close exec compil code file exec instal record tmp pip record fqjayvz instal record txt singl version extern manag compil instal header anaconda env includ python talib cwd tmp pip instal lof wuc talib complet output line run instal traceback recent file line file tmp pip instal lof wuc talib setup line talib talib cli cli file anaconda env lib python distutil core line setup dist run command file anaconda env lib python distutil dist line run command self run command cmd file anaconda env lib python distutil dist line run command cmd obj run file tmp pip instal lof wuc talib setup line run rais except probabl meant instal run lib except probabl meant instal run lib error command error exit statu anaconda env bin python import sy setuptool token sy argv tmp pip instal lof wuc talib setup file tmp pip instal lof wuc talib setup getattr token open open file code read replac close exec compil code file exec instal record tmp pip record fqjayvz instal record txt singl version extern manag compil instal header anaconda env includ python talib check log command output modulenotfounderror traceback recent ipython pip instal talib import talib modulenotfounderror modul name talib",
        "Question_preprocessed_content":"instal python librari juypyt notebook need us python librari explain try execut pip instal talib import talib lot error",
        "Question_gpt_summary_original":"The user is trying to install the ta-lib library in Azure Machine Learning Jupyter Notebooks to use it for recognizing over 50 candlestick patterns with Python. However, when the user tries to execute the installation command, they encounter multiple errors and are unable to install the library.",
        "Question_gpt_summary":"user try instal lib librari jupyt notebook us recogn candlestick pattern python user tri execut instal command encount multipl error unabl instal librari"
    },
    {
        "Question_title":"How to create a panel that reports number of successful runs per group",
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m currently performing a hyperparameter search (I\u2019m not using wandb\u2019s sweeep feature for that) on a GPU cluster.  I group the runs into different categories, let\u2019s say that\u2019s simply \u201cGroup A\u201d, \u201cGroup B\u201d, etc.<\/p>\n<p>Now, since jobs can crash for various reasons, I would love to have a panel that reports the number of successful runs <em>for each group<\/em>, so I know how each group is doing (\u201cGroup A has 200 successful runs, while Group B only has 50, so I need to start some more jobs for Group B\u201d). I know I can get there by using the general filter and group feature in my project\u2019s run-table, but this is rather tedious and it would be more convenient for me to have it as a panel for quick access (e.g., inside a report).<\/p>\n<p>I\u2019ve fiddled around with the \u201cscalar chart\u201d and the \u201cweave\u201d panel, but without success \u2013 any ideas?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1641293761794,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":234.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-create-a-panel-that-reports-number-of-successful-runs-per-group\/1683",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"creat panel report number success run group current perform hyperparamet search swp featur gpu cluster group run differ categori let that simpli group group job crash reason love panel report number success run group know group group success run group need start job group know gener filter group featur project run tabl tediou conveni panel quick access insid report iv fiddl scalar chart weav panel success idea",
        "Question_preprocessed_content":"creat panel report number success run group current perform hyperparamet search gpu cluster group run differ categori let that simpli group group job crash reason love panel report number success run group know group know gener filter group featur project tediou conveni panel quick access iv fiddl scalar chart weav panel success idea",
        "Question_gpt_summary_original":"The user is performing a hyperparameter search on a GPU cluster and grouping the runs into different categories. They want to create a panel that reports the number of successful runs for each group to easily track the progress of each group. The user has tried using the scalar chart and weave panel but has not been successful.",
        "Question_gpt_summary":"user perform hyperparamet search gpu cluster group run differ categori want creat panel report number success run group easili track progress group user tri scalar chart weav panel success"
    },
    {
        "Question_title":"How to add more photos to AWS Sagemaker Labelling job?",
        "Question_body":"<p>We wish to add more photos to the AWS Sagemaker Labelling job. We tried adding the photo's directly to the s3 path and editing the input manifest file but they don't seem to update on the Labelling task. Is there any workaround for it?<\/p>\n<p>Reference used: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-input-data-input-manifest.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-input-data-input-manifest.html<\/a><\/p>\n<p>On a side note: Is there any reference for chaining the jobs, since the documentation on Amazon is confusing and there are no practical examples out there either.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1631557077603,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1632136492540,
        "Question_score_count":1.0,
        "Question_view_count":51.0,
        "Poster_created_time":1631533148667,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69167374",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"add photo label job wish add photo label job tri ad photo directli path edit input manifest file updat label task workaround refer http doc aw amazon com latest sm input data input manifest html note refer chain job document amazon confus practic exampl",
        "Question_preprocessed_content":"add photo label job wish add photo label job tri ad photo directli path edit input manifest file updat label task workaround refer note refer chain job document amazon confus practic exampl",
        "Question_gpt_summary_original":"The user is facing challenges in adding more photos to their AWS Sagemaker Labelling job. They have tried adding the photos directly to the s3 path and editing the input manifest file, but the Labelling task does not update. They are looking for a workaround. Additionally, they are seeking references for chaining jobs, as the documentation on Amazon is confusing and lacks practical examples.",
        "Question_gpt_summary":"user face challeng ad photo label job tri ad photo directli path edit input manifest file label task updat look workaround addition seek refer chain job document amazon confus lack practic exampl"
    },
    {
        "Question_title":"Empty dictionary on AnnotationConsolidation lambda event for aws Sagemaker",
        "Question_body":"<p>I am starting to use aws sagemaker on the development of my machine learning model and I'm trying to build a lambda function to process the responses of a sagemaker labeling job. I already created my own lambda function but when I try to read the event contents I can see that the event dict is completely empty, so I'm not getting any data to read.<\/p>\n\n<p>I have already given enough permissions to the role of the lambda function. Including:\n- AmazonS3FullAccess.\n- AmazonSagemakerFullAccess.\n- AWSLambdaBasicExecutionRole<\/p>\n\n<p>I've tried using this code for the Post-annotation Lambda (adapted for python 3.6):<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step2-demo1.html#sms-custom-templates-step2-demo1-post-annotation\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step2-demo1.html#sms-custom-templates-step2-demo1-post-annotation<\/a><\/p>\n\n<p>As well as this one in this git repository:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py<\/a><\/p>\n\n<p>But none of them seemed to work.<\/p>\n\n<p>For creating the labeling job I'm using boto3's functions for sagemaker:\n<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_labeling_job\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_labeling_job<\/a><\/p>\n\n<p>This is the code i have for creating the labeling job:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def create_labeling_job(client,bucket_name ,labeling_job_name, manifest_uri, output_path):\n\n    print(\"Creating labeling job with name: %s\"%(labeling_job_name))\n\n    response = client.create_labeling_job(\n        LabelingJobName=labeling_job_name,\n        LabelAttributeName='annotations',\n        InputConfig={\n            'DataSource': {\n                'S3DataSource': {\n                    'ManifestS3Uri': manifest_uri\n                }\n            },\n            'DataAttributes': {\n                'ContentClassifiers': [\n                    'FreeOfAdultContent',\n                ]\n            }\n        },\n        OutputConfig={\n            'S3OutputPath': output_path\n        },\n        RoleArn='arn:aws:myrolearn',\n        LabelCategoryConfigS3Uri='s3:\/\/'+bucket_name+'\/config.json',\n        StoppingConditions={\n            'MaxPercentageOfInputDatasetLabeled': 100,\n        },\n        LabelingJobAlgorithmsConfig={\n            'LabelingJobAlgorithmSpecificationArn': 'arn:image-classification'\n        },\n        HumanTaskConfig={\n            'WorkteamArn': 'arn:my-private-workforce-arn',\n            'UiConfig': {\n                'UiTemplateS3Uri':'s3:\/\/'+bucket_name+'\/templatefile'\n            },\n            'PreHumanTaskLambdaArn': 'arn:aws:lambda:us-east-1:432418664414:function:PRE-BoundingBox',\n            'TaskTitle': 'Title',\n            'TaskDescription': 'Description',\n            'NumberOfHumanWorkersPerDataObject': 1,\n            'TaskTimeLimitInSeconds': 600,\n            'AnnotationConsolidationConfig': {\n                'AnnotationConsolidationLambdaArn': 'arn:aws:my-custom-post-annotation-lambda'\n            }\n        }\n    )\n\n    return response\n<\/code><\/pre>\n\n<p>And this is the one i have for the lambda function:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>    print(\"Received event: \" + json.dumps(event, indent=2))\n    print(\"event: %s\"%(event))\n    print(\"context: %s\"%(context))\n    print(\"event headers: %s\"%(event[\"headers\"]))\n\n    parsed_url = urlparse(event['payload']['s3Uri']);\n    print(\"parsed_url: \",parsed_url)\n\n    labeling_job_arn = event[\"labelingJobArn\"]\n    label_attribute_name = event[\"labelAttributeName\"]\n\n    label_categories = None\n    if \"label_categories\" in event:\n        label_categories = event[\"labelCategories\"]\n        print(\" Label Categories are : \" + label_categories)\n\n    payload = event[\"payload\"]\n    role_arn = event[\"roleArn\"]\n\n    output_config = None # Output s3 location. You can choose to write your annotation to this location\n    if \"outputConfig\" in event:\n        output_config = event[\"outputConfig\"]\n\n    # If you specified a KMS key in your labeling job, you can use the key to write\n    # consolidated_output to s3 location specified in outputConfig.\n    kms_key_id = None\n    if \"kmsKeyId\" in event:\n        kms_key_id = event[\"kmsKeyId\"]\n\n    # Create s3 client object\n    s3_client = S3Client(role_arn, kms_key_id)\n\n    # Perform consolidation\n    return do_consolidation(labeling_job_arn, payload, label_attribute_name, s3_client)\n<\/code><\/pre>\n\n<p>I've tried debugging the event object with:<\/p>\n\n<pre><code>    print(\"Received event: \" + json.dumps(event, indent=2))\n<\/code><\/pre>\n\n<p>But it just prints an empty dictionary: <code>Received event: {}<\/code><\/p>\n\n<p>I expect the output to be something like:<\/p>\n\n<pre><code>    #Content of an example event:\n    {\n        \"version\": \"2018-10-16\",\n        \"labelingJobArn\": &lt;labelingJobArn&gt;,\n        \"labelCategories\": [&lt;string&gt;],  # If you created labeling job using aws console, labelCategories will be null\n        \"labelAttributeName\": &lt;string&gt;,\n        \"roleArn\" : \"string\",\n        \"payload\": {\n            \"s3Uri\": &lt;string&gt;\n        }\n        \"outputConfig\":\"s3:\/\/&lt;consolidated_output configured for labeling job&gt;\"\n    }\n<\/code><\/pre>\n\n<p>Lastly, when I try yo get the labeling job ARN with:<\/p>\n\n<pre><code>    labeling_job_arn = event[\"labelingJobArn\"]\n<\/code><\/pre>\n\n<p>I just get a KeyError (which makes sense because the dictionary is empty).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1564494701657,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1575462189052,
        "Question_score_count":3.0,
        "Question_view_count":846.0,
        "Poster_created_time":1525449880547,
        "Poster_location":"Madrid, Spain",
        "Poster_reputation_count":81.0,
        "Poster_view_count":5.0,
        "Answer_body":"<p>I found the problem, I needed to add the ARN of the role used by my Lamda function as a Trusted Entity on the Role used for the Sagemaker Labeling Job.<\/p>\n\n<p>I just went to <code>Roles &gt; MySagemakerExecutionRole &gt; Trust Relationships<\/code> and added:<\/p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": [\n          \"arn:aws:iam::xxxxxxxxx:role\/My-Lambda-Role\",\n           ...\n        ],\n        \"Service\": [\n          \"lambda.amazonaws.com\",\n          \"sagemaker.amazonaws.com\",\n           ...\n        ]\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n<\/code><\/pre>\n\n<p>This made it work for me.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1564741689012,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1525449880547,
        "Answerer_location":"Madrid, Spain",
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57273357",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"dictionari annotationconsolid lambda event start us develop machin learn model try build lambda function process respons label job creat lambda function try read event content event dict complet get data read given permiss role lambda function includ amazonsfullaccess amazonfullaccess awslambdabasicexecutionrol tri code post annot lambda adapt python http doc aw amazon com latest sm custom templat step demo html sm custom templat step demo post annot git repositori http github com aw sampl aw ground truth recip blob master aw ground truth sampl lambda annot consolid lambda work creat label job boto function http boto amazonaw com document api latest refer servic html client creat label job code creat label job def creat label job client bucket label job manifest uri output path print creat label job label job respons client creat label job labelingjobnam label job labelattributenam annot inputconfig datasourc sdatasourc manifestsuri manifest uri dataattribut contentclassifi freeofadultcont outputconfig soutputpath output path rolearn arn aw myrolearn labelcategoryconfigsuri bucket config json stoppingcondit maxpercentageofinputdatasetlabel labelingjobalgorithmsconfig labelingjobalgorithmspecificationarn arn imag classif humantaskconfig workteamarn arn privat workforc arn uiconfig uitemplatesuri bucket templatefil prehumantasklambdaarn arn aw lambda east function pre boundingbox tasktitl titl taskdescript descript numberofhumanworkersperdataobject tasktimelimitinsecond annotationconsolidationconfig annotationconsolidationlambdaarn arn aw custom post annot lambda return respons lambda function print receiv event json dump event indent print event event print context context print event header event header pars url urlpars event payload suri print pars url pars url label job arn event labelingjobarn label attribut event labelattributenam label categori label categori event label categori event labelcategori print label categori label categori payload event payload role arn event rolearn output config output locat choos write annot locat outputconfig event output config event outputconfig specifi km kei label job us kei write consolid output locat specifi outputconfig km kei kmskeyid event km kei event kmskeyid creat client object client sclient role arn km kei perform consolid return consolid label job arn payload label attribut client tri debug event object print receiv event json dump event indent print dictionari receiv event expect output like content exampl event version labelingjobarn labelcategori creat label job aw consol labelcategori null labelattributenam rolearn string payload suri outputconfig lastli try label job arn label job arn event labelingjobarn keyerror make sens dictionari",
        "Question_preprocessed_content":"dictionari annotationconsolid lambda event start us develop machin learn model try build lambda function process respons label job creat lambda function try read event content event dict complet get data read given permiss role lambda function includ amazon fullaccess amazonfullaccess awslambdabasicexecutionrol tri code lambda git repositori work creat label job boto function code creat label job lambda function tri debug event object print dictionari expect output like lastli try label job arn keyerror",
        "Question_gpt_summary_original":"The user is encountering an issue with an empty dictionary on AnnotationConsolidation lambda event for AWS Sagemaker. They have created their own lambda function and given enough permissions to the role of the lambda function, but when they try to read the event contents, they can see that the event dict is completely empty, so they are not getting any data to read. They have tried using different codes for the Post-annotation Lambda, but none of them seemed to work. The user is also unable to get the labeling job ARN as they are getting a KeyError because the dictionary is empty.",
        "Question_gpt_summary":"user encount issu dictionari annotationconsolid lambda event creat lambda function given permiss role lambda function try read event content event dict complet get data read tri differ code post annot lambda work user unabl label job arn get keyerror dictionari"
    },
    {
        "Question_title":"Inconsistent keras model.summary() output shapes on AWS SageMaker and EC2",
        "Question_body":"<p>I have the following model in a jupyter notebook:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\n\n\n\nphysical_devices = tf.config.list_physical_devices('GPU')\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\n\nSIZE = (549, 549)\nSHUFFLE = False \nBATCH = 32\nEPOCHS = 20\n\ntrain_datagen =  DataGenerator(train_files, batch_size=BATCH, dim=SIZE, n_channels=1, shuffle=SHUFFLE)\ntest_datagen =  DataGenerator(test_files, batch_size=BATCH, dim=SIZE, n_channels=1, shuffle=SHUFFLE)\n\n\ninp = layers.Input(shape=(*SIZE, 1))\n\nx = layers.Conv2D(filters=549, kernel_size=(5,5), padding=&quot;same&quot;, activation=&quot;relu&quot;)(inp)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=549, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=549, kernel_size=(1, 1), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)\nx = layers.BatchNormalization()(x)\n\nx = layers.Conv2D(filters=549, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;sigmoid&quot;)(x)\n\nmodel = Model(inp, x)\n\nmodel.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam())\n\nmodel.summary()\n<\/code><\/pre>\n<p>Sagemaker and EC2 are running tensorflow 2.7.1. The EC2 instance is p3.2xlarge with Deep Learning AMI GPU TensorFlow 2.7.0 (Amazon Linux 2) 20220607. The SageMaker notebook is using ml.p3.2xlarge and I am using the conda_tensorflow2_p38 kernel. The notebook is in an FSx Lustre file system that is mounted to both SageMaker and EC2 so it is definitely the same code running on both machines.<\/p>\n<p>nvidia-smi output on SageMaker:<\/p>\n<pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n| N\/A   37C    P0    24W \/ 300W |      0MiB \/ 16384MiB |      0%      Default |\n|                               |                      |                  N\/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n<\/code><\/pre>\n<p>nvidia-smi output on EC2:<\/p>\n<pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n| N\/A   42C    P0    51W \/ 300W |   2460MiB \/ 16384MiB |      0%      Default |\n|                               |                      |                  N\/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N\/A  N\/A     11802      C   \/bin\/python3.8                    537MiB |\n|    0   N\/A  N\/A     26391      C   python3.8                        1921MiB |\n+-----------------------------------------------------------------------------+\n<\/code><\/pre>\n<p>The model.summary() output on SageMaker is (this is what I want it to be):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>Model: &quot;model&quot;\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d (Conv2D)             (None, 549, 549, 1)       7535574   \n                                                                 \n batch_normalization (BatchN  (None, 549, 549, 1)      4         \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (None, 549, 549, 1)       2713158   \n                                                                 \n batch_normalization_1 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_2 (Conv2D)           (None, 549, 549, 1)       301950    \n                                                                 \n batch_normalization_2 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_3 (Conv2D)           (None, 549, 549, 1)       2713158   \n                                                                 \n=================================================================\nTotal params: 13,263,852\nTrainable params: 13,263,846\nNon-trainable params: 6\n\n<\/code><\/pre>\n<p>The model.summary() output on EC2 is (notice the shape change):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nModel: &quot;model&quot;\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d (Conv2D)             (None, 549, 549, 549)     14274     \n                                                                 \n batch_normalization (BatchN  (None, 549, 549, 549)    2196      \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (None, 549, 549, 549)     2713158   \n                                                                 \n batch_normalization_1 (Batc  (None, 549, 549, 549)    2196      \n hNormalization)                                                 \n                                                                 \n conv2d_2 (Conv2D)           (None, 549, 549, 549)     301950    \n                                                                 \n batch_normalization_2 (Batc  (None, 549, 549, 549)    2196      \n hNormalization)                                                 \n                                                                 \n conv2d_3 (Conv2D)           (None, 549, 549, 549)     2713158   \n                                                                 \n=================================================================\nTotal params: 5,749,128\nTrainable params: 5,745,834\nNon-trainable params: 3,294\n_________________________________________________________________\n<\/code><\/pre>\n<p>One other thing that is interesting, if I change my model on the EC2 instance to:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>inp = layers.Input(shape=(*SIZE, 1))\n\nx = layers.Conv2D(filters=1, kernel_size=(5,5), padding=&quot;same&quot;, activation=&quot;relu&quot;)(inp)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=1, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=1, kernel_size=(1, 1), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)\nx = layers.BatchNormalization()(x)\n\nx = layers.Conv2D(filters=1, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;sigmoid&quot;)(x)\n\nmodel = Model(inp, x)\n\nmodel.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam())\n<\/code><\/pre>\n<p>My model.summary() output becomes:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>Model: &quot;model_2&quot;\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d_8 (Conv2D)           (None, 549, 549, 1)       26        \n                                                                 \n batch_normalization_6 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_9 (Conv2D)           (None, 549, 549, 1)       10        \n                                                                 \n batch_normalization_7 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_10 (Conv2D)          (None, 549, 549, 1)       2         \n                                                                 \n batch_normalization_8 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_11 (Conv2D)          (None, 549, 549, 1)       10        \n                                                                 \n=================================================================\nTotal params: 60\nTrainable params: 54\nNon-trainable params: 6\n_________________________________________________________________\n<\/code><\/pre>\n<p>In the last model the shape is correct but the trainable parameters is very low.<\/p>\n<p>Any ideas as to why the output shape is different and why this is happening with the filters?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1655433917067,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1655439555147,
        "Question_score_count":0.0,
        "Question_view_count":39.0,
        "Poster_created_time":1601598577080,
        "Poster_location":null,
        "Poster_reputation_count":131.0,
        "Poster_view_count":19.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72653823",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"inconsist kera model summari output shape follow model jupyt notebook import tensorflow tensorflow kera model import model tensorflow kera optim import adam tensorflow kera import layer physic devic config list physic devic gpu config experiment set memori growth physic devic true size shuffl fals batch epoch train datagen datagener train file batch size batch dim size channel shuffl shuffl test datagen datagener test file batch size batch dim size channel shuffl shuffl inp layer input shape size layer convd filter kernel size pad activ relu inp layer batchnorm layer convd filter kernel size pad activ relu layer batchnorm layer convd filter kernel size pad activ relu layer batchnorm layer convd filter kernel size pad activ sigmoid model model inp model compil loss kera loss binari crossentropi optim adam model summari run tensorflow instanc xlarg deep learn ami gpu tensorflow amazon linux notebook xlarg conda tensorflow kernel notebook fsx lustr file mount definit code run machin nvidia smi output nvidia smi driver version cuda version gpu persist bu disp volatil uncorr ecc fan temp perf pwr usag cap memori usag gpu util comput mig tesla sxm mib mib default process gpu pid type process gpu memori usag run process nvidia smi output nvidia smi driver version cuda version gpu persist bu disp volatil uncorr ecc fan temp perf pwr usag cap memori usag gpu util comput mig tesla sxm mib mib default process gpu pid type process gpu memori usag bin python mib python mib model summari output want model model layer type output shape param input inputlay convd convd batch normal batchn ormal convd convd batch normal batc hnormal convd convd batch normal batc hnormal convd convd total param trainabl param non trainabl param model summari output notic shape chang model model layer type output shape param input inputlay convd convd batch normal batchn ormal convd convd batch normal batc hnormal convd convd batch normal batc hnormal convd convd total param trainabl param non trainabl param thing interest chang model instanc inp layer input shape size layer convd filter kernel size pad activ relu inp layer batchnorm layer convd filter kernel size pad activ relu layer batchnorm layer convd filter kernel size pad activ relu layer batchnorm layer convd filter kernel size pad activ sigmoid model model inp model compil loss kera loss binari crossentropi optim adam model summari output model model layer type output shape param input inputlay convd convd batch normal batc hnormal convd convd batch normal batc hnormal convd convd batch normal batc hnormal convd convd total param trainabl param non trainabl param model shape correct trainabl paramet low idea output shape differ happen filter",
        "Question_preprocessed_content":"inconsist kera output shape follow model jupyt notebook run tensorflow instanc deep learn ami gpu tensorflow notebook kernel notebook fsx lustr file mount definit code run machin output output output output thing interest chang model instanc output model shape correct trainabl paramet low idea output shape differ happen filter",
        "Question_gpt_summary_original":"The user has encountered an issue with inconsistent output shapes and trainable parameters when using the Keras model.summary() function on AWS SageMaker and EC2 instances. The user has provided code examples and nvidia-smi output from both instances, and has noted that the same code is running on both machines. The user has also observed that changing the model architecture can affect the output shape and trainable parameters. The user is seeking advice on why this is happening and how to resolve the issue.",
        "Question_gpt_summary":"user encount issu inconsist output shape trainabl paramet kera model summari function instanc user provid code exampl nvidia smi output instanc note code run machin user observ chang model architectur affect output shape trainabl paramet user seek advic happen resolv issu"
    },
    {
        "Question_title":"How to configure our own inference.py for two different PyTorch models in MultiDataModel to build single endpoint and call both models from there?",
        "Question_body":"I have referred [this](https:\/\/github.com\/aws-samples\/sagemaker-multi-model-endpoint-tensorflow-computer-vision\/blob\/main\/multi-model-endpoint-tensorflow-cv.ipynb) notebook to deploy PyTorch model but as per this notebook they are just calling newly trained model to get predictions by passing input payload... but I want to configure my own inference.py file to pass an input there and get predictions from there...\nSo, can anyone help me achieving that?\n\nThanks",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1670259036956,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1670606543392,
        "Question_score_count":0.0,
        "Question_view_count":212.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEVxelof3TmimoLt1Kd1SBA\/how-to-configure-our-own-inference-py-for-two-different-pytorch-models-in-multidatamodel-to-build-single-endpoint-and-call-both-models-from-there",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"configur infer differ pytorch model multidatamodel build singl endpoint model refer http github com aw sampl multi model endpoint tensorflow vision blob main multi model endpoint tensorflow ipynb notebook deploi pytorch model notebook call newli train model predict pass input payload want configur infer file pass input predict help achiev thank",
        "Question_preprocessed_content":"configur differ pytorch model multidatamodel build singl endpoint model refer notebook deploi pytorch model notebook call newli train model predict pass input want configur file pass input predict help achiev thank",
        "Question_gpt_summary_original":"The user is seeking help in configuring their own inference.py file to pass input and get predictions from two different PyTorch models in MultiDataModel to build a single endpoint. They have referred to a notebook but it only calls newly trained models to get predictions.",
        "Question_gpt_summary":"user seek help configur infer file pass input predict differ pytorch model multidatamodel build singl endpoint refer notebook call newli train model predict"
    },
    {
        "Question_title":"Error code 0058 on permutation feature importance model",
        "Question_body":"<p>Hello, <\/p>\n<p>First off I have read the other threads on this error and seem to still have issues. I am currently attending a ML class using the Micrsoft Machine learning tool. This is my second week of this class and I have follow the instructions to build this experiment and the permutation feature importance will not run and errors out with 0058 with exit error 2. <\/p>\n<p><a href=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/39af9e02-401d-4688-8689-39b1e6d53863?platform=QnA\">PFeatureImportanceError.png<\/a><\/p>\n<p>I am not sure what I am doing wrong here provided I am just following instructions with no mention of getting this error. I have also tried cleaning any missing data, still produces the result. <\/p>\n<p>The readmi class column is what I am using in the train model. <a href=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/4f1e92d2-c51e-4cf9-8155-739a6e51b7b0?platform=QnA\">TrainModel.png<\/a><\/p>\n<p>Thank you. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1684368946973,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1286906\/error-code-0058-on-permutation-feature-importance",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error code permut featur import model hello read thread error issu current attend class micrsoft machin learn tool second week class follow instruct build experi permut featur import run error exit error pfeatureimportanceerror png sure wrong provid follow instruct mention get error tri clean miss data produc result readmi class column train model trainmodel png thank",
        "Question_preprocessed_content":"error code permut featur import model hello read thread error issu current attend class micrsoft machin learn tool second week class follow instruct build experi permut featur import run error exit error sure wrong provid follow instruct mention get error tri clean miss data produc result readmi class column train model thank",
        "Question_gpt_summary_original":"The user is encountering error code 0058 while trying to run the permutation feature importance model in Microsoft Machine Learning tool during an ML class. The user has followed the instructions provided but is still unable to resolve the issue. The user has also tried cleaning missing data but the error persists.",
        "Question_gpt_summary":"user encount error code try run permut featur import model microsoft machin learn tool class user follow instruct provid unabl resolv issu user tri clean miss data error persist"
    },
    {
        "Question_title":"Comprehend training input set",
        "Question_body":"We are labelling the pdf documents. We have limited resource. To complete all the labelling task, it might take more time. I am thinking of splitting the number of documents into smaller chunks and create mulitple jobs to complete the entire labelling work. \n\nThe output of this task will be feed into comprehend for training. I observed the comprehend will take only 5 jobs as input. Is there a way to increate this input number  ? Or Is there a way to retrain comprehend with next set of 5 jobs. ?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1671877500558,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1672223923776,
        "Question_score_count":0.0,
        "Question_view_count":54.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU9z3uA_p1SeuxLtQ3RB24jw\/comprehend-training-input-set",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"comprehend train input set label pdf document limit resourc complet label task time think split number document smaller chunk creat mulitpl job complet entir label work output task feed comprehend train observ comprehend job input wai increat input number wai retrain comprehend set job",
        "Question_preprocessed_content":"comprehend train input set label pdf document limit resourc complet label task time think split number document smaller chunk creat mulitpl job complet entir label work output task feed comprehend train observ comprehend job input wai increat input number wai retrain comprehend set job",
        "Question_gpt_summary_original":"The user is labeling PDF documents with limited resources and is considering splitting the documents into smaller chunks to create multiple jobs to complete the labeling work. The output will be fed into Comprehend for training, but the user has observed that Comprehend only takes 5 jobs as input. The user is seeking a way to increase the input number or retrain Comprehend with the next set of 5 jobs.",
        "Question_gpt_summary":"user label pdf document limit resourc consid split document smaller chunk creat multipl job complet label work output fed comprehend train user observ comprehend take job input user seek wai increas input number retrain comprehend set job"
    },
    {
        "Question_title":"Which connection method when using SageMaker Notebook through VPC Interface Endpoint?",
        "Question_body":"Hi, I see in this page of documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-interface-endpoint.html that:\n>\"*You can connect to your notebook instance from your VPC through an interface endpoint in your Virtual Private Cloud (VPC) instead of connecting over the internet. When you use a VPC interface endpoint, communication between your VPC and the notebook instance is conducted entirely and securely within the AWS network.*\"\n\nHow would customer interact on their laptop with the UI of a notebook instance sitting in a VPC?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1541494962000,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668438538512,
        "Question_score_count":0.0,
        "Question_view_count":459.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"If you are trying to access from within VPC, you'll have a direct connection. Otherwise, you'll need a configuration in place, such as Amazon VPN or AWS Direct Connect, to connect to your notebooks. Here is the blog post where we tried to explain how to set up AWS PrivateLink for Amazon SageMaker notebooks: https:\/\/aws.amazon.com\/blogs\/machine-learning\/direct-access-to-amazon-sagemaker-notebooks-from-amazon-vpc-by-using-an-aws-privatelink-endpoint\/",
        "Answer_comment_count":0.0,
        "Question_closed_time":1541516577000,
        "Answer_last_edit_time":1667925550451,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5z-7bQ9zQOi_NrVlHy_5oA\/which-connection-method-when-using-sagemaker-notebook-through-vpc-interface-endpoint",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":1.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":1.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_original_content":"connect method notebook vpc interfac endpoint page document http doc aw amazon com latest notebook interfac endpoint html connect notebook instanc vpc interfac endpoint virtual privat cloud vpc instead connect internet us vpc interfac endpoint commun vpc notebook instanc conduct entir secur aw network custom interact laptop notebook instanc sit vpc",
        "Question_preprocessed_content":"connect method notebook vpc interfac endpoint page document connect notebook instanc vpc interfac endpoint virtual privat cloud instead connect internet us vpc interfac endpoint commun vpc notebook instanc conduct entir secur aw custom interact laptop notebook instanc sit vpc",
        "Question_gpt_summary_original":"The user is seeking guidance on how to connect to a SageMaker Notebook instance through a VPC interface endpoint instead of over the internet. They are specifically asking how they can interact with the UI of the notebook instance on their laptop while it is located in a VPC.",
        "Question_gpt_summary":"user seek guidanc connect notebook instanc vpc interfac endpoint instead internet specif ask interact notebook instanc laptop locat vpc"
    },
    {
        "Question_title":"Loading non-s3 data source for Sagemaker Java SDK",
        "Question_body":"<p>I'm setting up a lambda function that performs SageMaker inferences using the Random Cut Forest algorithm. I successfully ran the python RCF example in a jupyter notebook, but I prefer to have my lambda written in Java, and I was hoping to not use an s3 bucket in the process. I know that if my lambda was in python, then I would be able to pass in my data, convert it to a pandas dataframe, then run <code>rcf_inference.predict(data)<\/code> to obtain my results. But with Java, this is seemingly impossible.<\/p>\n\n<p>I was able to make a batch transform job using the following Java code, but after examining the java aws-sdk documentation, it looks like my TransformDataSource <em>must<\/em> have an S3DataSource, and not any other kind of source. This is unfortunate because my lambda already has the data I need in memory, so uploading\/downloading to\/from an s3 bucket is unnecessary.<\/p>\n\n<pre><code>        String bucket = \"test-bucket441\";\n        String prefix = \"sagemaker\/rcf-benchmarks\";\n        String trainedModel = \"randomcutforest-2019-06-28-13-43-00-925\";\n\n        AmazonSageMaker sm = AmazonSageMakerClientBuilder.standard().build();\n\n        TransformS3DataSource s3Source = new TransformS3DataSource()\n                .withS3DataType(\"S3Prefix\")\n                .withS3Uri(\"s3:\/\/\" + bucket + \"\/\" + prefix);\n\n        TransformDataSource dataSource = new TransformDataSource()\n                .withS3DataSource(s3Source);\n\n        TransformInput input = new TransformInput()\n                .withContentType(\"text\/csv\")\n                .withDataSource(dataSource);\n\n        TransformOutput output = new TransformOutput()\n                .withS3OutputPath(\"s3:\/\/\" + bucket + \"\/\" + prefix + \"\/output\")\n                .withAssembleWith(\"Line\");\n\n        TransformResources resources = new TransformResources()\n                .withInstanceType(\"ml.m4.xlarge\")\n                .withInstanceCount(1);\n\n        CreateTransformJobRequest jobRequest = new CreateTransformJobRequest()\n                .withTransformJobName(\"test-job\")\n                .withModelName(trainedModel)\n                .withTransformInput(input)\n                .withTransformOutput(output)\n                .withTransformResources(resources);\n\n\n        sm.createTransformJob(jobRequest);\n<\/code><\/pre>\n\n<p>Does anyone know any way I can create a CreateTranformJobRequest without using an s3 bucket?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1562076887677,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":139.0,
        "Poster_created_time":1526489986910,
        "Poster_location":"Philadelphia, PA, USA",
        "Poster_reputation_count":926.0,
        "Poster_view_count":48.0,
        "Answer_body":"<p>your Python code uses a SageMaker HTTPS endpoint for real-time prediction: although the SageMaker SDK is Python only, you can absolutely do the same thing with the (lower level) AWS SDK for Java.<\/p>\n\n<p>Assuming you've already trained your model in SageMaker, you would:<\/p>\n\n<ul>\n<li>create an endpoint configuration, <\/li>\n<li>create an endpoint,<\/li>\n<li>invoke the endpoint.<\/li>\n<\/ul>\n\n<p>The corresponding APIs are detailed in:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/AWSJavaSDK\/latest\/javadoc\/com\/amazonaws\/services\/sagemaker\/AmazonSageMaker.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSJavaSDK\/latest\/javadoc\/com\/amazonaws\/services\/sagemaker\/AmazonSageMaker.html<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/AWSJavaSDK\/latest\/javadoc\/com\/amazonaws\/services\/sagemakerruntime\/AmazonSageMakerRuntime.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSJavaSDK\/latest\/javadoc\/com\/amazonaws\/services\/sagemakerruntime\/AmazonSageMakerRuntime.html<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1562592896756,
        "Answer_last_edit_time":1562647206950,
        "Answer_score_count":1.0,
        "Answerer_created_time":1426694564423,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56854482",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"load non data sourc java sdk set lambda function perform infer random cut forest algorithm successfulli ran python rcf exampl jupyt notebook prefer lambda written java hope us bucket process know lambda python abl pass data convert panda datafram run rcf infer predict data obtain result java seemingli imposs abl batch transform job follow java code examin java aw sdk document look like transformdatasourc sdatasourc kind sourc unfortun lambda data need memori upload download bucket unnecessari string bucket test bucket string prefix rcf benchmark string trainedmodel randomcutforest amazon amazonclientbuild standard build transformsdatasourc ssourc new transformsdatasourc withsdatatyp sprefix withsuri bucket prefix transformdatasourc datasourc new transformdatasourc withsdatasourc ssourc transforminput input new transforminput withcontenttyp text csv withdatasourc datasourc transformoutput output new transformoutput withsoutputpath bucket prefix output withassemblewith line transformresourc resourc new transformresourc withinstancetyp xlarg withinstancecount createtransformjobrequest jobrequest new createtransformjobrequest withtransformjobnam test job withmodelnam trainedmodel withtransforminput input withtransformoutput output withtransformresourc resourc createtransformjob jobrequest know wai creat createtranformjobrequest bucket",
        "Question_preprocessed_content":"load data sourc java sdk set lambda function perform infer random cut forest algorithm successfulli ran python rcf exampl jupyt notebook prefer lambda written java hope us bucket process know lambda python abl pass data convert panda datafram run obtain result java seemingli imposs abl batch transform job follow java code examin java document look like transformdatasourc datasourc kind sourc unfortun lambda data need memori bucket unnecessari know wai creat createtranformjobrequest bucket",
        "Question_gpt_summary_original":"The user is trying to set up a lambda function that performs SageMaker inferences using the Random Cut Forest algorithm in Java. They were able to make a batch transform job using Java code, but it looks like their TransformDataSource must have an S3DataSource, and not any other kind of source. The user is looking for a way to create a CreateTranformJobRequest without using an s3 bucket.",
        "Question_gpt_summary":"user try set lambda function perform infer random cut forest algorithm java abl batch transform job java code look like transformdatasourc sdatasourc kind sourc user look wai creat createtranformjobrequest bucket"
    },
    {
        "Question_title":"Getting error when I click on models in mlflow",
        "Question_body":"<p>I am using SQLite as back-end for ml-flow and I have registered a model. I got registered successful as output. but whenever I click on models I am getting below error. Not sure why I am getting this.<\/p>\n<p>INVALID_PARAMETER_VALUE: Invalid clause(s) in filter string: 'name', 'ilike', ''%%''. Expected search filter with single comparison operator. e.g. name='myModelName'<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1599126857387,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":252.0,
        "Poster_created_time":1532591725750,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":69.0,
        "Poster_view_count":12.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63721148",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"get error click model sqlite end flow regist model got regist success output click model get error sure get invalid paramet valu invalid claus filter string ilik expect search filter singl comparison oper mymodelnam",
        "Question_preprocessed_content":"get error click model sqlite regist model got regist success output click model get error sure get invalid claus filter string ilik expect search filter singl comparison oper mymodelnam",
        "Question_gpt_summary_original":"The user is encountering an error when clicking on models in mlflow after successfully registering a model using SQLite as the back-end. The error message states that there are invalid parameters in the filter string, specifically the 'name' and 'ilike' clauses. The user is unsure why this error is occurring.",
        "Question_gpt_summary":"user encount error click model successfulli regist model sqlite end error messag state invalid paramet filter string specif ilik claus user unsur error occur"
    },
    {
        "Question_title":"unable to identify current timezone 'C'",
        "Question_body":"<p>all,<\/p>\n\n<p>I am using R on the Azure machine learning, and I have some problems.<\/p>\n\n<p>I want to use program R to calculate the difference between two date, for example, 2014\/11\/01 and 2014\/11\/03.<\/p>\n\n<p>I using the function \"strptime\" in R to do this thing, it can work on my own computer, but when I want to run the same code on Azure ml, it came out the error.<\/p>\n\n<p>The error is : <\/p>\n\n<pre><code>[ModuleOutput] 1: In strptime(x, format, tz = tz) :\n[ModuleOutput] \n[ModuleOutput]   unable to identify current timezone 'C':\n[ModuleOutput] \n[ModuleOutput] please set environment variable 'TZ'\n[ModuleOutput] \n[ModuleOutput] 2: In strptime(x, format, tz = tz) : unknown timezone 'localtime'\n<\/code><\/pre>\n\n<p>I think the problem is that it can't detect the timezone on Azure ml, but I'm not sure.<\/p>\n\n<p>Is there any way to solve this problem?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_created_time":1415262044470,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":4.0,
        "Question_view_count":1456.0,
        "Poster_created_time":1413344613203,
        "Poster_location":"Taipei City, Taiwan",
        "Poster_reputation_count":579.0,
        "Poster_view_count":73.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/26774724",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unabl identifi current timezon problem want us program calcul differ date exampl function strptime thing work want run code came error error moduleoutput strptime format moduleoutput moduleoutput unabl identifi current timezon moduleoutput moduleoutput set environ variabl moduleoutput moduleoutput strptime format unknown timezon localtim think problem detect timezon sure wai solv problem thank advanc",
        "Question_preprocessed_content":"unabl identifi current timezon problem want us program calcul differ date exampl function strptime thing work want run code came error error think problem detect timezon sure wai solv problem thank advanc",
        "Question_gpt_summary_original":"The user is facing an issue while using the \"strptime\" function in R on Azure machine learning to calculate the difference between two dates. The error message suggests that the current timezone 'C' cannot be identified, and the user needs to set the environment variable 'TZ'. The user suspects that the problem is related to the inability to detect the timezone on Azure ml.",
        "Question_gpt_summary":"user face issu strptime function calcul differ date error messag suggest current timezon identifi user need set environ variabl user suspect problem relat inabl detect timezon"
    },
    {
        "Question_title":"Batch transform sparse matrix with AWS SageMaker Python SDK",
        "Question_body":"<p>I have successfully trained a Scikit-Learn LSVC model with AWS SageMaker.<br>\nI want to make batch prediction (aka. batch transform) on a relatively big dataset which is a scipy sparse matrix with shape 252772 x 185128. (The number of features is high because there is one-hot-encoding of bag-of-words and ngrams features).   <\/p>\n\n<p>I struggle because of:   <\/p>\n\n<ul>\n<li><p>the size of the data   <\/p><\/li>\n<li><p>the format of the data<\/p><\/li>\n<\/ul>\n\n<p>I did several experiments to check what was going on:    <\/p>\n\n<h3>1. predict locally on sample sparse matrix data<\/h3>\n\n<p><strong>It works<\/strong><br>\nDeserialize the model artifact locally on a SageMaker notebook and predict on a sample of the sparse matrix.<br>\nThis was just to check that the model can predict on this kind of data.<\/p>\n\n<h3>2. Batch Transform on a sample csv data<\/h3>\n\n<p><strong>It works<\/strong><br>\nLaunch a Batch Transform Job on SageMaker and request to transform a small sample in dense csv format : it works but does not scale, obviously.<br>\nThe code is:  <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>sklearn_model = SKLearnModel(\n    model_data=model_artifact_location_on_s3,\n    entry_point='my_script.py',\n    role=role,\n    sagemaker_session=sagemaker_session)\n\ntransformer = sklearn_model.transformer(\n   instance_count=1, \n   instance_type='ml.m4.xlarge', \n   max_payload=100)\n\ntransformer.transform(\n   data=batch_data, \n   content_type='text\/csv',\n   split_type=None)   \n\nprint('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\ntransformer.wait()\n\n<\/code><\/pre>\n\n<p>where:  <\/p>\n\n<ul>\n<li>'my_script.py' implements a simple <code>model_fn<\/code> to deserialize the model artifact: <\/li>\n<\/ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>def model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n    return clf\n<\/code><\/pre>\n\n<ul>\n<li><code>batch_data<\/code> is the s3 path for the csv file.  <\/li>\n<\/ul>\n\n<h3>3. Batch Transform of a sample dense numpy dataset.<\/h3>\n\n<p><strong>It works<\/strong><br>\nI prepared a sample of the data and saved it to s3 in Numpy <code>.npy<\/code> format. According to <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#sagemaker-scikit-learn-model-server\" rel=\"noreferrer\">this documentation<\/a>, SageMaker Scikit-learn model server can deserialize NPY-formatted data (along with JSON and CSV data).<br>\nThe only difference with the previous experiment (2) is the argument <code>content_type='application\/x-npy'<\/code> in <code>transformer.transform(...)<\/code>.   <\/p>\n\n<p>This solution does not scale and we would like to pass a Scipy sparse matrix: <\/p>\n\n<h3>4. Batch Transform of a big sparse matrix.<\/h3>\n\n<p><strong>Here is the problem<\/strong><br>\nSageMaker Python SDK does not support sparse matrix format out of the box.<br>\nFollowing this:  <\/p>\n\n<ul>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker\/<\/a>    <\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/55479366\/errors-running-sagemaker-batch-transformation-with-lda-model\">Errors running Sagemaker Batch Transformation with LDA model<\/a>  <\/li>\n<\/ul>\n\n<p>I used <code>write_spmatrix_to_sparse_tensor<\/code> to write the data to protobuf format on s3. The function I used is:  <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def write_protobuf(X_sparse, bucket, prefix, obj):\n    \"\"\"Write sparse matrix to protobuf format at location bucket\/prefix\/obj.\"\"\"\n    buf = io.BytesIO()\n    write_spmatrix_to_sparse_tensor(file=buf, array=X_sparse, labels=None)\n    buf.seek(0)\n    key = '{}\/{}'.format(prefix, obj)\n    boto3.resource('s3').Bucket(bucket).Object(key).upload_fileobj(buf)\n    return 's3:\/\/{}\/{}'.format(bucket, key)\n<\/code><\/pre>\n\n<p>Then the code used for launching the batch transform job is:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>sklearn_model = SKLearnModel(\n    model_data=model_artifact_location_on_s3,\n    entry_point='my_script.py',\n    role=role,\n    sagemaker_session=sagemaker_session)\n\ntransformer = sklearn_model.transformer(\n   instance_count=1, \n   instance_type='ml.m4.xlarge', \n   max_payload=100)\n\ntransformer.transform(\n   data=batch_data, \n   content_type='application\/x-recordio-protobuf',\n   split_type='RecordIO')   \n\nprint('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\ntransformer.wait()\n<\/code><\/pre>\n\n<p>I get the following error:<\/p>\n\n<pre><code>sagemaker_containers._errors.ClientError: Content type application\/x-recordio-protobuf is not supported by this framework.\n<\/code><\/pre>\n\n<p><strong>Questions:<\/strong><br>\n(Reference doc for Transformer: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/transformer.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/transformer.html<\/a>) <\/p>\n\n<ul>\n<li>If <code>content_type='application\/x-recordio-protobuf'<\/code> is not allowed, what should I use?<\/li>\n<li>Is <code>split_type='RecordIO'<\/code> the proper setting in this context?   <\/li>\n<li>Should I provide an <code>input_fn<\/code> function in my script to deserialize the data?<\/li>\n<li>Is there another better approach to tackle this problem?<\/li>\n<\/ul>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_created_time":1571219806713,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":5.0,
        "Question_view_count":834.0,
        "Poster_created_time":1406715922230,
        "Poster_location":"Belgium",
        "Poster_reputation_count":1714.0,
        "Poster_view_count":307.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58410583",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"batch transform spars matrix python sdk successfulli train scikit learn lsvc model want batch predict aka batch transform rel big dataset scipi spars matrix shape number featur high hot encod bag word ngram featur struggl size data format data experi check go predict local sampl spars matrix data work deseri model artifact local notebook predict sampl spars matrix check model predict kind data batch transform sampl csv data work launch batch transform job request transform small sampl dens csv format work scale obvious code sklearn model sklearnmodel model data model artifact locat entri point script role role session session transform sklearn model transform instanc count instanc type xlarg max payload transform transform data batch data content type text csv split type print wait transform job transform latest transform job job transform wait script implement simpl model deseri model artifact def model model dir clf joblib load path join model dir model joblib return clf batch data path csv file batch transform sampl dens numpi dataset work prepar sampl data save numpi npy format accord document scikit learn model server deseri npy format data json csv data differ previou experi argument content type applic npy transform transform solut scale like pass scipi spars matrix batch transform big spars matrix problem python sdk support spars matrix format box follow http aw amazon com blog machin learn build movi recommend factor machin amazon error run batch transform lda model write spmatrix spars tensor write data protobuf format function def write protobuf spars bucket prefix obj write spars matrix protobuf format locat bucket prefix obj buf bytesio write spmatrix spars tensor file buf arrai spars label buf seek kei format prefix obj boto resourc bucket bucket object kei upload fileobj buf return format bucket kei code launch batch transform job sklearn model sklearnmodel model data model artifact locat entri point script role role session session transform sklearn model transform instanc count instanc type xlarg max payload transform transform data batch data content type applic recordio protobuf split type recordio print wait transform job transform latest transform job job transform wait follow error contain error clienterror content type applic recordio protobuf support framework question refer doc transform http readthedoc stabl transform html content type applic recordio protobuf allow us split type recordio proper set context provid input function script deseri data better approach tackl problem",
        "Question_preprocessed_content":"batch transform spars matrix python sdk successfulli train lsvc model want batch predict rel big dataset scipi spars matrix shape struggl size data format data experi check go predict local sampl spars matrix data work deseri model artifact local notebook predict sampl spars matrix check model predict kind data batch transform sampl csv data work launch batch transform job request transform small sampl dens csv format work scale obvious code implement simpl deseri model artifact path csv file batch transform sampl dens numpi dataset work prepar sampl data save numpi format accord document model server deseri data differ previou experi argument solut scale like pass scipi spars matrix batch transform big spars matrix problem python sdk support spars matrix format box follow error run batch transform lda model write data protobuf format function code launch batch transform job follow error question refer doc transform allow us proper set context provid function script deseri data better approach tackl problem",
        "Question_gpt_summary_original":"The user has trained a Scikit-Learn LSVC model with AWS SageMaker and wants to make batch prediction on a large dataset which is a scipy sparse matrix. The user has tried several experiments to check the model's prediction on different data formats and sizes. However, the user is facing challenges in batch transforming a big sparse matrix as SageMaker Python SDK does not support sparse matrix format out of the box. The user has used write_spmatrix_to_sparse_tensor to write the data to protobuf format on s3 and is facing an error while launching the batch transform job. The user has several questions related to the content_type, split_type, input_fn, and better approaches to tackle this problem.",
        "Question_gpt_summary":"user train scikit learn lsvc model want batch predict larg dataset scipi spars matrix user tri experi check model predict differ data format size user face challeng batch transform big spars matrix python sdk support spars matrix format box user write spmatrix spars tensor write data protobuf format face error launch batch transform job user question relat content type split type input better approach tackl problem"
    },
    {
        "Question_title":"Class Label miscount while creating image masks for semantic segmentation",
        "Question_body":"<p>Hey all,<\/p>\n<p>When making a table of images with image masks, I realized wandb keeps the greatest amount of labels out of all the images as opposed to giving each image in a column its appropriate amount of labels. Ex: If I have 2 images, 1 with 5 labels and another with 11 labels, both image will have 11 labels as opposed to one with 5 and another with 11. I already debugged my code to make sure every time I created a dictionary with the appropriate amount of labels and passed it in as an argument, not sure why the final outcome in wandb still comes out with the wrong labels.<\/p>\n<p>Thanks!<br>\nUmama<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_created_time":1657549588807,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":126.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/class-label-miscount-while-creating-image-masks-for-semantic-segmentation\/2734",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"class label miscount creat imag mask semant segment hei make tabl imag imag mask realiz keep greatest label imag oppos give imag column appropri label imag label label imag label oppos debug code sure time creat dictionari appropri label pass argument sure final outcom come wrong label thank umama",
        "Question_preprocessed_content":"class label miscount creat imag mask semant segment hei make tabl imag imag mask realiz keep greatest label imag oppos give imag column appropri label imag label label imag label oppos debug code sure time creat dictionari appropri label pass argument sure final outcom come wrong label thank umama",
        "Question_gpt_summary_original":"The user is facing an issue with wandb while creating image masks for semantic segmentation. The problem is that wandb is keeping the greatest amount of labels out of all the images instead of giving each image its appropriate amount of labels. The user has already debugged their code and passed in the appropriate amount of labels, but the final outcome in wandb still comes out with the wrong labels.",
        "Question_gpt_summary":"user face issu creat imag mask semant segment problem keep greatest label imag instead give imag appropri label user debug code pass appropri label final outcom come wrong label"
    },
    {
        "Question_title":"Pushing docker image in AWS ECR from SageMaker Studio using AWS CLI",
        "Question_body":"<p>We can now publish Docker images to AWS ECR directly from <strong>SageMaker Studio<\/strong> using this code <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli<\/a>\nI did follow the easy installation instructions:<\/p>\n<pre><code>!pip install sagemaker-studio-image-build\nsm-docker build .\n<\/code><\/pre>\n<p>Also Trust policy and permissions have been set as described in the instructions.\nBut I'm getting the error &quot;<strong>Command did not exit successfully docker push<\/strong>&quot; at the stage where it is pushing the Docker image to AWS ECR. Any idea why? Here are the details print as output:<\/p>\n<pre><code>[Container] 2021\/05\/04 06:57:20 Running command echo Pushing the Docker image...\nPushing the Docker image...\n\n[Container] 2021\/05\/04 06:57:20 Running command docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG\nThe push refers to repository [752731038471.dkr.ecr.eu-central-1.amazonaws.com\/sagemaker-studio-d-tfbogtriaiml]\nAn image does not exist locally with the tag: 752731038471.dkr.ecr.eu-central-1.amazonaws.com\/sagemaker-studio-d-tfbogtriaiml\n\n[Container] 2021\/05\/04 06:57:20 Command did not exit successfully docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG exit status 1\n[Container] 2021\/05\/04 06:57:20 Phase complete: POST_BUILD State: FAILED\n[Container] 2021\/05\/04 06:57:20 Phase context status code: COMMAND_EXECUTION_ERROR Message: Error while executing command: docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG. Reason: exit status 1\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1620115524227,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":1046.0,
        "Poster_created_time":1469978721363,
        "Poster_location":null,
        "Poster_reputation_count":415.0,
        "Poster_view_count":48.0,
        "Answer_body":"<p>In the Dockerfile, there was a reference to another file that was not present in the directory from where the command <code>sm-docker build .<\/code> was launched.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1620378926343,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1469978721363,
        "Answerer_location":null,
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":48.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67380942",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"push docker imag aw ecr studio aw cli publish docker imag aw ecr directli studio code http github com aw sampl studio imag build cli follow easi instal instruct pip instal studio imag build docker build trust polici permiss set describ instruct get error command exit successfulli docker push stage push docker imag aw ecr idea detail print output contain run command echo push docker imag push docker imag contain run command docker push aw account dkr ecr aw default region amazonaw com imag repo imag tag push refer repositori dkr ecr central amazonaw com studio tfbogtriaiml imag exist local tag dkr ecr central amazonaw com studio tfbogtriaiml contain command exit successfulli docker push aw account dkr ecr aw default region amazonaw com imag repo imag tag exit statu contain phase complet post build state fail contain phase context statu code command execut error messag error execut command docker push aw account dkr ecr aw default region amazonaw com imag repo imag tag reason exit statu",
        "Question_preprocessed_content":"push docker imag aw ecr studio aw cli publish docker imag aw ecr directli studio code follow easi instal instruct trust polici permiss set describ instruct get error command exit successfulli docker push stage push docker imag aw ecr idea detail print output",
        "Question_gpt_summary_original":"The user is encountering an error while pushing a Docker image to AWS ECR from SageMaker Studio using AWS CLI. The error message states that the command did not exit successfully while pushing the Docker image and an image does not exist locally with the tag. The user has followed the installation instructions and set trust policy and permissions as described.",
        "Question_gpt_summary":"user encount error push docker imag aw ecr studio aw cli error messag state command exit successfulli push docker imag imag exist local tag user follow instal instruct set trust polici permiss describ"
    },
    {
        "Question_title":"Azure Machine Learning Data Transformation",
        "Question_body":"<p>Can machine learning be used to transform\/modifiy a list of numbers.<\/p>\n\n<p>I have many pairs of binary files read from vehicle ECUs, an original or stock file before the vehicle was tuned, and a modified file which has the engine parameters altered.  The files are basically lists of little or big endian 16 bit numbers.<\/p>\n\n<p>I was wondering if it is at all possible to feed these pairs of files into machine learning, and for it to take a new stock file and attempt to transform or tune that stock file.<\/p>\n\n<p>I would appreciate it if somebody could tell me if this is something which is at all possible.  All of the examples I've found appear to make decisions on data rather than do any sort of a transformation.<\/p>\n\n<p>Also I'm hoping to use azure for this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1455630592703,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1456850005583,
        "Question_score_count":0.0,
        "Question_view_count":113.0,
        "Poster_created_time":1330700625703,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":5.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35434371",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"data transform machin learn transform modifii list number pair binari file read vehicl ecu origin stock file vehicl tune modifi file engin paramet alter file basic list littl big endian bit number wonder possibl feed pair file machin learn new stock file attempt transform tune stock file appreci somebodi tell possibl exampl appear decis data sort transform hope us azur",
        "Question_preprocessed_content":"data transform machin learn list number pair binari file read vehicl ecu origin stock file vehicl tune modifi file engin paramet alter file basic list littl big endian bit number wonder possibl feed pair file machin learn new stock file attempt transform tune stock file appreci somebodi tell possibl exampl appear decis data sort transform hope us azur",
        "Question_gpt_summary_original":"The user is seeking to know if it is possible to use machine learning to transform a list of numbers, specifically binary files from vehicle ECUs containing little or big endian 16 bit numbers. They want to know if it is possible to feed these pairs of files into machine learning and use it to transform or tune a new stock file. The user is also interested in using Azure for this task.",
        "Question_gpt_summary":"user seek know possibl us machin learn transform list number specif binari file vehicl ecu contain littl big endian bit number want know possibl feed pair file machin learn us transform tune new stock file user interest azur task"
    },
    {
        "Question_title":"Azure Machine Learning: What error is this?",
        "Question_body":"<p>I am using a Classic Web Service with a non-default endpoint for a Update Resource activity on the Azure Data Factory. This is the error I get:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/shK0R.png\" rel=\"nofollow noreferrer\">Screenshot of Error<\/a><\/p>\n\n<p>I didn't find any info on the web and couldn't figure it out myself. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/data-factory-azure-ml-update-resource-activity\" rel=\"nofollow noreferrer\">This<\/a> website shows an example that I used by just filling in my values for mlEndpoint, apiKey and updateRessourceEndpoint:<\/p>\n\n<pre><code>{\n    \"name\": \"updatableScoringEndpoint2\",\n    \"properties\": {\n        \"type\": \"AzureML\",\n        \"typeProperties\": {\n            \"mlEndpoint\": \"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/xxx\/services\/--scoring experiment--\/jobs\",\n            \"apiKey\": \"endpoint2Key\",\n            \"updateResourceEndpoint\": \"https:\/\/management.azureml.net\/workspaces\/xxx\/webservices\/--scoring experiment--\/endpoints\/endpoint2\"\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>There is no mention of a token that needs to be passed...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1503316557373,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":253.0,
        "Poster_created_time":1476806455803,
        "Poster_location":"Holzkirchen, Deutschland",
        "Poster_reputation_count":3068.0,
        "Poster_view_count":386.0,
        "Answer_body":"<p>this error is basically saying the apiKey you provided is invalid to perform the update resource operation. Here is some posts for your reference: <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/3bb77e37-8860-43c6-bcaa-d6ebd70617b8\/retrain-predictive-web-service-programmatically-when-do-not-have-access-to-managementazuremlnet?forum=MachineLearning\" rel=\"nofollow noreferrer\">https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/3bb77e37-8860-43c6-bcaa-d6ebd70617b8\/retrain-predictive-web-service-programmatically-when-do-not-have-access-to-managementazuremlnet?forum=MachineLearning<\/a><\/p>\n\n<p>Please also be noted that if you modified your linked service in ADF, remember to re-deploy the pipeline as well to reflect your change in time.<\/p>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1503393687923,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1488711530187,
        "Answerer_location":null,
        "Answerer_reputation_count":29.0,
        "Answerer_view_count":12.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45796489",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error classic web servic non default endpoint updat resourc activ azur data factori error screenshot error info web couldn figur websit show exampl fill valu mlendpoint apikei updateressourceendpoint updatablescoringendpoint properti type typeproperti mlendpoint http ussouthcentr servic net workspac servic score experi job apikei endpointkei updateresourceendpoint http manag net workspac webservic score experi endpoint endpoint mention token need pass",
        "Question_preprocessed_content":"error classic web servic endpoint updat resourc activ azur data factori error screenshot error info web couldn figur websit show exampl fill valu mlendpoint apikei updateressourceendpoint mention token need",
        "Question_gpt_summary_original":"The user is encountering an error while using a Classic Web Service with a non-default endpoint for an Update Resource activity on the Azure Data Factory. The error message is displayed in the provided screenshot, and the user has not been able to find any information on the web or figure it out themselves. The user has followed an example from a website by filling in their values for mlEndpoint, apiKey, and updateResourceEndpoint, but there is no mention of a token that needs to be passed.",
        "Question_gpt_summary":"user encount error classic web servic non default endpoint updat resourc activ azur data factori error messag displai provid screenshot user abl inform web figur user follow exampl websit fill valu mlendpoint apikei updateresourceendpoint mention token need pass"
    },
    {
        "Question_title":"Check Azure ML Instance Type",
        "Question_body":"<p>How do i check current instance of AzureML type ? Where menu can i acces to see the type ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1636608553670,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/623173\/check-azure-ml-instance-type",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"check instanc type check current instanc type menu acc type",
        "Question_preprocessed_content":"check instanc type check current instanc type menu acc type",
        "Question_gpt_summary_original":"The user is seeking guidance on how to check the current instance type of Azure ML and where to access the menu to view the type.",
        "Question_gpt_summary":"user seek guidanc check current instanc type access menu view type"
    },
    {
        "Question_title":"SageMaker Estimator use_spot_instances causes Invalid MaxWaitTimeInSeconds",
        "Question_body":"<p><a href=\"https:\/\/aws.amazon.com\/blogs\/aws\/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs\/\" rel=\"nofollow noreferrer\">Managed Spot Training: Save Up to 90% On Your Amazon SageMaker Training Jobs<\/a> says:<\/p>\n<blockquote>\n<p>Setting it up is extremely simple, as it should be when working with a fully-managed service:<\/p>\n<ul>\n<li>If you\u2019re using the console, just switch the feature on.<\/li>\n<li>If you\u2019re working with the Amazon SageMaker SDK, <strong>just set the train_use_spot_instances to true<\/strong> in the Estimator constructor.<\/li>\n<\/ul>\n<\/blockquote>\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.Estimator\" rel=\"nofollow noreferrer\">SageMaker SDK sagemaker.estimator.Estimator<\/a> says:<\/p>\n<blockquote>\n<ul>\n<li>use_spot_instances (bool) \u2013\nSpecifies whether to use SageMaker Managed Spot instances for training. If enabled then the max_wait arg should also be set.<\/li>\n<li>max_wait (int) \u2013\nTimeout in seconds waiting for spot training instances (default: None). After this amount of time Amazon SageMaker will stop waiting for Spot instances to become available (default: None).<\/li>\n<\/ul>\n<\/blockquote>\n<p>As per the documentations, run below.<\/p>\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\n\nestimator = TensorFlow(\n    entry_point=&quot;fashion_mnist_training.py&quot;,\n    source_dir=&quot;src&quot;,\n    metric_definitions=metric_definitions,\n    hyperparameters=hyperparameters,\n    role=role,\n    input_mode='File',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    use_spot_instances=True,\n    max_wait= 23 * 60 * 60, \n    base_job_name=base_job_name,\n    checkpoint_s3_uri=checkpoint_s3_uri,\n    model_dir=False  # To avoid duplicate 'model_dir' command line argument\n)\n<\/code><\/pre>\n<p>However, error is caused.<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Invalid MaxWaitTimeInSeconds. It must be present and be greater than or equal to MaxRuntimeInSeconds\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1630660877280,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":269.0,
        "Poster_created_time":1416648155470,
        "Poster_location":null,
        "Poster_reputation_count":14749.0,
        "Poster_view_count":968.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69042524",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"estim us spot instanc caus invalid maxwaittimeinsecond manag spot train save train job sai set extrem simpl work fulli manag servic your consol switch featur your work sdk set train us spot instanc true estim constructor sdk estim estim sai us spot instanc bool specifi us manag spot instanc train enabl max wait arg set max wait int timeout second wait spot train instanc default time stop wait spot instanc avail default document run tensorflow import tensorflow estim tensorflow entri point fashion mnist train sourc dir src metric definit metric definit hyperparamet hyperparamet role role input mode file framework version version instanc count instanc type xlarg us spot instanc true max wait base job base job checkpoint uri checkpoint uri model dir fals avoid duplic model dir command line argument error caus clienterror error occur validationexcept call createtrainingjob oper invalid maxwaittimeinsecond present greater equal maxruntimeinsecond",
        "Question_preprocessed_content":"estim caus invalid maxwaittimeinsecond manag spot train save train job sai set extrem simpl work servic your consol switch featur your work sdk set true estim constructor sdk sai specifi us manag spot instanc train enabl arg set timeout second wait spot train instanc time stop wait spot instanc avail document run error caus",
        "Question_gpt_summary_original":"The user is encountering an error while using the SageMaker SDK to set up Managed Spot Training. Specifically, when setting the train_use_spot_instances to true in the Estimator constructor, the user is receiving a \"ValidationException\" error stating that the MaxWaitTimeInSeconds is invalid and must be present and greater than or equal to MaxRuntimeInSeconds.",
        "Question_gpt_summary":"user encount error sdk set manag spot train specif set train us spot instanc true estim constructor user receiv validationexcept error state maxwaittimeinsecond invalid present greater equal maxruntimeinsecond"
    },
    {
        "Question_title":"Registering and getting an environment in Azure Machine Learning Studio that derives from a self-created Docker image",
        "Question_body":"<p>I need to register an environment in Azure Machine Learning Studio which derives from a self-created Docker image. I have found this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-custom-image#use-a-private-container-registry-optional\" rel=\"nofollow noreferrer\">documentation<\/a> by Microsoft that describes the process of using a custom Docker image as an environment.<\/p>\n<p>Unfortunately, the documentation does not explain how to register the resulting environment into the workspace in order to get the environment when needed.<\/p>\n<p>Normally, you describe and register an environment like this:<\/p>\n<pre><code>from azureml.core import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n# Create a Python environment for the experiment\ndiabetes_env = Environment(&quot;diabetes-experiment-env&quot;)\ndiabetes_env.python.user_managed_dependencies = False # Let Azure ML manage dependencies\ndiabetes_env.docker.enabled = True # Use a docker container\n\n# Create a set of package dependencies (conda or pip as required)\ndiabetes_packages = CondaDependencies.create(conda_packages=['scikit-learn','ipykernel','matplotlib','pandas','pip'],\n                                             pip_packages=['azureml-sdk','pyarrow'])\n\n# Add the dependencies to the environment\ndiabetes_env.python.conda_dependencies = diabetes_packages\n\n# Register the environment\ndiabetes_env.register(workspace=ws)\n<\/code><\/pre>\n<p>Then, when you need the defined environment, you can get it with:<\/p>\n<pre><code># get the registered environment\nregistered_env = Environment.get(ws, 'diabetes-experiment-env')\n<\/code><\/pre>\n<p><a href=\"https:\/\/github.com\/MicrosoftLearning\/DP100\/blob\/master\/05A%20-%20Working%20with%20Environments.ipynb\" rel=\"nofollow noreferrer\">Source<\/a><\/p>\n<p>As it seems to me, this approach does not work for environments that derive from self-created Docker images.<\/p>\n<p>For example, I define and register an environment like this:<\/p>\n<pre><code>from azureml.core import Environment\nfrom azureml.core import ContainerRegistry\n\nregistry = ContainerRegistry()\nregistry.address = &quot;name_of_container_registry.azurecr.io&quot;\nregistry.username = &quot;username&quot;\nregistry.password = &quot;password&quot;\nmyenv = Environment.from_docker_image('env_name', 'name_of_container_registry.azurecr.io\/docker_image_name:latest', container_registry=registry, conda_specification=None, pip_requirements=None)\nmyenv.register(workspace=ws)\n<\/code><\/pre>\n<p>Then, in the script where I need to have the above-defined environment:<\/p>\n<pre><code>from azureml.core.model import Model\nfrom azureml.core import Environment\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.webservice import LocalWebservice\n\nmodel = Model(ws, 'exemplarily_model')\n\nregistered_env = Environment.get(ws, 'env_name')\n\ninference_config = InferenceConfig(environment=registered_env, \n                                   source_directory='.\/source_dir', \n                                   entry_script='.\/score.py') \n\ndeployment_config = LocalWebservice.deploy_configuration(port=6789)\n\nservice = Model.deploy(\n    ws,\n    &quot;myservice&quot;,\n    [model],\n    inference_config,\n    deployment_config,\n    overwrite=True,\n)\n\nservice.wait_for_deployment(show_output=True)\nprint(service.get_logs())\n<\/code><\/pre>\n<p>Everytime the inference service deployment fails with the exception &quot;Authentication failed for container registry name_of_container_registry.azurecr.io&quot;. However, when I change <code>environment=registered_env<\/code> to <code>environment=myenv<\/code>, the inference service starts successfully without any error.<\/p>\n<p>This brings me to the conclusion that registering and getting an environment in Azure Machine Learning Studio that derives from a self-created Docker image works differently. <br>\n<br>\nTherefore, I would like to ask for the correct commands for registering and getting an environment in Azure Machine Learning Studio that derives from a self-created Docker image.<\/p>\n<p><strong>Update:<\/strong> <br>\nI found a possibility to get the environments that derives from a self-created Docker image:<\/p>\n<p>Instead of registering the environment with the command <code>myenv.register(workspace=ws)<\/code>, you can save the environment to a directory with the command <code>myenv.save_to_directory(path=&quot;.\/env&quot;, overwrite=True)<\/code>.\nThen you can load the environment in a different script with <code>newenv = Environment.load_from_directory(path=&quot;.\/env&quot;)<\/code>.\nThis approach works for me to deploy my inference service.\nI found the possibility for getting a saved environment here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments#use-existing-environments\" rel=\"nofollow noreferrer\">Link<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1644948773413,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1644998305760,
        "Question_score_count":1.0,
        "Question_view_count":888.0,
        "Poster_created_time":1632899213152,
        "Poster_location":null,
        "Poster_reputation_count":139.0,
        "Poster_view_count":12.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71131403",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"regist get environ studio deriv self creat docker imag need regist environ studio deriv self creat docker imag document microsoft describ process custom docker imag environ unfortun document explain regist result environ workspac order environ need normal regist environ like core import environ core conda depend import condadepend creat python environ experi diabet env environ diabet experi env diabet env python user manag depend fals let manag depend diabet env docker enabl true us docker contain creat set packag depend conda pip requir diabet packag condadepend creat conda packag scikit learn ipykernel matplotlib panda pip pip packag sdk pyarrow add depend environ diabet env python conda depend diabet packag regist environ diabet env regist workspac need defin environ regist environ regist env environ diabet experi env sourc approach work environ deriv self creat docker imag exampl defin regist environ like core import environ core import containerregistri registri containerregistri registri address contain registri azurecr registri usernam usernam registri password password myenv environ docker imag env contain registri azurecr docker imag latest contain registri registri conda specif pip requir myenv regist workspac script need defin environ core model import model core import environ core model import inferenceconfig core webservic import localwebservic model model exemplarili model regist env environ env infer config inferenceconfig environ regist env sourc directori sourc dir entri script score deploy config localwebservic deploi configur port servic model deploi myservic model infer config deploy config overwrit true servic wait deploy output true print servic log everytim infer servic deploy fail except authent fail contain registri contain registri azurecr chang environ regist env environ myenv infer servic start successfulli error bring conclus regist get environ studio deriv self creat docker imag work differ like ask correct command regist get environ studio deriv self creat docker imag updat possibl environ deriv self creat docker imag instead regist environ command myenv regist workspac save environ directori command myenv save directori path env overwrit true load environ differ script newenv environ load directori path env approach work deploi infer servic possibl get save environ link",
        "Question_preprocessed_content":"regist get environ studio deriv docker imag need regist environ studio deriv docker imag document microsoft describ process custom docker imag environ unfortun document explain regist result environ workspac order environ need normal regist environ like need defin environ sourc approach work environ deriv docker imag exampl defin regist environ like script need environ everytim infer servic deploy fail except authent fail contain registri chang infer servic start successfulli error bring conclus regist get environ studio deriv docker imag work differ like ask correct command regist get environ studio deriv docker imag updat possibl environ deriv docker imag instead regist environ command save environ directori command load environ differ script approach work deploi infer servic possibl get save environ link",
        "Question_gpt_summary_original":"The user is trying to register an environment in Azure Machine Learning Studio that derives from a self-created Docker image. The Microsoft documentation does not provide clear instructions on how to register the resulting environment into the workspace. The user has tried to register the environment using the provided code but it fails with an authentication error. The user has found a workaround by saving the environment to a directory and loading it in a different script.",
        "Question_gpt_summary":"user try regist environ studio deriv self creat docker imag microsoft document provid clear instruct regist result environ workspac user tri regist environ provid code fail authent error user workaround save environ directori load differ script"
    },
    {
        "Question_title":"How to solve \"Encountered unresolved custom op: edgetpu-custom-op\" exception",
        "Question_body":"<p>Im trying to implement my custom ML model in a kotlin app.<\/p>\n<p>I first make and train my model in GCP Vertex AI.\nAfter my model was ready i've export it as tensor flow model to the edge and then upload to my Firebase Machine Learning proyect.<\/p>\n<p>After that i follow the <a href=\"https:\/\/firebase.google.com\/docs\/ml\/android\/use-custom-models?hl=es&amp;authuser=0\" rel=\"nofollow noreferrer\">guide to implement a custom model of tensor flow lite on android.<\/a><\/p>\n<p>Then when i execute my app it crash on this part of the code:<\/p>\n<pre><code>val conditions = CustomModelDownloadConditions.Builder()\n        .requireWifi()  \/\/ Also possible: .requireCharging() and .requireDeviceIdle()\n        .build()\nFirebaseModelDownloader.getInstance()\n        .getModel(&quot;your_model&quot;, DownloadType.LOCAL_MODEL_UPDATE_IN_BACKGROUND,\n            conditions)\n        .addOnSuccessListener { model: CustomModel? -&gt;\n            \/\/ Download complete. Depending on your app, you could enable the ML\n            \/\/ feature, or switch from the local model to the remote model, etc.\n\n            \/\/ The CustomModel object contains the local path of the model file,\n            \/\/ which you can use to instantiate a TensorFlow Lite interpreter.\n            val modelFile = model?.file\n            if (modelFile != null) {\n                interpreter = Interpreter(modelFile) \/\/ this line crash\n            }\n        }\n<\/code><\/pre>\n<p>More specific at the line &quot;interpreter = Interpreter(modelFile)&quot;.\nI get the following exception:<\/p>\n<blockquote>\n<p>java.lang.IllegalStateException: Internal error: Unexpected failure\nwhen preparing tensor allocations: Encountered unresolved custom op:\nedgetpu-custom-op. See instructions:\n<a href=\"https:\/\/www.tensorflow.org\/lite\/guide\/ops_custom\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/lite\/guide\/ops_custom<\/a>  Node number 0\n(edgetpu-custom-op) failed to prepare.<\/p>\n<\/blockquote>\n<p>What is the meaning of this error?  How can i solve it?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1663959349290,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":21.0,
        "Poster_created_time":1345651822772,
        "Poster_location":null,
        "Poster_reputation_count":109.0,
        "Poster_view_count":21.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73831871",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"solv encount unresolv custom edgetpu custom except try implement custom model kotlin app train model gcp model readi export tensor flow model edg upload firebas machin learn proyect follow guid implement custom model tensor flow lite android execut app crash code val condit custommodeldownloadcondit builder requirewifi possibl requirecharg requiredeviceidl build firebasemodeldownload getinst getmodel model downloadtyp local model updat background condit addonsuccesslisten model custommodel download complet depend app enabl featur switch local model remot model custommodel object contain local path model file us instanti tensorflow lite interpret val modelfil model file modelfil null interpret interpret modelfil line crash specif line interpret interpret modelfil follow except java lang illegalstateexcept intern error unexpect failur prepar tensor alloc encount unresolv custom edgetpu custom instruct http tensorflow org lite guid op custom node number edgetpu custom fail prepar mean error solv",
        "Question_preprocessed_content":"solv encount unresolv custom except try implement custom model kotlin app train model gcp model readi export tensor flow model edg upload firebas machin learn proyect follow guid implement custom model tensor flow lite android execut app crash code specif line interpret interpret follow except intern error unexpect failur prepar tensor alloc encount unresolv custom instruct node number fail prepar mean error solv",
        "Question_gpt_summary_original":"The user is trying to implement a custom ML model in a Kotlin app by following the guide to implement a custom model of TensorFlow Lite on Android. However, the app crashes at the line \"interpreter = Interpreter(modelFile)\" with the exception \"Encountered unresolved custom op: edgetpu-custom-op.\" The user is seeking help to understand the meaning of this error and how to solve it.",
        "Question_gpt_summary":"user try implement custom model kotlin app follow guid implement custom model tensorflow lite android app crash line interpret interpret modelfil except encount unresolv custom edgetpu custom user seek help understand mean error solv"
    },
    {
        "Question_title":"Azure Machine Learning profiling model errors",
        "Question_body":"<p>After successfully completing the image-classification-mnist-data tutorial in Azure Machine Learning Samples    <\/p>\n<blockquote>\n<p>Samples\/1.43.0\/tutorials\/image-classification-mnist-data\/img-classification-part1-training.ipynb    <\/p>\n<\/blockquote>\n<p> I would like to profile the resulting model as shown in this article <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-deploy-profile-model?pivots=py-sdk\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-deploy-profile-model?pivots=py-sdk<\/a>     <\/p>\n<p>However I keep getting an error saying <code>Running.....................................     Failed     \/tmp\/ipykernel_56534\/2365332213.py:15: UserWarning: Model Profiling operation failed with the following error: Model service has failed with status: CrashLoopBackOff: Back-off restarting failed. This may be caused by errors in your scoring file's init() function. Error logs URL: Log upload failed. Request ID: b5384f0f-8a3a-4f53-908e-0a028374b924. Inspect ModelProfile.error property for more information.       profile.wait_for_completion(True)     {'name': 'sklearn-08172022-143854',      'createdTime': '2022-08-17T14:38:56.706085+00:00',      'state': 'Failed',      'requestedCpu': 3.5,      'requestedMemoryInGB': 15.0,      'requestedQueriesPerSecond': 0,      'error': {'code': 'ModelTestBackendCrashLoopBackoff',       'statusCode': 400,       'message': &quot;Model service has failed with status: CrashLoopBackOff: Back-off restarting failed. This may be caused by errors in your scoring file's init() function. Error logs URL: Log upload failed.&quot;,       'details': []}}<\/code>    <\/p>\n<p>I only have 1 model in my workspace model list. So why am I getting an error and how can I see the error that is thrown inside the scoring file?    <\/p>\n<p><strong>scoring.py<\/strong>    <\/p>\n<pre><code>  %%writefile score.py  \n    import json  \n    import numpy as np  \n    import os  \n    import pickle  \n    import joblib  \n      \n    def init():  \n        global model  \n        # AZUREML_MODEL_DIR is an environment variable created during deployment.  \n        # It is the path to the model folder (.\/azureml-models\/$MODEL_NAME\/$VERSION)  \n        # For multiple models, it points to the folder containing all deployed models (.\/azureml-models)  \n        model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'sklearn_mnist_model.pkl')  \n        model = joblib.load(model_path)  \n      \n    def run(raw_data):  \n        data = np.array(json.loads(raw_data)['data'])  \n        # make prediction  \n        y_hat = model.predict(data)  \n        # you can return any data type as long as it is JSON-serializable  \n        return y_hat.tolist()  \n  \n<\/code><\/pre>\n<p><strong>profiling.py<\/strong>    <\/p>\n<pre><code>    import os  \n    from azureml.core import Dataset  \n    from azureml.opendatasets import MNIST  \n    from utils import load_data  \n    import os  \n    import glob  \n      \n      \n    data_folder = os.path.join(os.getcwd(), 'data')  \n    os.makedirs(data_folder, exist_ok=True)  \n      \n    mnist_file_dataset = MNIST.get_file_dataset()  \n    mnist_file_dataset.download(data_folder, overwrite=True)  \n      \n    data_folder = os.path.join(os.getcwd(), 'data')  \n    # note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the neural network converge faster  \n    X_test = load_data(glob.glob(os.path.join(data_folder,&quot;**\/t10k-images-idx3-ubyte.gz&quot;), recursive=True)[0], False) \/ 255.0  \n    y_test = load_data(glob.glob(os.path.join(data_folder,&quot;**\/t10k-labels-idx1-ubyte.gz&quot;), recursive=True)[0], True).reshape(-1)  \n      \n      \n      \n      \n    import json  \n    from azureml.core import Datastore  \n    from azureml.core.dataset import Dataset  \n    from azureml.data import dataset_type_definitions  \n      \n    random_index = np.random.randint(0, len(X_test)-1)  \n    input_json = &quot;{\\&quot;data\\&quot;: [&quot; + str(list(X_test[random_index])) + &quot;]}&quot;  \n    # create a string that can be utf-8 encoded and  \n    # put in the body of the request  \n    serialized_input_json = json.dumps(input_json)  \n    dataset_content = []  \n    for i in range(100):  \n        dataset_content.append(serialized_input_json)  \n    dataset_content = '\\n'.join(dataset_content)  \n    file_name = 'sample_request_data.txt'  \n    f = open(file_name, 'w')  \n    f.write(dataset_content)  \n    f.close()  \n      \n    # upload the txt file created above to the Datastore and create a dataset from it  \n    data_store = Datastore.get_default(ws)  \n    data_store.upload_files(['.\/' + file_name], target_path='sample_request_data')  \n    datastore_path = [(data_store, 'sample_request_data' +'\/' + file_name)]  \n    sample_request_data = Dataset.Tabular.from_delimited_files(  \n        datastore_path, separator='\\n',  \n        infer_column_types=True,  \n        header=dataset_type_definitions.PromoteHeadersBehavior.NO_HEADERS)  \n    sample_request_data = sample_request_data.register(workspace=ws,  \n                                                       name='sample_request_data',  \n                                                       create_new_version=True)  \n      \n      \n      \n    from azureml.core.model import InferenceConfig, Model  \n    from azureml.core.dataset import Dataset  \n    from datetime import datetime  \n      \n      \n    model = Model(ws, id='sklearn_mnist:1')  \n    inference_config = InferenceConfig(entry_script='score.py', environment=env)  \n    input_dataset = Dataset.get_by_name(workspace=ws, name='sample_request_data')  \n    profile = Model.profile(ws,  \n                'sklearn-%s' % datetime.now().strftime('%m%d%Y-%H%M%S'),  \n                [model],  \n                inference_config,  \n                input_dataset=input_dataset)  \n      \n    profile.wait_for_completion(True)  \n      \n    # see the result  \n    details = profile.get_details()  \n      \n  \n  \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1660748272003,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/970964\/azure-machine-learning-profiling-model-errors",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"profil model error successfulli complet imag classif mnist data tutori sampl sampl tutori imag classif mnist data img classif train ipynb like profil result model shown articl http learn microsoft com azur machin learn deploi profil model pivot sdk get error sai run fail tmp ipykernel userwarn model profil oper fail follow error model servic fail statu crashloopbackoff restart fail caus error score file init function error log url log upload fail request bff inspect modelprofil error properti inform profil wait complet true sklearn createdtim state fail requestedcpu requestedmemoryingb requestedqueriespersecond error code modeltestbackendcrashloopbackoff statuscod messag model servic fail statu crashloopbackoff restart fail caus error score file init function error log url log upload fail detail model workspac model list get error error thrown insid score file score writefil score import json import numpi import import pickl import joblib def init global model model dir environ variabl creat deploy path model folder model model version multipl model point folder contain deploi model model model path path join getenv model dir sklearn mnist model pkl model joblib load model path def run raw data data arrai json load raw data data predict hat model predict data return data type long json serializ return hat tolist profil import core import dataset opendataset import mnist util import load data import import glob data folder path join getcwd data makedir data folder exist true mnist file dataset mnist file dataset mnist file dataset download data folder overwrit true data folder path join getcwd data note shrink intens valu help neural network converg faster test load data glob glob path join data folder imag idx ubyt recurs true fals test load data glob glob path join data folder label idx ubyt recurs true true reshap import json core import datastor core dataset import dataset data import dataset type definit random index random randint len test input json data str list test random index creat string utf encod bodi request serial input json json dump input json dataset content rang dataset content append serial input json dataset content join dataset content file sampl request data txt open file write dataset content close upload txt file creat datastor creat dataset data store datastor default data store upload file file target path sampl request data datastor path data store sampl request data file sampl request data dataset tabular delimit file datastor path separ infer column type true header dataset type definit promoteheadersbehavior header sampl request data sampl request data regist workspac sampl request data creat new version true core model import inferenceconfig model core dataset import dataset datetim import datetim model model sklearn mnist infer config inferenceconfig entri script score environ env input dataset dataset workspac sampl request data profil model profil sklearn datetim strftime model infer config input dataset input dataset profil wait complet true result detail profil detail",
        "Question_preprocessed_content":"profil model error successfulli complet tutori sampl like profil result model shown articl get error sai model workspac model list get error error thrown insid score file",
        "Question_gpt_summary_original":"The user encountered an error while trying to profile a model in Azure Machine Learning. The error message suggests that the issue may be caused by errors in the scoring file's init() function. The user provided the scoring.py and profiling.py files for reference.",
        "Question_gpt_summary":"user encount error try profil model error messag suggest issu caus error score file init function user provid score profil file refer"
    },
    {
        "Question_title":"Dask scheduler not using multiple gpus on remote",
        "Question_body":"<p>Hi,<br>\nIt\u2019s a fresh feature and most likely I\u2019m doing something wrong, but I cannot get the Dask scheduler to work properly on a remote with 4 gpus.<\/p>\n<p>This is what I do, and apart from the remote part it is basically a copy of the steps in the How To guide:<\/p>\n<ol>\n<li>\n<p>Connect to remote. I have successfully staged runs on remote, run them directly, and also used multiple gpus by assigning runs to gpus manually using --gpus flag. So the remote works corectly.<\/p>\n<\/li>\n<li>\n<p>Start the Dask scheduler on the remote.<\/p>\n<\/li>\n<li>\n<p>Then I stage trials on remote, let\u2019s say 4, with different parameters, using a single command.<\/p>\n<\/li>\n<\/ol>\n<pre><code>guild run TABL:train window=[100,200,300,400] --remote cerberus --stage-trials\n<\/code><\/pre>\n<ol start=\"4\">\n<li>The trials are sent to remote, and the scheduler starts them. If workers is set to 4, it starts correctly 4 processes of loading data etc.<\/li>\n<\/ol>\n<p>And this is where something goes wrong. After doing the pre-procesing stage and creating 4 models concurrently, the scheduler places all 4 models and training processses on all 4 gpus (which can be seen on the screenshot - all gpus have allocated memory). And then only begins the training on 1 gpu.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6.png\" data-download-href=\"\/uploads\/short-url\/2PTbvO3vWfJgJUE57EqoCMII9Jc.png?dl=1\" title=\"Screenshot from 2021-03-29 18-26-25\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6_2_490x500.png\" alt=\"Screenshot from 2021-03-29 18-26-25\" data-base62-sha1=\"2PTbvO3vWfJgJUE57EqoCMII9Jc\" width=\"490\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6_2_490x500.png, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6.png 1.5x, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2021-03-29 18-26-25<\/span><span class=\"informations\">731\u00d7745 103 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p><strong>Expected<\/strong><br>\nI would expect the scheduler to assign the trials to available gpus and train them concurrently. Furthermore, when the number of trials is bigger than the number of gpus, I would expect the scheduler to automatically run the pending operation when a gpus becomes free.<\/p>\n<p>Did I understand what the scheduler is capable of correctly? Is there maybe some manual step somewhere that I missed?<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1617036688225,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":495.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/dask-scheduler-not-using-multiple-gpus-on-remote\/583",
        "Tool":"Guild AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"dask schedul multipl gpu remot fresh featur like wrong dask schedul work properli remot gpu apart remot basic copi step guid connect remot successfulli stage run remot run directli multipl gpu assign run gpu manual gpu flag remot work corectli start dask schedul remot stage trial remot let differ paramet singl command run tabl train window remot cerberu stage trial trial sent remot schedul start worker set start correctli process load data goe wrong pre proces stage creat model concurr schedul place model train proce gpu seen screenshot gpu alloc memori begin train gpu screenshot expect expect schedul assign trial avail gpu train concurr furthermor number trial bigger number gpu expect schedul automat run pend oper gpu free understand schedul capabl correctli mayb manual step miss thank advanc",
        "Question_preprocessed_content":"dask schedul multipl gpu remot fresh featur like wrong dask schedul work properli remot gpu apart remot basic copi step guid connect remot successfulli stage run remot run directli multipl gpu assign run gpu manual flag remot work corectli start dask schedul remot stage trial remot let differ paramet singl command trial sent remot schedul start worker set start correctli process load data goe wrong stage creat model concurr schedul place model train proce gpu begin train gpu screenshot expect expect schedul assign trial avail gpu train concurr furthermor number trial bigger number gpu expect schedul automat run pend oper gpu free understand schedul capabl correctli mayb manual step miss thank advanc",
        "Question_gpt_summary_original":"The user is having trouble getting the Dask scheduler to work properly on a remote with 4 GPUs. After staging trials on the remote, the scheduler places all 4 models and training processes on all 4 GPUs, but only begins training on 1 GPU. The user expected the scheduler to assign the trials to available GPUs and train them concurrently, and automatically run the pending operation when a GPU becomes free. The user is unsure if they missed a manual step or misunderstood the scheduler's capabilities.",
        "Question_gpt_summary":"user have troubl get dask schedul work properli remot gpu stage trial remot schedul place model train process gpu begin train gpu user expect schedul assign trial avail gpu train concurr automat run pend oper gpu free user unsur miss manual step misunderstood schedul capabl"
    },
    {
        "Question_title":"Is it possible to predict in sagemaker without using s3",
        "Question_body":"<p>I have a .pkl which I would like to put into production. I would like to do a daily query of my SQL server and do a prediction on about 1000 rows. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">documentation<\/a> implies I have to load the daily data into s3. Is there a way around this? It should be able to fit in memory no problem. <\/p>\n\n<p>The answer to \" <a href=\"https:\/\/stackoverflow.com\/questions\/48319893\/is-there-some-kind-of-persistent-local-storage-in-aws-sagemaker-model-training\">is there some kind of persistent local storage in aws sagemaker model training?<\/a> \" says that \"<em>The notebook instance is coming with a local EBS (5GB) that you can use to copy some data into it and run the fast development iterations without copying the data every time from S3.<\/em>\" The 5GB could be enough but I am not sure you can actually run from a notebook in this manner. If I set up a VPN could I just query using pyodbc?<\/p>\n\n<p>Is there sagemaker integration with AWS Lambda? That in combination with a docker container would suit my needs.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1531871216253,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1531893078607,
        "Question_score_count":2.0,
        "Question_view_count":1576.0,
        "Poster_created_time":1400315847156,
        "Poster_location":null,
        "Poster_reputation_count":4342.0,
        "Poster_view_count":315.0,
        "Answer_body":"<p>While you need to to specify a s3 \"folder\" as input, this folder can contain only a dummy file. \nAlso if you bring your own docker container for training like in <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">this example<\/a>, you can do pretty much everthing in it. So you could do your daily query inside your docker container, because they have access to the internet. <\/p>\n\n<p>Also inside this container you have access to all the other aws services. Your access is defined by the role you're passing to your training job.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1531919252070,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1517578984080,
        "Answerer_location":null,
        "Answerer_reputation_count":2090.0,
        "Answerer_view_count":163.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51391639",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"possibl predict pkl like product like daili queri sql server predict row document impli load daili data wai abl fit memori problem answer kind persist local storag model train sai notebook instanc come local eb us copi data run fast develop iter copi data time sure actual run notebook manner set vpn queri pyodbc integr aw lambda combin docker contain suit need",
        "Question_preprocessed_content":"possibl predict pkl like product like daili queri sql server predict row document impli load daili data wai abl fit memori problem answer kind persist local storag model train sai notebook instanc come local eb us copi data run fast develop iter copi data time sure actual run notebook manner set vpn queri pyodbc integr aw lambda combin docker contain suit need",
        "Question_gpt_summary_original":"The user has a .pkl file that they want to put into production for daily predictions on about 1000 rows from their SQL server. The documentation implies that the daily data needs to be loaded into s3, but the user wants to know if there is a way around this since the data can fit in memory. They are considering using a local EBS or setting up a VPN to query using pyodbc. They are also wondering if there is sagemaker integration with AWS Lambda and a docker container that would suit their needs.",
        "Question_gpt_summary":"user pkl file want product daili predict row sql server document impli daili data need load user want know wai data fit memori consid local eb set vpn queri pyodbc wonder integr aw lambda docker contain suit need"
    },
    {
        "Question_title":"Dropping Python 3.5 support in the upcoming release (1.14.0)",
        "Question_body":"Hi all,\n\n\nWe're planning to drop Python 3.5 support in the upcoming release (1.14.0):\nhttps:\/\/github.com\/mlflow\/mlflow\/issues\/3984\n\n\nPlease let us know if you have any issues with this plan.\n\n\n\nBest regards,\nHarutaka",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1611175955000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":null,
        "Question_view_count":10.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/po594D8-90g",
        "Tool":"MLflow",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"drop python support upcom releas plan drop python support upcom releas http github com issu let know issu plan best regard harutaka",
        "Question_preprocessed_content":"drop python support upcom releas plan drop python support upcom releas let know issu plan best regard harutaka",
        "Question_gpt_summary_original":"The MLflow team is planning to drop support for Python 3.5 in the upcoming release (1.14.0) and is seeking feedback from users who may be affected by this decision.",
        "Question_gpt_summary":"team plan drop support python upcom releas seek feedback user affect decis"
    },
    {
        "Question_title":"Custom Dockerfile on Azure Environment with python poetry",
        "Question_body":"<p>I am new to docker and environments. This could be basics but i have been trying to install packages in my pyproject.toml file in Dockerfile without success.   <\/p>\n<p>I have tried using poetry to export requirements.txt file  and using it with the   <br \/>\nEnvironment.from_pip_requirements('requirements.txt') function and a Dockerfile.   <\/p>\n<p>But could there be any elegant solution to use toml file directly for creating a custom environment ?  <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1638821054557,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>Thanks for the response, <a href=\"\/users\/na\/?userid=1cea772e-bffd-0003-0000-000000000000\">@Ram R  <\/a>     <br \/>\nUsing  the Dockerfile :     <\/p>\n<pre><code>FROM python:3.8-slim-buster  \nENV PYTHONUNBUFFERED=1 \\  \n    PYTHONDONTWRITEBYTECODE=1 \\  \n    PIP_NO_CACHE_DIR=1 \\  \n    PIP_DISABLE_PIP_VERSION_CHECK=1 \\  \n    POETRY_VERSION=1.1.7 \\  \n    PYLINT_VERSION=2.9.4  \n  \nRUN pip install pylint==$PYLINT_VERSION \\  \n    &amp;&amp; pip install &quot;poetry==$POETRY_VERSION&quot;   \n  \nCOPY pyproject.toml .\/  \nRUN poetry config virtualenvs.create false   \n<\/code><\/pre>\n",
        "Answer_comment_count":1.0,
        "Question_closed_time":1638981076963,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/653688\/custom-dockerfile-on-azure-environment-with-python",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"custom dockerfil azur environ python poetri new docker environ basic try instal packag pyproject toml file dockerfil success tri poetri export requir txt file environ pip requir requir txt function dockerfil eleg solut us toml file directli creat custom environ",
        "Question_preprocessed_content":"custom dockerfil azur environ python poetri new docker environ basic try instal packag file dockerfil success tri poetri export file function dockerfil eleg solut us toml file directli creat custom environ",
        "Question_gpt_summary_original":"The user is having trouble installing packages in their pyproject.toml file in a Dockerfile using poetry. They have tried exporting a requirements.txt file and using it with the Environment.from_pip_requirements function, but are looking for a more elegant solution to use the toml file directly for creating a custom environment.",
        "Question_gpt_summary":"user have troubl instal packag pyproject toml file dockerfil poetri tri export requir txt file environ pip requir function look eleg solut us toml file directli creat custom environ"
    },
    {
        "Question_title":"Public read-only s3 remote",
        "Question_body":"<p>Hey folks,<\/p>\n<p>The DVC <a href=\"https:\/\/github.com\/iterative\/example-get-started\" rel=\"nofollow noopener\">getting started example<\/a> has a read-only HTTP remote.<\/p>\n<p>Is it possible to do the same with S3 so the public can have read-only permission? I believe I\u2019ve set my bucket permissions correctly* (I can download w\/o AWS creds), but when I try <code>dvc pull<\/code> w\/o creds I get an error:<\/p>\n<pre><code class=\"lang-auto\">$ dvc pull\nERROR: unexpected error - Unable to locate credentials \n<\/code><\/pre>\n<p>It looks like <code>dvc pull<\/code> is checking for credentials regardless.<\/p>\n<ul>\n<li>\n<ul>\n<li>allowing s3:GetObject and s3:GetObject<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>If this isn\u2019t possible, does anyone have a suggestion on the easiest approach to allow read-only access to DVC data analogous to a public GitHub repo?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1587508457422,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":8.0,
        "Question_view_count":1836.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/public-read-only-s3-remote\/355",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"public read remot hei folk get start exampl read http remot possibl public read permiss believ iv set bucket permiss correctli download aw cred try pull cred error pull error unexpect error unabl locat credenti look like pull check credenti regardless allow getobject getobject isnt possibl suggest easiest approach allow read access data analog public github repo thank",
        "Question_preprocessed_content":"public remot hei folk get start exampl http remot possibl public permiss believ iv set bucket permiss correctli try cred error look like check credenti regardless allow getobject getobject isnt possibl suggest easiest approach allow access data analog public github repo thank",
        "Question_gpt_summary_original":"The user is trying to set up a read-only S3 remote for DVC, but is encountering an error when attempting to pull data without AWS credentials. The user has set bucket permissions correctly and is wondering if it is possible to allow public read-only access to DVC data similar to a public GitHub repo.",
        "Question_gpt_summary":"user try set read remot encount error attempt pull data aw credenti user set bucket permiss correctli wonder possibl allow public read access data similar public github repo"
    },
    {
        "Question_title":"Understanding Intermediate Values and Pruning in Optuna",
        "Question_body":"<p>I am just curious for more information on what an intermediate step actually is and how to use pruning if you're using a different ml library that isn't in the tutorial section eg) XGB, Pytorch etc.<\/p>\n<p>For example:<\/p>\n<pre><code>X, y = load_iris(return_X_y=True)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y)\nclasses = np.unique(y)\nn_train_iter = 100\n\ndef objective(trial):\n    global num_pruned\n    alpha = trial.suggest_float(&quot;alpha&quot;, 0.0, 1.0)\n    clf = SGDClassifier(alpha=alpha)\n    for step in range(n_train_iter):\n        clf.partial_fit(X_train, y_train, classes=classes)\n\n        intermediate_value = clf.score(X_valid, y_valid)\n        trial.report(intermediate_value, step)\n\n        if trial.should_prune():\n            raise optuna.TrialPruned()\n\n    return clf.score(X_valid, y_valid)\n\n\nstudy = optuna.create_study(\n    direction=&quot;maximize&quot;,\n    pruner=optuna.pruners.HyperbandPruner(\n        min_resource=1, max_resource=n_train_iter, reduction_factor=3\n    ),\n)\nstudy.optimize(objective, n_trials=30)\n<\/code><\/pre>\n<p>What is the point of the <code>for step in range()<\/code> section? Doesn't doing this just make the optimisation take more time and won't you yield the same result for every step in the loop?<\/p>\n<p>I'm really trying to figure out the need for <code>for step in range()<\/code> and is it required every time you wish to use pruning?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1637069214753,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":841.0,
        "Poster_created_time":1482279816096,
        "Poster_location":null,
        "Poster_reputation_count":138.0,
        "Poster_view_count":28.0,
        "Answer_body":"<p>The basic model creation can be done by passing a complete training datasets once. But there are models that can still be improved (an increase in accuracy) by re-training again on the same training datasets.<\/p>\n<p>To see to it that we are not wasting resources here, we would check the accuracy after every step using the validation datasets via <code>intermediate_score<\/code> if accuracy improves, if not we prune the whole trial skipping other steps. Then we go for next trial asking another value of alpha - the hyperparameter that we are trying to determine to have the greatest accuracy on the validation datasets.<\/p>\n<p>For other libraries, it is just a matter of asking ourselves what do we want with our model, accuracy for sure is a good criteria to measure the model's competency. There can be others.<\/p>\n<p>Example optuna pruning, I want the model to continue re-training but only at my specific conditions. If intermediate value cannot defeat my best_accuracy and if steps are already more than half of my max iteration then prune this trial.<\/p>\n<pre><code>best_accuracy = 0.0\n\n\ndef objective(trial):\n    global best_accuracy\n\n    alpha = trial.suggest_float(&quot;alpha&quot;, 0.0, 1.0)\n    clf = SGDClassifier(alpha=alpha)\n\n    for step in range(n_train_iter):\n        clf.partial_fit(X_train, y_train, classes=classes)\n\n        if step &gt; n_train_iter\/\/2:\n            intermediate_value = clf.score(X_valid, y_valid)\n\n            if intermediate_value &lt; best_accuracy:\n                raise optuna.TrialPruned()\n\n    best_accuracy = clf.score(X_valid, y_valid)\n\n    return best_accuracy\n<\/code><\/pre>\n<p>Optuna has specialized pruners at <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/pruners.html\" rel=\"nofollow noreferrer\">https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/pruners.html<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Question_closed_time":1637119128723,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1634692867416,
        "Answerer_location":"Philippines",
        "Answerer_reputation_count":3105.0,
        "Answerer_view_count":290.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69990009",
        "Tool":"Optuna",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"understand intermedi valu prune curiou inform intermedi step actual us prune differ librari isn tutori section xgb pytorch exampl load iri return true train valid train valid train test split class uniqu train iter def object trial global num prune alpha trial suggest float alpha clf sgdclassifi alpha alpha step rang train iter clf partial fit train train class class intermedi valu clf score valid valid trial report intermedi valu step trial prune rais trialprun return clf score valid valid studi creat studi direct maxim pruner pruner hyperbandprun min resourc max resourc train iter reduct factor studi optim object trial point step rang section optimis time won yield result step loop try figur need step rang requir time wish us prune",
        "Question_preprocessed_content":"understand intermedi valu prune curiou inform intermedi step actual us prune differ librari isn tutori section xgb pytorch exampl point section optimis time won yield result step loop try figur need requir time wish us prune",
        "Question_gpt_summary_original":"The user is seeking clarification on intermediate values and pruning in Optuna. They are specifically curious about the purpose of the \"for step in range()\" section in the code and whether it is necessary for using pruning. They also want to know how to use pruning with ML libraries other than those in the tutorial section, such as XGB and Pytorch.",
        "Question_gpt_summary":"user seek clarif intermedi valu prune specif curiou purpos step rang section code necessari prune want know us prune librari tutori section xgb pytorch"
    },
    {
        "Question_title":"Deploy Azure ML Designer Web Service model",
        "Question_body":"<p>Im trying to run R code to detect outliers in datasets.\nIm trying to publish as a webservice so it can be used to pre process data.<\/p>\n\n<p>In Azure ML Designer,I have a Execute R script which contains my model.<\/p>\n\n<p>When I try to publish as a web service I get the error \"Cannot create inference because there is no model on this pipeline\".\nI see python has a Create Python module.<\/p>\n\n<p>Can I use an Execute R Script to create a webservice with my model?<\/p>\n\n<p>Im good at R and still learning Azure and would really appreciate some advice on. the above.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1583419993597,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":244.0,
        "Poster_created_time":1542820108076,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60548089",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"deploi design web servic model try run code detect outlier dataset try publish webservic pre process data design execut script contain model try publish web servic error creat infer model pipelin python creat python modul us execut script creat webservic model good learn azur appreci advic",
        "Question_preprocessed_content":"deploi design web servic model try run code detect outlier dataset try publish webservic pre process data design execut script contain model try publish web servic error creat infer model pipelin python creat python modul us execut script creat webservic model good learn azur appreci advic",
        "Question_gpt_summary_original":"The user is trying to deploy an R code to detect outliers in datasets as a web service using Azure ML Designer. However, when trying to publish the web service, they receive an error message stating that there is no model on the pipeline. The user is seeking advice on whether they can use an Execute R Script to create a web service with their model.",
        "Question_gpt_summary":"user try deploi code detect outlier dataset web servic design try publish web servic receiv error messag state model pipelin user seek advic us execut script creat web servic model"
    },
    {
        "Question_title":"Add model description when registering model after hyperdrive successful run",
        "Question_body":"<p>I have successfully trained a model on Azure Machine Learning Service using Hyperdrive that has now yielded a hyperdrive run instance<\/p>\n\n<pre><code>hyperdrive_run = exp.submit(config=hypertune_config)\nhyperdrive_run\nbest_run = hyperdrive_run.get_best_run_by_primary_metric()\n<\/code><\/pre>\n\n<p>As a next step, I would like to register a model while adding a description to the model.:<\/p>\n\n<pre><code>pumps_rf = best_run.register_model(model_name='pumps_rf', model_path='outputs\/rf.pkl')\n<\/code><\/pre>\n\n<p>There is a <code>description<\/code> column in the Models section of my AML Workspace on Azure portal but the <code>register_model<\/code> method does not seem to have a <code>description<\/code> flag. So how do I go about adding a description to the model so I see it in Azure Portal?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1550539380057,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":453.0,
        "Poster_created_time":1408574571227,
        "Poster_location":"Toronto, Canada",
        "Poster_reputation_count":2754.0,
        "Poster_view_count":124.0,
        "Answer_body":"<p>Good question :).<\/p>\n\n<p>Looking at the current version of the API, it doesn't look like you can add the description using <code>Run.register_model<\/code>, as confirmed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#register-model-model-name--model-path-none--tags-none--properties-none----kwargs-\" rel=\"nofollow noreferrer\">by the docs<\/a>. <\/p>\n\n<p>You can go around this however by registering the model using the <code>Model.register<\/code> method which, fortunately, includes an argument for <code>description<\/code> as detailed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none-\" rel=\"nofollow noreferrer\">here<\/a>. In your case, you also need to <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py#download-file-name--output-file-path-none-\" rel=\"nofollow noreferrer\">download the files<\/a> first.<\/p>\n\n<p>In short, use something like:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>best_run.download_file('outputs\/rf.pkl', output_file_path='.\/rf.pkl')\n\nModel.register(workspace=ws, model_path='.\/rf.pkl', model_name=\"pumps_rf\", description=\"There are many models like it, but this one is mine.\")\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Question_closed_time":1550668517447,
        "Answer_last_edit_time":1550686459150,
        "Answer_score_count":1.0,
        "Answerer_created_time":1250158552416,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54757598",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"add model descript regist model hyperdr success run successfulli train model servic hyperdr yield hyperdr run instanc hyperdr run exp submit config hypertun config hyperdr run best run hyperdr run best run primari metric step like regist model ad descript model pump best run regist model model pump model path output pkl descript column model section aml workspac azur portal regist model method descript flag ad descript model azur portal",
        "Question_preprocessed_content":"add model descript regist model hyperdr success run successfulli train model servic hyperdr yield hyperdr run instanc step like regist model ad descript column model section aml workspac azur portal method flag ad descript model azur portal",
        "Question_gpt_summary_original":"The user has successfully trained a model on Azure Machine Learning Service using Hyperdrive and wants to register the model while adding a description to it. However, the register_model method does not seem to have a description flag, and the user is unsure how to add a description to the model so that it appears in the Azure Portal.",
        "Question_gpt_summary":"user successfulli train model servic hyperdr want regist model ad descript regist model method descript flag user unsur add descript model appear azur portal"
    },
    {
        "Question_title":"Serving models from mlflow registry to sagemaker",
        "Question_body":"<p>I have an mlflow server running locally and being exposed at port 80. I also have a model in the mlflow registry and I want to deploy it using the <code>mlflow sagemaker run-local<\/code> because after testing this locally, I am going to deploy everything to AWS and Sagemaker. My problem is that when I run:<\/p>\n<pre><code>export MODEL_PATH=models:\/churn-lgb-test\/2\nexport LOCAL_PORT=8000\nmlflow sagemaker run-local -m $MODEL_PATH -p $LOCAL_PORT -f python_function -i splicemachine\/mlflow-pyfunc:1.6.0\n<\/code><\/pre>\n<p>it starts the container and I immediately get this error:<\/p>\n<pre><code>2020-07-27 13:02:13 +0000] [827] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [828] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [828] [INFO] Worker exiting (pid: 828)\n[2020-07-27 13:02:13 +0000] [827] [INFO] Worker exiting (pid: 827)\n[2020-07-27 13:02:13 +0000] [829] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [829] [INFO] Worker exiting (pid: 829)\n[2020-07-27 13:02:13 +0000] [830] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [830] [INFO] Worker exiting (pid: 830)\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 209, in run\n    self.sleep()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 357, in sleep\n    ready = select.select([self.PIPE[0]], [], [], 1.0)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n    self.reap_workers()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n    raise HaltServer(reason, self.WORKER_BOOT_ERROR)\ngunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/bin\/gunicorn&quot;, line 8, in &lt;module&gt;\n    sys.exit(run())\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in run\n    WSGIApplication(&quot;%(prog)s [OPTIONS] [APP_MODULE]&quot;).run()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 228, in run\n    super().run()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 72, in run\n    Arbiter(self).run()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 229, in run\n    self.halt(reason=inst.reason, exit_status=inst.exit_status)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 342, in halt\n    self.stop()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 393, in stop\n    time.sleep(0.1)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n    self.reap_workers()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n    raise HaltServer(reason, self.WORKER_BOOT_ERROR)\ngunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\ncreating and activating custom environment\nGot sigterm signal, exiting.\n[2020-07-27 13:02:13 +0000] [831] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [831] [INFO] Worker exiting (pid: 831)\n[2020-07-27 13:02:14 +0000] [833] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:14 +0000] [833] [INFO] Worker exiting (pid: 833)\n[2020-07-27 13:02:14 +0000] [832] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:14 +0000] [832] [INFO] Worker exiting (pid: 832)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1595856379060,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":429.0,
        "Poster_created_time":1591385263856,
        "Poster_location":null,
        "Poster_reputation_count":21.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63116338",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"serv model registri server run local expos port model registri want deploi run local test local go deploi aw problem run export model path model churn lgb test export local port run local model path local port python function splicemachin pyfunc start contain immedi error error except worker process traceback recent file miniconda env custom env lib python site packag gunicorn arbit line spawn worker worker init process file miniconda env custom env lib python site packag gunicorn worker ggevent line init process super init process file miniconda env custom env lib python site packag gunicorn worker base line init process self load wsgi file miniconda env custom env lib python site packag gunicorn worker base line load wsgi self wsgi self app wsgi file miniconda env custom env lib python site packag gunicorn app base line wsgi self callabl self load file miniconda env custom env lib python site packag gunicorn app wsgiapp line load return self load wsgiapp file miniconda env custom env lib python site packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file miniconda env custom env lib python site packag gunicorn util line import app mod importlib import modul modul file miniconda env custom env lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file miniconda env custom env lib python site packag model contain score server wsgi line app score server init pyfunc load model opt model file miniconda env custom env lib python site packag pyfunc init line load model return importlib import modul conf main load pyfunc data path file miniconda env custom env lib python site packag sklearn line load pyfunc return load model local file path file miniconda env custom env lib python site packag sklearn line load model local file open path isadirectoryerror errno directori opt model error except worker process traceback recent file miniconda env custom env lib python site packag gunicorn arbit line spawn worker worker init process file miniconda env custom env lib python site packag gunicorn worker ggevent line init process super init process file miniconda env custom env lib python site packag gunicorn worker base line init process self load wsgi file miniconda env custom env lib python site packag gunicorn worker base line load wsgi self wsgi self app wsgi file miniconda env custom env lib python site packag gunicorn app base line wsgi self callabl self load file miniconda env custom env lib python site packag gunicorn app wsgiapp line load return self load wsgiapp file miniconda env custom env lib python site packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file miniconda env custom env lib python site packag gunicorn util line import app mod importlib import modul modul file miniconda env custom env lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file miniconda env custom env lib python site packag model contain score server wsgi line app score server init pyfunc load model opt model file miniconda env custom env lib python site packag pyfunc init line load model return importlib import modul conf main load pyfunc data path file miniconda env custom env lib python site packag sklearn line load pyfunc return load model local file path file miniconda env custom env lib python site packag sklearn line load model local file open path isadirectoryerror errno directori opt model info worker exit pid info worker exit pid error except worker process traceback recent file miniconda env custom env lib python site packag gunicorn arbit line spawn worker worker init process file miniconda env custom env lib python site packag gunicorn worker ggevent line init process super init process file miniconda env custom env lib python site packag gunicorn worker base line init process self load wsgi file miniconda env custom env lib python site packag gunicorn worker base line load wsgi self wsgi self app wsgi file miniconda env custom env lib python site packag gunicorn app base line wsgi self callabl self load file miniconda env custom env lib python site packag gunicorn app wsgiapp line load return self load wsgiapp file miniconda env custom env lib python site packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file miniconda env custom env lib python site packag gunicorn util line import app mod importlib import modul modul file miniconda env custom env lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file miniconda env custom env lib python site packag model contain score server wsgi line app score server init pyfunc load model opt model file miniconda env custom env lib python site packag pyfunc init line load model return importlib import modul conf main load pyfunc data path file miniconda env custom env lib python site packag sklearn line load pyfunc return load model local file path file miniconda env custom env lib python site packag sklearn line load model local file open path isadirectoryerror errno directori opt model info worker exit pid error except worker process traceback recent file miniconda env custom env lib python site packag gunicorn arbit line spawn worker worker init process file miniconda env custom env lib python site packag gunicorn worker ggevent line init process super init process file miniconda env custom env lib python site packag gunicorn worker base line init process self load wsgi file miniconda env custom env lib python site packag gunicorn worker base line load wsgi self wsgi self app wsgi file miniconda env custom env lib python site packag gunicorn app base line wsgi self callabl self load file miniconda env custom env lib python site packag gunicorn app wsgiapp line load return self load wsgiapp file miniconda env custom env lib python site packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file miniconda env custom env lib python site packag gunicorn util line import app mod importlib import modul modul file miniconda env custom env lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file miniconda env custom env lib python site packag model contain score server wsgi line app score server init pyfunc load model opt model file miniconda env custom env lib python site packag pyfunc init line load model return importlib import modul conf main load pyfunc data path file miniconda env custom env lib python site packag sklearn line load pyfunc return load model local file path file miniconda env custom env lib python site packag sklearn line load model local file open path isadirectoryerror errno directori opt model info worker exit pid traceback recent file miniconda env custom env lib python site packag gunicorn arbit line run self sleep file miniconda env custom env lib python site packag gunicorn arbit line sleep readi select select self pipe file miniconda env custom env lib python site packag gunicorn arbit line handl chld self reap worker file miniconda env custom env lib python site packag gunicorn arbit line reap worker rais haltserv reason self worker boot error gunicorn error haltserv handl except except occur traceback recent file miniconda env custom env bin gunicorn line sy exit run file miniconda env custom env lib python site packag gunicorn app wsgiapp line run wsgiapplic prog option app modul run file miniconda env custom env lib python site packag gunicorn app base line run super run file miniconda env custom env lib python site packag gunicorn app base line run arbit self run file miniconda env custom env lib python site packag gunicorn arbit line run self halt reason inst reason exit statu inst exit statu file miniconda env custom env lib python site packag gunicorn arbit line halt self stop file miniconda env custom env lib python site packag gunicorn arbit line stop time sleep file miniconda env custom env lib python site packag gunicorn arbit line handl chld self reap worker file miniconda env custom env lib python site packag gunicorn arbit line reap worker rais haltserv reason self worker boot error gunicorn error haltserv creat activ custom environ got sigterm signal exit error except worker process traceback recent file miniconda env custom env lib python site packag gunicorn arbit line spawn worker worker init process file miniconda env custom env lib python site packag gunicorn worker ggevent line init process super init process file miniconda env custom env lib python site packag gunicorn worker base line init process self load wsgi file miniconda env custom env lib python site packag gunicorn worker base line load wsgi self wsgi self app wsgi file miniconda env custom env lib python site packag gunicorn app base line wsgi self callabl self load file miniconda env custom env lib python site packag gunicorn app wsgiapp line load return self load wsgiapp file miniconda env custom env lib python site packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file miniconda env custom env lib python site packag gunicorn util line import app mod importlib import modul modul file miniconda env custom env lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file miniconda env custom env lib python site packag model contain score server wsgi line app score server init pyfunc load model opt model file miniconda env custom env lib python site packag pyfunc init line load model return importlib import modul conf main load pyfunc data path file miniconda env custom env lib python site packag sklearn line load pyfunc return load model local file path file miniconda env custom env lib python site packag sklearn line load model local file open path isadirectoryerror errno directori opt model info worker exit pid error except worker process traceback recent file miniconda env custom env lib python site packag gunicorn arbit line spawn worker worker init process file miniconda env custom env lib python site packag gunicorn worker ggevent line init process super init process file miniconda env custom env lib python site packag gunicorn worker base line init process self load wsgi file miniconda env custom env lib python site packag gunicorn worker base line load wsgi self wsgi self app wsgi file miniconda env custom env lib python site packag gunicorn app base line wsgi self callabl self load file miniconda env custom env lib python site packag gunicorn app wsgiapp line load return self load wsgiapp file miniconda env custom env lib python site packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file miniconda env custom env lib python site packag gunicorn util line import app mod importlib import modul modul file miniconda env custom env lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file miniconda env custom env lib python site packag model contain score server wsgi line app score server init pyfunc load model opt model file miniconda env custom env lib python site packag pyfunc init line load model return importlib import modul conf main load pyfunc data path file miniconda env custom env lib python site packag sklearn line load pyfunc return load model local file path file miniconda env custom env lib python site packag sklearn line load model local file open path isadirectoryerror errno directori opt model info worker exit pid error except worker process traceback recent file miniconda env custom env lib python site packag gunicorn arbit line spawn worker worker init process file miniconda env custom env lib python site packag gunicorn worker ggevent line init process super init process file miniconda env custom env lib python site packag gunicorn worker base line init process self load wsgi file miniconda env custom env lib python site packag gunicorn worker base line load wsgi self wsgi self app wsgi file miniconda env custom env lib python site packag gunicorn app base line wsgi self callabl self load file miniconda env custom env lib python site packag gunicorn app wsgiapp line load return self load wsgiapp file miniconda env custom env lib python site packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file miniconda env custom env lib python site packag gunicorn util line import app mod importlib import modul modul file miniconda env custom env lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file miniconda env custom env lib python site packag model contain score server wsgi line app score server init pyfunc load model opt model file miniconda env custom env lib python site packag pyfunc init line load model return importlib import modul conf main load pyfunc data path file miniconda env custom env lib python site packag sklearn line load pyfunc return load model local file path file miniconda env custom env lib python site packag sklearn line load model local file open path isadirectoryerror errno directori opt model info worker exit pid",
        "Question_preprocessed_content":"serv model registri server run local expos port model registri want deploi test local go deploi aw problem run start contain immedi error",
        "Question_gpt_summary_original":"the user encountered a challenge of serving models from a registry to , resulting in an \"isadirectoryerror\" when attempting to run the container.",
        "Question_gpt_summary":"user encount challeng serv model registri result isadirectoryerror attempt run contain"
    },
    {
        "Question_title":"Getting model metadata from SageMaker model endpoint",
        "Question_body":"<p>I have a TF model object deployed to SageMaker endpoint, and it's working fine when I invoke it to make a prediction. On its own, the model object itself contains key attributes that is accessible if I open it with <code>h5py.File()<\/code> like this:<\/p>\n<pre><code>with h5py.File(model_path2, 'r') as f:\n    labels = [n.decode(&quot;ascii&quot;, &quot;ignore&quot;) for n in f['labels']]\n    img_norm_vec = np.array(f['norm_vector'])\n<\/code><\/pre>\n<p>My question is, can I access the metadata attributes from a SM endpoint? I searched through the SM documentation and didn't see anything related to this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1631212272073,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":90.0,
        "Poster_created_time":1465664684867,
        "Poster_location":null,
        "Poster_reputation_count":101.0,
        "Poster_view_count":11.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69123156",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"get model metadata model endpoint model object deploi endpoint work fine invok predict model object contain kei attribut access open hpy file like hpy file model path label decod ascii ignor label img norm vec arrai norm vector question access metadata attribut endpoint search document relat",
        "Question_preprocessed_content":"get model metadata model endpoint model object deploi endpoint work fine invok predict model object contain kei attribut access open like question access metadata attribut endpoint search document relat",
        "Question_gpt_summary_original":"The user has deployed a TensorFlow model object to a SageMaker endpoint and is able to make predictions successfully. However, they are unable to access the metadata attributes of the model object from the endpoint and are seeking a solution.",
        "Question_gpt_summary":"user deploi tensorflow model object endpoint abl predict successfulli unabl access metadata attribut model object endpoint seek solut"
    },
    {
        "Question_title":"DataBricks + Kedro Vs GCP + Kubeflow Vs Server + Kedro + Airflow",
        "Question_body":"<p>We are deploying a data consortium between more than 10 companies. Wi will deploy several machine learning models (in general advanced analytics models) for all the companies and we will administrate all the models. We are looking for a solution that administrates several servers, clusters and data science pipelines. I love kedro, but not sure what is the best option to administrate all while using kedro.<\/p>\n<p>In summary, we are looking for the best solution to administrate several models, tasks and pipelines in different servers and possibly Spark clusters. Our current options are:<\/p>\n<ul>\n<li><p>AWS as our data warehouse and Databricks for administrating servers, clusters and tasks. I don't feel that the notebooks of databricks are a good solution for building pipelines and to work collaboratively, so I would like to connect kedro to databricks (is it good? is it easy to schedule the run of the kedro pipelines using databricks?)<\/p>\n<\/li>\n<li><p>Using GCP for data warehouse and use kubeflow (iin GCP) for deploying models and the administration and the schedule of the pipelines and the needed resources<\/p>\n<\/li>\n<li><p>Setting up servers from ASW or GCP, install kedro and schedule the pipelines with airflow (I see a big problem administrating 20 servers and 40 pipelines)<\/p>\n<\/li>\n<\/ul>\n<p>I would like to know if someone knows what is the best option between these alternatives, their  downsides and advantages, or if there are more possibilities.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1605830422657,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1605836750283,
        "Question_score_count":4.0,
        "Question_view_count":967.0,
        "Poster_created_time":1605828724552,
        "Poster_location":null,
        "Poster_reputation_count":59.0,
        "Poster_view_count":5.0,
        "Answer_body":"<p>I'll try and summarise what I know, but be aware that I've not been part of a KubeFlow project.<\/p>\n<h2>Kedro on Databricks<\/h2>\n<p>Our approach was to build our project with CI and then execute the pipeline from a notebook. We <em>did not<\/em> use the <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/11_tools_integration\/03_databricks.html\" rel=\"nofollow noreferrer\">kedro recommended approach<\/a> of using databricks-connect due to the <a href=\"https:\/\/databricks.com\/product\/aws-pricing\" rel=\"nofollow noreferrer\">large price difference<\/a> between Jobs and Interactive Clusters (which are needed for DB-connect). If you're working on several TB's of data, this quickly becomes relevant.<\/p>\n<p>As a DS, this approach may feel natural, as a SWE though it does not. Running pipelines in notebooks feels hacky. It works but it feels non-industrialised. Databricks performs well in automatically spinning up and down clusters &amp; taking care of the runtime for you. So their value add is abstracting IaaS away from you (more on that later).<\/p>\n<h2>GCP &amp; &quot;Cloud Native&quot;<\/h2>\n<p><strong>Pro<\/strong>: GCP's main selling point is BigQuery. It is an incredibly powerful platform, simply because you can be productive from day 0. I've seen people build entire web API's on top of it. KubeFlow isn't tied to GCP so you could port this somewhere else later on. Kubernetes will also allow you to run anything else you wish on the cluster, API's, streaming, web services, websites, you name it.<\/p>\n<p><strong>Con<\/strong>: Kubernetes is complex. If you have 10+ engineers to run this project long-term, you should be OK. But don't underestimate the complexity of Kubernetes. It is to the cloud what Linux is to the OS world. Think log management, noisy neighbours (one cluster for web APIs + batch spark jobs), multi-cluster management (one cluster per department\/project), security, resource access etc.<\/p>\n<h2>IaaS server approach<\/h2>\n<p>Your last alternative, the manual installation of servers is one I would recommend only if you have a large team, extremely large data and are building a long-term product who's revenue can sustain the large maintenance costs.<\/p>\n<h2>The people behind it<\/h2>\n<p>How does the talent market look like in your region? If you can hire experienced engineers with GCP knowledge, I'd go for the 2nd solution. GCP is a mature, &quot;native&quot; platform in the sense that it abstracts a lot away for customers. If your market has mainly AWS engineers, that may be a better road to take. If you have a number of kedro engineers, that also has relevance. Note that kedro is agnostic enough to run anywhere. It's really just python code.<\/p>\n<p><strong>Subjective advise<\/strong>:<\/p>\n<p>Having worked mostly on AWS projects and a few GCP projects, I'd go for GCP. I'd use the platform's components (BigQuery, Cloud Run, PubSub, Functions, K8S) as a toolbox to choose from and build an organisation around that. Kedro can run in any of these contexts, as a triggered job by the Scheduler, as a container on Kubernetes or as a ETL pipeline bringing data into (or out of) BigQuery.<\/p>\n<p>While Databricks is &quot;less management&quot; than raw AWS, it's still servers to think about and VPC networking charges to worry over. BigQuery is simply GB queried. Functions are simply invocation count. These high level components will allow you to quickly show value to customers and you only need to go deeper (RaaS -&gt; PaaS -&gt; IaaS) as you scale.<\/p>\n<p>AWS also has these higher level abstractions over IaaS but in general, it appears (to me) that Google's offering is the most mature. Mainly because they have published tools they've been using internally for almost a decade whereas AWS has built new tools for the market. AWS is the king of IaaS though.<\/p>\n<p>Finally, a bit of content, <a href=\"https:\/\/youtu.be\/kjhXMTOLtac?t=618\" rel=\"nofollow noreferrer\">two former colleagues have discussed ML industrialisation frameworks earlier this fall<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1605891844630,
        "Answer_last_edit_time":1605947117163,
        "Answer_score_count":4.0,
        "Answerer_created_time":1327570314367,
        "Answerer_location":"Berlin, Germany",
        "Answerer_reputation_count":2854.0,
        "Answerer_view_count":324.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64921833",
        "Tool":"Kedro",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"databrick gcp kubeflow server airflow deploi data consortium compani deploi machin learn model gener advanc analyt model compani administr model look solut administr server cluster data scienc pipelin love sure best option administr summari look best solut administr model task pipelin differ server possibl spark cluster current option aw data warehous databrick administr server cluster task feel notebook databrick good solut build pipelin work collabor like connect databrick good easi schedul run pipelin databrick gcp data warehous us kubeflow iin gcp deploi model administr schedul pipelin need resourc set server asw gcp instal schedul pipelin airflow big problem administr server pipelin like know know best option altern downsid advantag possibl",
        "Question_preprocessed_content":"databrick gcp kubeflow server airflow deploi data consortium compani deploi machin learn model compani administr model look solut administr server cluster data scienc pipelin love sure best option administr summari look best solut administr model task pipelin differ server possibl spark cluster current option aw data warehous databrick administr server cluster task feel notebook databrick good solut build pipelin work collabor like connect databrick gcp data warehous us kubeflow deploi model administr schedul pipelin need resourc set server asw gcp instal schedul pipelin airflow like know know best option altern downsid advantag possibl",
        "Question_gpt_summary_original":"The user is looking for a solution to administer several machine learning models and pipelines for a data consortium between more than 10 companies. They are considering using AWS with Databricks and Kedro, GCP with Kubeflow, or setting up servers with Kedro and Airflow. They are seeking advice on the best option and its advantages and disadvantages.",
        "Question_gpt_summary":"user look solut administ machin learn model pipelin data consortium compani consid aw databrick gcp kubeflow set server airflow seek advic best option advantag disadvantag"
    },
    {
        "Question_title":"How to use kfp Artifact with sklearn?",
        "Question_body":"<p>I'm trying to develop a custom pipeline with kubeflow pipeline (kfp) components inside Vertex AI (Google Cloud Platform). The steps of the pipeline are:<\/p>\n<ol>\n<li>read data from a big query table<\/li>\n<li>create a pandas <code>DataFrame<\/code><\/li>\n<li>use the <code>DataFrame<\/code> to train a K-Means model<\/li>\n<li>deploy the model to an endpoint<\/li>\n<\/ol>\n<p>Here there is the code of the step 2. I had to use <code>Output[Artifact]<\/code> as output because <code>pd.DataFrame<\/code> type that I found <a href=\"https:\/\/stackoverflow.com\/questions\/43890844\/pythonic-type-hints-with-pandas\">here<\/a> did not work.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(base_image=&quot;python:3.9&quot;, packages_to_install=[&quot;google-cloud-bigquery&quot;,&quot;pandas&quot;,&quot;pyarrow&quot;])\ndef create_dataframe(\n    project: str,\n    region: str,\n    destination_dataset: str,\n    destination_table_name: str,\n    df: Output[Artifact],\n):\n    \n    from google.cloud import bigquery\n    \n    client = bigquery.Client(project=project, location=region)\n    dataset_ref = bigquery.DatasetReference(project, destination_dataset)\n    table_ref = dataset_ref.table(destination_table_name)\n    table = client.get_table(table_ref)\n\n    df = client.list_rows(table).to_dataframe()\n<\/code><\/pre>\n<p>Here the code of the step 3:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(base_image=&quot;python:3.9&quot;, packages_to_install=['sklearn'])\ndef kmeans_training(\n        dataset: Input[Artifact],\n        model: Output[Model],\n        num_clusters: int,\n):\n    from sklearn.cluster import KMeans\n    model = KMeans(num_clusters, random_state=220417)\n    model.fit(dataset)\n<\/code><\/pre>\n<p>The run of the pipeline is stopped due to the following error:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>TypeError: float() argument must be a string or a number, not 'Artifact'\n<\/code><\/pre>\n<p>Is it possible to convert Artifact to <code>numpy array<\/code> or <code>Dataframe<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1636992603783,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":339.0,
        "Poster_created_time":1616589293616,
        "Poster_location":"Alatri, Frosinone, FR",
        "Poster_reputation_count":67.0,
        "Poster_view_count":33.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69977440",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"us kfp artifact sklearn try develop custom pipelin kubeflow pipelin kfp compon insid googl cloud platform step pipelin read data big queri tabl creat panda datafram us datafram train mean model deploi model endpoint code step us output artifact output datafram type work compon base imag python packag instal googl cloud bigqueri panda pyarrow def creat datafram project str region str destin dataset str destin tabl str output artifact googl cloud import bigqueri client bigqueri client project project locat region dataset ref bigqueri datasetrefer project destin dataset tabl ref dataset ref tabl destin tabl tabl client tabl tabl ref client list row tabl datafram code step compon base imag python packag instal sklearn def kmean train dataset input artifact model output model num cluster int sklearn cluster import kmean model kmean num cluster random state model fit dataset run pipelin stop follow error typeerror float argument string number artifact possibl convert artifact numpi arrai datafram",
        "Question_preprocessed_content":"us kfp artifact sklearn try develop custom pipelin kubeflow pipelin compon insid step pipelin read data big queri tabl creat panda us train model deploi model endpoint code step us output type work code step run pipelin stop follow error possibl convert artifact",
        "Question_gpt_summary_original":"The user is developing a custom pipeline with kubeflow pipeline (kfp) components inside Vertex AI (Google Cloud Platform) to read data from a big query table, create a pandas DataFrame, use the DataFrame to train a K-Means model, and deploy the model to an endpoint. However, the run of the pipeline is stopped due to a TypeError: float() argument must be a string or a number, not 'Artifact'. The user is seeking a solution to convert Artifact to numpy array or DataFrame.",
        "Question_gpt_summary":"user develop custom pipelin kubeflow pipelin kfp compon insid googl cloud platform read data big queri tabl creat panda datafram us datafram train mean model deploi model endpoint run pipelin stop typeerror float argument string number artifact user seek solut convert artifact numpi arrai datafram"
    },
    {
        "Question_title":"Wandb for Huggingface Trainer saves only first model",
        "Question_body":"<p>I am finetuning multiple models using for loop as follows.<\/p>\n<pre><code class=\"lang-auto\">for file in os.listdir(args.data_dir):\n    finetune(args, file)\n<\/code><\/pre>\n<p>BUT <code>wandb<\/code> shows logs only for the first file in <code>data_dir<\/code> although it is training and saving models for other files. It feels very strange behavior.<\/p>\n<pre><code class=\"lang-auto\">wandb: Synced bertweet-base-finetuned-file1: https:\/\/wandb.ai\/***\/huggingface\/runs\/***\n<\/code><\/pre>\n<p>This is a small snippet of <strong>finetuning<\/strong> code with Huggingface:<\/p>\n<pre><code class=\"lang-auto\">def finetune(args, file):\n    training_args = TrainingArguments(\n        output_dir=f'{model_name}-finetuned-{file}',\n        overwrite_output_dir=True,\n        evaluation_strategy='no',\n        num_train_epochs=args.epochs,\n        learning_rate=args.lr,\n        weight_decay=args.decay,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        fp16=True, # mixed-precision training to boost speed\n        save_strategy='no',\n        seed=args.seed,\n        dataloader_num_workers=4,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset['train'],\n        eval_dataset=None,\n        data_collator=data_collator,\n    )\n    trainer.train()\n    trainer.save_model()\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1650439111790,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":200.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p><code>wandb.init(reinit=True)<\/code> and <code>run.finish()<\/code> helped me to log the models <strong>separately<\/strong> on wandb website.<\/p>\n<p>The working code looks like below:<\/p>\n<pre><code class=\"lang-auto\">\nfor file in os.listdir(args.data_dir):\n    finetune(args, file)\n\nimport wandb\ndef finetune(args, file):\n    run = wandb.init(reinit=True)\n    ...\n    run.finish()\n<\/code><\/pre>\n<p>Reference: <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script\" class=\"inline-onebox\">Launch Experiments with wandb.init - Documentation<\/a><\/p>",
        "Answer_comment_count":null,
        "Question_closed_time":1650552651672,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-for-huggingface-trainer-saves-only-first-model\/2270",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"huggingfac trainer save model finetun multipl model loop follow file listdir arg data dir finetun arg file show log file data dir train save model file feel strang behavior sync bertweet base finetun file http huggingfac run small snippet finetun code huggingfac def finetun arg file train arg trainingargu output dir model finetun file overwrit output dir true evalu strategi num train epoch arg epoch learn rate arg weight decai arg decai devic train batch size arg batch size devic eval batch size arg batch size true mix precis train boost speed save strategi seed arg seed dataload num worker trainer trainer model model arg train arg train dataset token dataset train eval dataset data collat data collat trainer train trainer save model",
        "Question_preprocessed_content":"huggingfac trainer save model finetun multipl model loop follow show log file train save model file feel strang behavior small snippet finetun code huggingfac",
        "Question_gpt_summary_original":"The user is facing an issue while fine-tuning multiple models using a for loop with Huggingface Trainer and Wandb. Although the code is training and saving models for all files in the data directory, Wandb is only showing logs for the first file.",
        "Question_gpt_summary":"user face issu fine tune multipl model loop huggingfac trainer code train save model file data directori show log file"
    },
    {
        "Question_title":"Unable to stop Azure ML Compute instance",
        "Question_body":"<p>I'm unable to stop my Azure ML Compute instance as it fails with the following error:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/114520-image.png?platform=QnA\" alt=\"114520-image.png\" \/>    <\/p>\n<pre><code>Failed to stop compute  \nResourceNotReady: Request failed with status code 400.  \n  \nTrace ID : 698a7727-f014-4d38-9dc1-a058158a2b09  \nClient request ID : 7f491fb9-cdd3-4a82-86f3-3eebb19b8419  \nService request ID : |00-872699f1c7573e4bbb4b130d59759cab-9f1ee7b43cfb3f4c-01.ad2a11cf_  \n<\/code><\/pre>\n<p>In DevTools in the browser, I can see this additional information:    <\/p>\n<pre><code>message&quot;:&quot;There is already an active operation submitted.&quot;  \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1626257651013,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/475562\/unable-to-stop-azure-ml-compute-instance",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unabl stop comput instanc unabl stop comput instanc fail follow error fail stop comput resourcenotreadi request fail statu code trace aab client request ffb cdd eebbb servic request fcebdcab feebcfbfc adacf devtool browser addit inform messag activ oper submit",
        "Question_preprocessed_content":"unabl stop comput instanc unabl stop comput instanc fail follow error devtool browser addit inform",
        "Question_gpt_summary_original":"The user is facing an issue while trying to stop their Azure ML Compute instance, as it fails with an error message stating \"Failed to stop compute\" and \"ResourceNotReady\". The user also sees an additional message in DevTools stating \"There is already an active operation submitted\".",
        "Question_gpt_summary":"user face issu try stop comput instanc fail error messag state fail stop comput resourcenotreadi user see addit messag devtool state activ oper submit"
    },
    {
        "Question_title":"Update live AWS Sagemaker auto scaled endpoint instance type without putting it down",
        "Question_body":"<p>I have one live AWS Sagemaker endpoint where we have auto scaled enabled. \nNow I want to updated it from 'ml.t2.xlarge' to 'ml.t2.2xlarge' but it is showing this error <\/p>\n\n<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the \nUpdateEndpoint operation: The variant(s) \"[config1]\" must be deregistered as scalable targets with \nApplication Auto Scaling before they can be removed or have their instance type updated.\n<\/code><\/pre>\n\n<p>I believe we need to first de-register auto-scaling using this link \n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html<\/a><\/p>\n\n<p>but I doubt if will take our application down and the new model with training will take multiple hours. We can't afford this so please let me know if there are any better way to do it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1582793187027,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":869.0,
        "Poster_created_time":1501403168107,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Answer_body":"<p>You should have no problem updating your Endpoint instance type without taking the availability hit. The basic method looks like this when you have an active autoscaling policy:<\/p>\n\n<ol>\n<li>Create a new EndpointConfig that uses the new instance type, <code>ml.t2.2xlarge<\/code>\n\n<ol>\n<li>Do this by calling <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\"><code>CreateEndpointConfig<\/code><\/a>.<\/li>\n<li>Pass in the same values you used for your previous Endpoint config. You can point to the same <code>ModelName<\/code> that you did as well. By reusing the same model, you don't have to retrain it or anything<\/li>\n<\/ol><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html\" rel=\"nofollow noreferrer\">Delete the existing autoscaling policy<\/a>\n\n<ol>\n<li>Depending on your autoscaling, you might want to increase the desired count of your Endpoint in the event it needs to scale while you are doing this.<\/li>\n<li>If you are experience a spike in traffic while you are making these API calls, you risk an outage of your model if it can't keep up with traffic. Just keep this in mind and possibly scale in advance for this possibility.<\/li>\n<\/ol><\/li>\n<li>Call <code>UpdateEndpoint<\/code> like you did previously and specify this new <code>EndpointConfigName<\/code><\/li>\n<li>Wait for your Endpoint status to be <code>InService<\/code>. This should take 10-20 mins.<\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-add-policy.html\" rel=\"nofollow noreferrer\">Create a new autoscaling policy<\/a> for this new Endpoint and production variant<\/li>\n<\/ol>\n\n<p>You should be good to go without sacrificing availability.<\/p>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1582835892307,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1367358004727,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":6421.0,
        "Answerer_view_count":642.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60429339",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"updat live auto scale endpoint instanc type put live endpoint auto scale enabl want updat xlarg xlarg show error botocor except clienterror error occur validationexcept call updateendpoint oper variant config deregist scalabl target applic auto scale remov instanc type updat believ need regist auto scale link http doc aw amazon com latest endpoint auto scale delet html doubt applic new model train multipl hour afford let know better wai",
        "Question_preprocessed_content":"updat live auto scale endpoint instanc type put live endpoint auto scale enabl want updat show error believ need link doubt applic new model train multipl hour afford let know better wai",
        "Question_gpt_summary_original":"The user is facing an issue while updating the instance type of a live AWS Sagemaker endpoint with auto-scaling enabled. The error message suggests that the auto-scaling needs to be de-registered before updating the instance type. However, the user is concerned that de-registering auto-scaling may take the application down and the new model with training will take multiple hours. The user is seeking a better way to update the instance type without affecting the application.",
        "Question_gpt_summary":"user face issu updat instanc type live endpoint auto scale enabl error messag suggest auto scale need regist updat instanc type user concern regist auto scale applic new model train multipl hour user seek better wai updat instanc type affect applic"
    },
    {
        "Question_title":"CDK - How to create a SageMaer Model with VPC config",
        "Question_body":"<h1>Question<\/h1>\n<p>How to create a SageMaker model with the VPC config with CDK?<\/p>\n<p>CDK SageMaker <a href=\"https:\/\/docs.aws.amazon.com\/cdk\/api\/latest\/python\/aws_cdk.aws_sagemaker\/CfnModel.html\" rel=\"nofollow noreferrer\">CfnModel<\/a> has <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/host-vpc.html#host-vpc-configure\" rel=\"nofollow noreferrer\">vpc_config<\/a> argument to secure the resource access. This is required not to expose traffic to the internet. The vpc_config argument is specified below which does not tell anything how we can configure it.<\/p>\n<pre><code>vpc_config (Union[IResolvable, Forwardref, None]) \nAWS::SageMaker::Model.VpcConfig.\n<\/code><\/pre>\n<p>The actual class is <a href=\"https:\/\/docs.aws.amazon.com\/cdk\/api\/latest\/docs\/@aws-cdk_aws-sagemaker.CfnModel.VpcConfigProperty.html\" rel=\"nofollow noreferrer\">VpcConfigProperty<\/a> and the code is in aws\/cdk\/libcdk_elasticsearch.py of the CDK installation.<\/p>\n<pre><code>class VpcConfigProperty:\n    def __init__(\n        self, *, security_group_ids: typing.List[str], subnets: typing.List[str]\n    ) -&gt; None:\n        &quot;&quot;&quot;\n        :param security_group_ids: ``CfnModel.VpcConfigProperty.SecurityGroupIds``.\n        :param subnets: ``CfnModel.VpcConfigProperty.Subnets``.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1597300327853,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":403.0,
        "Poster_created_time":1416648155470,
        "Poster_location":null,
        "Poster_reputation_count":14749.0,
        "Poster_view_count":968.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63389383",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"cdk creat sagema model vpc config question creat model vpc config cdk cdk cfnmodel vpc config argument secur resourc access requir expos traffic internet vpc config argument specifi tell configur vpc config union iresolv forwardref aw model vpcconfig actual class vpcconfigproperti code aw cdk libcdk elasticsearch cdk instal class vpcconfigproperti def init self secur group id type list str subnet type list str param secur group id cfnmodel vpcconfigproperti securitygroupid param subnet cfnmodel vpcconfigproperti subnet",
        "Question_preprocessed_content":"cdk creat sagema model vpc config question creat model vpc config cdk cdk cfnmodel argument secur resourc access requir expos traffic internet argument specifi tell configur actual class vpcconfigproperti code cdk instal",
        "Question_gpt_summary_original":"The user is seeking guidance on how to create a SageMaker model with VPC config using CDK. The CDK SageMaker CfnModel has a vpc_config argument to secure resource access, but the user is unsure how to configure it. The actual class is VpcConfigProperty, and the code is in aws\/cdk\/libcdk_elasticsearch.py of the CDK installation.",
        "Question_gpt_summary":"user seek guidanc creat model vpc config cdk cdk cfnmodel vpc config argument secur resourc access user unsur configur actual class vpcconfigproperti code aw cdk libcdk elasticsearch cdk instal"
    },
    {
        "Question_title":"Executing existing Python Code in Azure ML",
        "Question_body":"<p>I have a code written Python and I need to deploy it on Azure ML. The python code consists of many function and returns the output in Jason format.<\/p>\n\n<p>However when implemented on Azure ML, there are no errors. But, there is no result as such generated. It is displaying the input dataset for both Result Dataset and Python device dataset.<\/p>\n\n<p>Can I know, how to go about it. I'm new to Azure ML, and have no clue how to go about it<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1483524253643,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1483525625560,
        "Question_score_count":1.0,
        "Question_view_count":111.0,
        "Poster_created_time":1479194627132,
        "Poster_location":null,
        "Poster_reputation_count":2713.0,
        "Poster_view_count":358.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41461066",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"execut exist python code code written python need deploi python code consist function return output jason format implement error result gener displai input dataset result dataset python devic dataset know new clue",
        "Question_preprocessed_content":"execut exist python code code written python need deploi python code consist function return output jason format implement error result gener displai input dataset result dataset python devic dataset know new clue",
        "Question_gpt_summary_original":"The user is facing challenges in deploying existing Python code on Azure ML. Although there are no errors, the output is not generated and only the input dataset is displayed for both Result Dataset and Python device dataset. The user is seeking guidance on how to proceed as they are new to Azure ML.",
        "Question_gpt_summary":"user face challeng deploi exist python code error output gener input dataset displai result dataset python devic dataset user seek guidanc proce new"
    },
    {
        "Question_title":"How to plot multiline in one plot with smoothing features?",
        "Question_body":"<p>I\u2019m trying to plot the figure as in [W&amp;B Smoothing Features], but it didn\u2019t provide any code:<\/p>\n<p>                    <a href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/9b0793db422eac619667bd11a7c56351d9d69149.png\" target=\"_blank\" rel=\"noopener nofollow ugc\" class=\"onebox\">\n            <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/9b0793db422eac619667bd11a7c56351d9d69149.png\" width=\"690\" height=\"301\">\n          <\/a>\n\n<\/p>\n<p>Tutorials I could find by searching <code>wandb multiline in one plot<\/code> is [Custom Multi-Line Plots] which introduces <code>wandb.plot.line_series()<\/code>.  So I tried the code following<\/p>\n<pre><code class=\"lang-python\">import wandb\nimport numpy as np\n\n\nwandb.init(project=\"test\", entity=\"xxxx\")\n\nwandb.log({\"my_custom_id\":\n           wandb.plot.line_series(\n               xs=range(100),\n               ys=[range(100), np.random.randint(100, size=100)],\n               keys=[\"y1\", \"y2\"],\n               title=\"Multiline\",\n               xname=\"steps\"\n           )})\n<\/code><\/pre>\n<p>It gives me the following pic after choosing <code>Edit panel<\/code><\/p>\n<p>                    <a href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/4e32b0d8ec1fb96df36dd2a7609f129875cb9e8e.png\" target=\"_blank\" rel=\"noopener nofollow ugc\" class=\"onebox\">\n            <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/4e32b0d8ec1fb96df36dd2a7609f129875cb9e8e.png\" width=\"690\" height=\"351\">\n          <\/a>\n\n<\/p>\n<p>Unlike the first picture:<\/p>\n<ol>\n<li>It <strong>doesn\u2019t<\/strong> have <code>Data<\/code>, <code>Group<\/code> etc tabs.<\/li>\n<li>There are <strong>two types<\/strong> of legend <code>name<\/code> and <code>lineKey<\/code> rather than one type.<\/li>\n<\/ol>\n<p>My question is how to plot exactly the same as the first picture with same function supported in wandb web?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_created_time":1639275305880,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":307.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>First I need to log the data I want<\/p>\n<pre><code class=\"lang-python\">import random\nimport wandb\n\nwandb.init(project=\"test\", entity=\"xxxx\")\nfor i in range(100):\n    wandb.log({\"y1\": random.random(), \"y2\": random.random(), \"x\": i})\n<\/code><\/pre>\n<p>Then, I need to mannually choosing y1 and y2 on Y Axis.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c.jpeg\" data-download-href=\"\/uploads\/short-url\/3mBq6eTgdFLO5BAFCPEQmOUHYoQ.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c_2_690x262.jpeg\" alt=\"image\" data-base62-sha1=\"3mBq6eTgdFLO5BAFCPEQmOUHYoQ\" width=\"690\" height=\"262\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c_2_690x262.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c_2_1035x393.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c_2_1380x524.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1807\u00d7687 214 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Answer_comment_count":null,
        "Question_closed_time":1639528370760,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-plot-multiline-in-one-plot-with-smoothing-features\/1512",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"plot multilin plot smooth featur try plot figur smooth featur didnt provid code tutori search multilin plot custom multi line plot introduc plot line seri tri code follow import import numpi init project test entiti log custom plot line seri rang rang random randint size kei titl multilin xname step give follow pic choos edit panel unlik pictur doesnt data group tab type legend linekei type question plot exactli pictur function support web",
        "Question_preprocessed_content":"plot multilin plot smooth featur try plot figur didnt provid code tutori search introduc tri code follow give follow pic choos unlik pictur doesnt tab type legend type question plot exactli pictur function support web",
        "Question_gpt_summary_original":"The user is trying to plot a multiline graph with smoothing features using wandb.plot.line_series() function. However, the resulting graph does not match the desired output as it lacks certain features such as Data and Group tabs and has two types of legends. The user is seeking guidance on how to plot the graph with the same function supported in wandb web.",
        "Question_gpt_summary":"user try plot multilin graph smooth featur plot line seri function result graph match desir output lack certain featur data group tab type legend user seek guidanc plot graph function support web"
    },
    {
        "Question_title":"'kedro' is not recognized as an internal or external command, operable program or batch file",
        "Question_body":"<p>I am trying to install Kedro but I am getting this error. I know most of the time this error arises because kedro is not in my PATH. I tried adding the file path to my PATH and still getting the same error.\nWhen I run:<\/p>\n\n<p><code>pip show kedro<\/code><\/p>\n\n<p>output:<\/p>\n\n<p>Name: kedro\nVersion: 0.15.5\nSummary: Kedro helps you build production-ready data and analytics pipelines\nHome-page: <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\" rel=\"nofollow noreferrer\">https:\/\/github.com\/quantumblacklabs\/kedro<\/a>\nAuthor: QuantumBlack Labs\nAuthor-email: None\nLicense: Apache Software License (Apache 2.0)\nLocation: c:\\users\\vince\\appdata\\roaming\\python\\python36\\site-packages\nRequires: pyarrow, anyconfig, PyYAML, requests, SQLAlchemy, tables, xlrd, cookiecutter, fsspec, pip-tools, toposort, s3fs, pandas, python-json-logger, xlsxwriter, click\nRequired-by:<\/p>\n\n<p>I get the file location, and that is what I was trying to add to my path. I am assuming I am using the wrong location, but I am not sure which file location to add to my PATH.<\/p>\n\n<p>What I added to my path: c:\\users\\vince\\appdata\\roaming\\python\\python36\\site-packages<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":8,
        "Question_created_time":1580205803793,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1580223418492,
        "Question_score_count":0.0,
        "Question_view_count":1647.0,
        "Poster_created_time":1551216516627,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":15.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59946245",
        "Tool":"Kedro",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"recogn intern extern command oper program batch file try instal get error know time error aris path tri ad file path path get error run pip output version summari help build product readi data analyt pipelin home page http github com quantumblacklab author quantumblack lab author email licens apach softwar licens apach locat user vinc appdata roam python python site packag requir pyarrow anyconfig pyyaml request sqlalchemi tabl xlrd cookiecutt fsspec pip tool toposort sf panda python json logger xlsxwriter click requir file locat try add path assum wrong locat sure file locat add path ad path user vinc appdata roam python python site packag",
        "Question_preprocessed_content":"recogn intern extern command oper program batch file try instal get error know time error aris path tri ad file path path get error run output version summari help build data analyt pipelin author quantumblack lab licens apach softwar licens locat requir pyarrow anyconfig pyyaml request sqlalchemi tabl xlrd cookiecutt fsspec toposort panda xlsxwriter click file locat try add path assum wrong locat sure file locat add path ad path",
        "Question_gpt_summary_original":"The user is encountering an error while trying to install Kedro and is unable to recognize it as an internal or external command. The user tried adding the file path to their PATH but still received the same error. The user is unsure which file location to add to their PATH.",
        "Question_gpt_summary":"user encount error try instal unabl recogn intern extern command user tri ad file path path receiv error user unsur file locat add path"
    },
    {
        "Question_title":"How to load an .RData file to AWS sagemaker notebook?",
        "Question_body":"<p>I just started using the AWS sagemaker and I have an xgboost model saved in my personal laptop using save as .Rdata, saveRDS, xgb.save commands. I have uploaded those files in my Sagemaker notebook instance where my different notebooks are. However, I am unable to load it to my environment and predict for test data by using the following commands:<\/p>\n<pre><code>load(&quot;Model.RData&quot;)\nmodel=xgb.load('model')\nmodel &lt;- readRDS(&quot;Model.rds&quot;)\n<\/code><\/pre>\n<p>When I predict, I get NAs as my prediction. These commands work fine on Rstudio but not on sagemaker notebook.Please help<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_created_time":1597789435370,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1597793319476,
        "Question_score_count":0.0,
        "Question_view_count":180.0,
        "Poster_created_time":1575119311447,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":7.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63477448",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"load rdata file notebook start xgboost model save person laptop save rdata saverd xgb save command upload file notebook instanc differ notebook unabl load environ predict test data follow command load model rdata model xgb load model model readrd model rd predict na predict command work fine rstudio notebook help",
        "Question_preprocessed_content":"load rdata file notebook start xgboost model save person laptop save rdata saverd command upload file notebook instanc differ notebook unabl load environ predict test data follow command predict na predict command work fine rstudio help",
        "Question_gpt_summary_original":"The user is facing challenges in loading an .RData file to AWS Sagemaker notebook. They have uploaded the file to their notebook instance but are unable to load it to their environment and predict for test data. The commands that work fine on Rstudio are not working on Sagemaker notebook, resulting in NAs as predictions.",
        "Question_gpt_summary":"user face challeng load rdata file notebook upload file notebook instanc unabl load environ predict test data command work fine rstudio work notebook result na predict"
    },
    {
        "Question_title":"Very Large numbers in Logged Metrics in Runs Table",
        "Question_body":"<p>I\u2019m running physics experiments and tracking it with WandB.<\/p>\n<p>I\u2019m tracking the logarithm of a very large number (logmetric) and a very large number directly (metric).<\/p>\n<p>The logarithmic value of the metric has a numerical value of logmetric=140, but when I track the exponentiated metric which is metric=1.3e+65, the dashboard says \u2018Infinity\u2019.<\/p>\n<p>Is my assumption correct that the dashboard simply translates very large numbers to \u2018Infinity\u2019 (which makes sense)?<br>\nIs there a way to enable scientific number representation?<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_created_time":1675934442253,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":87.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/very-large-numbers-in-logged-metrics-in-runs-table\/3853",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"larg number log metric run tabl run physic experi track track logarithm larg number logmetr larg number directli metric logarithm valu metric numer valu logmetr track exponenti metric metric dashboard sai infin assumpt correct dashboard simpli translat larg number infin make sens wai enabl scientif number represent",
        "Question_preprocessed_content":"larg number log metric run tabl run physic experi track track logarithm larg number larg number directli logarithm valu metric numer valu logmetr track exponenti metric dashboard sai infin assumpt correct dashboard simpli translat larg number infin wai enabl scientif number represent",
        "Question_gpt_summary_original":"The user is tracking the logarithm of a very large number and a very large number directly in their physics experiments using WandB. The logarithmic value of the metric has a numerical value of 140, but when the exponentiated metric is tracked, which is 1.3e+65, the dashboard shows 'Infinity'. The user is wondering if the dashboard simply translates very large numbers to 'Infinity' and if there is a way to enable scientific number representation.",
        "Question_gpt_summary":"user track logarithm larg number larg number directli physic experi logarithm valu metric numer valu exponenti metric track dashboard show infin user wonder dashboard simpli translat larg number infin wai enabl scientif number represent"
    },
    {
        "Question_title":"Access Kedro context from decorator",
        "Question_body":"<p>I am trying to create a decorator in which I need some information about the project and\/or catalog.  Is it possible to access the project context from inside of the decorator?  I am looking for things like project_name, catalog entry name, and pipeline node name.<\/p>\n\n<p>I created a way to find the root directory of the by getting the functions file path <code>inspect.getfile(func)<\/code>, then walking up the path until I find <code>.kedro.yml<\/code>, but this method breaks when using a function from a library.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1574277172107,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":581.0,
        "Poster_created_time":1479159384132,
        "Poster_location":"Illinois, United States",
        "Poster_reputation_count":513.0,
        "Poster_view_count":113.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58961729",
        "Tool":"Kedro",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"access context decor try creat decor need inform project catalog possibl access project context insid decor look thing like project catalog entri pipelin node creat wai root directori get function file path inspect getfil func walk path yml method break function librari",
        "Question_preprocessed_content":"access context decor try creat decor need inform project catalog possibl access project context insid decor look thing like catalog entri pipelin node creat wai root directori get function file path walk path method break function librari",
        "Question_gpt_summary_original":"The user is trying to create a decorator and needs to access information about the project and\/or catalog such as project name, catalog entry name, and pipeline node name. They have tried to find the root directory of the project by getting the function's file path but this method breaks when using a function from a library.",
        "Question_gpt_summary":"user try creat decor need access inform project catalog project catalog entri pipelin node tri root directori project get function file path method break function librari"
    },
    {
        "Question_title":"Workspace Authentication: More than one token matches the criteria",
        "Question_body":"<p>I constantly run into problems when working on Azure Compute Instances and trying to connect from the Jupyter Lab to the workspace.<\/p>\n<p>With InteractiveLoginAuthentication I get the following message:<\/p>\n<pre><code>AuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n<\/code><\/pre>\n<p>With a Service Principal this one (SP is owner in the ML Workspace):<\/p>\n<pre><code>WorkspaceException: WorkspaceException:\n    Message: No workspaces found with name=xxx in all the subscriptions that you have access to.\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;No workspaces found with name=xxx in all the subscriptions that you have access to.&quot;\n    }\n}\n<\/code><\/pre>\n<p>I had another workspace in a different subscription where I could resolve it by giving the tennant as an extra input to the InteractiveLoginAuthentication. This time, no chance.<\/p>\n<p>The funny thing is, though, that I can login to the workspace via InteractiveLoginAuthentication when doing it from my local computer.<\/p>\n<p>I supsected that some old tokens are cached somewhere so I tried to use the &quot;Private browsing&quot; function of my browser. Furthermore, I deleted <code>\/home\/azureuser\/.azure\/accessTokens.json<\/code> but no effect.<\/p>\n<p>Maybe some of you had this problem before and have an idea?<\/p>\n<p>For reference some sites I checked:<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-cli\/issues\/4618\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-cli\/issues\/4618<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-cli\/issues\/6147\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-cli\/issues\/6147<\/a><\/li>\n<\/ul>\n<h1>Update<\/h1>\n<p>When I run this code:<\/p>\n<pre><code>from azureml.core.authentication import InteractiveLoginAuthentication\ninteractive_auth = InteractiveLoginAuthentication(tenant_id='xxx')\n\nws = Workspace.get(name='xxx',\n                   subscription_id='xxx',\n                   resource_group='xxx',\n                   auth=interactive_auth)\n<\/code><\/pre>\n<p>I get the following trace:<\/p>\n<pre><code>---------------------------------------------------------------------------\nAdalError                                 Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1820         auth, _, _ = profile_object.get_login_credentials(resource)\n-&gt; 1821         access_token = auth._token_retriever()[1]\n   1822         if (_get_exp_time(access_token) - time.time()) &lt; _TOKEN_REFRESH_THRESHOLD_SEC:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in _retrieve_token()\n    525                     return self._creds_cache.retrieve_token_for_user(username_or_sp_id,\n--&gt; 526                                                                      account[_TENANT_ID], resource)\n    527                 use_cert_sn_issuer = account[_USER_ENTITY].get(_SERVICE_PRINCIPAL_CERT_SN_ISSUER_AUTH)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in retrieve_token_for_user(self, username, tenant, resource)\n    889         context = self._auth_ctx_factory(self._cloud_type, tenant, cache=self.adal_token_cache)\n--&gt; 890         token_entry = context.acquire_token(resource, username, _CLIENT_ID)\n    891         if not token_entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in acquire_token(self, resource, user_id, client_id)\n    144 \n--&gt; 145         return self._acquire_token(token_func)\n    146 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in _acquire_token(self, token_func, correlation_id)\n    127         self.authority.validate(self._call_context)\n--&gt; 128         return token_func(self)\n    129 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in token_func(self)\n    142             token_request = TokenRequest(self._call_context, self, client_id, resource)\n--&gt; 143             return token_request.get_token_from_cache_with_refresh(user_id)\n    144 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in get_token_from_cache_with_refresh(self, user_id)\n    346         self._user_id = user_id\n--&gt; 347         return self._find_token_from_cache()\n    348 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in _find_token_from_cache(self)\n    126         cache_query = self._create_cache_query()\n--&gt; 127         return self._cache_driver.find(cache_query)\n    128 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in find(self, query)\n    195                         {&quot;query&quot;: log.scrub_pii(query)})\n--&gt; 196         entry, is_resource_tenant_specific = self._load_single_entry_from_cache(query)\n    197         if entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in _load_single_entry_from_cache(self, query)\n    123             else:\n--&gt; 124                 raise AdalError('More than one token matches the criteria. The result is ambiguous.')\n    125 \n\nAdalError: More than one token matches the criteria. The result is ambiguous.\n\nDuring handling of the above exception, another exception occurred:\n\nAuthenticationException                   Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in wrapper(self, *args, **kwargs)\n    288                     module_logger.debug(&quot;{} acquired lock in {} s.&quot;.format(type(self).__name__, duration))\n--&gt; 289                 return test_function(self, *args, **kwargs)\n    290             except Exception as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token(self)\n    474         else:\n--&gt; 475             return self._get_arm_token_using_interactive_auth()\n    476 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_using_interactive_auth(self, force_reload, resource)\n    589         arm_token = _get_arm_token_with_refresh(profile_object, cloud_type, ACCOUNT, CONFIG, SESSION,\n--&gt; 590                                                 get_config_dir(), force_reload=force_reload, resource=resource)\n    591         # If a user has specified a tenant id then we need to check if this token is for that tenant.\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in connection_aborted_wrapper(*args, **kwargs)\n    325                 try:\n--&gt; 326                     return function(*args, **kwargs)\n    327                 except AuthenticationException as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1829             raise AuthenticationException(&quot;Could not retrieve user token. Please run 'az login'&quot;,\n-&gt; 1830                                           inner_exception=e)\n   1831 \n\nAuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n\nDuring handling of the above exception, another exception occurred:\n\nAdalError                                 Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1820         auth, _, _ = profile_object.get_login_credentials(resource)\n-&gt; 1821         access_token = auth._token_retriever()[1]\n   1822         if (_get_exp_time(access_token) - time.time()) &lt; _TOKEN_REFRESH_THRESHOLD_SEC:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in _retrieve_token()\n    525                     return self._creds_cache.retrieve_token_for_user(username_or_sp_id,\n--&gt; 526                                                                      account[_TENANT_ID], resource)\n    527                 use_cert_sn_issuer = account[_USER_ENTITY].get(_SERVICE_PRINCIPAL_CERT_SN_ISSUER_AUTH)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in retrieve_token_for_user(self, username, tenant, resource)\n    889         context = self._auth_ctx_factory(self._cloud_type, tenant, cache=self.adal_token_cache)\n--&gt; 890         token_entry = context.acquire_token(resource, username, _CLIENT_ID)\n    891         if not token_entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in acquire_token(self, resource, user_id, client_id)\n    144 \n--&gt; 145         return self._acquire_token(token_func)\n    146 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in _acquire_token(self, token_func, correlation_id)\n    127         self.authority.validate(self._call_context)\n--&gt; 128         return token_func(self)\n    129 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in token_func(self)\n    142             token_request = TokenRequest(self._call_context, self, client_id, resource)\n--&gt; 143             return token_request.get_token_from_cache_with_refresh(user_id)\n    144 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in get_token_from_cache_with_refresh(self, user_id)\n    346         self._user_id = user_id\n--&gt; 347         return self._find_token_from_cache()\n    348 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in _find_token_from_cache(self)\n    126         cache_query = self._create_cache_query()\n--&gt; 127         return self._cache_driver.find(cache_query)\n    128 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in find(self, query)\n    195                         {&quot;query&quot;: log.scrub_pii(query)})\n--&gt; 196         entry, is_resource_tenant_specific = self._load_single_entry_from_cache(query)\n    197         if entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in _load_single_entry_from_cache(self, query)\n    123             else:\n--&gt; 124                 raise AdalError('More than one token matches the criteria. The result is ambiguous.')\n    125 \n\nAdalError: More than one token matches the criteria. The result is ambiguous.\n\nDuring handling of the above exception, another exception occurred:\n\nAuthenticationException                   Traceback (most recent call last)\n&lt;ipython-input-2-fd1276999d15&gt; in &lt;module&gt;\n      5                    subscription_id='00c983e5-d766-480b-be75-abf95d1a46c3',\n      6                    resource_group='BusinessIntelligence',\n----&gt; 7                    auth=interactive_auth)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in get(name, auth, subscription_id, resource_group)\n    547 \n    548         result_dict = Workspace.list(\n--&gt; 549             subscription_id, auth=auth, resource_group=resource_group)\n    550         result_dict = {k.lower(): v for k, v in result_dict.items()}\n    551         name = name.lower()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in list(subscription_id, auth, resource_group)\n    637         elif subscription_id and resource_group:\n    638             workspaces_list = Workspace._list_legacy(\n--&gt; 639                 auth, subscription_id=subscription_id, resource_group_name=resource_group)\n    640 \n    641             Workspace._process_autorest_workspace_list(\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in _list_legacy(auth, subscription_id, resource_group_name, ignore_error)\n   1373                 return None\n   1374             else:\n-&gt; 1375                 raise e\n   1376 \n   1377     @staticmethod\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in _list_legacy(auth, subscription_id, resource_group_name, ignore_error)\n   1367             # azureml._base_sdk_common.workspace.models.workspace.Workspace\n   1368             workspace_autorest_list = _commands.list_workspace(\n-&gt; 1369                 auth, subscription_id=subscription_id, resource_group_name=resource_group_name)\n   1370             return workspace_autorest_list\n   1371         except Exception as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_project\/_commands.py in list_workspace(auth, subscription_id, resource_group_name)\n    386         if resource_group_name:\n    387             list_object = WorkspacesOperations.list_by_resource_group(\n--&gt; 388                 auth._get_service_client(AzureMachineLearningWorkspaces, subscription_id).workspaces,\n    389                 resource_group_name)\n    390             workspace_list = list_object.value\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_service_client(self, client_class, subscription_id, subscription_bound, base_url)\n    155         # in the multi-tenant case, which causes confusion.\n    156         if subscription_id:\n--&gt; 157             all_subscription_list, tenant_id = self._get_all_subscription_ids()\n    158             self._check_if_subscription_exists(subscription_id, all_subscription_list, tenant_id)\n    159 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_all_subscription_ids(self)\n    497         :rtype: list, str\n    498         &quot;&quot;&quot;\n--&gt; 499         arm_token = self._get_arm_token()\n    500         return self._get_all_subscription_ids_internal(arm_token)\n    501 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in wrapper(self, *args, **kwargs)\n    293                     InteractiveLoginAuthentication(force=True, tenant_id=self._tenant_id)\n    294                     # Try one more time\n--&gt; 295                     return test_function(self, *args, **kwargs)\n    296                 else:\n    297                     raise e\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token(self)\n    473             return self._ambient_auth._get_arm_token()\n    474         else:\n--&gt; 475             return self._get_arm_token_using_interactive_auth()\n    476 \n    477     @_login_on_failure_decorator(_interactive_auth_lock)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_using_interactive_auth(self, force_reload, resource)\n    588         profile_object = Profile(async_persist=False, cloud_type=cloud_type)\n    589         arm_token = _get_arm_token_with_refresh(profile_object, cloud_type, ACCOUNT, CONFIG, SESSION,\n--&gt; 590                                                 get_config_dir(), force_reload=force_reload, resource=resource)\n    591         # If a user has specified a tenant id then we need to check if this token is for that tenant.\n    592         if self._tenant_id and fetch_tenantid_from_aad_token(arm_token) != self._tenant_id:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in connection_aborted_wrapper(*args, **kwargs)\n    324             while True:\n    325                 try:\n--&gt; 326                     return function(*args, **kwargs)\n    327                 except AuthenticationException as e:\n    328                     if &quot;Connection aborted.&quot; in str(e) and attempt &lt;= retries:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1828         if not token_about_to_expire:\n   1829             raise AuthenticationException(&quot;Could not retrieve user token. Please run 'az login'&quot;,\n-&gt; 1830                                           inner_exception=e)\n   1831 \n   1832     try:\n\nAuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n<\/code><\/pre>\n<ul>\n<li><code>azureml-sdk<\/code> is on version 1.9.0<\/li>\n<li>I can connect an authenticate from my local machine. Problems only occur when I want to work on a compute instance.<\/li>\n<\/ul>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1591616299803,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1594308517016,
        "Question_score_count":3.0,
        "Question_view_count":2737.0,
        "Poster_created_time":1589205020747,
        "Poster_location":"Germany",
        "Poster_reputation_count":163.0,
        "Poster_view_count":16.0,
        "Answer_body":"<p>Okay, here is the answer:<\/p>\n<ul>\n<li>You work for company A which is on Azure.<\/li>\n<li>You get access to company B's subscription.<\/li>\n<li>Problem is: You are associated to A's AAD in ML-Studio.<\/li>\n<li>You need to specify the tenant ID in the <code>InteractiveLoginAuthentication<\/code> like so:<\/li>\n<\/ul>\n<pre><code>interactive_auth = InteractiveLoginAuthentication(tenant_id=tenant_id)\n\nworkspace = Workspace.get(name=workspace_name,\n                          subscription_id=subscription_id,\n                          resource_group=resource_group,\n                          auth=interactive_auth)\n<\/code><\/pre>\n<ul>\n<li>Now the <strong>important<\/strong> part: You need to use company B's <code>tenant_id<\/code> (I used company A's all the time since I thought that was my authentication point)<\/li>\n<li>Of course, this is obvious while you read it...as it is to me now :)<\/li>\n<\/ul>\n<p>Hope this helps you. Took me some time but learned a lot ;)<\/p>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1596034370720,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1589205020747,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":163.0,
        "Answerer_view_count":16.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62261222",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"workspac authent token match criteria constantli run problem work azur comput instanc try connect jupyt lab workspac interactiveloginauthent follow messag authenticationexcept authenticationexcept messag retriev user token run login innerexcept token match criteria result ambigu errorrespons error code usererror inner error code authent messag retriev user token run login servic princip owner workspac workspaceexcept workspaceexcept messag workspac subscript access innerexcept errorrespons error messag workspac subscript access workspac differ subscript resolv give tennant extra input interactiveloginauthent time chanc funni thing login workspac interactiveloginauthent local supsect old token cach tri us privat brows function browser furthermor delet home azureus azur accesstoken json effect mayb problem idea refer site check http doc microsoft com azur machin learn setup authent http github com azur machinelearningnotebook blob master us manag servic authent authent ipynb http github com azur azur cli issu http github com azur azur cli issu updat run code core authent import interactiveloginauthent interact auth interactiveloginauthent tenant workspac subscript resourc group auth interact auth follow trace adalerror traceback recent anaconda env lib python site packag core authent arm token refresh profil object cloud type account object config object session object config directori forc reload resourc auth profil object login credenti resourc access token auth token retriev exp time access token time time account tenant resourc us cert issuer account user entiti servic princip cert issuer auth anaconda env lib python site packag vendor azur cli core profil retriev token user self usernam tenant resourc context self auth ctx factori self cloud type tenant cach self adal token cach token entri context acquir token resourc usernam client token entri anaconda env lib python site packag adal authent context acquir token self resourc user client return self acquir token token func anaconda env lib python site packag adal authent context acquir token self token func correl self author valid self context return token func self anaconda env lib python site packag adal authent context token func self token request tokenrequest self context self client resourc return token request token cach refresh user anaconda env lib python site packag adal token request token cach refresh self user self user user return self token cach anaconda env lib python site packag adal token request token cach self cach queri self creat cach queri return self cach driver cach queri anaconda env lib python site packag adal cach driver self queri queri log scrub pii queri entri resourc tenant specif self load singl entri cach queri entri anaconda env lib python site packag adal cach driver load singl entri cach self queri rais adalerror token match criteria result ambigu adalerror token match criteria result ambigu handl except except occur authenticationexcept traceback recent anaconda env lib python site packag core authent wrapper self arg kwarg modul logger debug acquir lock format type self durat return test function self arg kwarg except anaconda env lib python site packag core authent arm token self return self arm token interact auth anaconda env lib python site packag core authent arm token interact auth self forc reload resourc arm token arm token refresh profil object cloud type account config session config dir forc reload forc reload resourc resourc user specifi tenant need check token tenant anaconda env lib python site packag core authent connect abort wrapper arg kwarg try return function arg kwarg authenticationexcept anaconda env lib python site packag core authent arm token refresh profil object cloud type account object config object session object config directori forc reload resourc rais authenticationexcept retriev user token run login inner except authenticationexcept authenticationexcept messag retriev user token run login innerexcept token match criteria result ambigu errorrespons error code usererror inner error code authent messag retriev user token run login handl except except occur adalerror traceback recent anaconda env lib python site packag core authent arm token refresh profil object cloud type account object config object session object config directori forc reload resourc auth profil object login credenti resourc access token auth token retriev exp time access token time time account tenant resourc us cert issuer account user entiti servic princip cert issuer auth anaconda env lib python site packag vendor azur cli core profil retriev token user self usernam tenant resourc context self auth ctx factori self cloud type tenant cach self adal token cach token entri context acquir token resourc usernam client token entri anaconda env lib python site packag adal authent context acquir token self resourc user client return self acquir token token func anaconda env lib python site packag adal authent context acquir token self token func correl self author valid self context return token func self anaconda env lib python site packag adal authent context token func self token request tokenrequest self context self client resourc return token request token cach refresh user anaconda env lib python site packag adal token request token cach refresh self user self user user return self token cach anaconda env lib python site packag adal token request token cach self cach queri self creat cach queri return self cach driver cach queri anaconda env lib python site packag adal cach driver self queri queri log scrub pii queri entri resourc tenant specif self load singl entri cach queri entri anaconda env lib python site packag adal cach driver load singl entri cach self queri rais adalerror token match criteria result ambigu adalerror token match criteria result ambigu handl except except occur authenticationexcept traceback recent subscript abfdac resourc group businessintellig auth interact auth anaconda env lib python site packag core workspac auth subscript resourc group result dict workspac list subscript auth auth resourc group resourc group result dict lower result dict item lower anaconda env lib python site packag core workspac list subscript auth resourc group elif subscript resourc group workspac list workspac list legaci auth subscript subscript resourc group resourc group workspac process autorest workspac list anaconda env lib python site packag core workspac list legaci auth subscript resourc group ignor error return rais staticmethod anaconda env lib python site packag core workspac list legaci auth subscript resourc group ignor error base sdk common workspac model workspac workspac workspac autorest list command list workspac auth subscript subscript resourc group resourc group return workspac autorest list except anaconda env lib python site packag project command list workspac auth subscript resourc group resourc group list object workspacesoper list resourc group auth servic client azuremachinelearningworkspac subscript workspac resourc group workspac list list object valu anaconda env lib python site packag core authent servic client self client class subscript subscript bound base url multi tenant case caus confus subscript subscript list tenant self subscript id self check subscript exist subscript subscript list tenant anaconda env lib python site packag core authent subscript id self rtype list str arm token self arm token return self subscript id intern arm token anaconda env lib python site packag core authent wrapper self arg kwarg interactiveloginauthent forc true tenant self tenant try time return test function self arg kwarg rais anaconda env lib python site packag core authent arm token self return self ambient auth arm token return self arm token interact auth login failur decor interact auth lock anaconda env lib python site packag core authent arm token interact auth self forc reload resourc profil object profil async persist fals cloud type cloud type arm token arm token refresh profil object cloud type account config session config dir forc reload forc reload resourc resourc user specifi tenant need check token tenant self tenant fetch tenantid aad token arm token self tenant anaconda env lib python site packag core authent connect abort wrapper arg kwarg true try return function arg kwarg authenticationexcept connect abort str attempt inner except try authenticationexcept authenticationexcept messag retriev user token run login innerexcept token match criteria result ambigu errorrespons error code usererror inner error code authent messag retriev user token run login sdk version connect authent local machin problem occur want work comput instanc",
        "Question_preprocessed_content":"workspac authent token match criteria constantli run problem work azur comput instanc try connect jupyt lab workspac interactiveloginauthent follow messag servic princip workspac differ subscript resolv give tennant extra input interactiveloginauthent time chanc funni thing login workspac interactiveloginauthent local supsect old token cach tri us privat brows function browser furthermor delet effect mayb problem idea refer site check updat run code follow trace version connect authent local machin problem occur want work comput instanc",
        "Question_gpt_summary_original":"the user is encountering challenges when trying to connect from the jupyter lab to the workspace, with authentication exceptions due to more than one token matching the criteria.",
        "Question_gpt_summary":"user encount challeng try connect jupyt lab workspac authent except token match criteria"
    },
    {
        "Question_title":"How to call Sagemaker training model endpoint API in C#",
        "Question_body":"<p>I have implemented machine learning algorithms through sagemaker.<\/p>\n\n<p>I have installed SDK for .net, and tried by executing below code.<\/p>\n\n<pre><code>Uri sagemakerEndPointURI = new Uri(\"https:\/\/runtime.sagemaker.us-east-2.amazonaws.com\/endpoints\/MyEndpointName\/invocations\");\nAmazon.SageMakerRuntime.Model.InvokeEndpointRequest request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest();\nrequest.EndpointName = \"MyEndpointName\";\nAmazonSageMakerRuntimeClient aawsClient = new AmazonSageMakerRuntimeClient(myAwsAccessKey,myAwsSecreteKey);            \nAmazon.SageMakerRuntime.Model.InvokeEndpointResponse resposnse= aawsClient.InvokeEndpoint(request);\n<\/code><\/pre>\n\n<p>By executing this, I am getting validation error as \"<code>1 validation error detected: Value at 'body' failed to satisfy constraint: Member must not be null<\/code>\"<\/p>\n\n<p>Can anyone guide me on how and what more input data I need to pass to call the given API?<\/p>\n\n<p>EDIT<\/p>\n\n<p>Further I'd tried by provinding body parameter which contains a MemoryStream written by a '.gz' or '.pkl' file, and it giving me error as : \"Error unmarshalling response back from AWS,  HTTP content length exceeded 5246976 bytes.\"<\/p>\n\n<p>EDIT 1\/23\/2018<\/p>\n\n<p>Further I came up with the error message as <\/p>\n\n<blockquote>\n  <p>ERROR - model server - 'TypeError' object has no attribute 'message'<\/p>\n<\/blockquote>\n\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_created_time":1516531050743,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1516720777516,
        "Question_score_count":8.0,
        "Question_view_count":2093.0,
        "Poster_created_time":1337759214688,
        "Poster_location":"Pune India",
        "Poster_reputation_count":1036.0,
        "Poster_view_count":124.0,
        "Answer_body":"<p>Later solved it by <code>Encoding.ASCII.GetBytes<\/code>as in below code.<\/p>\n\n<pre><code> byte[] bytes = System.IO.File.ReadAllBytes(@\"EXCEL_FILE_PATH\");\n    string listA = \"\";\n    while (!reader.EndOfStream)\n        {\n            var line = reader.ReadLine();\n            listA = listA + line + \"\\n\";\n        }\n    byte[] bytes = Encoding.ASCII.GetBytes(listA);\n    request.Body = new MemoryStream(bytes);\n    InvokeEndpointResponse response = sagemakerRunTimeClient.InvokeEndpoint(request);\n    string predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1519637555372,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1337759214688,
        "Answerer_location":"Pune India",
        "Answerer_reputation_count":1036.0,
        "Answerer_view_count":124.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48365866",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"train model endpoint api implement machin learn algorithm instal sdk net tri execut code uri endpointuri new uri http runtim east amazonaw com endpoint myendpointnam invoc amazon runtim model invokeendpointrequest request new amazon runtim model invokeendpointrequest request endpointnam myendpointnam amazonruntimecli aawsclient new amazonruntimecli myawsaccesskei myawssecretekei amazon runtim model invokeendpointrespons resposns aawsclient invokeendpoint request execut get valid error valid error detect valu bodi fail satisfi constraint member null guid input data need pass given api edit tri provind bodi paramet contain memorystream written pkl file give error error unmarshal respons aw http content length exceed byte edit came error messag error model server typeerror object attribut messag thank",
        "Question_preprocessed_content":"train model endpoint api implement machin learn algorithm instal sdk net tri execut code execut get valid error guid input data need pass given api edit tri provind bodi paramet contain memorystream written file give error error unmarshal respons aw http content length exceed edit came error messag error model server typeerror object attribut messag thank",
        "Question_gpt_summary_original":"The user is trying to call a Sagemaker training model endpoint API in C# using the SDK for .net. However, they are encountering a validation error stating that the member must not be null. They have tried passing a MemoryStream written by a '.gz' or '.pkl' file, but it is giving them an error stating that the HTTP content length exceeded 5246976 bytes. They have also encountered an error message stating that the 'TypeError' object has no attribute 'message'.",
        "Question_gpt_summary":"user try train model endpoint api sdk net encount valid error state member null tri pass memorystream written pkl file give error state http content length exceed byte encount error messag state typeerror object attribut messag"
    },
    {
        "Question_title":"venv on AWS sagemaker notebook not working",
        "Question_body":"<p>I just launched AWS Sagemaker studio and trying to create virtual environment to work with as below:<\/p>\n<pre><code>python3 -m venv venv\nsource venv\/bin\/activate\npip install -r requirements.txt \n<\/code><\/pre>\n<p>venv works for my python scripts on Sagemaker. I am able to import the packages in my script as &quot;import pandas as pd&quot;<\/p>\n<p>But When I tried opening a Jupyter notebook on Sagemaker and run <code>import pandas as pd<\/code> I am getting error as <code>ModuleNotFoundError: No module named 'pandas'<\/code><\/p>\n<p>How do I use venv in my notebook? Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1634606236283,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":364.0,
        "Poster_created_time":1565851119327,
        "Poster_location":null,
        "Poster_reputation_count":844.0,
        "Poster_view_count":148.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69624105",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"venv notebook work launch studio try creat virtual environ work python venv venv sourc venv bin activ pip instal requir txt venv work python script abl import packag script import panda tri open jupyt notebook run import panda get error modulenotfounderror modul name panda us venv notebook thank",
        "Question_preprocessed_content":"venv notebook work launch studio try creat virtual environ work venv work python script abl import packag script import panda tri open jupyt notebook run get error us venv notebook thank",
        "Question_gpt_summary_original":"The user is facing an issue with using venv on AWS Sagemaker notebook. While venv works for their Python scripts, they are unable to import packages like pandas in their Jupyter notebook and are getting a ModuleNotFoundError. They are seeking a solution to use venv in their notebook.",
        "Question_gpt_summary":"user face issu venv notebook venv work python script unabl import packag like panda jupyt notebook get modulenotfounderror seek solut us venv notebook"
    },
    {
        "Question_title":"Real-time Data Pre-processing in Lambda for SageMaker Endpoint",
        "Question_body":"<p>Am working on a project of consumer behaviour analysis on websites and predict the malicious activity of users in real-time.\nClick data is being collected for each click made by users.<\/p>\n<p>Am using multiple AWS services like kinesis stream, Lambda and sagemaker. I have created an autoencoder model and\ndeployed it as sagemaker endpoint which will be invoked using lambda when it receives new click data from the website through\nKinesis stream.<\/p>\n<p>Since sagemaker endpoint contains the only model but click data which lambda function receives is raw data with URLs, texts and\ndate. How can I pass raw data into required preprocessing steps and send processed data to sagemaker endpoint in the required format?<\/p>\n<p>Example of raw data:-<\/p>\n<p>{'URL':'www.amazon.com.au\/ref=nav_logo', 'Text':'Home', 'Information':'Computers'}<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1623043365217,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":475.0,
        "Poster_created_time":1604911047620,
        "Poster_location":"Victoria, Australia",
        "Poster_reputation_count":13.0,
        "Poster_view_count":4.0,
        "Answer_body":"<p>You can use Sagemaker inference Pipeline. You need to create preprocessing script comprising of your preprocessing steps and create a Pipeline including Preprocess and model. Deploy pipeline to an endpoint for real time inference.<\/p>\n<p>Reference:\n<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn\/<\/a><\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1624254151543,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1606374398152,
        "Answerer_location":null,
        "Answerer_reputation_count":54.0,
        "Answerer_view_count":12.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67866286",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"real time data pre process lambda endpoint work project consum behaviour analysi websit predict malici activ user real time click data collect click user multipl aw servic like kinesi stream lambda creat autoencod model deploi endpoint invok lambda receiv new click data websit kinesi stream endpoint contain model click data lambda function receiv raw data url text date pass raw data requir preprocess step send process data endpoint requir format exampl raw data url amazon com ref nav logo text home inform comput",
        "Question_preprocessed_content":"data lambda endpoint work project consum behaviour analysi websit predict malici activ user click data collect click user multipl aw servic like kinesi stream lambda creat autoencod model deploi endpoint invok lambda receiv new click data websit kinesi stream endpoint contain model click data lambda function receiv raw data url text date pass raw data requir preprocess step send process data endpoint requir format exampl raw data text home inform comput",
        "Question_gpt_summary_original":"The user is working on a project to analyze consumer behavior on websites and predict malicious activity in real-time. They are using AWS services like Kinesis stream, Lambda, and SageMaker. They have created an autoencoder model and deployed it as a SageMaker endpoint, which will be invoked using Lambda when it receives new click data from the website through Kinesis stream. However, the raw data received by the Lambda function contains URLs, texts, and dates, and the user needs to preprocess this data before sending it to the SageMaker endpoint in the required format.",
        "Question_gpt_summary":"user work project analyz consum behavior websit predict malici activ real time aw servic like kinesi stream lambda creat autoencod model deploi endpoint invok lambda receiv new click data websit kinesi stream raw data receiv lambda function contain url text date user need preprocess data send endpoint requir format"
    },
    {
        "Question_title":"Please check if pipelines with the same name were previously submitted to a different endpoint",
        "Question_body":"<p>I'm getting below error whenever I tried to run a pipeline job using vertex-ai managed Jupiter notebook.<\/p>\n<p>here I make sure that I'm creating a unique pipeline name every time by appending a timestamp in pipeline name sting. e.g my display name will be like <code>AutoML-Pipeline-DS-v4-1637251623<\/code> still I'm getting errors like <code> Please check if pipelines with the same name were previously submitted to a different endpoint.<\/code><\/p>\n<p>here I'm using <code>google-cloud-aiplatform==1.4.3<\/code> to run the pipeline job. also, I'm following this <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform#pipelines\" rel=\"nofollow noreferrer\">example<\/a> from GCP.<\/p>\n<pre><code>com.google.cloud.ai.platform.common.errors.AiPlatformException: code=INVALID_ARGUMENT, message=User-specified resource ID must match the regular expression '[a-z0-9][a-z0-9-]{0,127}', cause=null; Failed to update context (id = projects\/xxxx\/locations\/us-central1\/metadataStores\/default\/contexts\/AutoML-Pipeline-DS-v4-1637251623). Please check if pipelines with the same name were previously submitted to a different endpoint. If so, one may submit the current pipeline with a different name to avoid reusing the existing MLMD Context from the other endpoint.; Failed to update pipeline and run contexts: project_number=xxxx, job_id=xxxx.; Failed to handle the job: {project_number = xxxx, job_id = xxxx}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1637252862830,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":267.0,
        "Poster_created_time":1524034832407,
        "Poster_location":"Ahmedabad, Gujarat, India",
        "Poster_reputation_count":1.0,
        "Poster_view_count":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70023418",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"check pipelin previous submit differ endpoint get error tri run pipelin job vertex manag jupit notebook sure creat uniqu pipelin time append timestamp pipelin sting displai like automl pipelin get error like check pipelin previous submit differ endpoint googl cloud aiplatform run pipelin job follow exampl gcp com googl cloud platform common error aiplatformexcept code invalid argument messag user specifi resourc match regular express caus null fail updat context project locat central metadatastor default context automl pipelin check pipelin previous submit differ endpoint submit current pipelin differ avoid reus exist mlmd context endpoint fail updat pipelin run context project number job fail handl job project number job",
        "Question_preprocessed_content":"check pipelin previous submit differ endpoint get error tri run pipelin job manag jupit notebook sure creat uniqu pipelin time append timestamp pipelin sting displai like get error like run pipelin job follow exampl gcp",
        "Question_gpt_summary_original":"The user is encountering an error while running a pipeline job using vertex-ai managed Jupiter notebook. The error message suggests that the user should check if pipelines with the same name were previously submitted to a different endpoint. The user is using google-cloud-aiplatform==1.4.3 to run the pipeline job and has ensured that a unique pipeline name is created every time by appending a timestamp in the pipeline name string.",
        "Question_gpt_summary":"user encount error run pipelin job vertex manag jupit notebook error messag suggest user check pipelin previous submit differ endpoint user googl cloud aiplatform run pipelin job ensur uniqu pipelin creat time append timestamp pipelin string"
    },
    {
        "Question_title":"Translate text with a glossary (unsupported languages)",
        "Question_body":"Previously, in the list of supported languages (https:\/\/cloud.google.com\/translate\/docs\/languages), some languages were marked with an asterisk (*) and there was a note at the end of the list stating this: \u201c* You cannot use a glossary when translating to or from these languages. Glossaries aren't supported for these languages.\u201d\n\nNow that information has disappeared from the list. Does this mean all listed languages now support the glossary feature?\n\nThese were the languages listed as unsupported for this feature: \"Akan, Assamese, Aymara, Bhojpuri, Bambara, Kurdish (Sorani), Dogri, Divehi, Ewe, Guarani, Goan Konkani, Iloko, Krio, Ganda, Lingala, Mizo, Maithili, Manipuri (Meitei Mayek), Northern Sotho, Oromo, Quechua, Sanskrit, Tigrinya, Tsonga\"\n\nBR,\n\nJulian",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1677822120000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":90.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"According to the latest release note, there are 24 languages added to the glossary. All the languages provided here are all supported languages, that is why there is no asterisk(*) indication anymore. Also, there are still languages that were not added. I suggest to just check the release notes every now and then for any new announcements.\n\nView solution in original post",
        "Answer_comment_count":null,
        "Question_closed_time":1677846060000,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Translate-text-with-a-glossary-unsupported-languages\/td-p\/528448\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"translat text glossari unsupport languag previous list support languag http cloud googl com translat doc languag languag mark asterisk note end list state us glossari translat languag glossari aren support languag inform disappear list mean list languag support glossari featur languag list unsupport featur akan assames aymara bhojpuri bambara kurdish sorani dogri divehi ew guarani goan konkani iloko krio ganda lingala mizo maithili manipuri meitei mayek northern sotho oromo quechua sanskrit tigrinya tsonga julian",
        "Question_preprocessed_content":"translat text glossari previous list support languag languag mark asterisk note end list state us glossari translat languag glossari aren support languag inform disappear list mean list languag support glossari featur languag list unsupport featur akan assames aymara bhojpuri bambara kurdish dogri divehi ew guarani goan konkani iloko krio ganda lingala mizo maithili manipuri northern sotho oromo quechua sanskrit tigrinya tsonga julian",
        "Question_gpt_summary_original":"The user is questioning whether all languages listed on Google Translate's supported languages list now support the glossary feature, as the information about certain languages not supporting it has disappeared from the list. The user provides a list of languages that were previously marked as unsupported for the glossary feature.",
        "Question_gpt_summary":"user question languag list googl translat support languag list support glossari featur inform certain languag support disappear list user provid list languag previous mark unsupport glossari featur"
    },
    {
        "Question_title":"Azure ML for SAP ERP",
        "Question_body":"<p>I am trying to figure out about standard connectors between SAP ERP product and Azure ML especially for NLP scenarios. Can you please suggest on this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1664541861543,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p><a href=\"\/users\/na\/?userid=dfa9d536-725c-462d-87c8-47fbafb1a2bc\">@D-0887  <\/a> Thanks for the question. Here is the blog that could help and <a href=\"https:\/\/github.com\/microsoft\/nlp-recipes\">nlp recipes<\/a>.    <br \/>\n<a href=\"https:\/\/blogs.sap.com\/2022\/08\/03\/azure-machine-learning-triggering-calculations-ml-in-sap-data-warehouse-cloud\/\">https:\/\/blogs.sap.com\/2022\/08\/03\/azure-machine-learning-triggering-calculations-ml-in-sap-data-warehouse-cloud\/<\/a>    <\/p>\n",
        "Answer_comment_count":0.0,
        "Question_closed_time":1664542897547,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1030800\/azure-ml-for-sap-erp",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"sap erp try figur standard connector sap erp product especi nlp scenario suggest",
        "Question_preprocessed_content":"sap erp try figur standard connector sap erp product especi nlp scenario suggest",
        "Question_gpt_summary_original":"The user is seeking information about standard connectors between SAP ERP and Azure ML, specifically for NLP scenarios. They are requesting suggestions on this matter.",
        "Question_gpt_summary":"user seek inform standard connector sap erp specif nlp scenario request suggest matter"
    },
    {
        "Question_title":"How can I check whether an artifact is available?",
        "Question_body":"<p>Hi, just started to use W&amp;B and managed to refactor some code to use artifact versioning today. What I could not find is (and sorry if this is very basic): during the first run of the program I would like to check if there is already some artifact (raw data) f\u00fcr that project \/ artifact name \/ type available: If yes, use it. If no, prepare it (might take a while). I am looking for the equivalent of <code>&lt;filename&gt;.is_file()<\/code> but for artifacts. I could use\/download the artifact in a <code>try, except<\/code> clause but that\u2019s not very pretty (throwing errors on the console, not sure what the correct Exception is). The API does not seem to provide such a functionality?<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_created_time":1643300292817,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":215.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>Hey Stephan,<\/p>\n<p>Thanks for your response. I think the code you have written is the best way to check if an artifact exists if you do not know a priori if it really exists.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Question_closed_time":1644430595263,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-can-i-check-whether-an-artifact-is-available\/1826",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"check artifact avail start us manag refactor code us artifact version todai sorri basic run program like check artifact raw data project artifact type avail ye us prepar look equival file artifact us download artifact try claus that pretti throw error consol sure correct except api provid function",
        "Question_preprocessed_content":"check artifact avail start us manag refactor code us artifact version todai run program like check artifact project artifact type avail ye us prepar look equival artifact artifact claus that pretti api provid function",
        "Question_gpt_summary_original":"The user is new to using W&B and is trying to find a way to check if an artifact is available during the first run of their program. They are looking for an equivalent of <code>&lt;filename&gt;.is_file()<\/code> but for artifacts and are currently using a <code>try, except<\/code> clause which is not ideal. The user is asking if there is a better way to check if an artifact is available.",
        "Question_gpt_summary":"user new try wai check artifact avail run program look equival file artifact current try claus ideal user ask better wai check artifact avail"
    },
    {
        "Question_title":"Cant install imbalanced-learn on an Azure ML Environment",
        "Question_body":"<p>I have an Azure ML Workspace which comes by default with some pre-installed packages.<\/p>\n<p>I tried to install<\/p>\n<pre><code>!pip install -U imbalanced-learn\n<\/code><\/pre>\n<p>But I got this error<\/p>\n<pre><code>Requirement already up-to-date: scikit-learn in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (0.24.2)\nRequirement already satisfied, skipping upgrade: scipy&gt;=0.19.1 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (1.4.1)\nRequirement already satisfied, skipping upgrade: joblib&gt;=0.11 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (0.14.1)\nRequirement already satisfied, skipping upgrade: numpy&gt;=1.13.3 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (1.18.5)\nRequirement already satisfied, skipping upgrade: threadpoolctl&gt;=2.0.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (2.1.0)\nCollecting imbalanced-learn\n  Using cached imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\nRequirement already satisfied, skipping upgrade: threadpoolctl&gt;=2.0.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (2.1.0)\nRequirement already satisfied, skipping upgrade: joblib&gt;=0.11 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (0.14.1)\nRequirement already satisfied, skipping upgrade: scipy&gt;=1.1.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (1.4.1)\nERROR: Could not find a version that satisfies the requirement scikit-learn&gt;=1.0.1 (from imbalanced-learn) (from versions: 0.9, 0.10, 0.11, 0.12, 0.12.1, 0.13, 0.13.1, 0.14, 0.14.1, 0.15.0b1, 0.15.0b2, 0.15.0, 0.15.1, 0.15.2, 0.16b1, 0.16.0, 0.16.1, 0.17b1, 0.17, 0.17.1, 0.18, 0.18.1, 0.18.2, 0.19b2, 0.19.0, 0.19.1, 0.19.2, 0.20rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.20.4, 0.21rc2, 0.21.0, 0.21.1, 0.21.2, 0.21.3, 0.22rc2.post1, 0.22rc3, 0.22, 0.22.1, 0.22.2, 0.22.2.post1, 0.23.0rc1, 0.23.0, 0.23.1, 0.23.2, 0.24.dev0, 0.24.0rc1, 0.24.0, 0.24.1, 0.24.2)\nERROR: No matching distribution found for scikit-learn&gt;=1.0.1 (from imbalanced-\n<\/code><\/pre>\n<p>learn)<\/p>\n<p>Not sure how to solve this, I have read in other posts to use conda, but that didnt work either.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1644933997077,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1644960360047,
        "Question_score_count":1.0,
        "Question_view_count":219.0,
        "Poster_created_time":1302030303092,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Answer_body":"<p><a href=\"https:\/\/pypi.org\/project\/scikit-learn\/1.0.1\/\" rel=\"nofollow noreferrer\"><code>scikit-learn<\/code> 1.0.1<\/a> and up require Python &gt;= 3.7; you use Python 3.6. You need to upgrade Python or downgrade <code>imbalanced-learn<\/code>. <a href=\"https:\/\/pypi.org\/project\/imbalanced-learn\/0.8.1\/\" rel=\"nofollow noreferrer\"><code>imbalanced-learn<\/code> 0.8.1<\/a> allows Python 3.6 so<\/p>\n<pre><code>!pip install -U &quot;imbalanced-learn &lt; 0.9&quot;\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1644935036627,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1494171603136,
        "Answerer_location":null,
        "Answerer_reputation_count":73187.0,
        "Answerer_view_count":8473.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71127858",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"instal imbalanc learn environ workspac come default pre instal packag tri instal pip instal imbalanc learn got error requir date scikit learn anaconda env lib python site packag requir satisfi skip upgrad scipi anaconda env lib python site packag scikit learn requir satisfi skip upgrad joblib anaconda env lib python site packag scikit learn requir satisfi skip upgrad numpi anaconda env lib python site packag scikit learn requir satisfi skip upgrad threadpoolctl anaconda env lib python site packag scikit learn collect imbalanc learn cach imbalanc learn whl requir satisfi skip upgrad threadpoolctl anaconda env lib python site packag imbalanc learn requir satisfi skip upgrad joblib anaconda env lib python site packag imbalanc learn requir satisfi skip upgrad scipi anaconda env lib python site packag imbalanc learn error version satisfi requir scikit learn imbalanc learn version post post dev error match distribut scikit learn imbalanc learn sure solv read post us conda didnt work",
        "Question_preprocessed_content":"instal environ workspac come default packag tri instal got error learn sure solv read post us conda didnt work",
        "Question_gpt_summary_original":"The user is trying to install imbalanced-learn on an Azure ML Workspace, but is encountering an error stating that it cannot find a matching distribution for scikit-learn>=1.0.1. The user has tried using conda but it did not work.",
        "Question_gpt_summary":"user try instal imbalanc learn workspac encount error state match distribut scikit learn user tri conda work"
    },
    {
        "Question_title":"Unable to access data from S3 bucket to jupyter notebook of aws sagemaker",
        "Question_body":"<p>I need to train a model on aws sagemaker. I'm unable to access data in Jupiter notebook of sagemaker from S3 bucket. My bucket name is &quot;riceleaf&quot; there are four folders in the bucket named as s1,s2,s3,s4 and each folder contains 330 images named as 1.jpg and so on. It is created in Us-east zone. Bucket is private.<\/p>\n<p>One way i did was to access the object and when i displayed the key it shows me 1.jpg and so on. But when i try to open that image it didn't work. So i think I'm unable to get exact data path.<\/p>\n<p>In my code I need exact data path since I'm doing some random data generation in the code so need to access different folders. Therefore, I need a path till bucket so i can change next folder name and image name randomly in my code.<\/p>\n<p>Please help me to so that I can access the images in the Jupiter notebook of sagemaker.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1647841162080,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1647863096848,
        "Question_score_count":0.0,
        "Question_view_count":733.0,
        "Poster_created_time":1647840996452,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71553217",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unabl access data bucket jupyt notebook need train model unabl access data jupit notebook bucket bucket riceleaf folder bucket name folder contain imag name jpg creat east zone bucket privat wai access object displai kei show jpg try open imag work think unabl exact data path code need exact data path random data gener code need access differ folder need path till bucket chang folder imag randomli code help access imag jupit notebook",
        "Question_preprocessed_content":"unabl access data bucket jupyt notebook need train model unabl access data jupit notebook bucket bucket riceleaf folder bucket name folder contain imag name creat zone bucket privat wai access object displai kei show try open imag work think unabl exact data path code need exact data path random data gener code need access differ folder need path till bucket chang folder imag randomli code help access imag jupit notebook",
        "Question_gpt_summary_original":"The user is facing challenges in accessing data from an S3 bucket to a Jupyter notebook of AWS Sagemaker. The bucket contains four folders with 330 images each, and the user needs to access different folders for random data generation in their code. The user has tried accessing the object, but it did not work, and they need help in getting the exact data path to access the images in the Jupyter notebook.",
        "Question_gpt_summary":"user face challeng access data bucket jupyt notebook bucket contain folder imag user need access differ folder random data gener code user tri access object work need help get exact data path access imag jupyt notebook"
    },
    {
        "Question_title":"Trying to do multiple voice files with speech-to-text",
        "Question_body":"Hello.\n\nI'm someone who's trying to make speech-to-text work without being a coder in any way whatsoever. I have let's say hundreds of individual audio files and they go from 30 seconds to a minute and a half. The problem is that uploading them to the bucket makes it so there's hundreds of individual ones. And I need to create a transcriptions individually. what do I do? can I not just transcribe everything in one folder?",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1652292780000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":136.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Trying-to-do-multiple-voice-files-with-speech-to-text\/td-p\/422295\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"try multipl voic file speech text hello try speech text work coder wai whatsoev let hundr individu audio file second minut half problem upload bucket make hundr individu on need creat transcript individu transcrib folder",
        "Question_preprocessed_content":"try multipl voic file hello try work coder wai whatsoev let hundr individu audio file second minut half problem upload bucket make hundr individu on need creat transcript individu transcrib folder",
        "Question_gpt_summary_original":"The user has multiple audio files of varying lengths and wants to transcribe them using speech-to-text. However, uploading them to the bucket creates individual files, making it difficult to create transcriptions for each one. The user is seeking a solution to transcribe all the files in one folder.",
        "Question_gpt_summary":"user multipl audio file vari length want transcrib speech text upload bucket creat individu file make difficult creat transcript user seek solut transcrib file folder"
    },
    {
        "Question_title":"Best Approach to Clientside Machine Learning for Text Classification",
        "Question_body":"<p>I have approximately 100k rows of text data (initially PDF documents that have been OCR). Most are rows of less than 5000 characters. Each of the source documents are addressed to some department. These are typically in the form of the below examples where the target department would 'Urology' (there are several departments).    <\/p>\n<ul>\n<li> Urologly Department  <\/li>\n<li> Urologly Clinic  <\/li>\n<li> Urology Out Patients  <\/li>\n<li> Urology  <\/li>\n<li> Dear urology team  <\/li>\n<\/ul>\n<p>I have read a bit on ML Text Analysis and it seems I should be able to make a pretty good model by reviewing several hundred documents for each department (I have built an App to help me do this) and manually Classifying those documents. Some documents may mention urology but are actually addressed to another department. Typically the addressed department text is at the top third (first 3-7 lines) of the text body.               <\/p>\n<p>I cannot use any online tools, i.e. I can't upload any of the Document text to servers to process I need a client side library. I have read and completed several tutorials using the ML.net but these are pretty basic (sentiment, entity detection without any initial training), and read an excellent blog at <a href=\"+https:\/\/monkeylearn.com\/blog\/how-to-create-text-classifiers-machine-learning\/\">MonkeyLearn<\/a>: which seems to acknowledge that can do what I imagine I should be able to do.    <\/p>\n<p>So can anybody point me in the right direction, can I use some offline Microsoft client library to complete my task? Is there some other Open Source client library i should look at. Will I have to learn Go, or python to complete the task (currently a C# dev).   <\/p>\n<p>Note: I could get fairly good matches simply using SQL Text search and a bit of C# with plenty of hard coded rules, but I thought I'd try ML -- however its a nest of complications at the moment   and i am going around in circles.       <\/p>\n<p>Many Thanks  <br \/>\nMike.        <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1637112667940,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p><a href=\"\/users\/na\/?userid=68a37785-70de-4e30-ac8d-ab0658ca67f4\">@Mike Shapleski  <\/a> I see two possible solutions for your scenario.    <\/p>\n<ol>\n<li> Extracting text from your documents using the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/computer-vision\/vision-api-how-to-topics\/call-read-api\">computer vision API<\/a> and passing the required text as input to Azure <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/language-service\/text-analytics-for-health\/overview?tabs=ner\">Text Analytics for Health API<\/a>    <\/li>\n<li> Using <a href=\"https:\/\/learn.microsoft.com\/en-in\/azure\/search\/search-what-is-azure-search\">Azure cognitive search<\/a> to upload the documents and creating a search service and enabling specific <a href=\"https:\/\/learn.microsoft.com\/en-in\/azure\/search\/cognitive-search-predefined-skills\">skills<\/a> on the service to extract PII data or entities    <\/li>\n<\/ol>\n<p>The first solution can help you achieve this and ensure everything is offline or using docker containers without uploading any of your data to any storage externally. For billing purposes the containers need to connect to a metering endpoint on Azure to bill your usage of both these services(<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/computer-vision\/computer-vision-how-to-install-containers?tabs=version-3-2\">Computer Vision API<\/a> &amp; <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/language-service\/text-analytics-for-health\/how-to\/use-containers\">Azure text analytics<\/a> containers). Also, you can use C# client library to call the local endpoint of these containers. The setup could take time to configure docker containers and passing the PDF documents to the computer vision read API to extract text. The extracted text can then be directly used or stored, to call the text analytics for health API.    <\/p>\n<p>The second solution can be used to index all the documents by using the search service by having your data in the cloud or behind a firewall to index the documents and make them searchable. There are some skills that can be enabled on the search service to extract entities and other PII information but this may not extract the same data as text analytics for health. This solution can be faster to setup because you can directly query your data after uploading the documents.     <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Answer_comment_count":3.0,
        "Question_closed_time":1637144398316,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/629917\/best-approach-to-clientside-machine-learning-for-t",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"best approach clientsid machin learn text classif approxim row text data initi pdf document ocr row charact sourc document address depart typic form exampl target depart urolog depart urologli depart urologli clinic urolog patient urolog dear urolog team read bit text analysi abl pretti good model review document depart built app help manual classifi document document mention urolog actual address depart typic address depart text line text bodi us onlin tool upload document text server process need client librari read complet tutori net pretti basic sentiment entiti detect initi train read excel blog monkeylearn acknowledg imagin abl anybodi point right direct us offlin microsoft client librari complet task open sourc client librari look learn python complet task current dev note fairli good match simpli sql text search bit plenti hard code rule thought try nest complic moment go circl thank mike",
        "Question_preprocessed_content":"best approach clientsid machin learn text classif approxim row text data row charact sourc document address depart typic form exampl target depart urolog urologli depart urologli clinic urolog patient urolog dear urolog team read bit text analysi abl pretti good model review document depart manual classifi document document mention urolog actual address depart typic address depart text text bodi us onlin tool upload document text server process need client librari read complet tutori pretti basic read excel blog monkeylearn acknowledg imagin abl anybodi point right direct us offlin microsoft client librari complet task open sourc client librari look learn python complet task note fairli good match simpli sql text search bit plenti hard code rule thought try nest complic moment go circl thank mike",
        "Question_gpt_summary_original":"The user has 100k rows of text data that need to be classified based on the department they are addressed to. The user wants to use machine learning for text analysis but cannot use any online tools and needs a client-side library. The user has tried using ML.net but found it too basic and is looking for suggestions on offline Microsoft client libraries or other open-source client libraries. The user is currently a C# developer and is struggling with the complexity of the task.",
        "Question_gpt_summary":"user row text data need classifi base depart address user want us machin learn text analysi us onlin tool need client librari user tri net basic look suggest offlin microsoft client librari open sourc client librari user current develop struggl complex task"
    },
    {
        "Question_title":"Debugging R Scripts in azure-ml: Where can stdout and stderr logs be found? (or why are they empty?)",
        "Question_body":"<p>I'm using \"studio (preview)\" from Microsoft Azure Machine Learning to create a pipeline that applies machine learning to a dataset in a blob storage that is connected to our data warehouse.<\/p>\n\n<p>In the \"Designer\", an \"Exectue R Script\" action can be added to the pipeline. I'm using this functionality to execute some of my own machine learning algorithms.<\/p>\n\n<p>I've got a 'hello world' version of this script working (including using the \"script bundle\" to load the functions in my own R files). It applies a very simple manipulation (compute the days difference with the date in the date column and 'today'), and stores the output as a new file. Given that the exported file has the correct information, I know that the R script works well.<\/p>\n\n<p>The script looks like this:<\/p>\n\n<pre><code># R version: 3.5.1\n# The script MUST contain a function named azureml_main\n# which is the entry point for this module.\n\n# The entry point function can contain up to two input arguments:\n#   Param&lt;medals&gt;: a R DataFrame\n#   Param&lt;matches&gt;: a R DataFrame\n\nazureml_main &lt;- function(dataframe1, dataframe2){\n\n  message(\"STARTING R script run.\")\n\n  # If a zip file is connected to the third input port, it is\n  # unzipped under \".\/Script Bundle\". This directory is added\n  # to sys.path.\n\n  message('Adding functions as source...')\n\n  if (FALSE) {\n    # This works...\n      source(\".\/Script Bundle\/first_function_for_script_bundle.R\")\n  } else {\n    # And this works as well!\n    message('Sourcing all available functions...')\n    functions_folder = '.\/Script Bundle'\n\n    list.files(path = functions_folder)\n    list_of_R_functions &lt;- list.files(path = functions_folder, pattern = \"^.*[Rr]$\", include.dirs = FALSE, full.names = TRUE)\n    for (fun in list_of_R_functions) {\n\n      message(sprintf('Sourcing &lt;%s&gt;...', fun))\n\n      source(fun)\n\n    }\n  }\n\n  message('Executing R pipeline...')\n  dataframe1 = calculate_days_difference(dataframe = dataframe1)\n\n  # Return datasets as a Named List\n  return(list(dataset1=dataframe1, dataset2=dataframe2))\n}\n<\/code><\/pre>\n\n<p>And although I do print some messages in the R Script, I haven't been able to find the \"stdoutlogs\" nor the \"stderrlogs\" that should contain these printed messages.<\/p>\n\n<p>I need the printed messages for 1) information on how the analysis went and -most importantly- 2) debugging in case the code failed.<\/p>\n\n<p>Now, I have found (on multiple locations) the files \"stdoutlogs.txt\" and \"stderrlogs.txt\". These can be found under \"Logs\" when I click on \"Exectue R Script\" in the \"Designer\".\nI can also find \"stdoutlogs.txt\" and \"stderrlogs.txt\" files under \"Experiments\" when I click on a finished \"Run\" and then both under the tab \"Outputs\" and under the tab \"Logs\".\nHowever... all of these files are empty.<\/p>\n\n<p>Can anyone tell me how I can print messages from my R Script and help me locate where I can find the printed information?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1579792546180,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":287.0,
        "Poster_created_time":1534511592567,
        "Poster_location":"Netherlands",
        "Poster_reputation_count":423.0,
        "Poster_view_count":17.0,
        "Answer_body":"<p>Can you please click on the \"Execute R module\" and download the 70_driver.log? I tried message(\"STARTING R script run.\") in an R sample and can found the output there.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Z7s7h.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Z7s7h.png\" alt=\"view logs for a execute R script module\"><\/a><\/p>",
        "Answer_comment_count":5.0,
        "Question_closed_time":1579829908723,
        "Answer_last_edit_time":1579846977960,
        "Answer_score_count":2.0,
        "Answerer_created_time":1567164934556,
        "Answerer_location":null,
        "Answerer_reputation_count":96.0,
        "Answerer_view_count":25.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59881727",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"debug script azur stdout stderr log studio preview creat pipelin appli machin learn dataset blob storag connect data warehous design exectu script action ad pipelin function execut machin learn algorithm got hello world version script work includ script bundl load function file appli simpl manipul comput dai differ date date column todai store output new file given export file correct inform know script work script look like version script contain function name main entri point modul entri point function contain input argument param datafram param datafram main fun sourc fun messag execut pipelin datafram calcul dai differ datafram datafram return dataset name list return list dataset datafram dataset datafram print messag script haven abl stdoutlog stderrlog contain print messag need print messag inform analysi went importantli debug case code fail multipl locat file stdoutlog txt stderrlog txt log click exectu script design stdoutlog txt stderrlog txt file experi click finish run tab output tab log file tell print messag script help locat print inform",
        "Question_preprocessed_content":"debug script stdout stderr log studio creat pipelin appli machin learn dataset blob storag connect data warehous design exectu script action ad pipelin function execut machin learn algorithm got hello world version script work appli simpl manipul store output new file given export file correct inform know script work script look like print messag script haven abl stdoutlog stderrlog contain print messag need print messag inform analysi went importantli debug case code fail file log click exectu script design file experi click finish run tab output tab log file tell print messag script help locat print inform",
        "Question_gpt_summary_original":"The user is using \"studio (preview)\" from Microsoft Azure Machine Learning to create a pipeline that applies machine learning to a dataset in a blob storage that is connected to their data warehouse. They are using the \"Execute R Script\" action to execute their own machine learning algorithms. Although they print some messages in the R Script, they cannot find the \"stdoutlogs\" nor the \"stderrlogs\" that should contain these printed messages. They need the printed messages for debugging in case the code failed. The user has found the files \"stdoutlogs.txt\" and \"stderrlogs.txt\" in multiple locations, but all of these files are empty. They are seeking help to locate where they can find the printed information.",
        "Question_gpt_summary":"user studio preview creat pipelin appli machin learn dataset blob storag connect data warehous execut script action execut machin learn algorithm print messag script stdoutlog stderrlog contain print messag need print messag debug case code fail user file stdoutlog txt stderrlog txt multipl locat file seek help locat print inform"
    },
    {
        "Question_title":"\"Internal error encountered\" \"code: 13\"",
        "Question_body":"Hello friends, I would like to ask you a question.\nI am working with DocumentIA, and after uploading the documents and tagging them, when training the model, I got the following error:\n\nerror\ncode: 13\nmessage: \"Internal error encountered,\ndetails: []\n\nAs you can see there are no details. I didn't find anything in the documentation either.\n\nCan anyone tell me what that means and how to fix it.\n\nThank you very much!",
        "Question_answer_count":13,
        "Question_comment_count":0,
        "Question_created_time":1672058520000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":810.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/quot-Internal-error-encountered-quot-quot-code-13-quot\/td-p\/503519\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"intern error encount code hello friend like ask question work documentia upload document tag train model got follow error error code messag intern error encount detail detail document tell mean fix thank",
        "Question_preprocessed_content":"intern error encount code hello friend like ask question work documentia upload document tag train model got follow error error code messag intern error encount detail detail document tell mean fix thank",
        "Question_gpt_summary_original":"The user encountered an \"Internal error\" with \"code: 13\" while training a model in DocumentIA. The error message did not provide any details, and the user is seeking help to understand the issue and how to fix it.",
        "Question_gpt_summary":"user encount intern error code train model documentia error messag provid detail user seek help understand issu fix"
    },
    {
        "Question_title":"Deployment timeout while deploying the ML model in the Azure Container Instance with the azure pipeline",
        "Question_body":"<p>Our code was working fine till yesterday while deploying the ML model on Azure ACI, but facing the deployment timeout or sometimes 400 (Bad response) today.<\/p>\n<p>Is there any issue with microsoft datcenters or there is something else ?<\/p>\n<p>The errors are given below<\/p>\n<blockquote>\n<p>&quot;message&quot;: &quot;Received bad response from Model Management Service:\\nResponse Code: 400\\nHeaders: {'Date': 'Thu, 06 Aug 2020 21:05:31 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d', 'x-ms-client-request-id': '8aa4dcafb0ec40ef822b55fc4c7a196f', 'x-ms-client-session-id': 'bf2decff-8879-4a9c-bb5f-2fae7034946c', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'x-request-time': '0.044', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{&quot;code&quot;:&quot;BadRequest&quot;,&quot;statusCode&quot;:400,&quot;message&quot;:&quot;The request is invalid.&quot;,&quot;details&quot;:[{&quot;code&quot;:&quot;InvalidOverwriteRequest&quot;,&quot;message&quot;:&quot;Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.&quot;}],&quot;correlation&quot;:{&quot;RequestId&quot;:&quot;8aa4dcafb0ec40ef822b55fc4c7a196f&quot;}}'&quot;<\/p>\n<\/blockquote>\n<blockquote>\n<p>&quot;message&quot;: &quot;Service deployment polling reached non-successful terminal state, current service state: Unhealthy\\nOperation ID: 1b6746b3-cdb6-4e0c-a43d-b31b34ac99b2\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n &quot;code&quot;: &quot;DeploymentTimedOut&quot;,\\n &quot;statusCode&quot;: 504,\\n &quot;message&quot;: &quot;The deployment operation polling has TimedOut. The service creation is taking longer than our normal time. We are still trying to achieve the desired state for the web service. Please check the webservice state for the current webservice health. You can run print(service.state) from the python SDK to retrieve the current state of the webservice.&quot;\\n}&quot;<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1596785856050,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1596790304580,
        "Question_score_count":0.0,
        "Question_view_count":606.0,
        "Poster_created_time":1593185044876,
        "Poster_location":"Lahore, Pakistan",
        "Poster_reputation_count":11.0,
        "Poster_view_count":9.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63297308",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"deploy timeout deploi model azur contain instanc azur pipelin code work fine till yesterdai deploi model azur aci face deploy timeout bad respons todai issu microsoft datcent error given messag receiv bad respons model manag servic nrespons code nheader date thu aug gmt content type applic json transfer encod chunk connect aliv request context appid cid dee eea client request aadcafbecefbfccaf client session bfdecff bbf faec api support version preview request time strict transport secur max ag includesubdomain preload ncontent code badrequest statuscod messag request invalid detail code invalidoverwriterequest messag invalid overwrit request updat contain resourc requir dn label deploy type delet redeploi servic correl requestid aadcafbecefbfccaf messag servic deploy poll reach non success termin state current servic state unhealthi noper cdb bbacb nmore inform log nerror code deploymenttimedout statuscod messag deploy oper poll timedout servic creation take longer normal time try achiev desir state web servic check webservic state current webservic health run print servic state python sdk retriev current state webservic",
        "Question_preprocessed_content":"deploy timeout deploi model azur contain instanc azur pipelin code work fine till yesterdai deploi model azur aci face deploy timeout todai issu microsoft datcent error given messag receiv bad respons model manag code correl requestid dcafb messag servic deploy poll reach termin state current servic state inform code statuscod messag deploy oper poll timedout servic creation take longer normal time try achiev desir state web servic check webservic state current webservic health run python sdk retriev current state",
        "Question_gpt_summary_original":"The user is facing deployment timeout or 400 (Bad response) errors while deploying an ML model on Azure ACI. The error messages indicate that the deployment operation polling has timed out and the service creation is taking longer than normal. The user is unsure if there is an issue with Microsoft data centers or if there is something else causing the problem.",
        "Question_gpt_summary":"user face deploy timeout bad respons error deploi model azur aci error messag indic deploy oper poll time servic creation take longer normal user unsur issu microsoft data center caus problem"
    },
    {
        "Question_title":"Saving Machine Learning Logs on MongoDB",
        "Question_body":"<p>How can i save the logs of machine learing model trained in sagemaker like (Creation time, time taken for a job to learn etc.) on mongoDB<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1578479966177,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1578519215528,
        "Question_score_count":0.0,
        "Question_view_count":56.0,
        "Poster_created_time":1578479768727,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59644059",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"save machin learn log mongodb save log machin lear model train like creation time time taken job learn mongodb",
        "Question_preprocessed_content":"save machin learn log mongodb save log machin lear model train like mongodb",
        "Question_gpt_summary_original":"The user is seeking guidance on how to save machine learning logs, including creation time and job duration, on MongoDB after training a model in Sagemaker.",
        "Question_gpt_summary":"user seek guidanc save machin learn log includ creation time job durat mongodb train model"
    },
    {
        "Question_title":"How to create a python package inside a SageMaker?",
        "Question_body":"<p>Python packages can be created easily by having a <strong>init<\/strong> module and a combination of other modules, then we can import a function from one module to another. Now the question is can the same thing be done in Jupyter notebook? Like can all the modules (instead of being .py file being a .ipynb file. The motivation for this question is, can we create a python package inside a SageMaker? By package I mean init and bunch of other modules and a higher level module to call other modules.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1642745647980,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":137.0,
        "Poster_created_time":1406319465827,
        "Poster_location":null,
        "Poster_reputation_count":701.0,
        "Poster_view_count":68.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70797126",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"creat python packag insid python packag creat easili have init modul combin modul import function modul question thing jupyt notebook like modul instead file ipynb file motiv question creat python packag insid packag mean init bunch modul higher level modul modul",
        "Question_preprocessed_content":"creat python packag insid python packag creat easili have init modul combin modul import function modul question thing jupyt notebook like modul instead file ipynb file motiv question creat python packag insid packag mean init bunch modul higher level modul modul",
        "Question_gpt_summary_original":"The user is trying to create a Python package inside a SageMaker and is wondering if it can be done in a Jupyter notebook by having an init module and other modules, with the modules being .ipynb files instead of .py files. The user wants to know if a higher level module can be created to call other modules.",
        "Question_gpt_summary":"user try creat python packag insid wonder jupyt notebook have init modul modul modul ipynb file instead file user want know higher level modul creat modul"
    },
    {
        "Question_title":"Splitting data into test and train",
        "Question_body":"Please guide me with a tutorial for splitting the data into train and test data.\nAWS is using Sagemaker for machine learning for the new customers.I am a beginner in machine learning.\n\nPlease throw some light",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1676529352231,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1676875618464,
        "Question_score_count":0.0,
        "Question_view_count":31.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHVqqziWJTZG8YkJmYU6piQ\/splitting-data-into-test-and-train",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"split data test train guid tutori split data train test data aw machin learn new custom beginn machin learn throw light",
        "Question_preprocessed_content":"split data test train guid tutori split data train test data aw machin learn new beginn machin learn throw light",
        "Question_gpt_summary_original":"The user is a beginner in machine learning and is seeking guidance on how to split data into train and test data using Sagemaker, which is being used by AWS for machine learning for new customers.",
        "Question_gpt_summary":"user beginn machin learn seek guidanc split data train test data aw machin learn new custom"
    },
    {
        "Question_title":"'azureml.logging' module not found",
        "Question_body":"<p>I can't seem to get the logger to work in the online azure notesbooks workspace. I'm using python 3.6 environment.\nRunning this import:<\/p>\n\n<pre><code>from azureml.logging import get_azureml_logger\n<\/code><\/pre>\n\n<p>gives me the following error:<\/p>\n\n<pre><code>ModuleNotFoundError: No module named 'azureml.logging\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1521752364630,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":1597.0,
        "Poster_created_time":1326743808803,
        "Poster_location":null,
        "Poster_reputation_count":635.0,
        "Poster_view_count":50.0,
        "Answer_body":"<p>The solution is <\/p>\n\n<p>pip install \"<a href=\"https:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\" rel=\"nofollow noreferrer\">https:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D<\/a>\"<\/p>\n\n<p>I found the blob url here. It's inside docker container dependencies.<\/p>\n\n<p><a href=\"https:\/\/github.com\/Azure\/LearnAI-Bootcamp\/blob\/master\/lab03.3_manage_conda_envs_in_aml\/0_README.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/LearnAI-Bootcamp\/blob\/master\/lab03.3_manage_conda_envs_in_aml\/0_README.md<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1526723551880,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1340784274407,
        "Answerer_location":null,
        "Answerer_reputation_count":150.0,
        "Answerer_view_count":12.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49438358",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"log modul logger work onlin azur notesbook workspac python environ run import log import logger give follow error modulenotfounderror modul name log",
        "Question_preprocessed_content":"modul logger work onlin azur notesbook workspac python environ run import give follow error",
        "Question_gpt_summary_original":"The user is encountering an issue with the 'azureml.logging' module not being found while trying to use the logger in the online Azure notebooks workspace. The error message \"ModuleNotFoundError: No module named 'azureml.logging'\" is displayed when trying to import the module.",
        "Question_gpt_summary":"user encount issu log modul try us logger onlin azur notebook workspac error messag modulenotfounderror modul name log displai try import modul"
    },
    {
        "Question_title":"What is the best and correct way of hosting an endpoint running R code?",
        "Question_body":"<p>I think it must be a relatively common use case to load a model and invoke an endpoint to call R's <code>predict(object, newdata, ...)<\/code> function.  I wanted to do this with a custom AWS Sagemaker container, using <code>plumber<\/code> on the R side.  This <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/r_examples\/r_byo_r_algo_hpo\" rel=\"nofollow noreferrer\">example<\/a> gives all the details, I think, and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html\" rel=\"nofollow noreferrer\">this bit of documentation<\/a> also explains how the container should be built and react.\nI followed the steps of these documents, but I get<\/p>\n<blockquote>\n<p>The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.<\/p>\n<\/blockquote>\n<p>in the Sagemaker console after a couple of long minutes, and the endpoint creation fails.<\/p>\n<p>This is my container:<\/p>\n<pre><code># --- Dockerfile\nFROM rocker\/r-base\nRUN apt-get -y update &amp;&amp; apt-get install -y libsodium-dev libcurl4-openssl-dev\nRUN apt-get install -y  \\\n    ca-certificates\n\nRUN R -e &quot;install.packages(c('lme4', 'plumber'))&quot;\n\nADD .\/plumber.R \/\n\nENTRYPOINT [&quot;R&quot;, &quot;-e&quot;, &quot;plumber::pr_run(plumber::pr('plumber.R'), port=8080)&quot;, \\\n            &quot;--no-save&quot;]\n<\/code><\/pre>\n<pre><code># --- plumber.R\nlibrary(plumber)\nlibrary(lme4)\n\nprefix &lt;- '\/opt\/ml'\nprint(dir('\/opt\/ml', recursive = TRUE))\nmodel &lt;- readRDS(file.path(prefix, 'model', 'model.RDS'))\n\n#* @apiTitle Guess the likelihood of something\n\n#' Ping to show server is there\n#' @get \/ping\nfunction() {\n  print(paste('successfully pinged at', Sys.time()))\n  return('')}\n\n#' Parse input and return prediction from model\n#' @param req The http request sent\n#' @post \/invocations\nfunction(req) {\n  print(paste('invocation triggered at', Sys.time()))\n  conn &lt;- textConnection(gsub('\\\\\\\\n', '\\n', req$postBody))\n  data &lt;- read.csv(conn)\n  close(conn)\n  \n  print(data)\n  \n  predict(model, data,\n          allow.new.levels = TRUE,\n          type = 'response')\n}\n\n<\/code><\/pre>\n<p>And then the endpoint is created using this code:<\/p>\n<pre><code># run_on_sagemaker.py\n# [...]\ncreate_model_response = sm.create_model(\n    ModelName=model_name,\n    ExecutionRoleArn=role,\n    PrimaryContainer={\n        'Image': image_uri,\n        'ModelDataUrl': s3_model_location\n    }\n)\ncreate_endpoint_config_response = sm.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[{\n        'InstanceType': instance_type,\n        'InitialInstanceCount': 1,\n        'ModelName': model_name,\n        'VariantName': 'AllTraffic'}])\n\nprint(&quot;Endpoint Config Arn: &quot; + create_endpoint_config_response['EndpointConfigArn'])\n\nprint('Endpoint Response:')\ncreate_endpoint_response = sm.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=endpoint_config_name)\nprint(create_endpoint_response['EndpointArn'])\n\nresp = sm.describe_endpoint(EndpointName=endpoint_name)\nstatus = resp['EndpointStatus']\nprint(&quot;Status: &quot; + status)\n\ntry:\n    sm.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\nfinally:\n    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n    status = resp['EndpointStatus']\n    print(&quot;Arn: &quot; + resp['EndpointArn'])\n    print(&quot;Status: &quot; + status)\n    if status != 'InService':\n        raise Exception('Endpoint creation did not succeed')\n    print(create_model_response['ModelArn'])\n<\/code><\/pre>\n<p>Most code is actually copied from the abovementioned example, the most significant difference I note is that in my container the model is loaded right away while in the example it loads the model object every time an invocation is made (which must be slowing responses down, so i wonder, why?).<\/p>\n<p>The logs on Cloudwatch equal the output of the container when it's run locally and indicate no failure.  Locally I can query the container with\n<code>curl -d &quot;data\\nin\\ncsv\\nformat&quot; -i localhost:8080\/invocations<\/code> and it works fine and gives back a prediction for every row in the POST data.  Also, <code>curl localhost:8080\/ping<\/code> returns <code>[&quot;&quot;]<\/code>, as it should, I think.  And it shows no signs of being slow, the model object is a 4.4MiB in size (although this is to be extended greatly once this simple version runs).<\/p>\n<p>The error on the terminal is<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;run_on_sagemaker.py&quot;, line 57, in &lt;module&gt;\n    sm.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n  File &quot;[...]\/lib\/python3.8\/site-packages\/botocore\/waiter.py&quot;, line 53, in wait\n    Waiter.wait(self, **kwargs)\n  File &quot;[...]\/lib\/python3.8\/site-packages\/botocore\/waiter.py&quot;, line 320, in wait\n    raise WaiterError(\nbotocore.exceptions.WaiterError: Waiter EndpointInService failed: Waiter encountered a terminal failure state\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;run_on_sagemaker.py&quot;, line 64, in &lt;module&gt;\n    raise Exception('Endpoint creation did not succeed')\n\n<\/code><\/pre>\n<p>So, why is this failing on the Sagemaker console?  Is this a good way, are there better ways, and how can I do further diagnostics?  Generally, I also could not get the AWS example (see above) for your own R container running, so I wonder what the best way to run R predictions of a Sagemaker model is.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1612450842990,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1612451306600,
        "Question_score_count":1.0,
        "Question_view_count":163.0,
        "Poster_created_time":1508013612543,
        "Poster_location":null,
        "Poster_reputation_count":121.0,
        "Poster_view_count":10.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66048199",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"best correct wai host endpoint run code think rel common us case load model invok endpoint predict object newdata function want custom contain plumber exampl give detail think bit document explain contain built react follow step document primari contain product variant alltraff pass ping health check check cloudwatch log endpoint consol coupl long minut endpoint creation fail contain dockerfil rocker base run apt updat apt instal libsodium dev libcurl openssl dev run apt instal certif run instal packag lme plumber add plumber entrypoint plumber run plumber plumber port save plumber librari plumber librari lme prefix waiter endpoint servic wait endpointnam endpoint file lib python site packag botocor waiter line wait waiter wait self kwarg file lib python site packag botocor waiter line wait rais waitererror botocor except waitererror waiter endpointinservic fail waiter encount termin failur state handl except except occur traceback recent file run line rais except endpoint creation succe fail consol good wai better wai diagnost gener aw exampl contain run wonder best wai run predict model",
        "Question_preprocessed_content":"best correct wai host endpoint run code think rel common us case load model invok endpoint function want custom contain exampl give detail think bit document explain contain built react follow step document primari contain product variant alltraff pass ping health check check cloudwatch log endpoint consol coupl long minut endpoint creation fail contain endpoint creat code code actual copi abovement exampl signific differ note contain model load right awai exampl load model object time invoc log cloudwatch equal output contain run local indic failur local queri contain work fine give predict row post data return think show sign slow model object size error termin fail consol good wai better wai diagnost gener aw exampl contain run wonder best wai run predict model",
        "Question_gpt_summary_original":"The user is trying to host an endpoint running R code using a custom AWS Sagemaker container with Plumber on the R side. The user followed the steps in the documentation but encountered an error message stating that the primary container for production variant AllTraffic did not pass the ping health check. The user's container loads the model right away, unlike the example which loads the model object every time an invocation is made. The user is seeking advice on why the endpoint creation is failing and how to do further diagnostics.",
        "Question_gpt_summary":"user try host endpoint run code custom contain plumber user follow step document encount error messag state primari contain product variant alltraff pass ping health check user contain load model right awai unlik exampl load model object time invoc user seek advic endpoint creation fail diagnost"
    },
    {
        "Question_title":"sizing cloud resources for an AI credit risk analysis application",
        "Question_body":"<p>AI SaaS\/CRM solution focused on enhancing b2b credit risk analysis. The customer is trying to size his solution in Azure cloud for investors  <br \/>\nbut isn\u2019t clear on how much compute, networking, etc he\u2019ll need and if any of that is included in the AI\/ML framework solutions Azure cloud  offers.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1632292183837,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/561581\/sizing-cloud-resources-for-an-ai-credit-risk-analy",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"size cloud resourc credit risk analysi applic saa crm solut focus enhanc credit risk analysi custom try size solut azur cloud investor isnt clear comput network hell need includ framework solut azur cloud offer",
        "Question_preprocessed_content":"size cloud resourc credit risk analysi applic solut focus enhanc credit risk analysi custom try size solut azur cloud investor isnt clear comput network hell need includ framework solut azur cloud offer",
        "Question_gpt_summary_original":"The user is facing challenges in determining the appropriate amount of compute and networking resources required for an AI credit risk analysis application in Azure cloud. They are also unsure if any of these resources are included in the AI\/ML framework solutions offered by Azure cloud.",
        "Question_gpt_summary":"user face challeng determin appropri comput network resourc requir credit risk analysi applic azur cloud unsur resourc includ framework solut offer azur cloud"
    },
    {
        "Question_title":"How to debug invocation timeout in sagemaker?",
        "Question_body":"I am testing inference in sagemaker , by using one of the container listed here ->  https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md. the model is zipped up as below  and with in  inference.py file , i am overwriting functions like  model_fn method and predict_fn. I tested this with batch transform and it worked but for few small input files but for other larger files, i keep getting \"Model server did not respond to \/invocations request within 3600 seconds\" . I'm trying to find out what is the cause of it? 3600 is the max we can set for \"invocation timeout in seconds\" parameter and the default input size for batch  is 6mb , the input files i'm using are way smaller than that but i still get that error. \n\nDirectory structure \n```\nmodel.tar.gz\/\n|- model.pth\n|- code\/\n  |- inference.py\n  |- requirements.txt  \n\n```\nfile : inference.py\n```\nimport torch\nimport os\n\ndef model_fn(model_dir):\n    model = Your_Model()\n    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\n        model.load_state_dict(torch.load(f))\n    return model\n\ndef predict_fn():\n    \/\/\n```\n based on docs here, https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html#your-algorithms-batch-code-how-containers-should-respond-to-inferences, do we need to install flask and have an \/invocations endpoint , that responds 200 ok , when we are using custom container?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1649727502799,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668588423840,
        "Question_score_count":0.0,
        "Question_view_count":1125.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE4UPZjwNQveIG8zuZeXIgA\/how-to-debug-invocation-timeout-in-sagemaker",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"debug invoc timeout test infer contain list http github com aw deep learn contain blob master avail imag model zip infer file overwrit function like model method predict test batch transform work small input file larger file get model server respond invoc request second try caus max set invoc timeout second paramet default input size batch input file wai smaller error directori structur model tar model pth code infer requir txt file infer import torch import def model model dir model model open path join model dir model pth model load state dict torch load return model def predict base doc http doc aw amazon com latest algorithm batch code html algorithm batch code contain respond infer need instal flask invoc endpoint respond custom contain",
        "Question_preprocessed_content":"debug invoc timeout test infer contain list model zip file overwrit function like method test batch transform work small input file larger file get model server respond invoc request second try caus max set invoc timeout second paramet default input size batch input file wai smaller error directori structur file base doc need instal flask invoc endpoint respond custom contain",
        "Question_gpt_summary_original":"The user is encountering an issue with invocation timeout while testing inference in sagemaker using a custom container. The user has set the maximum invocation timeout to 3600 seconds, but still receives the error message \"Model server did not respond to \/invocations request within 3600 seconds\" for larger input files. The user is trying to find the cause of the issue and wonders if they need to install Flask and have an \/invocations endpoint that responds 200 OK when using a custom container.",
        "Question_gpt_summary":"user encount issu invoc timeout test infer custom contain user set maximum invoc timeout second receiv error messag model server respond invoc request second larger input file user try caus issu wonder need instal flask invoc endpoint respond custom contain"
    },
    {
        "Question_title":"Pickle Load- File Not Found when deploying using Azure ML Studio",
        "Question_body":"<p>I have saved a classifier model with pickle using the following code-  <\/p>\n<pre><code>import pickle\nwith open('skm.pickle', 'wb') as fid:\n    pickle.dump(clf, fid) \n<\/code><\/pre>\n<p>Now, during deployment, when I try to load this same model, it is giving an error-  <\/p>\n<pre><code>Error:\n{\n  &quot;code&quot;: &quot;AciDeploymentFailed&quot;,\n  &quot;statusCode&quot;: 400,\n  &quot;message&quot;: &quot;Aci Deployment failed with exception: Error in entry script, FileNotFoundError: [Errno 2] No such file or directory: '.\/skm.pickle', please run print(service.get_logs()) to get details.&quot;,\n  &quot;details&quot;: [\n    {\n      &quot;code&quot;: &quot;CrashLoopBackOff&quot;,\n      &quot;message&quot;: &quot;Error in entry script, FileNotFoundError: [Errno 2] No such file or directory: '.\/skm.pickle', please run print(service.get_logs()) to get details.&quot;\n    }\n  ]\n}\n<\/code><\/pre>\n<p>This is the score.py file where I am loading the pickle model and the same file is called during deployment. Also note that, all these files (code, pickle file and related files) are in the same directory.  <\/p>\n<pre><code>%%writefile sklearnscore.py\n\nimport json\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nimport pickle\n\n# Initialize the deployment environment\ndef init():\n    # read in the model file\n    from sklearn.pipeline import Pipeline\n    global obj\n    with open('.\/skm.pickle', 'rb') as f:\n        obj = pickle.load(f)\n<\/code><\/pre>\n<p>I am registering the model using- <code>model = Model.register(ws, model_name=&quot;utility15&quot;, model_path=&quot;.\/skm.pickle&quot;)<\/code>  <\/p>\n<p>And the deployment code is-  <\/p>\n<pre><code>service_name = 'my-custom-env-service-4'\nsklearn_env = Environment.from_conda_specification(name='sklearn-env', file_path='Sklearn.yaml')\n\ninference_config = InferenceConfig(entry_script='sklearnscore.py', environment=sklearn_env)\n\naci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=4,tags={'Createdby':'xyz'})\n\nservice = Model.deploy(workspace=ws,\n                        name=service_name,\n                        models=[model],\n                        inference_config=inference_config,\n                        deployment_config=aci_config,\n                        overwrite=True)\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n<p>When this script is run, it calls the score.py file and the file not found error comes up for pickle file. I have even tried loading the model without the .\/ thing, but the same error comes up.  <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1653375158970,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/861537\/pickle-load-file-not-found-when-deploying-using-az",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"pickl load file deploi studio save classifi model pickl follow code import pickl open skm pickl fid pickl dump clf fid deploy try load model give error error code acideploymentfail statuscod messag aci deploy fail except error entri script filenotfounderror errno file directori skm pickl run print servic log detail detail code crashloopbackoff messag error entri script filenotfounderror errno file directori skm pickl run print servic log detail score file load pickl model file call deploy note file code pickl file relat file directori writefil sklearnscor import json import panda sklearn preprocess import minmaxscal sklearn ensembl import randomforestclassifi import pickl initi deploy environ def init read model file sklearn pipelin import pipelin global obj open skm pickl obj pickl load regist model model model regist model util model path skm pickl deploy code servic custom env servic sklearn env environ conda specif sklearn env file path sklearn yaml infer config inferenceconfig entri script sklearnscor environ sklearn env aci config aciwebservic deploi configur cpu core memori tag createdbi xyz servic model deploi workspac servic model model infer config infer config deploy config aci config overwrit true servic wait deploy output true script run call score file file error come pickl file tri load model thing error come",
        "Question_preprocessed_content":"pickl load file deploi studio save classifi model pickl follow code deploy try load model give error file load pickl model file call deploy note file directori regist model deploy code script run call file file error come pickl file tri load model thing error come",
        "Question_gpt_summary_original":"The user has encountered an error while deploying a classifier model using Azure ML Studio. The model was saved with pickle, but during deployment, the user is getting a \"File Not Found\" error for the pickle file. The score.py file is used to load the pickle model during deployment, and all the related files are in the same directory. The user has registered the model and used the deployment code, but the error persists.",
        "Question_gpt_summary":"user encount error deploi classifi model studio model save pickl deploy user get file error pickl file score file load pickl model deploy relat file directori user regist model deploy code error persist"
    },
    {
        "Question_title":"Guild runs very slow",
        "Question_body":"<p>Similarly to what has been described in <a href=\"https:\/\/my.guild.ai\/t\/guild-run-hangs-very-slow\/362\">this thread<\/a>, running the following command results in hanging for a while before we see any results:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild run train\n<\/code><\/pre>\n<p>The Guild file contains the following:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">train:\n  main: script\n<\/code><\/pre>\n<p>The script is simply:<\/p>\n<pre data-code-wrap=\"py\"><code class=\"lang-nohighlight\">print(\"Hello, world!\")\n<\/code><\/pre>\n<p>I believe the hanging is related to the fact that I have a data folder which contains thousands of files each corresponding to a sample of the dataset I\u2019m using in my experiments. This is because:<\/p>\n<ol>\n<li>When using the <code>--debug<\/code> flag, Guild hangs just after copying one <code>.csv<\/code> file within the data folder (none of the thousands of sample files are copied to the target directory, though).<\/li>\n<li>When I remove the data folder, the hanging stops.<\/li>\n<\/ol>\n<hr>\n<p>My questions are:<\/p>\n<ol>\n<li>Is this a bug? If so, how can I circumvent this issue?<\/li>\n<li>How am I supposed to make my dataset accessible by the training script, since it is run from a totally different directory and there are thousands of files it needs to access?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1663441155671,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":161.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-runs-very-slow\/919",
        "Tool":"Guild AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"run slow similarli describ thread run follow command result hang result run train file contain follow train main script script simpli print hello world believ hang relat fact data folder contain thousand file correspond sampl dataset experi debug flag hang copi csv file data folder thousand sampl file copi target directori remov data folder hang stop question bug circumv issu suppos dataset access train script run total differ directori thousand file need access",
        "Question_preprocessed_content":"run slow similarli describ thread run follow command result hang result file contain follow script simpli believ hang relat fact data folder contain thousand file correspond sampl dataset experi flag hang copi file data folder remov data folder hang stop question bug circumv issu suppos dataset access train script run total differ directori thousand file need access",
        "Question_gpt_summary_original":"The user is experiencing slow performance while running the \"guild run train\" command, which is believed to be related to the presence of a data folder containing thousands of files. The user suspects that the issue is a bug and is seeking a solution to make the dataset accessible to the training script.",
        "Question_gpt_summary":"user experienc slow perform run run train command believ relat presenc data folder contain thousand file user suspect issu bug seek solut dataset access train script"
    },
    {
        "Question_title":"Trying to understand data storage",
        "Question_body":"<p>Hi, I am a newcomer to DVC. I read through the tutorial for Data and Model Versioning, and I want to understand how data is stored.<\/p>\n<p>Specifically, say I pull from my remote storage 1000 images, and add 1000 images to create a new dataset of 2000 images. If I dvc add the new data folder and dvc push, how is it stored in remote? Is it like 1000 images (original folder), and then add 1000 images, or is it 1000 images, and then a folder with 2000 images? The reason for the confusion is because at <a href=\"https:\/\/dvc.org\/doc\/start\/data-management\/data-versioning\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Get Started: Data Versioning<\/a> (7:54), it shows that there are 2 folders in the remote google drive. I want to know this because I am a ML engineer for a computer vision company and we would have huge datasets, so we don\u2019t want any duplicates across versions.<\/p>\n<p>Furthermore, if I do a dvc checkout to a previous version (1000 images) of the dataset in my local, do 1000 images just disappear from my newer dataset (2000 images). I see this happen in the above video, where dvc checkout with a previous data\/data.xml.dvc causes the data to be 36M instead of 72M. How exactly does dvc do this switch so quickly, and does it work for a folder of images vs just a single xml files?<\/p>\n<p>Sorry if these are stupid questions, I am completely new to this.<\/p>",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_created_time":1667019094738,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":5.0,
        "Question_view_count":168.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/trying-to-understand-data-storage\/1378",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"try understand data storag newcom read tutori data model version want understand data store specif pull remot storag imag add imag creat new dataset imag add new data folder push store remot like imag origin folder add imag imag folder imag reason confus start data version show folder remot googl drive want know engin vision compani huge dataset dont want duplic version furthermor checkout previou version imag dataset local imag disappear newer dataset imag happen video checkout previou data data xml caus data instead exactli switch quickli work folder imag singl xml file sorri stupid question complet new",
        "Question_preprocessed_content":"try understand data storag newcom read tutori data model version want understand data store specif pull remot storag imag add imag creat new dataset imag add new data folder push store remot like imag add imag imag folder imag reason confus start data version show folder remot googl drive want know engin vision compani huge dataset dont want duplic version furthermor checkout previou version dataset local imag disappear newer dataset happen video checkout previou caus data instead exactli switch quickli work folder imag singl xml file sorri stupid question complet new",
        "Question_gpt_summary_original":"The user is a newcomer to DVC and is trying to understand how data is stored when using DVC. They are specifically concerned about avoiding duplicates across versions when dealing with large datasets. They are also curious about how DVC handles switching between different versions of a dataset and whether it works for folders of images or just single files.",
        "Question_gpt_summary":"user newcom try understand data store specif concern avoid duplic version deal larg dataset curiou handl switch differ version dataset work folder imag singl file"
    },
    {
        "Question_title":"Bug in JSON parsing of SageMaker endpoint?",
        "Question_body":"<p>I am using a TF model that is deployed in SageMaker to make predictions.\nThe input for prediction is an array of Strings.\nE.g. I can call it successfully with:<\/p>\n\n<pre><code>input = {\n  'instances': [\"Hello world\"]\n}\nfrom sagemaker.tensorflow.model import TensorFlowPredictor\npredictor = TensorFlowPredictor('endpoint-name', sagemaker_session)\npredictor.predict(input)\n<\/code><\/pre>\n\n<p>However, if a String in the String array includes '][' I get an error message about the validity of the Json array<\/p>\n\n<pre><code>input = {\n  'instances': [\"][\"]\n}\nfrom sagemaker.tensorflow.model import TensorFlowPredictor\npredictor = TensorFlowPredictor('endpoint-name', sagemaker_session)\npredictor.predict(input)\n\n<\/code><\/pre>\n\n<p>The error message is:<\/p>\n\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message \"{ \"error\": \"JSON Value: {\\n    \\\"instances\\\": {\\n        \\\"instances\\\": [\\n            \\\"][\\\"\\n        ]\\n    }\\n} Excepting 'instances' to be an list\/array\" }\". See https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/endpoint-name in account XXXXXXXXX for more information.\n<\/code><\/pre>\n\n<p>As far as I can tell the JSON I'm sending<\/p>\n\n<pre><code>{\n  'instances': [\"Hello world\"]\n}\n<\/code><\/pre>\n\n<p>is a valid JSON (I also checked it with some online json viewers that accept it as legal JSON).\nIs this some bug in SageMaker's JSON parsing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1582811350950,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":417.0,
        "Poster_created_time":1458550179920,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60434735",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"bug json pars endpoint model deploi predict input predict arrai string successfulli input instanc hello world tensorflow model import tensorflowpredictor predictor tensorflowpredictor endpoint session predictor predict input string string arrai includ error messag valid json arrai input instanc tensorflow model import tensorflowpredictor predictor tensorflowpredictor endpoint session predictor predict input error messag error occur modelerror call invokeendpoint oper receiv client error model messag error json valu instanc instanc except instanc list arrai http east consol aw amazon com cloudwatch home region east logeventview group aw endpoint endpoint account inform far tell json send instanc hello world valid json check onlin json viewer accept legal json bug json pars",
        "Question_preprocessed_content":"bug json pars endpoint model deploi predict input predict arrai string successfulli string string arrai includ error messag valid json arrai error messag far tell json send valid json bug json pars",
        "Question_gpt_summary_original":"The user is facing an issue with JSON parsing in SageMaker endpoint while using a TensorFlow model for prediction. The input for prediction is an array of strings, and if a string in the array includes '][' the user gets an error message about the validity of the JSON array. The user suspects it to be a bug in SageMaker's JSON parsing.",
        "Question_gpt_summary":"user face issu json pars endpoint tensorflow model predict input predict arrai string string arrai includ user get error messag valid json arrai user suspect bug json pars"
    },
    {
        "Question_title":"AWS Sagemaker Kernel appears to have died and restarts",
        "Question_body":"<p>I am getting a kernel error while trying to retrieve the data from an API that includes 100 pages. The data size is huge but the code runs well when executed on Google Colab or on local machine.<\/p>\n\n<p>The error I see in a window is-<\/p>\n\n<p><strong>Kernel Restarting\nThe kernel appears to have died. It will restart automatically.<\/strong><\/p>\n\n<p>I am using an ml.m5.xlarge machine with a memory allocation of 1000GB and there are no pre-saved datasets in the instance. Also, the expected data size is around 60 GB split into multiple datasets of 4 GB each.<\/p>\n\n<p>Can anyone help?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1584774759840,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":1026.0,
        "Poster_created_time":1570163855220,
        "Poster_location":"Mumbai, Maharashtra, India",
        "Poster_reputation_count":11.0,
        "Poster_view_count":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60785324",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"kernel appear di restart get kernel error try retriev data api includ page data size huge code run execut googl colab local machin error window kernel restart kernel appear di restart automat xlarg machin memori alloc pre save dataset instanc expect data size split multipl dataset help",
        "Question_preprocessed_content":"kernel appear di restart get kernel error try retriev data api includ page data size huge code run execut googl colab local machin error window kernel restart kernel appear di restart automat machin memori alloc dataset instanc expect data size split multipl dataset help",
        "Question_gpt_summary_original":"The user is encountering a kernel error while trying to retrieve data from an API that includes 100 pages. The error message states that the kernel appears to have died and will restart automatically. The user is using an ml.m5.xlarge machine with a memory allocation of 1000GB and there are no pre-saved datasets in the instance. The expected data size is around 60 GB split into multiple datasets of 4 GB each. The code runs well on Google Colab or on a local machine.",
        "Question_gpt_summary":"user encount kernel error try retriev data api includ page error messag state kernel appear di restart automat user xlarg machin memori alloc pre save dataset instanc expect data size split multipl dataset code run googl colab local machin"
    },
    {
        "Question_title":"Training data on AWS",
        "Question_body":"<p>I am relatively new to AWS and I'm trying to analyze credit fraud data set available on kaggle. I'm using doing it by looking at the example mentioned in the link \"<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn\/<\/a>\". I uploaded my credit card data set into my s3 bucket and done the following<\/p>\n\n<pre><code>import boto3 \nimport botocore \nimport pandas as pd\nimport sagemaker\nfrom sagemaker import get_execution_role \nrole = get_execution_role() \nbucket = '&lt;mybucket&gt;' \nprefix = 'test'\nkey = 'creditcard.csv' \ndata_location = 's3:\/\/{}\/{}\/train\/{}'.format(bucket,prefix,key)  \noutput_location = 's3:\/\/{}\/{}\/output'.format(bucket, prefix)\ncontainers = {\n          &lt;mycontainer&gt;\n          }\nsess = sagemaker.Session()\nlinear = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n                                   role, \n                                   train_instance_count=1, \n                                   train_instance_type='ml.c4.xlarge',\n                                   output_path=output_location,\n                                   sagemaker_session=sess)\nlinear.set_hyperparameters(feature_dim=31,\n                       predictor_type='binary_classifier',\n                       mini_batch_size=50)\nlinear.fit({'train': data_location})\n<\/code><\/pre>\n\n<p>When I try to run this code, an error message is popping up showing \"Either the training channel is empty or the mini-batch size is too high. Verify that training data contains non-empty files and the mini-batch size is less than the number of records per training host\"\nMy training data doesn't have any empty files. How to deal with this error ?\n*I uploaded the data set from my PC and stored in my bucket inside the folder named test\n*The container is standard one used for linear learner\n* I think I somehow couldn't connect the file from s3 bucket to the notebook instance I created. Can anyone verify that<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1569235758043,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1569248771947,
        "Question_score_count":0.0,
        "Question_view_count":66.0,
        "Poster_created_time":1568318861627,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":486.0,
        "Poster_view_count":75.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58060764",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"train data aw rel new aw try analyz credit fraud data set avail kaggl look exampl mention link http aw amazon com blog machin learn preprocess input data make predict amazon infer pipelin scikit learn upload credit card data set bucket follow import boto import botocor import panda import import execut role role execut role bucket prefix test kei creditcard csv data locat train format bucket prefix kei output locat output format bucket prefix contain sess session linear estim estim contain boto session region role train instanc count train instanc type xlarg output path output locat session sess linear set hyperparamet featur dim predictor type binari classifi mini batch size linear fit train data locat try run code error messag pop show train channel mini batch size high verifi train data contain non file mini batch size number record train host train data file deal error upload data set store bucket insid folder name test contain standard linear learner think couldn connect file bucket notebook instanc creat verifi",
        "Question_preprocessed_content":"train data aw rel new aw try analyz credit fraud data set avail kaggl look exampl mention link upload credit card data set bucket follow try run code error messag pop show train channel size high verifi train data contain file size number record train host train data file deal error upload data set store bucket insid folder name test contain standard linear learner think couldn connect file bucket notebook instanc creat verifi",
        "Question_gpt_summary_original":"The user is encountering an error message while trying to analyze a credit fraud dataset on AWS using the example mentioned in a blog post. The error message states that either the training channel is empty or the mini-batch size is too high. The user has verified that the training data doesn't have any empty files and suspects that there might be an issue with connecting the file from the S3 bucket to the notebook instance.",
        "Question_gpt_summary":"user encount error messag try analyz credit fraud dataset aw exampl mention blog post error messag state train channel mini batch size high user verifi train data file suspect issu connect file bucket notebook instanc"
    },
    {
        "Question_title":"Many Azure ML subresources do not support tags and tags in cost report",
        "Question_body":"<p>Tags are supported on Azure Machine learning, but the most cost generating part in Azure ML usage is related to sub-resources that do not support tags in cost report as shown here.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/azure-resource-manager\/management\/tag-support#microsoftmachinelearningservices\">https:\/\/learn.microsoft.com\/en-us\/azure\/azure-resource-manager\/management\/tag-support#microsoftmachinelearningservices<\/a>     <\/p>\n<p>It seems that Azure ML also leverages other services and hence flagged with the resource name of the emitting service (Azure ML workspace) with no tags propagated, and unfortunately these are the most expensive and are not covered by tagging mechanism.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1638463996317,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/649517\/many-azure-ml-subresources-do-not-support-tags-and",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"subresourc support tag tag cost report tag support cost gener usag relat sub resourc support tag cost report shown http learn microsoft com azur azur resourc manag manag tag support microsoftmachinelearningservic leverag servic flag resourc emit servic workspac tag propag unfortun expens cover tag mechan",
        "Question_preprocessed_content":"subresourc support tag tag cost report tag support cost gener usag relat support tag cost report shown leverag servic flag resourc emit servic tag propag unfortun expens cover tag mechan",
        "Question_gpt_summary_original":"Azure Machine Learning supports tags, but many of its sub-resources do not support tags in cost reports. These sub-resources are the most expensive and are not covered by the tagging mechanism.",
        "Question_gpt_summary":"support tag sub resourc support tag cost report sub resourc expens cover tag mechan"
    },
    {
        "Question_title":"Retrieve Sagemaker instance metrics from cloudwatch via CLI or API",
        "Question_body":"<p>I've got a training job running on Sagemaker. I would like to retrieve instance metrics like MemoryUtilization etc by CLI or boto3 client.<\/p>\n\n<p>Obviously I can see them in the console. However, I cannot see them in the CLI\/API. For example, when running:<\/p>\n\n<pre><code>aws cloudwatch list-metrics --namespace \"AWS\/SageMaker\"\n<\/code><\/pre>\n\n<p>I can see only metrics regarding endpoint invocation but not any training job related metrics.<\/p>\n\n<p>Any idea?<\/p>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1575975365270,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":200.0,
        "Poster_created_time":1461583482736,
        "Poster_location":"Israel",
        "Poster_reputation_count":169.0,
        "Poster_view_count":39.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59265637",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"retriev instanc metric cloudwatch cli api got train job run like retriev instanc metric like memoryutil cli boto client obvious consol cli api exampl run aw cloudwatch list metric namespac aw metric endpoint invoc train job relat metric idea thank",
        "Question_preprocessed_content":"retriev instanc metric cloudwatch cli api got train job run like retriev instanc metric like memoryutil cli boto client obvious consol exampl run metric endpoint invoc train job relat metric idea thank",
        "Question_gpt_summary_original":"The user is running a training job on Sagemaker and wants to retrieve instance metrics like MemoryUtilization via CLI or boto3 client. However, the user is unable to see these metrics in the CLI\/API and can only see metrics related to endpoint invocation. The user is seeking suggestions to resolve this issue.",
        "Question_gpt_summary":"user run train job want retriev instanc metric like memoryutil cli boto client user unabl metric cli api metric relat endpoint invoc user seek suggest resolv issu"
    },
    {
        "Question_title":"What is the \"BigQuery project for exporting data\" in Vertex AI Model training",
        "Question_body":"<p>I am following <a href=\"https:\/\/codelabs.developers.google.com\/codelabs\/vertex-ai-custom-code-training#4\" rel=\"nofollow noreferrer\">this guide<\/a> in setting up a vertex AI pipeline. I am told to supply the &quot;BigQuery project for exporting data&quot;, except that the guide is not clear on what this BigQuery project is. I have tried supplying the name of my GCP Project to no avail.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/0X5Gt.png\" rel=\"nofollow noreferrer\">Here is the photo of the field needing this BigQuery project<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/UZvX3.png\" rel=\"nofollow noreferrer\">Here is a photo of the guide that requests this information<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_created_time":1652456249073,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":37.0,
        "Poster_created_time":1652456020876,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72232134",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"bigqueri project export data model train follow guid set pipelin told suppli bigqueri project export data guid clear bigqueri project tri suppli gcp project avail photo field need bigqueri project photo guid request inform",
        "Question_preprocessed_content":"bigqueri project export data model train follow guid set pipelin told suppli bigqueri project export data guid clear bigqueri project tri suppli gcp project avail photo field need bigqueri project photo guid request inform",
        "Question_gpt_summary_original":"The user is following a guide to set up a Vertex AI pipeline and is asked to provide a \"BigQuery project for exporting data\" but the guide does not provide clear instructions on what this project is. The user has tried supplying the name of their GCP project but it did not work.",
        "Question_gpt_summary":"user follow guid set pipelin ask provid bigqueri project export data guid provid clear instruct project user tri suppli gcp project work"
    },
    {
        "Question_title":"Is there a way to access compute quotas with the Azure CLI or Python SDK?",
        "Question_body":"<p>I want to tabulate the compute quotas for each Azure ML workspace, in each Azure location, for my organization's Azure subscription. Although it is possible to look at the quotas manually through the Azure Portal (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-quotas#workspace-level-quota\" rel=\"nofollow noreferrer\">link<\/a>), I have not found a way to do this with the Azure CLI or Python SDK for Azure. Since there are many resource groups and AML workspaces for different teams under my Azure subscription, it would be much more efficient to do this programmatically rather than manually through the portal. Is this even possible, and if so how can it be done?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1597248542537,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1597249191852,
        "Question_score_count":2.0,
        "Question_view_count":418.0,
        "Poster_created_time":1369863777596,
        "Poster_location":"Cambridge, MA",
        "Poster_reputation_count":335.0,
        "Poster_view_count":23.0,
        "Answer_body":"<p>It does look like these commands are currently in the CLI or the Python SDK. The CLI uses the Python SDK, so what's missing from one does tend to be missing from the other.<\/p>\n<p>Fortunately, you can invoke the rest endpoints directly, either in Python or by using the <code>az rest<\/code> command in the CLI.<\/p>\n<p>There are a few commands that may interest you:<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/workspacesandcomputes\/usages\/list\" rel=\"nofollow noreferrer\">Usage<\/a> and Quotas for a region:\n<code>\/subscriptions\/{subscriptionId}\/providers\/Microsoft.MachineLearningServices\/locations\/{location}\/usages?api-version=2019-05-01<\/code>\n<code>\/subscriptions\/{subscriptionId}\/providers\/Microsoft.MachineLearningServices\/locations\/{location}\/quotas?api-version=2020-04-01<\/code><\/p>\n<p>The process for updating REST specs to the offical documentation is fairly lengthy so it isn't published yet, but if you are willing to use Swagger docs to explore what is available, the 2020-06-01 version of the API is on Github, which includes endpoints for updating quotas as well as retrieving them: <a href=\"https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/resource-manager\/Microsoft.MachineLearningServices\/stable\/2020-06-01\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/resource-manager\/Microsoft.MachineLearningServices\/stable\/2020-06-01<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Question_closed_time":1597341970283,
        "Answer_last_edit_time":1597439016992,
        "Answer_score_count":1.0,
        "Answerer_created_time":1549041651583,
        "Answerer_location":"Atlanta, GA, USA",
        "Answerer_reputation_count":3106.0,
        "Answerer_view_count":428.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63380531",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"wai access comput quota azur cli python sdk want tabul comput quota workspac azur locat organ azur subscript possibl look quota manual azur portal link wai azur cli python sdk azur resourc group aml workspac differ team azur subscript effici programmat manual portal possibl",
        "Question_preprocessed_content":"wai access comput quota azur cli python sdk want tabul comput quota workspac azur locat organ azur subscript possibl look quota manual azur portal wai azur cli python sdk azur resourc group aml workspac differ team azur subscript effici programmat manual portal possibl",
        "Question_gpt_summary_original":"The user wants to access compute quotas for each Azure ML workspace in each Azure location for their organization's Azure subscription. They are looking for a way to do this programmatically using the Azure CLI or Python SDK, as it would be more efficient than doing it manually through the Azure Portal.",
        "Question_gpt_summary":"user want access comput quota workspac azur locat organ azur subscript look wai programmat azur cli python sdk effici manual azur portal"
    },
    {
        "Question_title":"Dvc exp list shows nothing, but VS Code extension still shows experiments",
        "Question_body":"<p>Hi,<br>\nI want to remove all the experiments. I removed some experiments using <code>dvc exp remove<\/code> and there is nothing showing when executing <code>dvc exp list<\/code>, however there are still experiments which were committed showing in the VS Code extension, how can I remove them as well?<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_created_time":1680466360596,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":4.0,
        "Question_view_count":65.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-exp-list-shows-nothing-but-vs-code-extension-still-shows-experiments\/1583",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"exp list show code extens show experi want remov experi remov experi exp remov show execut exp list experi commit show code extens remov",
        "Question_preprocessed_content":"exp list show code extens show experi want remov experi remov experi show execut experi commit show code extens remov",
        "Question_gpt_summary_original":"The user has removed some experiments using \"dvc exp remove\" and there is nothing showing when executing \"dvc exp list\", but experiments that were committed are still showing in the VS Code extension. The user wants to know how to remove them as well.",
        "Question_gpt_summary":"user remov experi exp remov show execut exp list experi commit show code extens user want know remov"
    },
    {
        "Question_title":"Azure ML - Deployed to Inference Cluster throws 500 Server Error - MissingFeaturesError",
        "Question_body":"<p>We have an Azure ML model we are ready to deploy to an http endpoint for consumption and testing.    <\/p>\n<p>We are using this tutorial (<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy<\/a>) as the jumping off point for deploying our ml model. We have created an inference cluster, converted the training model to a real-time inference model and deployed. Deployment looks successful. However, when testing (both via the Test tab in the Azure ML Workspace and via http POST) the server throws a 500. The MissingFeaturesError follows:    <\/p>\n<blockquote>\n<p>File &quot;\/azureml-envs\/azureml_9b50686470a92ca74f0d62e2629faaec\/lib\/python3.6\/site-packages\/azureml\/studio\/modules\/ml\/common\/base_learner.py&quot;, line 289, in _validate_no_missing_feature    <br \/>\n    ErrorMapping.throw(MissingFeaturesError(required_feature_name=';'.join(missing_feature_list)))    <br \/>\n      &gt; missing_feature_list = ['Miles', 'Age', 'Gender', 'MarriagetPlans']    <\/p>\n<p>  File &quot;\/azureml-envs\/azureml_9b50686470a92ca74f0d62e2629faaec\/lib\/python3.6\/site-packages\/azureml\/studio\/common\/error.py&quot;, line 814, in throw    <br \/>\n    raise err    <br \/>\n      &gt; err = MissingFeaturesError('Features for Miles;Age;Gender;MarriagetPlans required but not provided.',)    <\/p>\n<p>  MissingFeaturesError: Features for Miles;Age;Gender;MarriagetPlans required but not provided.    <\/p>\n<\/blockquote>\n<p>In both test cases (via Test tab in Azure and http POST to the endpoint) all the required data is indeed provided. The request body definitely includes Miles, Age, Gender, MarriagetPlans.    <\/p>\n<p>What is going on here?    <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1614113525043,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/285607\/azure-ml-deployed-to-inference-cluster-throws-500",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"deploi infer cluster throw server error missingfeatureserror model readi deploi http endpoint consumpt test tutori http learn microsoft com azur machin learn tutori design automobil price deploi jump point deploi model creat infer cluster convert train model real time infer model deploi deploy look success test test tab workspac http post server throw missingfeatureserror follow file env bacafdefaaec lib python site packag studio modul common base learner line valid miss featur errormap throw missingfeatureserror requir featur join miss featur list miss featur list mile ag gender marriagetplan file env bacafdefaaec lib python site packag studio common error line throw rais err err missingfeatureserror featur mile ag gender marriagetplan requir provid missingfeatureserror featur mile ag gender marriagetplan requir provid test case test tab azur http post endpoint requir data provid request bodi definit includ mile ag gender marriagetplan go",
        "Question_preprocessed_content":"deploi infer cluster throw server error missingfeatureserror model readi deploi http endpoint consumpt test tutori jump point deploi model creat infer cluster convert train model infer model deploi deploy look success test server throw missingfeatureserror follow file line file line throw rais err err missingfeatureserror missingfeatureserror featur mile ag gender marriagetplan requir provid test case requir data provid request bodi definit includ mile ag gender marriagetplan go",
        "Question_gpt_summary_original":"The user is encountering a 500 server error while testing an Azure ML model that has been deployed to an inference cluster. The error message indicates a MissingFeaturesError, even though all the required data is provided in the request body. The user is seeking assistance in resolving this issue.",
        "Question_gpt_summary":"user encount server error test model deploi infer cluster error messag indic missingfeatureserror requir data provid request bodi user seek assist resolv issu"
    },
    {
        "Question_title":"Hide Command from Overview Run Page Bug - Reopen",
        "Question_body":"<aside class=\"quote\" data-post=\"1\" data-topic=\"2231\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"><\/div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/k\/ed8c4c\/40.png\" class=\"avatar\">\n    <a href=\"https:\/\/community.wandb.ai\/t\/hide-command-from-overview-run-page\/2231\">Hide Command from Overview Run Page<\/a> <a class=\"badge-wrapper  bullet\" href=\"\/c\/w-b-support\/36\"><span class=\"badge-category-bg\" style=\"background-color: #0088CC;\"><\/span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"Ask your W&amp;B questions here. Let us know if you have an issue and one of our engineers or community experts will get back to you ASAP.\">W&amp;B Support<\/span><\/a>\n  <\/div>\n  <blockquote>\n    On the Run Page (<a href=\"https:\/\/docs.wandb.ai\/ref\/app\/pages\/run-page\">https:\/\/docs.wandb.ai\/ref\/app\/pages\/run-page<\/a>) it shows on the left incognito that it shouldn\u2019t show your command when the public is viewing your page. \nHowever, on my page, when public and I view as not-me, it still shows the command that launched it, and that includes my Windows username, which I\u2019d rather not. I can\u2019t find anything to override or hide this. What am I missing? \nThanks.\n  <\/blockquote>\n<\/aside>\n\n<p>Reopening the above, this still isn\u2019t fixed and it\u2019s still bothering me.<\/p>\n<p>Any updates?<\/p>\n<p><a class=\"mention\" href=\"\/u\/armanharutyunyan\">@armanharutyunyan<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1658864314475,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":213.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/hide-command-from-overview-run-page-bug-reopen\/2802",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"hide command overview run page bug reopen hide command overview run page support run page http doc ref app page run page show left incognito shouldnt command public view page page public view show command launch includ window usernam overrid hide miss thank reopen isnt fix bother updat armanharutyunyan",
        "Question_preprocessed_content":"hide command overview run page bug reopen hide command overview run page support run page show left incognito shouldnt command public view page page public view show command launch includ window usernam overrid hide miss thank reopen isnt fix bother updat",
        "Question_gpt_summary_original":"The user is facing an issue with the \"Hide Command from Overview Run Page\" feature on the WandB platform. Despite the feature being designed to hide the command when viewed by the public, the user's Windows username is still visible. The user is seeking a solution to override or hide this information. The issue remains unresolved, and the user is requesting an update.",
        "Question_gpt_summary":"user face issu hide command overview run page featur platform despit featur design hide command view public user window usernam visibl user seek solut overrid hide inform issu remain unresolv user request updat"
    },
    {
        "Question_title":"Amazon sagemaker. SKlearn estimator vs Tensorflow estimator - why requirements_file is not present in one of them?",
        "Question_body":"<p>I am looking at definitions of two estimators SKLearn and Tensorflow in Amazon Sagemaker:<\/p>\n\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/sagemaker.sklearn.html\" rel=\"nofollow noreferrer\">SKLearn<\/a><\/p>\n\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/sagemaker.tensorflow.html\" rel=\"nofollow noreferrer\">Tensorflow<\/a><\/p>\n\n<pre><code>class sagemaker.sklearn.estimator.SKLearn(entry_point, framework_version='0.20.0', source_dir=None, hyperparameters=None, py_version='py3', image_name=None, **kwargs)\n\nclass sagemaker.tensorflow.estimator.TensorFlow(training_steps=None, evaluation_steps=None, checkpoint_path=None, py_version='py2', framework_version=None, model_dir=None, requirements_file='', image_name=None, script_mode=False, distributions=None, **kwargs)\n<\/code><\/pre>\n\n<p>Tensorflow has requirements_file parameter, while SKLearn does not. Is there reason why? How can I add <code>requirements.txt<\/code> to SKLearn estimator?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1557166184387,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":221.0,
        "Poster_created_time":1348677250368,
        "Poster_location":null,
        "Poster_reputation_count":6498.0,
        "Poster_view_count":988.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56010337",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"sklearn estim tensorflow estim requir file present look definit estim sklearn tensorflow sklearn tensorflow class sklearn estim sklearn entri point framework version sourc dir hyperparamet version imag kwarg class tensorflow estim tensorflow train step evalu step checkpoint path version framework version model dir requir file imag script mode fals distribut kwarg tensorflow requir file paramet sklearn reason add requir txt sklearn estim",
        "Question_preprocessed_content":"sklearn estim tensorflow estim present look definit estim sklearn tensorflow sklearn tensorflow tensorflow paramet sklearn reason add sklearn estim",
        "Question_gpt_summary_original":"The user is comparing the definitions of two Amazon Sagemaker estimators, SKLearn and Tensorflow, and has noticed that Tensorflow has a \"requirements_file\" parameter while SKLearn does not. The user is seeking to understand the reason for this difference and how to add a \"requirements.txt\" file to the SKLearn estimator.",
        "Question_gpt_summary":"user compar definit estim sklearn tensorflow notic tensorflow requir file paramet sklearn user seek understand reason differ add requir txt file sklearn estim"
    },
    {
        "Question_title":"Automatically repeating experiments and plots with error bars",
        "Question_body":"<p>Is it possible to use wandb to do repeats of certain machine learning experiments and automatically plot a learning curve with uncertainty regions?<\/p>\n<p>Something like this <a href=\"https:\/\/stackoverflow.com\/questions\/43064524\/plotting-shaded-uncertainty-region-in-line-plot-in-matplotlib-when-data-has-nans\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">python - Plotting shaded uncertainty region in line plot in matplotlib when data has NaNs - Stack Overflow<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1658489678841,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":109.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/automatically-repeating-experiments-and-plots-with-error-bars\/2791",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"automat repeat experi plot error bar possibl us repeat certain machin learn experi automat plot learn curv uncertainti region like python plot shade uncertainti region line plot matplotlib data nan stack overflow",
        "Question_preprocessed_content":"automat repeat experi plot error bar possibl us repeat certain machin learn experi automat plot learn curv uncertainti region like python plot shade uncertainti region line plot matplotlib data nan stack overflow",
        "Question_gpt_summary_original":"The user is asking if it is possible to use wandb to automatically repeat machine learning experiments and plot a learning curve with uncertainty regions, similar to a python example provided in the post.",
        "Question_gpt_summary":"user ask possibl us automat repeat machin learn experi plot learn curv uncertainti region similar python exampl provid post"
    },
    {
        "Question_title":"Copying run from a project to another and adding the right hyperparameter",
        "Question_body":"<p>Hello,<\/p>\n<p>I have been running several test on differents on different projects and I am struggling for doing something pretty simple I think.<br>\nI have run a sweep in a project:\u201cproject 1\u201d for testing for hyperparameter.<br>\nA bit later I modified a bit my architecture and run another test and send it to another project: \u201cproject 2\u201d.<br>\nNow I want to compare some of the run of project 1 with the one of project 2. I tried to use the option move, however the runs I moved disappeared from project1\u2026 and I don\u2019t want that\u2026 How can I copy the run from one project to another one (Q1)<br>\nMoreover I did not logged my hyperparameter for project 2. Can I add them manually from wandb interface ? (Q2)<\/p>\n<p>Finally I don\u2019t understand why some hyperparameters of my run for project 2 were not logged I did the following:<\/p>\n<pre><code class=\"lang-auto\">\nimport wandb\nwandb.login()\n\nimport stuff\ndefine model and dataloaders\n\nwandb.init(project=\"Structural encoder\", entity=\"barthelemymp\")\nwandb.config = {\n  \"num_layers\": 6,\n  \"forward_expansion\": 2048,\n  \"batch_size\": 10,\n  \"Encoder\": \"Structural\"\n}\n\nfor epoch in range(num_epochs):\n      ...\n      ...\n      ...\n      wandb.log({\"Train loss\": mean_lossTrain, \"Val Loss\":mean_lossVal, \"epoch\":epoch})\n<\/code><\/pre>\n<p>In the end on wandb website for this run I see only the trainloss valloss and epoch but not the config. What have I done wrong ? (Q3)<\/p>\n<p>Lot\u2019s of Question in one post, but any help will be appreciated,<\/p>\n<p>Best<\/p>\n<p>Barth<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1634738052400,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":340.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/copying-run-from-a-project-to-another-and-adding-the-right-hyperparameter\/1049",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"copi run project ad right hyperparamet hello run test differ differ project struggl pretti simpl think run sweep project project test hyperparamet bit later modifi bit architectur run test send project project want compar run project project tri us option run move disappear project dont want copi run project log hyperparamet project add manual interfac final dont understand hyperparamet run project log follow import login import stuff defin model dataload init project structur encod entiti barthelemymp config num layer forward expans batch size encod structur epoch rang num epoch log train loss mean losstrain val loss mean lossval epoch epoch end websit run trainloss valloss epoch config wrong lot question post help appreci best barth",
        "Question_preprocessed_content":"copi run project ad right hyperparamet hello run test differ differ project struggl pretti simpl think run sweep project project test hyperparamet bit later modifi bit architectur run test send project project want compar run project project tri us option run move disappear project dont want copi run project log hyperparamet project add manual interfac final dont understand hyperparamet run project log follow end websit run trainloss valloss epoch config wrong lot question post help appreci best barth",
        "Question_gpt_summary_original":"The user is having trouble copying a run from one project to another in WandB without losing the original run. They also want to know if they can manually add hyperparameters to a project and why some hyperparameters were not logged in a run.",
        "Question_gpt_summary":"user have troubl copi run project lose origin run want know manual add hyperparamet project hyperparamet log run"
    },
    {
        "Question_title":"SageMaker example access denied",
        "Question_body":"<p>I am running the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">k-means example in SageMaker<\/a>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker import KMeans\n\ndata_location = 's3:\/\/{}\/kmeans_highlevel_example\/data'.format(bucket)\noutput_location = 's3:\/\/{}\/kmeans_example\/output'.format(bucket)\n\nkmeans = KMeans(role=role, \n                train_instance_count=2,\n                train_instance_type='ml.c4.8xlarge',\n                output_path=output_location,\n                k=10, \n                data_location=data_location)\n<\/code><\/pre>\n<p>When I run this line, it appears access denied error.<\/p>\n<pre><code>%%time\n\nkmeans.fit(kmeans.record_set(train_set[0]))\n<\/code><\/pre>\n<p>The error returns:<\/p>\n<blockquote>\n<p>ClientError: An error occurred (AccessDenied) when calling the\nPutObject operation: Access Denied<\/p>\n<\/blockquote>\n<p>I also read other questions, but their answers do not solve my problem.<br \/>\nWould you please look at my case?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1522012018310,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1618306977208,
        "Question_score_count":1.0,
        "Question_view_count":5183.0,
        "Poster_created_time":1520999196230,
        "Poster_location":"Atlanta, GA, USA",
        "Poster_reputation_count":21.0,
        "Poster_view_count":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49480931",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"exampl access deni run mean exampl import kmean data locat kmean highlevel exampl data format bucket output locat kmean exampl output format bucket kmean kmean role role train instanc count train instanc type xlarg output path output locat data locat data locat run line appear access deni error time kmean fit kmean record set train set error return clienterror error occur accessdeni call putobject oper access deni read question answer solv problem look case",
        "Question_preprocessed_content":"exampl access deni run exampl run line appear access deni error error return clienterror error occur call putobject oper access deni read question answer solv problem look case",
        "Question_gpt_summary_original":"The user is encountering an access denied error while running the k-means example in SageMaker. The error occurs when calling the PutObject operation and the user's attempts to solve the issue using other solutions have been unsuccessful.",
        "Question_gpt_summary":"user encount access deni error run mean exampl error occur call putobject oper user attempt solv issu solut unsuccess"
    },
    {
        "Question_title":"Sagemaker KMeans Built-In - List of files csv as input",
        "Question_body":"<p>I Want to use <strong>Sagemaker KMeans BuilIn Algorithm<\/strong> in one of my applications. I have a large CSV file in S3 (raw data) that I split into several parts to be easy to clean. Before I had cleaned, I tried to use it as the input of Kmeans to perform the training job but It doesn't work.<\/p>\n\n<p>My manifest file:<\/p>\n\n<pre><code>[\n    {\"prefix\": \"s3:\/\/&lt;BUCKET_NAME&gt;\/kmeans_data\/KMeans-2019-28-07-13-40-00-001\/\"}, \n    \"file1.csv\", \n    \"file2.csv\"\n]\n<\/code><\/pre>\n\n<p>The error I've got:<\/p>\n\n<pre><code>Failure reason: ClientError: Unable to read data channel 'train'. Requested content-type is 'application\/x-recordio-protobuf'. Please verify the data matches the requested content-type. (caused by MXNetError) Caused by: [16:47:31] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsCppLibs\/AIAlgorithmsCppLibs-2.0.1620.0\/AL2012\/generic-flavor\/src\/src\/aialgs\/io\/iterator_base.cpp:100: (Input Error) The header of the MXNet RecordIO record at position 0 in the dataset does not start with a valid magic number. Stack trace returned 10 entries: [bt] (0) \/opt\/amazon\/lib\/libaialgs.so(+0xb1f0) [0x7fb5674c31f0] [bt] (1) \/opt\/amazon\/lib\/libaialgs.so(+0xb54a) [0x7fb5674c354a] [bt] (2) \/opt\/amazon\/lib\/libaialgs.so(aialgs::iterator_base::Next()+0x4a6) [0x7fb5674cc436] [bt] (3) \/opt\/amazon\/lib\/libmxnet.so(MXDataIterNext+0x21) [0x7fb54ecbcdb1] [bt] (4) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call_unix64+0x4c) [0x7fb567a1e858] [bt] (5) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call+0x15f) [0x7fb567a1d95f\n<\/code><\/pre>\n\n<p>My question is: It's possible to use multiple CSV files as input in Sagemaker KMeans BuilIn Algorithm only in GUI? If it's possible, How should I format my manifest?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1564336514917,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1564342604716,
        "Question_score_count":3.0,
        "Question_view_count":522.0,
        "Poster_created_time":1464391892936,
        "Poster_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Poster_reputation_count":2243.0,
        "Poster_view_count":148.0,
        "Answer_body":"<p>the manifest looks fine, but based on the error message, it looks like you haven't set the right data format for you S3 data. It's expecting protobuf, which is the default format :)<\/p>\n\n<p>You have to set the CSV data format explicitly. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.s3_input\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.s3_input<\/a>. <\/p>\n\n<p>It should look something like this:<\/p>\n\n<pre><code>s3_input_train = sagemaker.s3_input(\n  s3_data='s3:\/\/{}\/{}\/train\/manifest_file'.format(bucket, prefix),    \n  s3_data_type='ManifestFile',\n  content_type='csv')\n\n...\n\nkmeans_estimator = sagemaker.estimator.Estimator(kmeans_image, ...)\nkmeans_estimator.set_hyperparameters(...)\n\ns3_data = {'train': s3_input_train}\nkmeans_estimator.fit(s3_data)\n<\/code><\/pre>\n\n<p>Please note the KMeans estimator in the SDK only supports protobuf, see <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/kmeans.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/kmeans.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1564340123623,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1426694564423,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57243583",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"kmean built list file csv input want us kmean builin algorithm applic larg csv file raw data split part easi clean clean tri us input kmean perform train job work manifest file prefix kmean data kmean file csv file csv error got failur reason clienterror unabl read data channel train request content type applic recordio protobuf verifi data match request content type caus mxneterror caus opt brazil pkg cach packag aialgorithmscpplib aialgorithmscpplib gener flavor src src aialg iter base cpp input error header mxnet recordio record posit dataset start valid magic number stack trace return entri opt amazon lib libaialg xbf xfbcf opt amazon lib libaialg xba xfbca opt amazon lib libaialg aialg iter base xfbcc opt amazon lib libmxnet mxdataiternext xfbecbcdb opt amazon python lib python lib dynload ctype ffi unix xfbae opt amazon python lib python lib dynload ctype ffi xfbadf question possibl us multipl csv file input kmean builin algorithm gui possibl format manifest",
        "Question_preprocessed_content":"kmean list file csv input want us kmean builin algorithm applic larg csv file split part easi clean clean tri us input kmean perform train job work manifest file error got question possibl us multipl csv file input kmean builin algorithm gui possibl format manifest",
        "Question_gpt_summary_original":"The user wants to use Sagemaker KMeans Built-In Algorithm in their application and has a large CSV file in S3 that they split into several parts. However, when they tried to use it as input for KMeans, they encountered an error related to the content-type. The user is asking if it's possible to use multiple CSV files as input in Sagemaker KMeans Built-In Algorithm and how to format the manifest file.",
        "Question_gpt_summary":"user want us kmean built algorithm applic larg csv file split part tri us input kmean encount error relat content type user ask possibl us multipl csv file input kmean built algorithm format manifest file"
    },
    {
        "Question_title":"How to schedule tasks on SageMaker",
        "Question_body":"<p>I have a notebook on SageMaker I would like to run every night. What's the best way to schedule this task. Is there a way to run a bash script and schedule Cron job from SageMaker?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_created_time":1522449441927,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":null,
        "Question_score_count":14.0,
        "Question_view_count":17339.0,
        "Poster_created_time":1420001102892,
        "Poster_location":null,
        "Poster_reputation_count":173.0,
        "Poster_view_count":25.0,
        "Answer_body":"<p>Amazon SageMaker is a set of API that can help various machine learning and data science tasks. These API can be invoked from various sources, such as CLI, <a href=\"https:\/\/aws.amazon.com\/tools\/\" rel=\"noreferrer\">SDK<\/a> or specifically from schedule AWS Lambda functions (see here for documentation: <a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-scheduled-events.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-scheduled-events.html<\/a> )<\/p>\n\n<p>The main parts of Amazon SageMaker are notebook instances, training and tuning jobs, and model hosting for real-time predictions. Each one has different types of schedules that you might want to have. The most popular are:<\/p>\n\n<ul>\n<li><strong>Stopping and Starting Notebook Instances<\/strong> - Since the notebook instances are used for interactive ML models development, you don't really need them running during the nights or weekends. You can schedule a Lambda function to call the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html\" rel=\"noreferrer\">stop-notebook-instance<\/a> API at the end of the working day (8PM, for example), and the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StartNotebookInstance.html\" rel=\"noreferrer\">start-notebook-instance<\/a> API in the morning. Please note that you can also run crontab on the notebook instances (after opening the local terminal from the Jupyter interface).<\/li>\n<li><strong>Refreshing an ML Model<\/strong> - Automating the re-training of models, on new data that is flowing into the system all the time, is a common issue that with SageMaker is easier to solve. Calling <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"noreferrer\">create-training-job<\/a> API from a scheduled Lambda function (or even from a <a href=\"https:\/\/docs.aws.amazon.com\/AmazonCloudWatch\/latest\/events\/WhatIsCloudWatchEvents.html\" rel=\"noreferrer\">CloudWatch Event<\/a> that is monitoring the performance of the existing models), pointing to the S3 bucket where the old and new data resides, can <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateModel.html\" rel=\"noreferrer\">create a refreshed model<\/a> that you can now deploy into an <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_UpdateEndpointWeightsAndCapacities.html\" rel=\"noreferrer\">A\/B testing environment<\/a> .<\/li>\n<\/ul>\n\n<p>----- UPDATE (thanks to @snat2100 comment) -----<\/p>\n\n<ul>\n<li><strong>Creating and Deleting Real-time Endpoints<\/strong> - If your realtime endpoints are not needed 24\/7 (for example, serving internal company users working during workdays and hours), you can also <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpoint.html\" rel=\"noreferrer\">create the endpoints<\/a> in the morning and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_DeleteEndpoint.html\" rel=\"noreferrer\">delete them<\/a> at night. <\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1523213115336,
        "Answer_last_edit_time":1570200701000,
        "Answer_score_count":19.0,
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49582307",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"schedul task notebook like run night best wai schedul task wai run bash script schedul cron job",
        "Question_preprocessed_content":"schedul task notebook like run night best wai schedul task wai run bash script schedul cron job",
        "Question_gpt_summary_original":"The user wants to schedule a notebook on SageMaker to run every night and is seeking advice on the best way to do so, including the possibility of running a bash script and scheduling a Cron job from SageMaker.",
        "Question_gpt_summary":"user want schedul notebook run night seek advic best wai includ possibl run bash script schedul cron job"
    },
    {
        "Question_title":"Tensorflow Parameter Servers on SageMaker",
        "Question_body":"<p>I am trying to understand how parameters servers (PS's) work for distributed training in Tensorflow on Amazon SageMaker. <\/p>\n\n<p>To make things more concrete, I am able to run the example from AWS using PS's: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/tf-distributed-training.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/tf-distributed-training.ipynb<\/a><\/p>\n\n<p>Here is the code block that initializes the estimator for Tensorflow:<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ngit_config = {'repo': 'https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode', 'branch': 'master'}\n\nps_instance_type = 'ml.p3.2xlarge'\nps_instance_count = 2\n\nmodel_dir = \"\/opt\/ml\/model\"\n\ndistributions = {'parameter_server': {\n                    'enabled': True}\n                }\nhyperparameters = {'epochs': 60, 'batch-size' : 256}\n\nestimator_ps = TensorFlow(\n                       git_config=git_config,\n                       source_dir='tf-distribution-options\/code',\n                       entry_point='train_ps.py', \n                       base_job_name='ps-cifar10-tf',\n                       role=role,\n                       framework_version='1.13',\n                       py_version='py3',\n                       hyperparameters=hyperparameters,\n                       train_instance_count=ps_instance_count, \n                       train_instance_type=ps_instance_type,\n                       model_dir=model_dir,\n                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'dist'}],\n                       distributions=distributions)\n<\/code><\/pre>\n\n<p>Going through the documentation for Tensorflow, it seems that a device scope can be used for assigning a variable to a particular worker. However, I never see this done when running training jobs on SageMaker. In the example from AWS, the model is defined by:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/code\/model_def.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/code\/model_def.py<\/a><\/p>\n\n<p>Here is a snippet:<\/p>\n\n<pre><code>def get_model(learning_rate, weight_decay, optimizer, momentum, size, mpi=False, hvd=False):\n\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), padding='same', input_shape=(HEIGHT, WIDTH, DEPTH)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Conv2D(32, (3, 3)))\n\n    ...\n\n    model.add(Flatten())\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(NUM_CLASSES))\n    model.add(Activation('softmax'))\n\n    if mpi:\n        size = hvd.size()\n\n    if optimizer.lower() == 'sgd':\n        ...\n\n    if mpi:\n        opt = hvd.DistributedOptimizer(opt)\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=opt,\n                  metrics=['accuracy'])\n\n    return model\n<\/code><\/pre>\n\n<p>Here, there are no references to distribution strategies (except with MPI, but that flag is set to False for PS's). Somehow, Tensorflow or the SageMaker container is able to decide where the variables for each layer should be stored. However, I'm not seeing anything in the container code that does anything with the distribution strategy.<\/p>\n\n<p>I am able to run this code and train the model using 1 and 2 instances. When i do so, I see a decrease of almost 50% in the runtime, suggesting that a distributed training is occurring.<\/p>\n\n<p>My questions are:<\/p>\n\n<ol>\n<li>How does Tensorflow decide the distribution of variables on the PS's? In the example code, there is no explicit reference to devices. Somehow the distribution is done automatically.<\/li>\n<li>Is it possible to see which parameters have been assigned to each PS? Or to see what the communication between PS's looks like? If so, how?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1581363222683,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":307.0,
        "Poster_created_time":1416337478872,
        "Poster_location":null,
        "Poster_reputation_count":101.0,
        "Poster_view_count":13.0,
        "Answer_body":"<blockquote>\n<p>My questions are:<\/p>\n<p>How does Tensorflow decide the distribution of variables on the PS's?\nIn the example code, there is no explicit reference to devices.\nSomehow the distribution is done automatically.<\/p>\n<\/blockquote>\n<p>The TensorFlow image provided by SageMaker has the code to setup TF_CONFIG and launching parameter server for multi work training. See the code [here][1] The setup is for each node in the cluster there is a PS and a worker thread configured.<\/p>\n<p>It's not using any DistributionStrategy so the default strategy is used. See [here][2].<\/p>\n<p>If you would like to use a different DistributionStrategy or different TF_CONFIG you will need to disable <code>parameter_server<\/code> option when launching the SageMaker training job and set everything up in your training script.<\/p>\n<blockquote>\n<p>Is it possible to see which parameters have been assigned to each PS?\nOr to see what the communication between PS's looks like? If so, how?<\/p>\n<\/blockquote>\n<p>You should be able to get some information from the output log which can be found in CloudWatch. The link is available on the Training Job console page.\n[1]: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-training-toolkit\/blob\/master\/src\/sagemaker_tensorflow_container\/training.py#L37\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-training-toolkit\/blob\/master\/src\/sagemaker_tensorflow_container\/training.py#L37<\/a>\n[2]: <a href=\"https:\/\/www.tensorflow.org\/guide\/distributed_training#default_strategy\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/guide\/distributed_training#default_strategy<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1600204151310,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1539125320752,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":16.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60157184",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"tensorflow paramet server try understand paramet server work distribut train tensorflow thing concret abl run exampl aw http github com aw sampl amazon script mode blob master distribut option distribut train ipynb code block initi estim tensorflow tensorflow import tensorflow git config repo http github com aw sampl amazon script mode branch master instanc type xlarg instanc count model dir opt model distribut paramet server enabl true hyperparamet epoch batch size estim tensorflow git config git config sourc dir distribut option code entri point train base job cifar role role framework version version hyperparamet hyperparamet train instanc count instanc count train instanc type instanc type model dir model dir tag kei project valu cifar kei tensorboard valu dist distribut distribut go document tensorflow devic scope assign variabl particular worker run train job exampl aw model defin http github com aw sampl amazon script mode blob master distribut option code model def snippet def model learn rate weight decai optim momentum size mpi fals hvd fals model sequenti model add convd pad input shape height width depth model add batchnorm model add activ relu model add convd model add flatten model add dens model add activ relu model add dropout model add dens num class model add activ softmax mpi size hvd size optim lower sgd mpi opt hvd distributedoptim opt model compil loss categor crossentropi optim opt metric accuraci return model refer distribut strategi mpi flag set fals tensorflow contain abl decid variabl layer store see contain code distribut strategi abl run code train model instanc decreas runtim suggest distribut train occur question tensorflow decid distribut variabl exampl code explicit refer devic distribut automat possibl paramet assign commun look like",
        "Question_preprocessed_content":"tensorflow paramet server try understand paramet server work distribut train tensorflow thing concret abl run exampl aw code block initi estim tensorflow go document tensorflow devic scope assign variabl particular worker run train job exampl aw model defin snippet refer distribut strategi tensorflow contain abl decid variabl layer store see contain code distribut strategi abl run code train model instanc decreas runtim suggest distribut train occur question tensorflow decid distribut variabl exampl code explicit refer devic distribut automat possibl paramet assign commun look like",
        "Question_gpt_summary_original":"The user is trying to understand how parameter servers work for distributed training in Tensorflow on Amazon SageMaker. They are able to run an example from AWS using parameter servers, but they are unsure how Tensorflow decides the distribution of variables on the parameter servers. They are also wondering if it is possible to see which parameters have been assigned to each parameter server and what the communication between parameter servers looks like.",
        "Question_gpt_summary":"user try understand paramet server work distribut train tensorflow abl run exampl aw paramet server unsur tensorflow decid distribut variabl paramet server wonder possibl paramet assign paramet server commun paramet server look like"
    },
    {
        "Question_title":"Wandb sweeep training error",
        "Question_body":"<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/b\/b2b7478189f41aaca4f1c7ba31c9ab544d26e8e9.png\" data-download-href=\"\/uploads\/short-url\/puZF9VtzGIPQ8W24OigDTk9RKiJ.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/b\/b2b7478189f41aaca4f1c7ba31c9ab544d26e8e9.png\" alt=\"image\" data-base62-sha1=\"puZF9VtzGIPQ8W24OigDTk9RKiJ\" width=\"690\" height=\"413\" data-dominant-color=\"262727\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1010\u00d7605 29.4 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>this error happened while I was training and I know nothing about this. Help me on this :))<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1672808543979,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":126.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-sweeep-training-error\/3636",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"swp train error imag error happen train know help",
        "Question_preprocessed_content":"swp train error imag error happen train know help",
        "Question_gpt_summary_original":"The user encountered an error while training with Wandb sweep, but did not provide any specific details about the error.",
        "Question_gpt_summary":"user encount error train sweep provid specif detail error"
    },
    {
        "Question_title":"Key Error in wandb.config when using wandb.sweep in pytorch",
        "Question_body":"<p>I am trying to run this codebase <a href=\"https:\/\/github.com\/devzhk\/LMCTS\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - devzhk\/LMCTS<\/a>. To run hyperparameter sweep, I ran the following:<\/p>\n<pre><code class=\"lang-auto\">wandb sweep sweep\/simulation\/linear-lmcts.yaml\nwandb agent [agent id]\n<\/code><\/pre>\n<p><a href=\"https:\/\/github.com\/devzhk\/LMCTS\/blob\/master\/sweep\/simulation\/linear-lmcts.yaml\" rel=\"noopener nofollow ugc\">The config file<\/a> seems to have the same style as the sweep tutorial in W&amp;B website has. However, when I run sweep command as above, I get the following error<\/p>\n<pre><code class=\"lang-auto\">2022-08-10 02:12:55,113 - wandb.wandb_agent - INFO - Running runs: []\n2022-08-10 02:12:55,352 - wandb.wandb_agent - INFO - Agent received command: run\n2022-08-10 02:12:55,362 - wandb.wandb_agent - INFO - Agent starting run with config:\n\tT: 10000\n\talgo: LMCTS\n\tbeta_inv: 0.0001\n\tdatapath: data\/gaussian50-20-1-1.pt\n\tdim_context: 20\n\tfunc: linear\n\tlr: 0.1\n\tmodel: linear\n\tnum_arm: 50\n\tnum_iter: 70\n\tsigma: 0.5\n2022-08-10 02:12:55,372 - wandb.wandb_agent - INFO - About to run command: \/usr\/bin\/env python3 run_simulation.py\n2022-08-10 02:13:00,385 - wandb.wandb_agent - INFO - Running runs: ['ngxmksza']\n\/home\/mila\/i\/ishfaqha\/code\/LMCTS\/LCMTS\/lib\/python3.7\/site-packages\/sklearn\/feature_extraction\/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\n  dtype=np.int):\nRandom seed: 720\nTraceback (most recent call last):\n  File \"run_simulation.py\", line 107, in &lt;module&gt;\n    run(config, args)\n  File \"run_simulation.py\", line 41, in run\n    data = torch.load(config['datapath'])\nKeyError: 'datapath'\n\n<\/code><\/pre>\n<p>Shouldn\u2019t it automatically parse the config?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1660196584671,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":159.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/key-error-in-wandb-config-when-using-wandb-sweep-in-pytorch\/2898",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"kei error config sweep pytorch try run codebas github devzhk lmct run hyperparamet sweep ran follow sweep sweep simul linear lmct yaml agent agent config file style sweep tutori websit run sweep command follow error agent info run run agent info agent receiv command run agent info agent start run config algo lmct beta inv datapath data gaussian dim context func linear model linear num arm num iter sigma agent info run command usr bin env python run simul agent info run run ngxmksza home mila ishfaqha code lmct lcmt lib python site packag sklearn featur extract imag deprecationwarn int deprec alia builtin int silenc warn us int modifi behavior safe replac int wish us int int specifi precis wish review current us check releas note link addit inform deprec numpi detail guidanc http numpi org devdoc releas note html deprec dtype int random seed traceback recent file run simul line run config arg file run simul line run data torch load config datapath keyerror datapath shouldnt automat pars config",
        "Question_preprocessed_content":"kei error config sweep pytorch try run codebas github run hyperparamet sweep ran follow config file style sweep tutori websit run sweep command follow error shouldnt automat pars config",
        "Question_gpt_summary_original":"The user is encountering a KeyError in wandb.config while using wandb.sweep in PyTorch. The user is trying to run a codebase for hyperparameter sweep, but when running the sweep command, they are getting an error related to the config file. The error message suggests that the config file is not being parsed automatically, resulting in a KeyError for the 'datapath' key.",
        "Question_gpt_summary":"user encount keyerror config sweep pytorch user try run codebas hyperparamet sweep run sweep command get error relat config file error messag suggest config file pars automat result keyerror datapath kei"
    },
    {
        "Question_title":"How to save a .html file to S3 that is created in a Sagemaker processing container",
        "Question_body":"**Error message:**\n\"FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/processing\/output\/profile_case.html'\"\n\n**Background:**\nI am working in Sagemaker using python trying to profile a dataframe that is saved in a S3 bucket with pandas profiling. The data is very large so instead of spinning up a large EC2 instance, I am using a SKLearn processor.\n\nEverything runs fine but when the job finishes it does not save the pandas profile (a .html file) in a S3 bucket or back in the instance Sagemaker is running in.\n\nWhen I try to export the .html file that is created from the pandas profile, I keep getting errors saying that the file cannot be found.\n\nDoes anyone know of a way to export the .html file out of the temporary 24xl instance that the SKLearn processor is running in to S3? Below is the exact code I am using:\n\n\n```\nimport os\nimport sys\nimport subprocess\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall('awswrangler')\ninstall('tqdm')\ninstall('pandas')\ninstall('botocore==1.19.4')\ninstall('ruamel.yaml')\ninstall('pandas-profiling==2.13.0')\nimport awswrangler as wr\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom string import Template\nimport gc\nimport boto3\n\nfrom pandas_profiling import ProfileReport\n\nclient = boto3.client('s3')\nsession = boto3.Session(region_name=\"eu-west-2\")\n```\n\n```\n%%writefile casetableprofile.py\n\nimport os\nimport sys\nimport subprocess\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall('awswrangler')\ninstall('tqdm')\ninstall('pandas')\ninstall('botocore')\ninstall('ruamel.yaml')\ninstall('pandas-profiling')\nimport awswrangler as wr\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom string import Template\nimport gc\nimport boto3\n\nfrom pandas_profiling import ProfileReport\n\nclient = boto3.client('s3')\nsession = boto3.Session(region_name=\"eu-west-2\")\n\n\n\n\ndef run_profile():\n\n\n\n    query = \"\"\"\n    SELECT  * FROM \"healthcloud-refined\".\"case\"\n    ;\n    \"\"\"\n    tableforprofile = wr.athena.read_sql_query(query,\n                                            database=\"healthcloud-refined\",\n                                            boto3_session=session,\n                                            ctas_approach=False,\n                                            workgroup='DataScientists')\n    print(\"read in the table queried above\")\n\n    print(\"got rid of missing and added a new index\")\n\n    profile_tblforprofile = ProfileReport(tableforprofile, \n                                  title=\"Pandas Profiling Report\", \n                                  minimal=True)\n\n    print(\"Generated carerequest profile\")\n                                      \n    return profile_tblforprofile\n\n\nif __name__ == '__main__':\n\n    profile_tblforprofile = run_profile()\n    \n    print(\"Generated outputs\")\n\n    output_path_tblforprofile = ('profile_case.html')\n    print(output_path_tblforprofile)\n    \n    profile_tblforprofile.to_file(output_path_tblforprofile)\n\n    \n    #Below is the only part where I am getting errors\nimport boto3\nimport os   \ns3 = boto3.resource('s3')\ns3.meta.client.upload_file('\/opt\/ml\/processing\/output\/profile_case.html', 'intl-euro-uk-datascientist-prod','Mark\/healthclouddataprofiles\/{}'.format(output_path_tblforprofile))  \n```\n\n```\nimport sagemaker\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsession = boto3.Session(region_name=\"eu-west-2\")\n\nbucket = 'intl-euro-uk-datascientist-prod'\n\nprefix = 'Mark'\n\nsm_session = sagemaker.Session(boto_session=session, default_bucket=bucket)\nsm_session.upload_data(path='.\/casetableprofile.py',\n                                bucket=bucket,\n                                key_prefix=f'{prefix}\/source')\n```\n\n\n\n```\nimport boto3\n#import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nregion = boto3.session.Session().region_name\n\n\nS3_ROOT_PATH = \"s3:\/\/{}\/{}\".format(bucket, prefix)\n\nrole = get_execution_role()\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     sagemaker_session=sm_session,\n                                     instance_type='ml.m5.24xlarge',\n                                     instance_count=1)\n```\n\n```\nsklearn_processor.run(code='s3:\/\/{}\/{}\/source\/casetableprofile.py'.format(bucket, prefix),\n                      inputs=[],\n                      outputs=[ProcessingOutput(output_name='output',\n                                                source='\/opt\/ml\/processing\/output',\n                                                destination='s3:\/\/intl-euro-uk-datascientist-prod\/Mark\/')])\n```\n\n\nThank you in advance!!!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1660653174738,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668605561608,
        "Question_score_count":0.0,
        "Question_view_count":426.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"Hi,\n\nFirstly, you should not (usually) need to directly interact with S3 from your processing script: The fact that you've configured your `ProcessingOutput` means that any files your script saves in `\/opt\/ml\/processing\/output` should automatically get uploaded to your `s3:\/\/...` destination URL. Of course there might be particular special cases where you want to directly access S3 from your script, but in general the processing job inputs and outputs should do it for you, to keep your code nice and simple.\n\nI'm no Pandas Profiler expert, but I *think* the error might be coming from here:\n\n```python\n    output_path_tblforprofile = ('profile_case.html')\n    print(output_path_tblforprofile)\n    \n    profile_tblforprofile.to_file(output_path_tblforprofile)\n```\n\nDoesn't this just save the report to `profile_case.html` in your current working directory? That's not the `\/opt\/ml\/processing\/output` directory: It's usually the folder where the script is downloaded to the container I believe. The FileNotFound error is telling you that the HTML file is not getting created in the folder you expect, I think.\n\nSo I would suggest to make your output path explicit e.g. `\/opt\/ml\/processing\/output\/profile_case.html`, and also remove the boto3\/s3 section at the end - hope that helps!",
        "Answer_comment_count":1.0,
        "Question_closed_time":1660704602950,
        "Answer_last_edit_time":1660704602951,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5abOieUyQZSFvyRwfApRVA\/how-to-save-a-html-file-to-s3-that-is-created-in-a-sagemaker-processing-container",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_original_content":"save html file creat process contain error messag filenotfounderror errno file directori opt process output profil case html background work python try profil datafram save bucket panda profil data larg instead spin larg instanc sklearn processor run fine job finish save panda profil html file bucket instanc run try export html file creat panda profil get error sai file know wai export html file temporari instanc sklearn processor run exact code import import sy import subprocess def instal packag subprocess check sy execut pip instal packag instal awswrangl instal tqdm instal panda instal botocor instal ruamel yaml instal panda profil import awswrangl import panda import numpi import datetim dateutil relativedelta import relativedelta string import templat import import boto panda profil import profilereport client boto client session boto session region west writefil casetableprofil import import sy import subprocess def instal packag subprocess check sy execut pip instal packag instal awswrangl instal tqdm instal panda instal botocor instal ruamel yaml instal panda profil import awswrangl import panda import numpi import datetim dateutil relativedelta import relativedelta string import templat import import boto panda profil import profilereport client boto client session boto session region west def run profil queri select healthcloud refin case tableforprofil athena read sql queri queri databas healthcloud refin boto session session cta approach fals workgroup datascientist print read tabl queri print got rid miss ad new index profil tblforprofil profilereport tableforprofil titl panda profil report minim true print gener carerequest profil return profil tblforprofil main profil tblforprofil run profil print gener output output path tblforprofil profil case html print output path tblforprofil profil tblforprofil file output path tblforprofil get error import boto import boto resourc meta client upload file opt process output profil case html intl euro datascientist prod mark healthclouddataprofil format output path tblforprofil import process import processinginput processingoutput session boto session region west bucket intl euro datascientist prod prefix mark session session boto session session default bucket bucket session upload data path casetableprofil bucket bucket kei prefix prefix sourc import boto import import execut role sklearn process import sklearnprocessor region boto session session region root path format bucket prefix role execut role sklearn processor sklearnprocessor framework version role role session session instanc type xlarg instanc count sklearn processor run code sourc casetableprofil format bucket prefix input output processingoutput output output sourc opt process output destin intl euro datascientist prod mark thank advanc",
        "Question_preprocessed_content":"save html file creat process contain error messag filenotfounderror file directori background work python try profil datafram save bucket panda profil data larg instead spin larg instanc sklearn processor run fine job finish save panda profil bucket instanc run try export html file creat panda profil get error sai file know wai export html file temporari instanc sklearn processor run exact code thank advanc",
        "Question_gpt_summary_original":"The user is trying to save a .html file to S3 that is created in a Sagemaker processing container. They are using pandas profiling to profile a large dataframe and are running the code in a SKLearn processor. However, when the job finishes, the pandas profile is not saved in S3 or in the instance Sagemaker is running in. The user is encountering an error message stating that the file cannot be found when trying to export the .html file. They are seeking a way to export the .html file out of the temporary 24xl instance that the SKLearn processor is running in to S3.",
        "Question_gpt_summary":"user try save html file creat process contain panda profil profil larg datafram run code sklearn processor job finish panda profil save instanc run user encount error messag state file try export html file seek wai export html file temporari instanc sklearn processor run"
    },
    {
        "Question_title":"A small UI design issue, which is uncomfortable to my eyes",
        "Question_body":"<p>Let me first say that I really like the <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a> and have &gt;900 runs logged into its platform. my small issue is that  I use wandb in Night mode. However, when I want to delete a run (screenshot attached), the entire screen turns white. This sudden transition is painful to my eyes. It would be great if this could as well be in black.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/ac461cf4c2ffcf0921999390ec87b7a73867bb9d.jpeg\" data-download-href=\"\/uploads\/short-url\/oA0laXOG86Pc97aBsxsFFySDV4p.jpeg?dl=1\" title=\"Screenshot 2023-05-06 at 22.48.15\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ac461cf4c2ffcf0921999390ec87b7a73867bb9d_2_690x393.jpeg\" alt=\"Screenshot 2023-05-06 at 22.48.15\" data-base62-sha1=\"oA0laXOG86Pc97aBsxsFFySDV4p\" width=\"690\" height=\"393\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ac461cf4c2ffcf0921999390ec87b7a73867bb9d_2_690x393.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ac461cf4c2ffcf0921999390ec87b7a73867bb9d_2_1035x589.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ac461cf4c2ffcf0921999390ec87b7a73867bb9d_2_1380x786.jpeg 2x\" data-dominant-color=\"D6D6D6\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2023-05-06 at 22.48.15<\/span><span class=\"informations\">2652\u00d71514 169 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1683406233483,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":29.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/a-small-ui-design-issue-which-is-uncomfortable-to-my-eyes\/4348",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"small design issu uncomfort ey let like run log platform small issu us night mode want delet run screenshot attach entir screen turn white sudden transit pain ey great black screenshot",
        "Question_preprocessed_content":"small design issu uncomfort ey let like run log platform small issu us night mode want delet run entir screen turn white sudden transit pain ey great black screenshot",
        "Question_gpt_summary_original":"The user is experiencing discomfort while using wandb.ai in Night mode because the entire screen turns white when they try to delete a run, causing a sudden transition that is painful to their eyes. They suggest that it would be helpful if the delete function could also be in black.",
        "Question_gpt_summary":"user experienc discomfort night mode entir screen turn white try delet run caus sudden transit pain ey suggest help delet function black"
    },
    {
        "Question_title":"How to generate batch forecasts using model created by AWS SageMaker Autopilot?",
        "Question_body":"<p>I created a complete model using Amazon Web Services (AWS) SageMaker Autopilot. I would like to see what forecasts the model makes on my training data. I'm running this in a SageMaker Studio notebook. Here's my code.<\/p>\n<pre><code>import sagemaker\n\nimage = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, sagemaker.session.Session().boto_region_name, version=&quot;latest&quot;)\n\nmodel = sagemaker.model.Model(\n    image_uri = image,\n    model_data = &quot;s3:\/\/sagemaker-us-east-...\/batch-prediction\/sagemaker-xgboost-2021-...\/output\/model.tar.gz&quot;\n)\n\ntransformer = model.transformer(\n    instance_count = 1,\n    instance_type = &quot;ml.c4.xlarge&quot;\n)\n<\/code><\/pre>\n<p>Here's the full error stack.<\/p>\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-14-23386d1fc99a&gt; in &lt;module&gt;\n      1 transformer = my_model.transformer(\n      2     instance_count = 1,\n----&gt; 3     instance_type = &quot;ml.c4.xlarge&quot;\n      4 )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/model.py in transformer(self, instance_count, instance_type, strategy, assemble_with, output_path, output_kms_key, accept, env, max_concurrent_transforms, max_payload, tags, volume_kms_key)\n    772         self._init_sagemaker_session_if_does_not_exist(instance_type)\n    773 \n--&gt; 774         self._create_sagemaker_model(instance_type, tags=tags)\n    775         if self.enable_network_isolation():\n    776             env = None\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/model.py in _create_sagemaker_model(self, instance_type, accelerator_type, tags)\n    259             vpc_config=self.vpc_config,\n    260             enable_network_isolation=enable_network_isolation,\n--&gt; 261             tags=tags,\n    262         )\n    263 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in create_model(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags)\n   2596             enable_network_isolation=enable_network_isolation,\n   2597             primary_container=primary_container,\n-&gt; 2598             tags=tags,\n   2599         )\n   2600         LOGGER.info(&quot;Creating model with name: %s&quot;, name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in _create_model_request(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags)\n   2512             container_defs = primary_container\n   2513 \n-&gt; 2514         role = self.expand_role(role)\n   2515 \n   2516         if isinstance(container_defs, list):\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in expand_role(self, role)\n   3466             str: The corresponding AWS IAM role ARN.\n   3467         &quot;&quot;&quot;\n-&gt; 3468         if &quot;\/&quot; in role:\n   3469             return role\n   3470         return self.boto_session.resource(&quot;iam&quot;).Role(role).arn\n\nTypeError: argument of type 'NoneType' is not iterable\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1629080151253,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1629146200556,
        "Question_score_count":0.0,
        "Question_view_count":66.0,
        "Poster_created_time":1413115438712,
        "Poster_location":null,
        "Poster_reputation_count":139.0,
        "Poster_view_count":31.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68796753",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"gener batch forecast model creat autopilot creat complet model amazon web servic aw autopilot like forecast model make train data run studio notebook code import imag imag uri retriev xgboost session session boto region version latest model model model imag uri imag model data east batch predict xgboost output model tar transform model transform instanc count instanc type xlarg error stack typeerror traceback recent transform model transform instanc count instanc type xlarg opt conda lib python site packag model transform self instanc count instanc type strategi assembl output path output km kei accept env max concurr transform max payload tag volum km kei self init session exist instanc type self creat model instanc type tag tag self enabl network isol env opt conda lib python site packag model creat model self instanc type acceler type tag vpc config self vpc config enabl network isol enabl network isol tag tag opt conda lib python site packag session creat model self role contain def vpc config enabl network isol primari contain tag enabl network isol enabl network isol primari contain primari contain tag tag logger info creat model opt conda lib python site packag session creat model request self role contain def vpc config enabl network isol primari contain tag contain def primari contain role self expand role role isinst contain def list opt conda lib python site packag session expand role self role str correspond aw iam role arn role return role return self boto session resourc iam role role arn typeerror argument type nonetyp iter",
        "Question_preprocessed_content":"gener batch forecast model creat autopilot creat complet model amazon web servic autopilot like forecast model make train data run studio notebook code error stack",
        "Question_gpt_summary_original":"The user is trying to generate batch forecasts using a model created by AWS SageMaker Autopilot in a SageMaker Studio notebook. However, they are encountering a TypeError stating that the argument of type 'NoneType' is not iterable.",
        "Question_gpt_summary":"user try gener batch forecast model creat autopilot studio notebook encount typeerror state argument type nonetyp iter"
    },
    {
        "Question_title":"Azure Model",
        "Question_body":"<p>I want to use Gaussian NB, Bernoulli NB, and K nearest neighbor but I don't know if Azure Machine Learning has those models.  <br \/>\nPlease reply.   <br \/>\nThank you very much.  <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1631260693017,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/546916\/azure-model",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"azur model want us gaussian bernoulli nearest neighbor know model repli thank",
        "Question_preprocessed_content":"azur model want us gaussian bernoulli nearest neighbor know model repli thank",
        "Question_gpt_summary_original":"The user is unsure if Azure Machine Learning has Gaussian NB, Bernoulli NB, and K nearest neighbor models and is seeking clarification.",
        "Question_gpt_summary":"user unsur gaussian bernoulli nearest neighbor model seek clarif"
    },
    {
        "Question_title":"Automated ML model - endPoint goes for toss",
        "Question_body":"<p>Hi     <\/p>\n<p>I created &quot;Automated ML &quot; model for the given set of data. After creating the model, I deployed model as ACI , while test the model under &quot;Home &gt; endpoint&gt;modelname, I am facing below problem     <\/p>\n<ol>\n<li> Whole page hangs indefinitely , refer to attach snap shot    <\/li>\n<li> Sometime i get he page , input like to forecast time is not editable  refer to snapshot    <br \/>\n <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/121926-snap3.jpg?platform=QnA\" alt=\"121926-snap3.jpg\" \/>    <\/li>\n<\/ol>\n<p>Thanks in advance for the support      <br \/>\nRamabadran     <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1628591307843,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/507900\/automated-ml-model-endpoint-goes-for-toss",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"autom model endpoint goe toss creat autom model given set data creat model deploi model aci test model home endpoint modelnam face problem page hang indefinit refer attach snap shot page input like forecast time edit refer snapshot thank advanc support ramabadran",
        "Question_preprocessed_content":"autom model endpoint goe toss creat autom model given set data creat model deploi model aci test model home endpoint modelnam face problem page hang indefinit refer attach snap shot page input like forecast time edit refer snapshot thank advanc support ramabadran",
        "Question_gpt_summary_original":"The user created an \"Automated ML\" model for a given set of data and deployed it as ACI. However, while testing the model under \"Home > endpoint > modelname,\" the user is facing issues such as the whole page hanging indefinitely and the input for forecasting time not being editable.",
        "Question_gpt_summary":"user creat autom model given set data deploi aci test model home endpoint modelnam user face issu page hang indefinit input forecast time edit"
    },
    {
        "Question_title":"SageMaker: AccessDeniedException ClientError when calling CreateModel",
        "Question_body":"<p>ClientError: An error occurred (AccessDeniedException) when calling the CreateModel operation: User: arn:aws:sts::0123456789:assumed-role\/sagemakeraccesstoservices\/SageMaker is not authorized to perform: sagemaker:CreateModel on resource: arn:aws:sagemaker:us-east-2:0123456789:model\/blazingtext-2022-08-09-13-58-21-739 because no identity-based policy allows the sagemaker:CreateModel action<\/p>\n<p>I made the following policy<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;sagemaker:*&quot;,\n                &quot;iam:PassRole&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>However, it still doesn't work.\nAny pointers?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1660162558070,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1660231466927,
        "Question_score_count":0.0,
        "Question_view_count":61.0,
        "Poster_created_time":1354307337688,
        "Poster_location":null,
        "Poster_reputation_count":403.0,
        "Poster_view_count":70.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73312231",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"accessdeniedexcept clienterror call createmodel clienterror error occur accessdeniedexcept call createmodel oper user arn aw st assum role accesstoservic author perform createmodel resourc arn aw east model blazingtext ident base polici allow createmodel action follow polici version statement sid visualeditor effect allow action iam passrol resourc work pointer",
        "Question_preprocessed_content":"accessdeniedexcept clienterror call createmodel clienterror error occur call createmodel oper user author perform createmodel resourc polici allow createmodel action follow polici work pointer",
        "Question_gpt_summary_original":"The user is encountering an AccessDeniedException ClientError when trying to call the CreateModel operation in SageMaker. The error message indicates that the user is not authorized to perform the action because no identity-based policy allows the sagemaker:CreateModel action. The user has created a policy that allows sagemaker:* and iam:PassRole actions, but it is not working. The user is seeking guidance on how to resolve the issue.",
        "Question_gpt_summary":"user encount accessdeniedexcept clienterror try createmodel oper error messag indic user author perform action ident base polici allow createmodel action user creat polici allow iam passrol action work user seek guidanc resolv issu"
    },
    {
        "Question_title":"MLFlow: cannot specify base image for \"mlflow models build-docker\"",
        "Question_body":"<p>With mlflow models build-docker, it always creates the image from a hardcoded definition (the _DOCKERFILE_TEMPLATE string in mlflow.models.docker_utils.py), which is a pretty 'heavy' image of about 3GB.\nIt here a way to overrule this build-in dockerfile definition so you can use more lightweight docker images?\nFurthermore, it looks like the docker image is serving the model via java which pulls in a lot of additional dependencies. Why can't the default just use the python-func concept, just like mlflow serve on a local system?<\/p>\n<p>Thanks, and by the way, I really like mlflow!<\/p>\n<p><strong>Update Sept 27, 2021: (in response to comment)<\/strong><\/p>\n<p>How to reproduce: (from CLI)<\/p>\n<pre><code>    mlflow models build-docker -m {model uri) -n {docker name}    \n<\/code><\/pre>\n<p>The docker file is always based on what has been hardcoded in<\/p>\n<pre><code>    mlflow.models.docker_utils.py\n<\/code><\/pre>\n<p>in a string called<\/p>\n<pre><code>    _DOCKERFILE_TEMPLATE\n<\/code><\/pre>\n<p>this string is used in the function<\/p>\n<pre><code> _build_image(....)   (line 80)\n<\/code><\/pre>\n<p>where in line 103 it is actually used for creating a local docker file based on this string<\/p>\n<pre><code>        with open(os.path.join(cwd, &quot;Dockerfile&quot;), &quot;w&quot;) as f:\n            f.write(\n                _DOCKERFILE_TEMPLATE.format(\n<\/code><\/pre>\n<p>I would have expected that it would take a user-specified Dockerfile as input instead of the hardcoded _DOCKERFILE_TEMPLATE<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_created_time":1631700593203,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1632753806270,
        "Question_score_count":0.0,
        "Question_view_count":190.0,
        "Poster_created_time":1532616612928,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69191099",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"specifi base imag model build docker model build docker creat imag hardcod definit dockerfil templat string model docker util pretti heavi imag wai overrul build dockerfil definit us lightweight docker imag furthermor look like docker imag serv model java pull lot addit depend default us python func concept like serv local thank wai like updat sept respons comment reproduc cli model build docker model uri docker docker file base hardcod model docker util string call dockerfil templat string function build imag line line actual creat local docker file base string open path join cwd dockerfil write dockerfil templat format expect user specifi dockerfil input instead hardcod dockerfil templat",
        "Question_preprocessed_content":"specifi base imag model model creat imag hardcod definit pretti heavi imag wai overrul dockerfil definit us lightweight docker imag furthermor look like docker imag serv model java pull lot addit depend default us concept like serv local thank wai like updat sept reproduc docker file base hardcod string call string function line actual creat local docker file base string expect dockerfil input instead hardcod",
        "Question_gpt_summary_original":"The user is facing an issue with MLFlow where the mlflow models build-docker command always creates a heavy image of about 3GB from a hardcoded definition. The user is looking for a way to use more lightweight docker images and is questioning why the default cannot use the python-func concept instead of serving the model via java, which pulls in a lot of additional dependencies. The user has provided details on how to reproduce the issue and has suggested that MLFlow should take a user-specified Dockerfile as input instead of the hardcoded definition.",
        "Question_gpt_summary":"user face issu model build docker command creat heavi imag hardcod definit user look wai us lightweight docker imag question default us python func concept instead serv model java pull lot addit depend user provid detail reproduc issu suggest user specifi dockerfil input instead hardcod definit"
    },
    {
        "Question_title":"How to get the status of a pipeline run within a component, running on Vertex AI?",
        "Question_body":"<p>Previously, using Kubeflow Pipelines SDK v1, the status of a pipeline could be inferred during pipeline execution by passing an Argo placeholder, <code>{{workflow.status}}<\/code>, to the component, as shown below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import kfp.dsl as dsl\n\ncomponent_1 = dsl.ContainerOp(\n    name='An example component',\n    image='eu.gcr.io\/...\/my-component-img',\n    arguments=[\n               'python3', 'main.py',\n               '--status', &quot;{{workflow.status}}&quot;\n              ]\n)\n<\/code><\/pre>\n<p>This placeholder would take the value <code>Succeeded<\/code> or <code>Failed<\/code> when passed to the component. One use-case for this would be to send a failure-warning to eg. Slack, in combination with <code>dsl.ExitHandler<\/code>.<\/p>\n<p>However, when using Pipeline SDK version 2, <code>kfp.v2<\/code>, together with Vertex AI to compile and run the pipeline the Argo placeholders no longer work, as described by <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/7614\" rel=\"nofollow noreferrer\">this open issue<\/a>. Because of this, I would need another way to check the status of the pipeline within the component. I was thinking I could use the <code>kfp.Client<\/code> <a href=\"https:\/\/kubeflow-pipelines.readthedocs.io\/en\/latest\/source\/kfp.client.html\" rel=\"nofollow noreferrer\">class<\/a>, but I'm assuming this won't work using Vertex AI, since there is no &quot;host&quot; really. Also, there seems to be supported placeholders for to pass the run id (<code>dsl.PIPELINE_JOB_ID_PLACEHOLDER<\/code>) as a placeholder, as per <a href=\"https:\/\/stackoverflow.com\/questions\/68348026\/run-id-in-kubeflow-pipelines-on-vertex-ai\">this SO post<\/a>, but I can't find anything around <code>status<\/code>.<\/p>\n<p>Any ideas how to get the status of a pipeline run within a component, running on Vertex AI?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1651551486133,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1654867910903,
        "Question_score_count":0.0,
        "Question_view_count":520.0,
        "Poster_created_time":1562750927332,
        "Poster_location":"Stockholm, Sverige",
        "Poster_reputation_count":803.0,
        "Poster_view_count":73.0,
        "Answer_body":"<p>Each pipeline run is automatically logged to Google Logging, and so are also the failed pipeline runs.\nThe error logs also contain information about the pipeline and the component that failed.<\/p>\n<p>We can use this information to monitor our logs and set up an alert via email for example.<\/p>\n<p>The logs for our Vertex AI Pipeline runs we get with the following filter<\/p>\n<p>resource.type=\u201daiplatform.googleapis.com\/PipelineJob\u201d\nseverity=(ERROR OR CRITICAL OR ALERT OR EMERGENCY)<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/e4jFR.png\" rel=\"nofollow noreferrer\">Vertex AI Pipeline Logs<\/a><\/p>\n<p>Based on those logs you can set up log-based alerts <a href=\"https:\/\/cloud.google.com\/logging\/docs\/alerting\/log-based-alerts\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/logging\/docs\/alerting\/log-based-alerts<\/a>. Notifications via email, Slack, SMS, and many more are possible.<\/p>\n<p>source:\n<a href=\"https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1655447541936,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1655446100500,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72094768",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"statu pipelin run compon run previous kubeflow pipelin sdk statu pipelin infer pipelin execut pass argo placehold workflow statu compon shown import kfp dsl dsl compon dsl containerop exampl compon imag gcr compon img argument python main statu workflow statu placehold valu succeed fail pass compon us case send failur warn slack combin dsl exithandl pipelin sdk version kfp compil run pipelin argo placehold longer work describ open issu need wai check statu pipelin compon think us kfp client class assum won work host support placehold pass run dsl pipelin job placehold placehold post statu idea statu pipelin run compon run",
        "Question_preprocessed_content":"statu pipelin run compon run previous kubeflow pipelin sdk statu pipelin infer pipelin execut pass argo placehold compon shown placehold valu pass compon send slack combin pipelin sdk version compil run pipelin argo placehold longer work describ open issu need wai check statu pipelin compon think us class assum won work host support placehold pass run placehold post idea statu pipelin run compon run",
        "Question_gpt_summary_original":"The user is facing a challenge in getting the status of a pipeline run within a component while using Pipeline SDK version 2 with Vertex AI. The Argo placeholders that were used previously to infer the status of a pipeline during execution are no longer working. The user is looking for alternative ways to check the status of the pipeline within the component, but is unsure if using the kfp.Client class would work with Vertex AI. The user is seeking suggestions on how to get the status of a pipeline run within a component on Vertex AI.",
        "Question_gpt_summary":"user face challeng get statu pipelin run compon pipelin sdk version argo placehold previous infer statu pipelin execut longer work user look altern wai check statu pipelin compon unsur kfp client class work user seek suggest statu pipelin run compon"
    },
    {
        "Question_title":"Edit DateTime to be relative to a different date",
        "Question_body":"<p>Is there a way using Azure ML to edit a column of DateTime values so that the dates are set relative to a different date, e.g. the min date in the data set?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1441318942047,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":255.0,
        "Poster_created_time":1305602611663,
        "Poster_location":"Redmond, WA",
        "Poster_reputation_count":1322.0,
        "Poster_view_count":137.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32386351",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"edit datetim rel differ date wai edit column datetim valu date set rel differ date min date data set",
        "Question_preprocessed_content":"edit datetim rel differ date wai edit column datetim valu date set rel differ date min date data set",
        "Question_gpt_summary_original":"The user is seeking a way to edit a column of DateTime values in Azure ML so that the dates are relative to a different date, such as the minimum date in the dataset.",
        "Question_gpt_summary":"user seek wai edit column datetim valu date rel differ date minimum date dataset"
    },
    {
        "Question_title":"Vertex AI - how to monitor training progress?",
        "Question_body":"<h1>Question<\/h1>\n<p>Is there a way to monitor the console output of model training progress during the Vertex AI training?<\/p>\n<h2>Background<\/h2>\n<p>Suppose we have a Tensorflow\/Keras model training code:<\/p>\n<pre><code>model = keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n])\n\noptimizer = tf.keras.optimizers.RMSprop(0.001)\nmodel.compile(\n    loss='mse',\n    optimizer=optimizer,\n    metrics=['mae', 'mse']\n)\n\nEPOCHS = 1000\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\nearly_history = model.fit(normed_train_data, train_labels, \n                    epochs=EPOCHS, validation_split = 0.2, \n                    callbacks=[early_stop])\n<\/code><\/pre>\n<p>When run the model training from the command line, we can see the progress in the console.<\/p>\n<pre><code>Epoch 1\/1000\nOMP: Info #211: KMP_AFFINITY: decoding x2APIC ids.\nOMP: Info #209: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\nOMP: Info #156: KMP_AFFINITY: 4 available OS procs\nOMP: Info #157: KMP_AFFINITY: Uniform topology\nOMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores\/pkg x 2 threads\/core (2 total cores)\nOMP: Info #213: KMP_AFFINITY: OS proc to physical thread map:\nOMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \nOMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \nOMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \nOMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \nOMP: Info #249: KMP_AFFINITY: pid 1 tid 17 thread 0 bound to OS proc set 0\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 17 thread 1 bound to OS proc set 1\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 28 thread 2 bound to OS proc set 2\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 29 thread 3 bound to OS proc set 3\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 30 thread 4 bound to OS proc set 0\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 18 thread 5 bound to OS proc set 1\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 31 thread 6 bound to OS proc set 2\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 32 thread 7 bound to OS proc set 3\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 33 thread 8 bound to OS proc set 0\n8\/8 [==============================] - 2s 31ms\/step - loss: 579.6393 - mae: 22.7661 - mse: 579.6393 - val_loss: 571.7239 - val_mae: 22.5494 - val_mse: 571.7239\nEpoch 2\/1000\n8\/8 [==============================] - 0s 7ms\/step - loss: 527.9056 - mae: 21.6268 - mse: 527.9056 - val_loss: 520.5531 - val_mae: 21.3917 - val_mse: 520.5531\n...\n<\/code><\/pre>\n<p>However, if we run the training in the Vertex AI training, there looks to be no menu\/option to see the console output. Not sure if it is logged in Log Explorer. Please help understand how to monitor the training progress realtime.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/tbaBD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tbaBD.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1651193148017,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":339.0,
        "Poster_created_time":1416648155470,
        "Poster_location":null,
        "Poster_reputation_count":14749.0,
        "Poster_view_count":968.0,
        "Answer_body":"<p>You may view training logs in the <strong>GCP Logs Explorer<\/strong> by using below query.<\/p>\n<pre><code>resource.type=&quot;ml_job&quot;\nresource.labels.job_id=&quot;your-training-custom-job-ID&quot;\n<\/code><\/pre>\n<p>The <em><strong>your-training-custom-job-ID<\/strong><\/em> can be found on the ongoing Vertex AI Training in GCP console as seen on the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ks4Yf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ks4Yf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Below is the screenshot of the logs for the Vertex AI training in GCP logs explorer using the above query.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/waHu6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/waHu6.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You may click on <strong>Jump to now<\/strong> to immediately view the latest logs. Also, you may use <strong>Stream Logs<\/strong> option to view <strong>REAL TIME<\/strong> log data which you can also adjust the buffer window in which has certain trade offs. You may refer to this <a href=\"https:\/\/cloud.google.com\/logging\/docs\/view\/streaming-live-tailing\" rel=\"nofollow noreferrer\">documentation<\/a> for more information on streaming logs in GCP logs explorer.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1651562908648,
        "Answer_last_edit_time":1651563481836,
        "Answer_score_count":1.0,
        "Answerer_created_time":1639972620503,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72051655",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"monitor train progress question wai monitor consol output model train progress train background suppos tensorflow kera model train code model kera sequenti layer dens activ relu input shape len train dataset kei layer dens activ relu layer dens optim kera optim rmsprop model compil loss mse optim optim metric mae mse epoch earli stop kera callback earlystop monitor val loss patienc earli histori model fit norm train data train label epoch epoch valid split callback earli stop run model train command line progress consol epoch omp info kmp affin decod xapic id omp info kmp affin affin capabl global cpuid leaf info omp info kmp affin initi proc set respect omp info kmp affin avail proc omp info kmp affin uniform topolog omp info kmp affin packag core pkg thread core total core omp info kmp affin proc physic thread map omp info kmp affin proc map packag core thread omp info kmp affin proc map packag core thread omp info kmp affin proc map packag core thread omp info kmp affin proc map packag core thread omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set step loss mae mse val loss val mae val mse epoch step loss mae mse val loss val mae val mse run train train look menu option consol output sure log log explor help understand monitor train progress realtim",
        "Question_preprocessed_content":"monitor train progress question wai monitor consol output model train progress train background suppos model train code run model train command line progress consol run train train look consol output sure log log explor help understand monitor train progress realtim",
        "Question_gpt_summary_original":"The user is asking if there is a way to monitor the console output of model training progress during the Vertex AI training. They have provided a Tensorflow\/Keras model training code and explained that when run from the command line, the progress can be seen in the console. However, when running the training in Vertex AI, there seems to be no menu\/option to see the console output. The user is seeking help to understand how to monitor the training progress in real-time.",
        "Question_gpt_summary":"user ask wai monitor consol output model train progress train provid tensorflow kera model train code explain run command line progress seen consol run train menu option consol output user seek help understand monitor train progress real time"
    },
    {
        "Question_title":"Data Import error for Azure table storage to Azure ML studio ?",
        "Question_body":"<p>Hi Team,  <\/p>\n<p>I tried connecting to Azure table storage in Azure ML Studio. It shows connection successful after updating all credentials but after hitting run, import is landing to internal system error.  <br \/>\nBelow is the message :  <br \/>\n[Critical]     Error: Sorry, it seems that you have encountered an internal system error. Please contact amlforum@microsoft.com with the full URL in the browser and the time you experienced the failure. We can locate this error with your help and investigate further. Thank you.  <\/p>\n<p>Requesting you to please assist in this case.  <\/p>\n<p>Regards,  <br \/>\nSachin<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_created_time":1616070530733,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>Hello,    <\/p>\n<p>There is a known issue that Azure ML Studio only supports \u201chttp\u201d protocol when connecting with Azure Storage Account. You might hit this issue when using the Import Data module.    <\/p>\n<p>Here is a quick work around:    <br \/>\nPlease check the \u201cConfiguration\u201d of your Storage Account, and make sure the \u201cSecure transfer required\u201d is disabled (see the figure below).    <\/p>\n<p>If still encountering error after taking these steps, please double check and make sure the account key is correct.    <\/p>\n<p><a href=\"\/users\/na\/?userid=520e72bc-f33a-4fa2-84f8-4795fd5f44af\">@Sachin Gaikwad  <\/a> Please accept the answer if you feel the work around works. Thank you!    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/80028-image.png?platform=QnA\" alt=\"80028-image.png\" \/>    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Answer_comment_count":0.0,
        "Question_closed_time":1616395982956,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/320696\/data-import-error-for-azure-table-storage-to-azure",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"data import error azur tabl storag studio team tri connect azur tabl storag studio show connect success updat credenti hit run import land intern error messag critic error sorri encount intern error contact amlforum microsoft com url browser time experienc failur locat error help investig thank request assist case regard sachin",
        "Question_preprocessed_content":"data import error azur tabl storag studio team tri connect azur tabl storag studio show connect success updat credenti hit run import land intern error messag critic error sorri encount intern error contact url browser time experienc failur locat error help investig thank request assist case regard sachin",
        "Question_gpt_summary_original":"The user is facing an issue while importing data from Azure table storage to Azure ML Studio. The connection is successful but the import is resulting in an internal system error. The user has requested assistance in resolving the issue.",
        "Question_gpt_summary":"user face issu import data azur tabl storag studio connect success import result intern error user request assist resolv issu"
    },
    {
        "Question_title":"Predictive Web Service disabled",
        "Question_body":"<p>Hello, I'm trying to use the function &quot;Predictive Web Service&quot;    <br \/>\nBut appears disabled:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/113471-image.png?platform=QnA\" alt=\"113471-image.png\" \/>    <\/p>\n<p>Why does this is happening?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_created_time":1625852190597,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/470099\/predictive-web-service-disabled",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"predict web servic disabl hello try us function predict web servic appear disabl happen",
        "Question_preprocessed_content":"predict web servic disabl hello try us function predict web servic appear disabl happen",
        "Question_gpt_summary_original":"The user is facing an issue where the \"Predictive Web Service\" function appears to be disabled and is seeking an explanation for why this is happening.",
        "Question_gpt_summary":"user face issu predict web servic function appear disabl seek explan happen"
    },
    {
        "Question_title":"Azure ML Two-Class Decision Forest causes Score Model Error 1000",
        "Question_body":"<p>Can anybody explain to me why Two-Class Decision Forest causes Score Model Error 1000: AFx Library library exception: col: Invalid index: 1, expected bounds [0, 1)? My dataset is normalized, column names doesn't contains spaces...<\/p>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_created_time":1432796841477,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":174.0,
        "Poster_created_time":1290168495692,
        "Poster_location":"Slovenia",
        "Poster_reputation_count":189.0,
        "Poster_view_count":30.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/30499481",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"class decis forest caus score model error anybodi explain class decis forest caus score model error afx librari librari except col invalid index expect bound dataset normal column name contain space thank",
        "Question_preprocessed_content":"decis forest caus score model error anybodi explain decis forest caus score model error afx librari librari except col invalid index expect bound dataset normal column name contain thank",
        "Question_gpt_summary_original":"The user is facing an issue with the Two-Class Decision Forest in Azure ML, which is causing a Score Model Error 1000 with an AFx Library exception related to an invalid index. The dataset is normalized and column names do not contain spaces.",
        "Question_gpt_summary":"user face issu class decis forest caus score model error afx librari except relat invalid index dataset normal column name contain space"
    },
    {
        "Question_title":"Sagemaker tensorflow endpoint not calling the input_handler when being invoked for a prediction",
        "Question_body":"<p>I'm deploying a <code>tensorflow.serving<\/code> endpoint with a custom <code>inference.py<\/code> script via the <code>entry point<\/code> parameter<\/p>\n<pre><code>model = Model(role='xxx',\n              framework_version='2.2.0',\n              entry_point='inference.py',\n              model_data='xxx')\n\npredictor = model.deploy(instance_type='xxx',\n                         initial_instance_count=1,\n                         endpoint_name='xxx')\n<\/code><\/pre>\n<p>inference.py constains an <code>input_handler<\/code> and an <code>output_handler<\/code> functions, but when i call predict with:<\/p>\n<pre><code>model = Predictor(endpoint_name='xxx')\nurl = 'xxx'\n\ninput = {\n    'instances': [url]\n}\n\npredictions = model.predict(input)\n<\/code><\/pre>\n<p>I'm getting the following <code>error<\/code>:<\/p>\n<p><em>botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;{&quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: &quot;xxx&quot; Type: String is not of expected type: float&quot; }&quot;<\/em><\/p>\n<p>It seems the function is never calling the <code>input_handler<\/code> function in inference.py script. Do you know why this might be happening?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_created_time":1595851293460,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1595931884943,
        "Question_score_count":2.0,
        "Question_view_count":474.0,
        "Poster_created_time":1402755934688,
        "Poster_location":"Porto, Portugal",
        "Poster_reputation_count":5998.0,
        "Poster_view_count":426.0,
        "Answer_body":"<p>Found the problem thanks to AWS support:<\/p>\n<p>I was creating an endpoint that already had an endpoint configuration with the same name and the new configuration wasn't being utilized.<\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1595952661830,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1402755934688,
        "Answerer_location":"Porto, Portugal",
        "Answerer_reputation_count":5998.0,
        "Answerer_view_count":426.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63114905",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"tensorflow endpoint call input handler invok predict deploi tensorflow serv endpoint custom infer script entri point paramet model model role framework version entri point infer model data predictor model deploi instanc type initi instanc count endpoint infer constain input handler output handler function predict model predictor endpoint url input instanc url predict model predict input get follow error botocor errorfactori modelerror error occur modelerror call invokeendpoint oper receiv client error model messag error fail process element instanc list error invalid argument json valu type string expect type float function call input handler function infer script know happen",
        "Question_preprocessed_content":"tensorflow endpoint call invok predict deploi endpoint custom script paramet constain function predict get follow error occur call invokeendpoint oper receiv client error model messag error fail process element instanc list error invalid argument json valu type string expect type float function call function script know happen",
        "Question_gpt_summary_original":"The user is deploying a tensorflow.serving endpoint with a custom inference.py script via the entry point parameter. The inference.py script contains an input_handler and an output_handler functions, but when the user calls predict, they are getting an error that suggests the function is never calling the input_handler function in the inference.py script.",
        "Question_gpt_summary":"user deploi tensorflow serv endpoint custom infer script entri point paramet infer script contain input handler output handler function user call predict get error suggest function call input handler function infer script"
    },
    {
        "Question_title":"Is Azure Machine Learning Python SDK v1 still supported",
        "Question_body":"<p>I followed this page and example (section &quot;Service Principal Authentication&quot;)<\/p>\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb<\/a><\/p>\n<pre><code>import os\nfrom azureml.core.authentication import ServicePrincipalAuthentication\n\nsvc_pr_password = os.environ.get(&quot;AZUREML_PASSWORD&quot;)\n\nsvc_pr = ServicePrincipalAuthentication(\n    tenant_id=&quot;my-tenant-id&quot;,\n    service_principal_id=&quot;my-application-id&quot;,\n    service_principal_password=svc_pr_password)\n\n\nws = Workspace(\n    subscription_id=&quot;my-subscription-id&quot;,\n    resource_group=&quot;my-ml-rg&quot;,\n    workspace_name=&quot;my-ml-workspace&quot;,\n    auth=svc_pr\n    )\n\nprint(&quot;Found workspace {} at location {}&quot;.format(ws.name, ws.location))\n<\/code><\/pre>\n<p>The print works fine. However, when I try to get datastore like below, it times out after 60sec. Why is that? <\/p>\n<pre><code>print(ws.datastores)\n\n&gt;&gt;&gt;\n.\n.\n\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/azureml\/core\/workspace.py&quot;, line 1102, in datastores\n    datastore.name: datastore for datastore in _DatastoreClient.list(self)}\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/azureml\/data\/datastore_client.py&quot;, line 668, in list\n    dss, ct = _DatastoreClient._list(workspace, ct, 100)\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/azureml\/data\/_exception_handler.py&quot;, line 16, in decorated\n    return f(*args, **kwargs)\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/azureml\/data\/datastore_client.py&quot;, line 938, in _list\n    client = _DatastoreClient._get_client(ws, auth, host)\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/azureml\/data\/datastore_client.py&quot;, line 977, in _get_client\n    host = host or host_env or get_service_url(\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/azureml\/_base_sdk_common\/service_discovery.py&quot;, line 120, in get_service_url\n    return cached_service_object.get_cached_service_url(workspace_scope, service_name,\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/azureml\/_base_sdk_common\/service_discovery.py&quot;, line 282, in get_cached_service_url\n    return self.get_cached_services_uris(arm_scope, service_name, unique_id=unique_id,\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/azureml\/_base_sdk_common\/service_discovery.py&quot;, line 182, in wrapper\n    return test_function(self, *args, **kwargs)\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/azureml\/_base_sdk_common\/service_discovery.py&quot;, line 257, in get_cached_services_uris\n    cache[cache_key][DEFAULT_FLIGHT] = super(CachedServiceDiscovery, self).discover_services_uris_from_arm_scope(arm_scope, discovery_url)\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/azureml\/_base_sdk_common\/service_discovery.py&quot;, line 138, in discover_services_uris_from_arm_scope\n    return self.discover_services_uris(discovery_url)\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/azureml\/_base_sdk_common\/service_discovery.py&quot;, line 141, in discover_services_uris\n    status = ClientBase._execute_func(requests.get, discovery_url)\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/azureml\/_restclient\/clientbase.py&quot;, line 375, in _execute_func\n    return cls._execute_func_internal(\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/azureml\/_restclient\/clientbase.py&quot;, line 368, in _execute_func_internal\n    left_retry = cls._handle_retry(back_off, left_retry, total_retry, error, logger, func)\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/azureml\/_restclient\/clientbase.py&quot;, line 400, in _handle_retry\n    raise error\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/azureml\/_restclient\/clientbase.py&quot;, line 359, in _execute_func_internal\n    response = func(*args, **kwargs)\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/requests\/api.py&quot;, line 73, in get\n    return request(&quot;get&quot;, url, params=params, **kwargs)\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/requests\/api.py&quot;, line 59, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/requests\/sessions.py&quot;, line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/requests\/sessions.py&quot;, line 701, in send\n    r = adapter.send(request, **kwargs)\n  File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/requests\/adapters.py&quot;, line 547, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n<\/code><\/pre>\n<p>I tried SDK v2 and I can access datastore. Configuration like tenant_id, service principal id\/password, subscriptionid, resourcegroup, workspace_name are the same.<\/p>\n<pre><code>\nml_client = MLClient.from_config(credential=credential)\n.\n.\nprint(ml_client.datastores.get(&quot;workspaceblobstore&quot;))\n\n&gt;&gt;&gt;\nAzureBlobDatastore({'type': &lt;DatastoreType.AZURE_BLOB: 'AzureBlob'&gt;, 'name': 'workspaceblobstore', 'descriptio\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1683743681150,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1282220\/is-azure-machine-learning-python-sdk-v1-still-supp",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"python sdk support follow page exampl section servic princip authent http github com azur machinelearningnotebook blob master us manag servic authent authent ipynb import core authent import serviceprincipalauthent svc password environ password svc serviceprincipalauthent tenant tenant servic princip applic servic princip password svc password workspac subscript subscript resourc group workspac workspac auth svc print workspac locat format locat print work fine try datastor like time sec print datastor file opt anaconda lib python site packag core workspac line datastor datastor datastor datastor datastorecli list self file opt anaconda lib python site packag data datastor client line list dss datastorecli list workspac file opt anaconda lib python site packag data except handler line decor return arg kwarg file opt anaconda lib python site packag data datastor client line list client datastorecli client auth host file opt anaconda lib python site packag data datastor client line client host host host env servic url file opt anaconda lib python site packag base sdk common servic discoveri line servic url return cach servic object cach servic url workspac scope servic file opt anaconda lib python site packag base sdk common servic discoveri line cach servic url return self cach servic uri arm scope servic uniqu uniqu file opt anaconda lib python site packag base sdk common servic discoveri line wrapper return test function self arg kwarg file opt anaconda lib python site packag base sdk common servic discoveri line cach servic uri cach cach kei default flight super cachedservicediscoveri self discov servic uri arm scope arm scope discoveri url file opt anaconda lib python site packag base sdk common servic discoveri line discov servic uri arm scope return self discov servic uri discoveri url file opt anaconda lib python site packag base sdk common servic discoveri line discov servic uri statu clientbas execut func request discoveri url file opt anaconda lib python site packag restclient clientbas line execut func return cl execut func intern file opt anaconda lib python site packag restclient clientbas line execut func intern left retri cl handl retri left retri total retri error logger func file opt anaconda lib python site packag restclient clientbas line handl retri rais error file opt anaconda lib python site packag restclient clientbas line execut func intern respons func arg kwarg file opt anaconda lib python site packag request api line return request url param param kwarg file opt anaconda lib python site packag request api line request return session request method method url url kwarg file opt anaconda lib python site packag request session line request resp self send prep send kwarg file opt anaconda lib python site packag request session line send adapt send request kwarg file opt anaconda lib python site packag request adapt line send rais connectionerror err request request request except connectionerror connect abort timeouterror oper time tri sdk access datastor configur like tenant servic princip password subscriptionid resourcegroup workspac client mlclient config credenti credenti print client datastor workspaceblobstor azureblobdatastor type workspaceblobstor descriptio",
        "Question_preprocessed_content":"python sdk support follow page exampl print work fine try datastor like time sec tri sdk access datastor configur like servic princip subscriptionid resourcegroup",
        "Question_gpt_summary_original":"The user is facing a timeout issue while trying to access the data store using Azure Machine Learning Python SDK v1, even though the workspace is found successfully. However, the user was able to access the data store using SDK v2 with the same configuration.",
        "Question_gpt_summary":"user face timeout issu try access data store python sdk workspac successfulli user abl access data store sdk configur"
    },
    {
        "Question_title":"Does Sagemaker support private GitLab repos?",
        "Question_body":"I have a private GitLab and would like to connect it to Sagemaker Studio and\/or Notebooks. Is that supported? What is it is through a VPC? If so, how can it be done?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1650995574723,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668605819551,
        "Question_score_count":1.0,
        "Question_view_count":823.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUI8X0yZHTQ1uiS-TxOJe4ww\/does-sagemaker-support-private-gitlab-repos",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"support privat gitlab repo privat gitlab like connect studio notebook support vpc",
        "Question_preprocessed_content":"support privat gitlab repo privat gitlab like connect studio notebook support vpc",
        "Question_gpt_summary_original":"The user is inquiring if Sagemaker supports private GitLab repos and how to connect it to Sagemaker Studio and\/or Notebooks, especially if it is through a VPC.",
        "Question_gpt_summary":"user inquir support privat gitlab repo connect studio notebook especi vpc"
    },
    {
        "Question_title":"How to fix 'KeyError: dtype('float32')' in LDAviz",
        "Question_body":"<p>I use LDAvis library to visualize my LDA topics. It works fine before, but it gets me this error when I download the saved model files from Sagemaker to the local computer. I don't know why does this happen? Does that relate to Sagemaker?<\/p>\n\n<p>If I run from the local, and saved the model from local, and then run LDAviz library, it works fine.<\/p>\n\n<hr>\n\n<p>KeyError                                  Traceback (most recent call last)\n in ()<\/p>\n\n<p>~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyLDAvis\\gensim.py in prepare(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\n    116     See <code>pyLDAvis.prepare<\/code> for **kwargs.\n    117     \"\"\"\n--> 118     opts = fp.merge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n    119     return vis_prepare(**opts)<\/p>\n\n<p>~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyLDAvis\\gensim.py in _extract_data(topic_model, corpus, dictionary, doc_topic_dists)\n     46           gamma = topic_model.inference(corpus)\n     47       else:\n---> 48           gamma, _ = topic_model.inference(corpus)\n     49       doc_topic_dists = gamma \/ gamma.sum(axis=1)[:, None]\n     50    else:<\/p>\n\n<p>~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py in inference(self, chunk, collect_sstats)\n    665             # phinorm is the normalizer.\n    666             # TODO treat zeros explicitly, instead of adding epsilon?\n--> 667             eps = DTYPE_TO_EPS[self.dtype]\n    668             phinorm = np.dot(expElogthetad, expElogbetad) + eps\n    669 <\/p>\n\n<p>KeyError: dtype('float32')<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1561573597343,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":577.0,
        "Poster_created_time":1524091215152,
        "Poster_location":"Hoboken, NJ, USA",
        "Poster_reputation_count":187.0,
        "Poster_view_count":28.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56779011",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"fix keyerror dtype float ldaviz us ldavi librari visual lda topic work fine get error download save model file local know happen relat run local save model local run ldaviz librari work fine keyerror traceback recent appdata local continuum anaconda lib site packag pyldavi gensim prepar topic model corpu dictionari doc topic dist kwarg pyldavi prepar kwarg opt merg extract data topic model corpu dictionari doc topic dist kwarg return vi prepar opt appdata local continuum anaconda lib site packag pyldavi gensim extract data topic model corpu dictionari doc topic dist gamma topic model infer corpu gamma topic model infer corpu doc topic dist gamma gamma sum axi appdata local continuum anaconda lib site packag gensim model ldamodel infer self chunk collect sstat phinorm normal todo treat zero explicitli instead ad epsilon ep dtype ep self dtype phinorm dot expelogthetad expelogbetad ep keyerror dtype float",
        "Question_preprocessed_content":"fix keyerror dtype ldaviz us ldavi librari visual lda topic work fine get error download save model file local know happen relat run local save model local run ldaviz librari work fine keyerror traceback corpu dictionari kwarg kwarg opt corpu dictionari kwarg return corpu dictionari gamma gamma gamma infer phinorm normal todo treat zero explicitli instead ad epsilon ep phinorm expelogbetad ep keyerror dtype",
        "Question_gpt_summary_original":"The user is encountering a 'KeyError: dtype('float32')' error while using the LDAvis library to visualize their LDA topics. The error occurs when they download saved model files from Sagemaker to their local computer. The user is unsure why this is happening and if it is related to Sagemaker. However, when they run the library from the local and save the model from there, it works fine.",
        "Question_gpt_summary":"user encount keyerror dtype float error ldavi librari visual lda topic error occur download save model file local user unsur happen relat run librari local save model work fine"
    },
    {
        "Question_title":"How to use Azure Data Lake Store as an input data set for Azure ML?",
        "Question_body":"<p>I am moving data into Azure Data Lake Store and processing it using Azure Data Lake Analytics. Data is in form of XML and I am reading it through <a href=\"https:\/\/github.com\/Azure\/usql\/tree\/master\/Examples\/DataFormats\/Microsoft.Analytics.Samples.Formats\" rel=\"nofollow\">XML Extractor<\/a>. Now I want to access this data from Azure ML and it looks like Azure Data Lake store is not directly supported at the moment. <\/p>\n\n<p>What are the possible ways to use Azure Data Lake Store with Azure ML?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1458553338210,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":963.0,
        "Poster_created_time":1274637971227,
        "Poster_location":"Pakistan",
        "Poster_reputation_count":838.0,
        "Poster_view_count":101.0,
        "Answer_body":"<p>Right now, Azure Data Lake Store is not a supported source, as you note.  That said, Azure Data Lake Analytics can also be used to write data out to Azure Blob Store, and so you can use that as an approach to process the data in U-SQL and then stage it for Azure Machine Learning to process it from Blob store.  When Azure ML supports Data Lake store, then you can switch that over. <\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1458750128120,
        "Answer_last_edit_time":null,
        "Answer_score_count":4.0,
        "Answerer_created_time":1289236634983,
        "Answerer_location":null,
        "Answerer_reputation_count":315.0,
        "Answerer_view_count":82.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36127510",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"us azur data lake store input data set move data azur data lake store process azur data lake analyt data form xml read xml extractor want access data look like azur data lake store directli support moment possibl wai us azur data lake store",
        "Question_preprocessed_content":"us azur data lake store input data set move data azur data lake store process azur data lake analyt data form xml read xml extractor want access data look like azur data lake store directli support moment possibl wai us azur data lake store",
        "Question_gpt_summary_original":"The user is facing a challenge in using Azure Data Lake Store as an input data set for Azure ML. They are currently processing XML data using Azure Data Lake Analytics and want to access this data from Azure ML, but it appears that Azure Data Lake Store is not directly supported. The user is seeking possible solutions to this issue.",
        "Question_gpt_summary":"user face challeng azur data lake store input data set current process xml data azur data lake analyt want access data appear azur data lake store directli support user seek possibl solut issu"
    },
    {
        "Question_title":"How send to image payload to the Deep learning model using lambda and send get the Image respone",
        "Question_body":"<p>I am working on Deploying background removal using AWS lambda, I didn't understand how to create a proper image payload to model and get the response back as an image.\nplease help me with this, I am struggling with this for one week.\nhere I tried:<\/p>\n<pre><code> model_dir='model\/u2net.pth'\n    model=load_models(model_dir)\n    \n    def lambda_handler(event, context):\n        \n        url = event['url']\n        img = io.imread(url)\n        # classify image\n        img = u2net_api_call(img, model)\n        output_image = img.numpy()\n        output_image = np.uint8(output_image.transpose(1, 2, 0) * 255)\n        output_image = Image.fromarray(output_image)\n    \n      # convert the PIL image to base64\n        result = {\n          &quot;output&quot;: img_to_base64_str(output_image)\n      }\n    \n      # send the result back to the client inside the body field\n        return {\n          &quot;statusCode&quot;: 200,\n          &quot;body&quot;: json.dumps(result),\n          &quot;headers&quot;: {\n              'Content-Type': 'application\/json',\n              'Access-Control-Allow-Origin': '*'\n          }\n      }\n<\/code><\/pre>\n<p>I have deployed these with lambda and docker images, I have created the docker image and push ECR and I am adding with lambda.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1646168848977,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":32.0,
        "Poster_created_time":1583415050007,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":1.0,
        "Poster_view_count":15.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71314659",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"send imag payload deep learn model lambda send imag respon work deploi background remov aw lambda understand creat proper imag payload model respons imag help struggl week tri model dir model unet pth model load model model dir def lambda handler event context url event url img imread url classifi imag img unet api img model output imag img numpi output imag uint output imag transpos output imag imag fromarrai output imag convert pil imag base result output img base str output imag send result client insid bodi field return statuscod bodi json dump result header content type applic json access control allow origin deploi lambda docker imag creat docker imag push ecr ad lambda",
        "Question_preprocessed_content":"send imag payload deep learn model lambda send imag respon work deploi background remov aw lambda understand creat proper imag payload model respons imag help struggl week tri deploi lambda docker imag creat docker imag push ecr ad lambda",
        "Question_gpt_summary_original":"The user is struggling to create a proper image payload to send to a deep learning model using AWS Lambda and receive the response back as an image. They have tried deploying the model with Lambda and Docker images, but are still facing difficulties.",
        "Question_gpt_summary":"user struggl creat proper imag payload send deep learn model aw lambda receiv respons imag tri deploi model lambda docker imag face difficulti"
    },
    {
        "Question_title":"Unable to run conda package manager. AzureML uses conda to provision python",
        "Question_body":"<p>Hi I was trying to run the 14. 'Interpret Models' exercise from  <a href=\"https:\/\/aka.ms\/mslearn-dp100\">https:\/\/aka.ms\/mslearn-dp100<\/a> in Azure ML Jupyter notebook, and was met with following error:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/189503-image.png?platform=QnA\" alt=\"189503-image.png\" \/>    <\/p>\n<p>i have installed the relevant packages and was unsure how to resolve it<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1648968542177,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/798162\/unable-to-run-conda-package-manager-azureml-uses-c",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unabl run conda packag manag us conda provis python try run interpret model exercis http aka mslearn jupyt notebook met follow error instal relev packag unsur resolv",
        "Question_preprocessed_content":"unabl run conda packag manag us conda provis python try run interpret model exercis jupyt notebook met follow error instal relev packag unsur resolv",
        "Question_gpt_summary_original":"The user is unable to run the conda package manager while trying to run the 'Interpret Models' exercise from Azure ML Jupyter notebook. The user has installed the relevant packages but is still facing an error.",
        "Question_gpt_summary":"user unabl run conda packag manag try run interpret model exercis jupyt notebook user instal relev packag face error"
    },
    {
        "Question_title":"How to define concurrency in SageMaker real-time inference",
        "Question_body":"Hi Team,\n\nGreetings!!\n\nCould you please confirm, how can we define concurrency in SageMaker real-time inference?\n\nThis is how we define concurrency in SageMaker serverless inference.\n\n\u201cServerlessConfig\u201d: {\n\u201cMemorySizeInMB\u201d: 4096,\n\u201cMaxConcurrency\u201d: 10,\n}\n\n\nThanks,\nVinayak",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1645076515894,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1668575046931,
        "Question_score_count":0.0,
        "Question_view_count":586.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOW_MzWx4QWaQeIqVYMEyeQ\/how-to-define-concurrency-in-sagemaker-real-time-inference",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"defin concurr real time infer team greet confirm defin concurr real time infer defin concurr serverless infer serverlessconfig memorysizeinmb maxconcurr thank vinayak",
        "Question_preprocessed_content":"defin concurr infer team greet confirm defin concurr infer defin concurr serverless infer serverlessconfig thank vinayak",
        "Question_gpt_summary_original":"The user is seeking clarification on how to define concurrency in SageMaker real-time inference and has provided an example of how it is defined in SageMaker serverless inference.",
        "Question_gpt_summary":"user seek clarif defin concurr real time infer provid exampl defin serverless infer"
    },
    {
        "Question_title":"Is SageMaker Distributed Data-Parallel (SMDDP) supported for keras models?",
        "Question_body":"<p>Is SageMaker Distributed Data-Parallel (SMDDP) supported for keras models?<\/p>\n<p>In documentation it says &quot;SageMaker distributed data parallel is adaptable to TensorFlow training scripts composed of tf core modules except tf.keras modules. SageMaker distributed data parallel does not support TensorFlow with Keras implementation.&quot;\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-parallel-modify-sdp.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-parallel-modify-sdp.html<\/a><\/p>\n<p>But inside the training script and how to modify it, I can see the tf.keras and tf.keras.model is used.\u00a0<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/sdp_versions\/v1.0.0\/smd_data_parallel_tensorflow.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/sdp_versions\/v1.0.0\/smd_data_parallel_tensorflow.html<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1662702971423,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":12.0,
        "Poster_created_time":1662621266503,
        "Poster_location":null,
        "Poster_reputation_count":48.0,
        "Poster_view_count":6.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73657854",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"distribut data parallel smddp support kera model distribut data parallel smddp support kera model document sai distribut data parallel adapt tensorflow train script compos core modul kera modul distribut data parallel support tensorflow kera implement http doc aw amazon com latest data parallel modifi sdp html insid train script modifi kera kera model http readthedoc stabl api train sdp version smd data parallel tensorflow html",
        "Question_preprocessed_content":"distribut support kera model distribut support kera model document sai distribut data parallel adapt tensorflow train script compos core modul modul distribut data parallel support tensorflow kera insid train script modifi",
        "Question_gpt_summary_original":"The user is questioning whether SageMaker Distributed Data-Parallel (SMDDP) is supported for Keras models. The documentation states that SMDDP does not support TensorFlow with Keras implementation, but the training script and modification guide use tf.keras and tf.keras.model.",
        "Question_gpt_summary":"user question distribut data parallel smddp support kera model document state smddp support tensorflow kera implement train script modif guid us kera kera model"
    },
    {
        "Question_title":"Azure ml notebook does not show widgets from ipywidgets",
        "Question_body":"<p>ipywidgets seems to work fine for the simplest usages, i.e. just using a slider. However, when trying to use more complex functionality the notebook does not show \/ display the widgets anymore.    <\/p>\n<p>See in the picture:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/96865-image.png?platform=QnA\" alt=\"96865-image.png\" \/>    <\/p>\n<p>The simple usage    <\/p>\n<pre><code>widgets.IntSlider()  \n<\/code><\/pre>\n<p>works fine.    <br \/>\nHowever, using ipywidget's interact does not show any widget:    <\/p>\n<pre><code>def f(x):  \n    return x  \n  \ninteract(f, x=10)  \n<\/code><\/pre>\n<p>When I change the editor using the dropdown    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/96817-image.png?platform=QnA\" alt=\"96817-image.png\" \/>    <\/p>\n<p>and use Jupyter or JupyterLab, everything works as expected without flaws.    <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_created_time":1621085408217,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/396317\/azure-ml-notebook-does-not-show-widgets-from-ipywi",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"notebook widget ipywidget ipywidget work fine simplest usag slider try us complex function notebook displai widget anymor pictur simpl usag widget intslid work fine ipywidget interact widget def return interact chang editor dropdown us jupyt jupyterlab work expect flaw",
        "Question_preprocessed_content":"notebook widget ipywidget ipywidget work fine simplest usag slider try us complex function notebook displai widget anymor pictur simpl usag work fine ipywidget interact widget chang editor dropdown us jupyt jupyterlab work expect flaw",
        "Question_gpt_summary_original":"The user is facing an issue with Azure ml notebook where ipywidgets work fine for simple usage but fail to display widgets for more complex functionality. The user has provided an example of using interact which does not show any widget. The issue is resolved when using Jupyter or JupyterLab.",
        "Question_gpt_summary":"user face issu notebook ipywidget work fine simpl usag fail displai widget complex function user provid exampl interact widget issu resolv jupyt jupyterlab"
    },
    {
        "Question_title":"AZURE ML getting model weights",
        "Question_body":"<p>I have deployed a regression model on azure ML , is it possible to get the model weights\/coefficients of the model programatically from azure, rather than getting predicted value? .<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1458651323657,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":4.0,
        "Question_view_count":1044.0,
        "Poster_created_time":1422821109972,
        "Poster_location":"India",
        "Poster_reputation_count":1238.0,
        "Poster_view_count":172.0,
        "Answer_body":"<p>No. Currently we do not feature exporting weights from the models including with Azure Machine Learning. <\/p>\n\n<p>If you have a method for extracting weights from Python models, you may be able to work this out using the execute Python Script module.<\/p>\n\n<p>The primary purpose of Azure Machine Learning is to make deployable and scalable web services from the machine learning modules. Though the authoring experience for creating ML models is great, it is not intended to be a place to create and export models, but instead a place to create and operationalize your models. <\/p>\n\n<p><em>UPDATE<\/em> New features may make this answer outdated. <\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1458737377168,
        "Answer_last_edit_time":1495601794132,
        "Answer_score_count":2.0,
        "Answerer_created_time":1334762714136,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36154971",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"get model weight deploi regress model possibl model weight coeffici model programat azur get predict valu",
        "Question_preprocessed_content":"get model weight deploi regress model possibl model model programat azur get predict valu",
        "Question_gpt_summary_original":"The user has deployed a regression model on Azure ML and is seeking a way to programmatically obtain the model weights\/coefficients instead of just the predicted value.",
        "Question_gpt_summary":"user deploi regress model seek wai programmat obtain model weight coeffici instead predict valu"
    },
    {
        "Question_title":"WebScrapping error in Google Cloud - Vertex AI Workbench (using Python3 & Selenium)",
        "Question_body":"<p>I am trying to use Workbench Managed Notebooks to schedule some Jupyter notebooks to run Selenium for webscrapping some pages.<\/p>\n<p>My code is below:<\/p>\n<pre><code>from get_gecko_driver import GetGeckoDriver\nget_driver = GetGeckoDriver()\nget_driver = GetGeckoDriver()\nget_driver.install()\n\nfrom selenium import webdriver\nfrom selenium.webdriver import FirefoxOptions\n\nopts = FirefoxOptions()\nopts.add_argument(&quot;--headless&quot;)\nbrowser = webdriver.Firefox(options=opts)\n<\/code><\/pre>\n<p>I get the error:<\/p>\n<pre><code>---------------------------------------------------------------------------\nSessionNotCreatedException                Traceback (most recent call last)\n\/tmp\/ipykernel_1\/3371773802.py in &lt;module&gt;\n      4 opts = FirefoxOptions()\n      5 opts.add_argument(&quot;--headless&quot;)\n----&gt; 6 browser = webdriver.Firefox(options=opts)\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/firefox\/webdriver.py in __init__(self, firefox_profile, firefox_binary, capabilities, proxy, executable_path, options, service_log_path, service_args, service, desired_capabilities, log_path, keep_alive)\n    178             command_executor=executor,\n    179             options=options,\n--&gt; 180             keep_alive=True)\n    181 \n    182         self._is_remote = False\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/webdriver.py in __init__(self, command_executor, desired_capabilities, browser_profile, proxy, keep_alive, file_detector, options)\n    275         self._authenticator_id = None\n    276         self.start_client()\n--&gt; 277         self.start_session(capabilities, browser_profile)\n    278 \n    279     def __repr__(self):\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/webdriver.py in start_session(self, capabilities, browser_profile)\n    368         w3c_caps = _make_w3c_caps(capabilities)\n    369         parameters = {&quot;capabilities&quot;: w3c_caps}\n--&gt; 370         response = self.execute(Command.NEW_SESSION, parameters)\n    371         if 'sessionId' not in response:\n    372             response = response['value']\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/webdriver.py in execute(self, driver_command, params)\n    433         response = self.command_executor.execute(driver_command, params)\n    434         if response:\n--&gt; 435             self.error_handler.check_response(response)\n    436             response['value'] = self._unwrap_value(\n    437                 response.get('value', None))\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/errorhandler.py in check_response(self, response)\n    245                 alert_text = value['alert'].get('text')\n    246             raise exception_class(message, screen, stacktrace, alert_text)  # type: ignore[call-arg]  # mypy is not smart enough here\n--&gt; 247         raise exception_class(message, screen, stacktrace)\n    248 \n    249     def _value_or_default(self, obj: Mapping[_KT, _VT], key: _KT, default: _VT) -&gt; _VT:\n\nSessionNotCreatedException: Message: Expected browser binary location, but unable to find binary in default location, no 'moz:firefoxOptions.binary' capability provided, and no binary flag set on the command line\n<\/code><\/pre>\n<p>It seems that I need to setup somehow the location of Firefox, but I don't know if this is possible and how to do it (or if there are some alternatives that are easier to setup e.g. chromium).<\/p>\n<p><em>Please Note: this code is running perfectly fine when I run Jupyter notebooks in VM instances but I cannot schedule those notebooks to run automatically so I guess my only option is to go with the Vertex AI &gt; Workbench &gt; Managed Notebooks solution.<\/em><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1657864043243,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":137.0,
        "Poster_created_time":1623914392096,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":4.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72989531",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"webscrap error googl cloud workbench python selenium try us workbench manag notebook schedul jupyt notebook run selenium webscrap page code gecko driver import getgeckodriv driver getgeckodriv driver getgeckodriv driver instal selenium import webdriv selenium webdriv import firefoxopt opt firefoxopt opt add argument headless browser webdriv firefox option opt error sessionnotcreatedexcept traceback recent tmp ipykernel opt firefoxopt opt add argument headless browser webdriv firefox option opt local lib python site packag selenium webdriv firefox webdriv init self firefox profil firefox binari capabl proxi execut path option servic log path servic arg servic desir capabl log path aliv command executor executor option option aliv true self remot fals local lib python site packag selenium webdriv remot webdriv init self command executor desir capabl browser profil proxi aliv file detector option self authent self start client self start session capabl browser profil def repr self local lib python site packag selenium webdriv remot webdriv start session self capabl browser profil cap cap capabl paramet capabl cap respons self execut command new session paramet sessionid respons respons respons valu local lib python site packag selenium webdriv remot webdriv execut self driver command param respons self command executor execut driver command param respons self error handler check respons respons respons valu self unwrap valu respons valu local lib python site packag selenium webdriv remot errorhandl check respons self respons alert text valu alert text rais except class messag screen stacktrac alert text type ignor arg mypi smart rais except class messag screen stacktrac def valu default self obj map kei default sessionnotcreatedexcept messag expect browser binari locat unabl binari default locat moz firefoxopt binari capabl provid binari flag set command line need setup locat firefox know possibl altern easier setup chromium note code run perfectli fine run jupyt notebook instanc schedul notebook run automat guess option workbench manag notebook solut",
        "Question_preprocessed_content":"webscrap error googl cloud workbench try us workbench manag notebook schedul jupyt notebook run selenium webscrap page code error need setup locat firefox know possibl note code run perfectli fine run jupyt notebook instanc schedul notebook run automat guess option workbench manag notebook solut",
        "Question_gpt_summary_original":"The user is encountering a SessionNotCreatedException error while trying to use Workbench Managed Notebooks to schedule Jupyter notebooks to run Selenium for web scraping some pages. The error message suggests that the user needs to set up the location of Firefox, but they are unsure how to do it or if there are any alternatives such as Chromium. The code runs fine when the user runs Jupyter notebooks in VM instances, but they cannot schedule those notebooks to run automatically, so they are trying to use the Vertex AI > Workbench > Managed Notebooks solution.",
        "Question_gpt_summary":"user encount sessionnotcreatedexcept error try us workbench manag notebook schedul jupyt notebook run selenium web scrape page error messag suggest user need set locat firefox unsur altern chromium code run fine user run jupyt notebook instanc schedul notebook run automat try us workbench manag notebook solut"
    },
    {
        "Question_title":"Quota increase for training on Vertex AI",
        "Question_body":"I am trying to use the custom container method to train a simple model. I am executing a config.yaml file through which I am running a job but I received the following error.\u00a0\n\nError:\u00a0\"error\": { \"code\": 429, \"message\": \"The following quota metrics exceed quota limits: aiplatform.googleapis.com\/custom_model_training_n2_cpus\", \"status\": \"RESOURCE_EXHAUSTED\" } }\n\nThe config.yaml file looks like this:\u00a0\n\nworkerPoolSpecs:\n\u00a0\u00a0machineSpec:\n\u00a0\u00a0\u00a0\u00a0machineType:\u00a0n1-standard-4\n\u00a0\u00a0\u00a0\u00a0acceleratorType:\u00a0NVIDIA_TESLA_T4\n\u00a0\u00a0\u00a0\u00a0acceleratorCount:\u00a01\n\u00a0\u00a0replicaCount:\u00a01\n\u00a0\u00a0containerSpec:\n\u00a0\u00a0\u00a0\u00a0imageUri:\u00a0gcr.io\/vertexAIdemo\/vertexai-bert-training:latest\n\u00a0\n\u00a0\nI tried to increase quota and it cant be increased. I tried to contact several teams and it was to no help. Please provide me a way to how I can be able to increase the quota to execute the job.\n\u00a0\nPS: no vertex Ai Api quota is available to edit and is currently 0 for all regions be it CPU or GPU",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_created_time":1673579700000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":231.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Quota-increase-for-training-on-Vertex-AI\/td-p\/509872\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"quota increas train try us custom contain method train simpl model execut config yaml file run job receiv follow error error error code messag follow quota metric exce quota limit aiplatform googleapi com custom model train cpu statu resourc exhaust config yaml file look like workerpoolspec machinespec machinetyp standard acceleratortyp nvidia tesla acceleratorcount replicacount containerspec imageuri gcr vertexaidemo vertexai bert train latest tri increas quota increas tri contact team help provid wai abl increas quota execut job api quota avail edit current region cpu gpu",
        "Question_preprocessed_content":"quota increas train try us custom contain method train simpl model execut file run job receiv follow error error error file look like workerpoolspec machinespec acceleratorcount replicacount containerspec tri increas quota increas tri contact team help provid wai abl increas quota execut job api quota avail edit current region cpu gpu",
        "Question_gpt_summary_original":"The user is encountering a quota limit error while trying to train a model using the custom container method on Vertex AI. The error message indicates that the custom model training n2 CPUs quota limit has been exceeded. The user has tried to increase the quota but has been unsuccessful and is seeking help to execute the job. Additionally, the user mentions that no Vertex AI API quota is available to edit and is currently 0 for all regions, whether CPU or GPU.",
        "Question_gpt_summary":"user encount quota limit error try train model custom contain method error messag indic custom model train cpu quota limit exceed user tri increas quota unsuccess seek help execut job addition user mention api quota avail edit current region cpu gpu"
    },
    {
        "Question_title":"Issue with MLFlow remote tracking server and Databricks-connect",
        "Question_body":"Hi all,\n\n\nI'm not able\u00a0 to load a MLFlow model from a remote server located in Databricks dbfs.\n\n\nI'm using Kedro package locally for my project structure and I run the pipelines using the package databricks-connect to send the computation instructions to my cluster on Databricks.\n\n\nCurrently, I have a MLFLow model in a dbfs path which I can retrieve successfully from the Databricks workspace (Databricks web interface) using the command: mlflow.spark.load_model('dbfs:\/<path>) on the notebooks.\n\n\nUnfortunately, when I try to replicate this locally (from my kedro project) with these lines:\nmlflow.set_tracking_uri('databricks')\nmlflow.spark.load_model('dbfs:\/<path>)\n\n\nI get the following error message:\n\"An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: java.io.IOException: Illegal file pattern: Illegal\/unsupported escape sequence near index 8\nsparkml\\metadata\"\n\n\nThanks,\nRodrigo.",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1617205200000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":null,
        "Question_view_count":89.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/nrQ7-1wDTO4",
        "Tool":"MLflow",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"issu remot track server databrick connect abl load model remot server locat databrick dbf packag local project structur run pipelin packag databrick connect send comput instruct cluster databrick current model dbf path retriev successfulli databrick workspac databrick web interfac command spark load model dbf notebook unfortun try replic local project line set track uri databrick spark load model dbf follow error messag error occur call org apach spark api python pythonrdd runjob java ioexcept illeg file pattern illeg unsupport escap sequenc near index sparkml metadata thank rodrigo",
        "Question_preprocessed_content":"issu remot track server abl load model remot server locat databrick dbf packag local project structur run pipelin packag send comput instruct cluster databrick current model dbf path retriev successfulli databrick workspac command notebook unfortun try replic local line follow error messag error occur call illeg file pattern escap sequenc near index thank rodrigo",
        "Question_gpt_summary_original":"The user is facing an issue while trying to load an MLFlow model from a remote server located in Databricks dbfs. They are using Kedro package locally for their project structure and running pipelines using the package databricks-connect to send the computation instructions to their cluster on Databricks. However, they are encountering an error message while trying to replicate the process locally.",
        "Question_gpt_summary":"user face issu try load model remot server locat databrick dbf packag local project structur run pipelin packag databrick connect send comput instruct cluster databrick encount error messag try replic process local"
    },
    {
        "Question_title":"Difference in performance between Mlflow run and manual training",
        "Question_body":"<p>I'm working in Databricks trying to train a <code>XGBClassifier<\/code> model in an sklearn <code>Pipeline<\/code> using both manual code and within an Mlflow run. I have a training set <code>X_train<\/code>, a validation set <code>X_val<\/code>, and a test set <code>X_test<\/code>. When I run the model on my test set, the performance metrics for each method are different, with the Mlflow metrics being much better. Below is my code for the manual fitting and testing (preprocessor is a standard scaler and one-hot encoder):<\/p>\n<pre><code>model = Pipeline([\n    (&quot;column_selector&quot;, col_selector),\n    (&quot;preprocessor&quot;, preprocessor),\n    (&quot;classifier&quot;, xgb_model),\n])\n\nmodel.fit(X_train, y_train, classifier__early_stopping_rounds=5,\nclassifier__eval_set=[(X_val_processed,y_val)], classifier__verbose=False)\n\ny_pred = model.predict(X_test)\ny_probs = model.predict_proba(X_test)\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, log_loss\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_f1 = f1_score(y_test, y_pred)\ntest_prec = precision_score(y_test, y_pred)\ntest_rec = recall_score(y_test, y_pred)\ntest_logloss = log_loss(y_test, y_pred)\ntest_rocauc = roc_auc_score(y_test, y_probs[:, 1])\nmetric_cols = ['accuracy', 'f1_score', 'precision', 'recall', 'log_loss', 'roc_auc']\npd.DataFrame([[test_acc, test_f1, test_prec, test_rec, test_logloss, test_rocauc]], columns=metric_cols, index=['test'])\n<\/code><\/pre>\n<p>Which gets me the following:<\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th><\/th>\n<th>accuracy<\/th>\n<th>f1_score<\/th>\n<th>precision<\/th>\n<th>recall<\/th>\n<th>log_loss<\/th>\n<th>roc_auc<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>test<\/td>\n<td>0.865784<\/td>\n<td>0.326917<\/td>\n<td>0.753635<\/td>\n<td>0.208731<\/td>\n<td>4.635675<\/td>\n<td>0.738647<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>Here is my code for the mlflow run:<\/p>\n<pre><code>import mlflow\nimport databricks.automl_runtime\n\nmlflow.sklearn.autolog(disable=True)\n\nmlflow.sklearn.autolog(log_input_examples=True, silent=True)\n\nwith mlflow.start_run(experiment_id=&quot;363669966797473&quot;, run_name=&quot;xgb-mod-3-run&quot;) as mlflow_run:\n    model.fit(X_train, y_train, classifier__early_stopping_rounds=5, classifier__eval_set=[(X_val_processed,y_val)], classifier__verbose=False)\n\n    xgbc_test_metrics = mlflow.sklearn.eval_and_log_metrics(model, X_test, y_test, prefix=&quot;test_&quot;)\n\n    xgbc_test_metrics = {k.replace(&quot;test_&quot;, &quot;&quot;): v for k, v in xgbc_test_metrics.items()}\n\n    display(pd.DataFrame([xgbc_test_metrics], index=[&quot;test&quot;]))\n<\/code><\/pre>\n<p>After which I get much better results for precision, recall, f1_score, and log_loss:<\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th><\/th>\n<th>accuracy<\/th>\n<th>f1_score<\/th>\n<th>precision<\/th>\n<th>recall<\/th>\n<th>log_loss<\/th>\n<th>roc_auc<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>test<\/td>\n<td>0.865784<\/td>\n<td>0.83134<\/td>\n<td>0.85255<\/td>\n<td>0.86578<\/td>\n<td>0.36501<\/td>\n<td>0.738647<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>While accuracy and ROC AUC remain unchanged.<\/p>\n<p>What is causing this difference in performance? Is Mlflow performing any sort of tuning\/optimization under the hood? Is the experiment run actually fitting multiple models other than the <code>xgb_model<\/code> I instantiated? Let me know if I need to provide more code.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_created_time":1655997509287,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":65.0,
        "Poster_created_time":1623786908787,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72732635",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"differ perform run manual train work databrick try train xgbclassifi model sklearn pipelin manual code run train set train valid set val test set test run model test set perform metric method differ metric better code manual fit test preprocessor standard scaler hot encod model pipelin column selector col selector preprocessor preprocessor classifi xgb model model fit train train classifi earli stop round classifi eval set val process val classifi verbos fals pred model predict test prob model predict proba test sklearn metric import accuraci score score precis score recal score roc auc score log loss test acc accuraci score test pred test score test pred test prec precis score test pred test rec recal score test pred test logloss log loss test pred test rocauc roc auc score test prob metric col accuraci score precis recal log loss roc auc datafram test acc test test prec test rec test logloss test rocauc column metric col index test get follow accuraci score precis recal log loss roc auc test code run import import databrick automl runtim sklearn autolog disabl true sklearn autolog log input exampl true silent true start run experi run xgb mod run run model fit train train classifi earli stop round classifi eval set val process val classifi verbos fals xgbc test metric sklearn eval log metric model test test prefix test xgbc test metric replac test xgbc test metric item displai datafram xgbc test metric index test better result precis recal score log loss accuraci score precis recal log loss roc auc test accuraci roc auc remain unchang caus differ perform perform sort tune optim hood experi run actual fit multipl model xgb model instanti let know need provid code",
        "Question_preprocessed_content":"differ perform run manual train work databrick try train model sklearn manual code run train set valid set test set run model test set perform metric method differ metric better code manual fit test get follow accuraci precis recal test code run better result precis recal accuraci precis recal test accuraci roc auc remain unchang caus differ perform perform sort hood experi run actual fit multipl model instanti let know need provid code",
        "Question_gpt_summary_original":"The user is encountering a difference in performance between manual training and Mlflow run while training an XGBClassifier model in an sklearn Pipeline. The performance metrics for each method are different, with the Mlflow metrics being much better. The user is unsure about the cause of this difference and whether Mlflow is performing any tuning\/optimization under the hood or fitting multiple models other than the xgb_model instantiated.",
        "Question_gpt_summary":"user encount differ perform manual train run train xgbclassifi model sklearn pipelin perform metric method differ metric better user unsur caus differ perform tune optim hood fit multipl model xgb model instanti"
    },
    {
        "Question_title":"How can I invoke a SageMaker model, trained with TensorFlow, using a csv file in the body of the call?",
        "Question_body":"<p>I have deployed a TensorFlow model on AWS SageMaker, and I want to be able to invoke it using a csv file as the body of the call. The documentation says about creating a <code>serving_input_function<\/code> like the one below: <\/p>\n\n<pre><code>def serving_input_fn(hyperparameters):\n  # Logic to the following:\n  # 1. Defines placeholders that TensorFlow serving will feed with inference requests\n  # 2. Preprocess input data\n  # 3. Returns a tf.estimator.export.ServingInputReceiver or tf.estimator.export.TensorServingInputReceiver,\n  # which packages the placeholders and the resulting feature Tensors together.\n<\/code><\/pre>\n\n<p>In step 2, where it says preprocess input data, how do I get a handle on input data to process them?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1523450778890,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":1249.0,
        "Poster_created_time":1423231864696,
        "Poster_location":"Athens, Greece",
        "Poster_reputation_count":1109.0,
        "Poster_view_count":157.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49775557",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"invok model train tensorflow csv file bodi deploi tensorflow model want abl invok csv file bodi document sai creat serv input function like def serv input hyperparamet logic follow defin placehold tensorflow serv feed infer request preprocess input data return estim export servinginputreceiv estim export tensorservinginputreceiv packag placehold result featur tensor step sai preprocess input data handl input data process",
        "Question_preprocessed_content":"invok model train tensorflow csv file bodi deploi tensorflow model want abl invok csv file bodi document sai creat like step sai preprocess input data handl input data process",
        "Question_gpt_summary_original":"The user has deployed a TensorFlow model on AWS SageMaker and wants to invoke it using a csv file as the body of the call. They are facing challenges in understanding how to preprocess input data in the <code>serving_input_function<\/code> as they are unsure how to get a handle on the input data.",
        "Question_gpt_summary":"user deploi tensorflow model want invok csv file bodi face challeng understand preprocess input data serv input function unsur handl input data"
    },
    {
        "Question_title":"How do I verify that an interface VPC endpoint is functioning well?",
        "Question_body":"<p>I am pretty new to AWS, and here is my scenario:\nI create a sagemaker endpoint that can be called with AWS SDK IAmazonSageMakerRuntime::InvokeEndpointAsync() from a EC2 web server in a private subnet over public internet via NAT instance. <\/p>\n\n<pre><code>using (MemoryStream ms = new MemoryStream(Encoding.UTF8.GetBytes(mydata)))\n{\n   InvokeEndpointRequest req = new InvokeEndpointRequest()\n   {\n      EndpointName = \"MyEndpoint\",\n      ContentType = \"application\/json\",\n      Body = ms\n   };\n   var resp = await _sageMakerRuntime.InvokeEndpointAsync(req);\n}\n<\/code><\/pre>\n\n<p>And to further secure the connection and avoid traffic over public internet, I set up a interface VPC endpoint(the associated security group inbound rule allows the cidr ip range in my VPC with port 80\/443) for SageMakerRuntime service to take advantage of the benefits from using AWS PrivteConnection. <\/p>\n\n<pre><code>EndpointSageMakerSecurityGroup:\nType: AWS::EC2::SecurityGroup\nProperties:\n  GroupDescription: Enable access from app to interface VPC endpoint for sagemaker runtime\n  VpcId: !Ref VPC\n  SecurityGroupIngress:\n  - CidrIp: 10.50.0.0\/16\n    IpProtocol: tcp\n    ToPort: 80\n    FromPort: 80\n  - CidrIp: 10.50.0.0\/16\n    IpProtocol: tcp\n    ToPort: 443\n    FromPort: 443\n  Tags:\n  - Key: Name\n    Value: !Sub \"${AWS::StackName}-EndpointSageMakerSecurityGroup\"\n\nEndpointSageMakerRuntime:\n    Type: AWS::EC2::VPCEndpoint\n    Properties:\n      PolicyDocument:\n        Version: 2012-10-17\n        Statement:\n          - Effect: Allow\n            Principal: '*'\n            Action:\n              - sagemaker:InvokeEndpoint\n            Resource: !Sub arn:${AWS::Partition}:sagemaker:${AWS::Region}:${AWS::AccountId}:endpoint\/MyEndpoint      \n      SubnetIds: !Ref PrivateSubnets\n      SecurityGroupIds:\n      - !Ref EndpointSageMakerSecurityGroup\n      ServiceName: !Sub com.amazonaws.${AWS::Region}.sagemaker.runtime #runtime.sagemaker.${AWS::Region}.amazonaws.com\n      PrivateDnsEnabled: true\n      VpcEndpointType: Interface\n      VpcId: !Ref VPC\n<\/code><\/pre>\n\n<p>However, I really have no idea whether the interface VPC endpoint does take effect and my function call InvokeEndpointAsync() indeed calls to the interface VPC endpoint into AWS SageMakerRuntime service. I tried creating a notification in the interface VPC endpoint to publish message to a SNS topic and subscribing that SNS topic with my email, but I was not able to get any email from it. I guess there is something wrong but can't not figure out how to get the interface VPC endpoint right. Any ideas would be appreciated. <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":6,
        "Question_created_time":1591954808800,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1591955392603,
        "Question_score_count":1.0,
        "Question_view_count":261.0,
        "Poster_created_time":1375928870100,
        "Poster_location":null,
        "Poster_reputation_count":145.0,
        "Poster_view_count":39.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62341557",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"verifi interfac vpc endpoint function pretti new aw scenario creat endpoint call aw sdk iamazonruntim invokeendpointasync web server privat subnet public internet nat instanc memorystream new memorystream encod utf getbyt mydata invokeendpointrequest req new invokeendpointrequest endpointnam myendpoint contenttyp applic json bodi var resp await runtim invokeendpointasync req secur connect avoid traffic public internet set interfac vpc endpoint associ secur group inbound rule allow cidr rang vpc port runtim servic advantag benefit aw privteconnect endpointsecuritygroup type aw securitygroup properti groupdescript enabl access app interfac vpc endpoint runtim vpcid ref vpc securitygroupingress cidrip ipprotocol tcp toport fromport cidrip ipprotocol tcp toport fromport tag kei valu sub aw stacknam endpointsecuritygroup endpointruntim type aw vpcendpoint properti policydocu version statement effect allow princip action invokeendpoint resourc sub arn aw partit aw region aw accountid endpoint myendpoint subnetid ref privatesubnet securitygroupid ref endpointsecuritygroup servicenam sub com amazonaw aw region runtim runtim aw region amazonaw com privatednsen true vpcendpointtyp interfac vpcid ref vpc idea interfac vpc endpoint effect function invokeendpointasync call interfac vpc endpoint runtim servic tri creat notif interfac vpc endpoint publish messag sn topic subscrib sn topic email abl email guess wrong figur interfac vpc endpoint right idea appreci",
        "Question_preprocessed_content":"verifi interfac vpc endpoint function pretti new aw scenario creat endpoint call aw sdk iamazonruntim invokeendpointasync web server privat subnet public internet nat instanc secur connect avoid traffic public internet set interfac vpc endpoint runtim servic advantag benefit aw privteconnect idea interfac vpc endpoint effect function invokeendpointasync call interfac vpc endpoint runtim servic tri creat notif interfac vpc endpoint publish messag sn topic subscrib sn topic email abl email guess wrong figur interfac vpc endpoint right idea appreci",
        "Question_gpt_summary_original":"The user is new to AWS and has created a sagemaker endpoint that can be called with AWS SDK IAmazonSageMakerRuntime::InvokeEndpointAsync() from an EC2 web server in a private subnet over public internet via NAT instance. To secure the connection and avoid traffic over public internet, the user has set up an interface VPC endpoint for SageMakerRuntime service. However, the user is unsure whether the interface VPC endpoint is functioning well and how to verify it. The user has tried creating a notification in the interface VPC endpoint to publish a message to an SNS topic and subscribing to that SNS topic with their email, but they have not received any email from it.",
        "Question_gpt_summary":"user new aw creat endpoint call aw sdk iamazonruntim invokeendpointasync web server privat subnet public internet nat instanc secur connect avoid traffic public internet user set interfac vpc endpoint runtim servic user unsur interfac vpc endpoint function verifi user tri creat notif interfac vpc endpoint publish messag sn topic subscrib sn topic email receiv email"
    },
    {
        "Question_title":"MLflow remote tracking",
        "Question_body":"Hi folks,\n\n\nI am using mlflow 'version 1.1.0' in a model written in python for tracking purpose. I have created a project for the same with MLproject file inside it. I want to store the tracking details in a remote server and hence I am making use of set_tracking_uri() function. Below is the sample code;\n\n\nset_tracking_uri('172.16.0.80:5000')\n\ntry:\n\u00a0 \u00a0 \u00a0 \u00a0 create_experiment(\"SampleExp\")\nexcept:\n\u00a0 \u00a0 \u00a0 \u00a0 set_experiment(\"SampleExp\")\nset_tag(\"Script\", \"SampleExp1\")\n\n\nThe mlflow server is running in the remote server. When I execute the model with 'mlflow run' command, only the experiment name gets created in the remote server path but mlruns directory and other tracking data gets stored in the path from where mlflow run command is getting executed locally. Kindly help me in resolving this issue.",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1579651256000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":null,
        "Question_view_count":37.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/9wmaxy3no30",
        "Tool":"MLflow",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"remot track folk version model written python track purpos creat project mlproject file insid want store track detail remot server make us set track uri function sampl code set track uri try creat experi sampleexp set experi sampleexp set tag script sampleexp server run remot server execut model run command experi get creat remot server path mlrun directori track data get store path run command get execut local kindli help resolv issu",
        "Question_preprocessed_content":"remot track folk version model written python track purpos creat project mlproject file insid want store track detail remot server make us function sampl code try sampleexp server run remot server execut model run command experi get creat remot server path mlrun directori track data get store path run command get execut local kindli help resolv issu",
        "Question_gpt_summary_original":"The user is facing an issue with MLflow remote tracking while using version 1.1.0. They have created a project with an MLproject file and are trying to store tracking details in a remote server using the set_tracking_uri() function. However, when executing the model with 'mlflow run' command, only the experiment name gets created in the remote server path, and the mlruns directory and other tracking data get stored in the path from where mlflow run command is getting executed locally.",
        "Question_gpt_summary":"user face issu remot track version creat project mlproject file try store track detail remot server set track uri function execut model run command experi get creat remot server path mlrun directori track data store path run command get execut local"
    },
    {
        "Question_title":"How to get list of services in Azure ML?",
        "Question_body":"<p>Is there an API to receive list of published webservices?<\/p>\n<p>I have workspace id and auth token, I can get list of projects and experiments, but I can't get list of services created from experiments. Specifically I need the URL in order to post requests.\n<a href=\"https:\/\/i.stack.imgur.com\/6YHP1.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6YHP1.png\" alt=\"\" \/><\/a><br \/>\n<sub>(source: <a href=\"https:\/\/msdnshared.blob.core.windows.net\/media\/TNBlogsFS\/prod.evol.blogs.technet.com\/CommunityServer.Blogs.Components.WeblogFiles\/00\/00\/01\/02\/52\/JupRay-2.png\" rel=\"nofollow noreferrer\">windows.net<\/a>)<\/sub><\/p>\n<p>In client api I see if we publish a new service we can get it, but do we have more options?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1462267670620,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1639596690136,
        "Question_score_count":0.0,
        "Question_view_count":42.0,
        "Poster_created_time":1369681858647,
        "Poster_location":null,
        "Poster_reputation_count":464.0,
        "Poster_view_count":34.0,
        "Answer_body":"<p>The R Azure ML API has that. Excerpt from <a href=\"https:\/\/htmlpreview.github.io\/?https:\/\/github.com\/RevolutionAnalytics\/AzureML\/blob\/master\/vignettes\/getting_started.html\" rel=\"nofollow\">https:\/\/htmlpreview.github.io\/?https:\/\/github.com\/RevolutionAnalytics\/AzureML\/blob\/master\/vignettes\/getting_started.html<\/a> : <\/p>\n\n<p><code>\n(webservices &lt;- services(ws, name = \"AzureML-vignette-silly\"))\n<\/code><\/p>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1462276416390,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1298044626600,
        "Answerer_location":null,
        "Answerer_reputation_count":1476.0,
        "Answerer_view_count":105.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37000397",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"list servic api receiv list publish webservic workspac auth token list project experi list servic creat experi specif need url order post request sourc window net client api publish new servic option",
        "Question_preprocessed_content":"list servic api receiv list publish webservic workspac auth token list project experi list servic creat experi specif need url order post request sourc client api publish new servic option",
        "Question_gpt_summary_original":"The user is looking for an API to receive a list of published webservices in Azure ML, as they have the workspace id and auth token but cannot get the list of services created from experiments. They specifically need the URL to post requests and are wondering if there are more options available besides publishing a new service.",
        "Question_gpt_summary":"user look api receiv list publish webservic workspac auth token list servic creat experi specif need url post request wonder option avail publish new servic"
    },
    {
        "Question_title":"Is there a way to list out my datastores if I've deployed to a VNET?",
        "Question_body":"<p>I followed the instructions in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-enable-virtual-network\" rel=\"nofollow noreferrer\">MSFT Docs<\/a>, but now I can't list our my Datastores either via the SDK nor the Azure Machine Learning studio. <\/p>\n\n<p>Instead, in the studio I see this:\n<a href=\"https:\/\/i.stack.imgur.com\/im2oe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/im2oe.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Is there a way to make this work? Did I miss a step?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1573145127473,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":164.0,
        "Poster_created_time":1529439461716,
        "Poster_location":null,
        "Poster_reputation_count":392.0,
        "Poster_view_count":39.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58753430",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"wai list datastor deploi vnet follow instruct msft doc list datastor sdk studio instead studio wai work miss step",
        "Question_preprocessed_content":"wai list datastor deploi vnet follow instruct msft doc list datastor sdk studio instead studio wai work miss step",
        "Question_gpt_summary_original":"The user followed the instructions in the MSFT Docs to deploy to a VNET, but is now unable to list out their Datastores using the SDK or Azure Machine Learning studio. They are seeking assistance to make this work and wondering if they missed a step.",
        "Question_gpt_summary":"user follow instruct msft doc deploi vnet unabl list datastor sdk studio seek assist work wonder miss step"
    },
    {
        "Question_title":"ValueError: Value out of range: 15263439326086220952",
        "Question_body":"<p>I am trying sweep in a remote server,whose system is Centos 7.6.<br>\nAfter the first run ,it occered  an  ERROR as folow:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/6a868c651662464dfba15f99745bf91fd688ff00.png\" data-download-href=\"\/uploads\/short-url\/fcmSAJb5vDbYn8vgBpfwfFB8mZO.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/6a868c651662464dfba15f99745bf91fd688ff00.png\" alt=\"image\" data-base62-sha1=\"fcmSAJb5vDbYn8vgBpfwfFB8mZO\" width=\"690\" height=\"265\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/6a868c651662464dfba15f99745bf91fd688ff00_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1134\u00d7436 19.9 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nAnd  the code is ok to run in my conputer.<br>\nwhen i remove all the sweep code,the code is able ro run successfully in the remote server too.<br>\nIs this error attributed to my remote server?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1657289339847,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":140.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/valueerror-value-out-of-range-15263439326086220952\/2729",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"valueerror valu rang try sweep remot server cento run occer error folow imag code run conput remov sweep code code abl run successfulli remot server error attribut remot server",
        "Question_preprocessed_content":"valueerror valu rang try sweep remot server cento run occer error folow imag code run conput remov sweep code code abl run successfulli remot server error attribut remot server",
        "Question_gpt_summary_original":"The user is encountering a ValueError while trying to sweep in a remote server running on Centos 7.6. The error message indicates that the value is out of range. The code runs successfully on the user's computer, but not on the remote server. The user is unsure if the error is due to the remote server.",
        "Question_gpt_summary":"user encount valueerror try sweep remot server run cento error messag indic valu rang code run successfulli user remot server user unsur error remot server"
    },
    {
        "Question_title":"Uploading xgboost to azure machine learning: %1 is not a valid Win32 application\\r\\nProcess returned with non-zero exit code 1",
        "Question_body":"<p>I've tried to upload the xgboost python library to Azure ML, however it claim that my library is not a Win32 application.\nI've made sure to install the 32 bit version of the package and i'm running conda 32 bit as well.\nI downloaded the library from:\n<a href=\"http:\/\/www.lfd.uci.edu\/~gohlke\/pythonlibs\/#xgboost\" rel=\"nofollow noreferrer\">http:\/\/www.lfd.uci.edu\/~gohlke\/pythonlibs\/#xgboost<\/a>\nand chose the 32 bit python 3.5 version.\nPython installation as below.\n<img src=\"https:\/\/i.stack.imgur.com\/NkKRk.jpg\" alt=\"python installation\"><\/p>\n\n<p>This is the error I get returned azure ml error\n<img src=\"https:\/\/i.stack.imgur.com\/V9M1f.jpg\" alt=\"azure ml error\"><\/p>\n\n<p>Here is my installation of anaconda conda installation\n<img src=\"https:\/\/i.stack.imgur.com\/MsLyg.jpg\" alt=\"conda installation\"><\/p>\n\n<p>Can anyone see where I went wrong? <\/p>\n\n<p>Best Regards<\/p>\n\n<p>EDIT:\nYes I followed the document and uploaded a zip file containing the wheel file. When I run the following it works just fine:\n\"import pip\" and   \"pip.main(['install', '.\/Script Bundle\/xgboost-0.6-cp35-cp35m-win32.whl'])\"\nBut when I add \"import xgboost\" I get this error.<a href=\"https:\/\/i.stack.imgur.com\/coYnH.jpg\" rel=\"nofollow noreferrer\">Import error<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1496231820517,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1496308899830,
        "Question_score_count":0.0,
        "Question_view_count":650.0,
        "Poster_created_time":1405417345768,
        "Poster_location":"Copenhagen, Denmark",
        "Poster_reputation_count":9.0,
        "Poster_view_count":7.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44284183",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"upload xgboost valid win applic nprocess return non zero exit code tri upload xgboost python librari claim librari win applic sure instal bit version packag run conda bit download librari http lfd uci edu gohlk pythonlib xgboost chose bit python version python instal error return error instal anaconda conda instal went wrong best regard edit ye follow document upload zip file contain wheel file run follow work fine import pip pip main instal script bundl xgboost cpm win whl add import xgboost error import error",
        "Question_preprocessed_content":"upload xgboost valid win return exit code tri upload xgboost python librari claim librari win applic sure instal bit version packag run conda bit download librari chose bit python version python instal error return error instal anaconda conda instal went wrong best regard edit ye follow document upload zip file contain wheel file run follow work fine import pip add import xgboost error",
        "Question_gpt_summary_original":"The user is facing an issue while uploading the xgboost python library to Azure ML, as it is not being recognized as a valid Win32 application. The user has installed the 32-bit version of the package and is running conda 32-bit as well. The user has followed the document and uploaded a zip file containing the wheel file, which works fine with \"import pip\" and \"pip.main(['install', '.\/Script Bundle\/xgboost-0.6-cp35-cp35m-win32.whl'])\", but when \"import xgboost\" is added, an import error occurs.",
        "Question_gpt_summary":"user face issu upload xgboost python librari recogn valid win applic user instal bit version packag run conda bit user follow document upload zip file contain wheel file work fine import pip pip main instal script bundl xgboost cpm win whl import xgboost ad import error occur"
    },
    {
        "Question_title":"Google TTS keeps having a high-pitched, robotic voice crack intermittently",
        "Question_body":"Google TTS keeps having a high-pitched, robotic voice crack intermittently, regardless of the voice model I use. Why is that happening?\n\nIs there no way to resolve this issue?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1683505320000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":33.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-TTS-keeps-having-a-high-pitched-robotic-voice-crack\/td-p\/551205\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"googl tt keep have high pitch robot voic crack intermitt googl tt keep have high pitch robot voic crack intermitt regardless voic model us happen wai resolv issu",
        "Question_preprocessed_content":"googl tt keep have robot voic crack intermitt googl tt keep have robot voic crack intermitt regardless voic model us happen wai resolv issu",
        "Question_gpt_summary_original":"The user is experiencing a problem with Google TTS where the voice crack intermittently and sounds high-pitched and robotic, regardless of the voice model used. The user is seeking a solution to this issue.",
        "Question_gpt_summary":"user experienc problem googl tt voic crack intermitt sound high pitch robot regardless voic model user seek solut issu"
    },
    {
        "Question_title":"optuna.integration.lightGBM custom optimization metric",
        "Question_body":"<p>I am trying to optimize a lightGBM model using optuna.<\/p>\n<p>Reading the docs I noticed that there are two approaches that can be used, as mentioned here: <a href=\"https:\/\/medium.com\/optuna\/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258\" rel=\"nofollow noreferrer\">LightGBM Tuner: New Optuna Integration for Hyperparameter Optimization<\/a>.<\/p>\n<p>The first approach uses the &quot;standard&quot; way of optimizing with optuna (objective function + trials), the second one wrappes everything together with the <code>.train()<\/code> function. The first one basically tries combination of hyper-parameters values, while the second one optimizes following a step-wise approach on the hyperparameters.<\/p>\n<p>The two approaches are showed in the following code examples in the optuna github repository:<\/p>\n<ol>\n<li><a href=\"https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/lightgbm\/lightgbm_simple.py\" rel=\"nofollow noreferrer\">First approach<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/lightgbm\/lightgbm_tuner_simple.py\" rel=\"nofollow noreferrer\">Second approach<\/a><\/li>\n<\/ol>\n<p>Both codes perform the exact same optimization on the same parameters (optimized parameters by the second approach are decribed <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.integration.lightgbm.LightGBMTuner.html\" rel=\"nofollow noreferrer\">here<\/a>), but in different ways (combinatorial vs step-wise).<\/p>\n<p><strong>My questions is:<\/strong><\/p>\n<ol>\n<li><p>Is it possible in the second approach to specify a custom evaluation metric? In the first one I can easily change the accuracy used inside the github examples with any custom metric.<br \/>\nAs an example I could write:<\/p>\n<pre><code> import lightgbm as lgb\n import numpy as np\n import sklearn.datasets\n import sklearn.metrics\n from sklearn.model_selection import train_test_split\n\n import optuna\n\n def my_eval_metric(valid_y, pred_labels):\n     # my custom metric\n     ..........\n     ..........\n\n     return my_metric\n\n def objective(trial):\n     data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n     train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n     dtrain = lgb.Dataset(train_x, label=train_y)\n\n     param = {\n         &quot;objective&quot;: &quot;binary&quot;,\n         &quot;metric&quot;: &quot;binary_logloss&quot;,\n         &quot;verbosity&quot;: -1,\n         &quot;boosting_type&quot;: &quot;gbdt&quot;,\n         &quot;lambda_l1&quot;: trial.suggest_float(&quot;lambda_l1&quot;, 1e-8, 10.0, log=True),\n         &quot;lambda_l2&quot;: trial.suggest_float(&quot;lambda_l2&quot;, 1e-8, 10.0, log=True),\n         &quot;num_leaves&quot;: trial.suggest_int(&quot;num_leaves&quot;, 2, 256),\n         &quot;feature_fraction&quot;: trial.suggest_float(&quot;feature_fraction&quot;, 0.4, 1.0),\n         &quot;bagging_fraction&quot;: trial.suggest_float(&quot;bagging_fraction&quot;, 0.4, 1.0),\n         &quot;bagging_freq&quot;: trial.suggest_int(&quot;bagging_freq&quot;, 1, 7),\n         &quot;min_child_samples&quot;: trial.suggest_int(&quot;min_child_samples&quot;, 5, 100),\n     }\n\n     gbm = lgb.train(param, dtrain)\n     preds = gbm.predict(valid_x)\n     pred_labels = np.rint(preds)\n     my_eval_metric_value = my_eval_metric(valid_y, pred_labels)\n     return custom_metric_value \n\n\n if __name__ == &quot;__main__&quot;:\n     study = optuna.create_study(direction=&quot;maximize&quot;)\n     study.optimize(objective, n_trials=100)\n\n     print(&quot;Number of finished trials: {}&quot;.format(len(study.trials)))\n\n     print(&quot;Best trial:&quot;)\n     trial = study.best_trial\n\n     print(&quot;  Value: {}&quot;.format(trial.value))\n\n     print(&quot;  Params: &quot;)\n     for key, value in trial.params.items():\n         print(&quot;    {}: {}&quot;.format(key, value))\n<\/code><\/pre>\n<\/li>\n<\/ol>\n<p>This code will return the parameters of the lightGBM model that maximizes my custom metric. However in the second approach I haven't been able to specify my own custom metric.<\/p>\n<p><strong>UPDATE:<\/strong>\nI managed to define my own custom metric and its usage inside the second approach. A minimal reproducible code is the following (just pass the data using <code>train_test_split<\/code> by scikit):<\/p>\n<pre><code>from sklearn.metrics import average_precision_score\nimport optuna.integration.lightgbm as lgb_sequential\n\ndef tune_lightGBM_sequential(X_train, X_val, y_train, y_val):\n    \n    def calculate_ctr(gt):\n        positive = len([x for x in gt if x == 1])\n        ctr = positive\/float(len(gt))\n        return ctr\n\n    def compute_rce(preds, train_data):\n        gt = train_data.get_label()\n        cross_entropy = log_loss(gt, preds)\n        data_ctr = calculate_ctr(gt)\n        strawman_cross_entropy = log_loss(gt, [data_ctr for _ in range(len(gt))])\n        rce = (1.0 - cross_entropy\/strawman_cross_entropy)*100.0\n        return ('rce', rce, True)\n\n    def compute_avg_precision(preds, train_data):\n        gt = train_data.get_label()\n        avg_precision= average_precision_score(gt, preds)\n        return('avg_precision', avg_precision, True)\n    \n    params = {\n        &quot;objective&quot;: &quot;binary&quot;,\n        &quot;metric&quot;: 'custom',\n        &quot;boosting_type&quot;: &quot;gbdt&quot;,\n        &quot;verbose&quot; : 2\n    }\n    \n    dtrain = lgb_sequential.Dataset(X_train, label=y_train)\n    dval = lgb_sequential.Dataset(X_val, label=y_val)\n    \n    print('Starting training lightGBM sequential')\n    model = lgb_sequential.train(\n        params, dtrain, valid_sets=[dtrain, dval], verbose_eval=True,num_boost_round =2, early_stopping_rounds=100, feval = [compute_rce, compute_avg_precision]\n    )\n    \n    return model.params\n<\/code><\/pre>\n<p>However Optuna doesn't seem to be able to select the best trial based on my custom metrics, in fact, I get the following error:<\/p>\n<blockquote>\n<p>[W 2021-05-16 15:56:48,759] Trial 0 failed because of the following\nerror: KeyError('custom') Traceback (most recent call last): File\n&quot;C:\\Users\\Mattia\\anaconda3\\envs\\rec_sys_challenge\\lib\\site-packages\\optuna_optimize.py&quot;,\nline 217, in _run_trial value_or_values = func(trial) File\n&quot;C:\\Users\\Mattia\\anaconda3\\envs\\rec_sys_challenge\\lib\\site-packages\\optuna\\integration_lightgbm_tuner\\optimize.py&quot;,\nline 251, in call val_score = self._get_booster_best_score(booster)\nFile\n&quot;C:\\Users\\Mattia\\anaconda3\\envs\\rec_sys_challenge\\lib\\site-packages\\optuna\\integration_lightgbm_tuner\\optimize.py&quot;,\nline 118, in _get_booster_best_score val_score =\nbooster.best_score[valid_name][metric] KeyError: 'custom'<\/p>\n<\/blockquote>\n<p>It seems a issue with the library (you can find more here: <a href=\"https:\/\/github.com\/optuna\/optuna\/issues\/1351\" rel=\"nofollow noreferrer\">GitHub Issue<\/a>), I tried many proposed solutions, but none of them worked.<\/p>\n<p>Any help?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_created_time":1621084691557,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1621176110623,
        "Question_score_count":0.0,
        "Question_view_count":1217.0,
        "Poster_created_time":1522782849496,
        "Poster_location":"Milano, MI, Italia",
        "Poster_reputation_count":1144.0,
        "Poster_view_count":305.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67547127",
        "Tool":"Optuna",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"integr lightgbm custom optim metric try optim lightgbm model read doc notic approach mention lightgbm tuner new integr hyperparamet optim approach us standard wai optim object function trial second wrapp train function basic tri combin hyper paramet valu second optim follow step wise approach hyperparamet approach show follow code exampl github repositori approach second approach code perform exact optim paramet optim paramet second approach decrib differ wai combinatori step wise question possibl second approach specifi custom evalu metric easili chang accuraci insid github exampl custom metric exampl write import lightgbm lgb import numpi import sklearn dataset import sklearn metric sklearn model select import train test split import def eval metric valid pred label custom metric return metric def object trial data target sklearn dataset load breast cancer return true train valid train valid train test split data target test size dtrain lgb dataset train label train param object binari metric binari logloss verbos boost type gbdt lambda trial suggest float lambda log true lambda trial suggest float lambda log true num leav trial suggest int num leav featur fraction trial suggest float featur fraction bag fraction trial suggest float bag fraction bag freq trial suggest int bag freq min child sampl trial suggest int min child sampl gbm lgb train param dtrain pred gbm predict valid pred label rint pred eval metric valu eval metric valid pred label return custom metric valu main studi creat studi direct maxim studi optim object trial print number finish trial format len studi trial print best trial trial studi best trial print valu format trial valu print param kei valu trial param item print format kei valu code return paramet lightgbm model maxim custom metric second approach haven abl specifi custom metric updat manag defin custom metric usag insid second approach minim reproduc code follow pass data train test split scikit sklearn metric import averag precis score import integr lightgbm lgb sequenti def tune lightgbm sequenti train val train val def calcul ctr posit len ctr posit float len return ctr def comput rce pred train data train data label cross entropi log loss pred data ctr calcul ctr strawman cross entropi log loss data ctr rang len rce cross entropi strawman cross entropi return rce rce true def comput avg precis pred train data train data label avg precis averag precis score pred return avg precis avg precis true param object binari metric custom boost type gbdt verbos dtrain lgb sequenti dataset train label train dval lgb sequenti dataset val label val print start train lightgbm sequenti model lgb sequenti train param dtrain valid set dtrain dval verbos eval true num boost round earli stop round feval comput rce comput avg precis return model param abl select best trial base custom metric fact follow error trial fail follow error keyerror custom traceback recent file user mattia anaconda env rec sy challeng lib site packag optim line run trial valu valu func trial file user mattia anaconda env rec sy challeng lib site packag integr lightgbm tuner optim line val score self booster best score booster file user mattia anaconda env rec sy challeng lib site packag integr lightgbm tuner optim line booster best score val score booster best score valid metric keyerror custom issu librari github issu tri propos solut work help",
        "Question_preprocessed_content":"custom optim metric try optim lightgbm model read doc notic approach mention lightgbm tuner new integr hyperparamet optim approach us standard wai optim second wrapp function basic tri combin valu second optim follow approach hyperparamet approach show follow code exampl github repositori approach second approach code perform exact optim paramet differ wai question possibl second approach specifi custom evalu metric easili chang accuraci insid github exampl custom metric exampl write code return paramet lightgbm model maxim custom metric second approach haven abl specifi custom metric updat manag defin custom metric usag insid second approach minim reproduc code follow abl select best trial base custom metric fact follow error trial fail follow error keyerror traceback file line func file line file line keyerror custom issu librari tri propos solut work help",
        "Question_gpt_summary_original":"The user is trying to optimize a lightGBM model using optuna and is exploring two approaches mentioned in the documentation. The first approach involves optimizing with optuna's objective function and trials, while the second approach wraps everything together with the .train() function. The user is trying to specify a custom evaluation metric in the second approach but is encountering an error. The user has tried various solutions proposed in the GitHub issue but none of them have worked.",
        "Question_gpt_summary":"user try optim lightgbm model explor approach mention document approach involv optim object function trial second approach wrap train function user try specifi custom evalu metric second approach encount error user tri solut propos github issu work"
    },
    {
        "Question_title":"__main__ has no attribute __spec__ pytorch-lightning multiGPU",
        "Question_body":"<p>When using guild to run experiments on multiple GPUs with pytorch-lightning I get the following error:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/main.py\", line 219, in &lt;module&gt;\n10\/21\/2022 5:54:52 PM\tmain()\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/main.py\", line 95, in main\n10\/21\/2022 5:54:52 PM\timputed_data = get_imputation_logic(args)(args, data)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/task_logic\/ae_imputation.py\", line 119, in ae_imputation_logic\n10\/21\/2022 5:54:52 PM\tae_imputer = create_autoencoder(args, data, settings)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/task_logic\/tuner.py\", line 69, in create_autoencoder\n10\/21\/2022 5:54:52 PM\tae_imputer.fit(data)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/models\/ap.py\", line 148, in fit\n10\/21\/2022 5:54:52 PM\tself._fit(data)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/models\/ap.py\", line 166, in _fit\n10\/21\/2022 5:54:52 PM\tself.trainer.fit(self.ae, datamodule=data)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 770, in fit\n10\/21\/2022 5:54:52 PM\tself._call_and_handle_interrupt(\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 721, in _call_and_handle_interrupt\n10\/21\/2022 5:54:52 PM\treturn self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/strategies\/launchers\/subprocess_script.py\", line 92, in launch\n10\/21\/2022 5:54:52 PM\tself._call_children_scripts()\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/strategies\/launchers\/subprocess_script.py\", line 109, in _call_children_scripts\n10\/21\/2022 5:54:52 PM\tif __main__.__spec__ is None: # pragma: no-cover\n10\/21\/2022 5:54:52 PM\tAttributeError: 'dict' object has no attribute '__spec__'\n<\/code><\/pre>\n<p>So I went into the file <code>\/home\/davina\/mambaforge\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/strategies\/launchers\/subprocess_script.py\", line 109, in _call_children_scripts<\/code> and changed <code>if __main__.__spec__ is None<\/code> to <code>if True<\/code> as a stopgap, and it seemed to work. For some reason <code>__main__<\/code> is a dictionary.<\/p>\n<p>This isn\u2019t the exact same problem, but I happened to find a somewhat relevant problem <a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/3859\" rel=\"noopener nofollow ugc\">here<\/a>.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_created_time":1666401932704,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":222.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/main-has-no-attribute-spec-pytorch-lightning-multigpu\/946",
        "Tool":"Guild AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"main attribut spec pytorch lightn multigpu run experi multipl gpu pytorch lightn follow error file home davina mambaforg env run dccbbdbedbd sourcecod autopopulu main line main file home davina mambaforg env run dccbbdbedbd sourcecod autopopulu main line main imput data imput logic arg arg data file home davina mambaforg env run dccbbdbedbd sourcecod autopopulu task logic imput line imput logic imput creat autoencod arg data set file home davina mambaforg env run dccbbdbedbd sourcecod autopopulu task logic tuner line creat autoencod imput fit data file home davina mambaforg env run dccbbdbedbd sourcecod autopopulu model line fit self fit data file home davina mambaforg env run dccbbdbedbd sourcecod autopopulu model line fit self trainer fit self datamodul data file home davina mambaforg env lib python site packag pytorch lightn trainer trainer line fit self handl interrupt file home davina mambaforg env lib python site packag pytorch lightn trainer trainer line handl interrupt return self strategi launcher launch trainer arg trainer self kwarg file home davina mambaforg env lib python site packag pytorch lightn strategi launcher subprocess script line launch self children script file home davina mambaforg env lib python site packag pytorch lightn strategi launcher subprocess script line children script main spec pragma cover attributeerror dict object attribut spec went file home davina mambaforg env lib python site packag pytorch lightn strategi launcher subprocess script line children script chang main spec true stopgap work reason main dictionari isnt exact problem happen somewhat relev problem",
        "Question_preprocessed_content":"attribut multigpu run experi multipl gpu follow error went file chang stopgap work reason dictionari isnt exact problem happen somewhat relev problem",
        "Question_gpt_summary_original":"The user encountered an error while using guild to run experiments on multiple GPUs with pytorch-lightning. The error occurred in the subprocess_script.py file and was caused by the __main__ attribute being a dictionary instead of a module. The user made a temporary fix by changing the code to \"if True\".",
        "Question_gpt_summary":"user encount error run experi multipl gpu pytorch lightn error occur subprocess script file caus main attribut dictionari instead modul user temporari fix chang code true"
    },
    {
        "Question_title":"Machine Learning\/Performance issue with Writing to CSV file - C#",
        "Question_body":"<p>Hi,  <\/p>\n<p>I developed a Machine Learning (ML.NET) -- .NET Core console application (using Multiple classes prediction template).  I've created the ML Model class and I'm now applying the ML Model to another .NET Core console application to predict the class\/type of stream bed.  This app is residing on my desktop, not Azure.  <\/p>\n<p>The ML app reads each row from the CSV input data and predicts the type of stream bed.  As it makes the prediction row by row, I store each row prediction in a StringBuilder.  When all rows have been read, I call the File.WriteAllText() function.  <\/p>\n<p>The Machine Learning console app is working fine, but now, my issue is how do I improve performance?  When I use CSV input file for my app that consists of over 100K rows, the app runs very slowly  (it writes the result to a CSV file at the rate of 1,000+ rows per hour!).  All my data are over 100K rows each, and I need to process about 50 separate CSV files.  <\/p>\n<p>Is there a better way of doing this?  Should I read\/write each row first, instead of storing all the prediction rows to a StringBuilder before writing to CSV file?  <\/p>\n<p>Or, is there a faster way of writing the results to a CSV file?    <\/p>\n<p>Appreciate any advice.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1590149443723,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/28770\/machine-learning-performance-issue-with-writing-to",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"machin learn perform issu write csv file develop machin learn net net core consol applic multipl class predict templat creat model class appli model net core consol applic predict class type stream bed app resid desktop azur app read row csv input data predict type stream bed make predict row row store row predict stringbuild row read file writealltext function machin learn consol app work fine issu improv perform us csv input file app consist row app run slowli write result csv file rate row hour data row need process separ csv file better wai read write row instead store predict row stringbuild write csv file faster wai write result csv file appreci advic",
        "Question_preprocessed_content":"machin issu write csv file develop machin learn net core consol applic creat model class appli model net core consol applic predict stream bed app resid desktop azur app read row csv input data predict type stream bed make predict row row store row predict stringbuild row read function machin learn consol app work fine issu improv perform us csv input file app consist row app run slowli data row need process separ csv file better wai row instead store predict row stringbuild write csv file faster wai write result csv file appreci advic",
        "Question_gpt_summary_original":"The user has developed a Machine Learning (ML.NET) console application that reads each row from a CSV input data and predicts the type of stream bed. The application stores each row prediction in a StringBuilder and calls the File.WriteAllText() function to write the result to a CSV file. However, when the CSV input file consists of over 100K rows, the application runs very slowly. The user is seeking advice on how to improve the performance, such as reading\/writing each row first or finding a faster way of writing the results to a CSV file.",
        "Question_gpt_summary":"user develop machin learn net consol applic read row csv input data predict type stream bed applic store row predict stringbuild call file writealltext function write result csv file csv input file consist row applic run slowli user seek advic improv perform read write row find faster wai write result csv file"
    },
    {
        "Question_title":"ModuleNotFoundError: No module named 'pyspark.dbutils' while running multiple.py file\/notebook on job clusters in databricks",
        "Question_body":"<p>I am working in TravisCI, MlFlow and Databricks environment where .tavis.yml sits at git master branch and detects any change in <code>.py<\/code> file and whenever it gets updated, It will run mlflow command to run .py file in databricks environment. \nmy MLProject file looks as following:<\/p>\n\n<pre><code>name: mercury_cltv_lib\nconda_env: conda-env.yml\n\n\nentry_points:    \n  main:\n    command: \"python3 run-multiple-notebooks.py\"\n<\/code><\/pre>\n\n<p>Workflow is as following:\nTravisCI detects change in master branch-->triggers build which will run MLFlow command and it'll spin up a job cluster in databricks to run .py file from repo.<\/p>\n\n<p>It worked fine with one .py file but when I tried to run multiple notebook using dbutils, it is throwing <\/p>\n\n<pre><code>  File \"run-multiple-notebooks.py\", line 3, in &lt;module&gt;\n    from pyspark.dbutils import DBUtils\nModuleNotFoundError: No module named 'pyspark.dbutils'\n<\/code><\/pre>\n\n<p>Please find below the relevant code section from run-multiple-notebooks.py<\/p>\n\n<pre><code>  def get_spark_session():\n    from pyspark.sql import SparkSession\n    return SparkSession.builder.getOrCreate()\n\n  def get_dbutils(self, spark = None):\n    try:\n        if spark == None:\n            spark = spark\n\n        from pyspark.dbutils import DBUtils #error line\n        dbutils = DBUtils(spark) #error line\n    except ImportError:\n        import IPython\n        dbutils = IPython.get_ipython().user_ns[\"dbutils\"]\n    return dbutils\n\n  def submitNotebook(notebook):\n    print(\"Running notebook %s\" % notebook.path)\n    spark = get_spark_session()\n    dbutils = get_dbutils(spark)\n<\/code><\/pre>\n\n<p>I tried all the options and tried <\/p>\n\n<pre><code>https:\/\/stackoverflow.com\/questions\/61546680\/modulenotfounderror-no-module-named-pyspark-dbutils\n<\/code><\/pre>\n\n<p>as well. It is not working :(<\/p>\n\n<p>Can someone please suggest if there is fix for the above-mentioned error while running .py in job cluster. My code works fine inside databricks local notebook but running from outside using TravisCI and MLFlow isn't working which is must requirement for pipeline automation.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_created_time":1589840102237,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1589982476476,
        "Question_score_count":2.0,
        "Question_view_count":401.0,
        "Poster_created_time":1555347036127,
        "Poster_location":"Minnesota, USA",
        "Poster_reputation_count":352.0,
        "Poster_view_count":88.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61879913",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"modulenotfounderror modul name pyspark dbutil run multipl file notebook job cluster databrick work travisci databrick environ tavi yml sit git master branch detect chang file get updat run command run file databrick environ mlproject file look follow mercuri cltv lib conda env conda env yml entri point main command python run multipl notebook workflow follow travisci detect chang master branch trigger build run command spin job cluster databrick run file repo work fine file tri run multipl notebook dbutil throw file run multipl notebook line pyspark dbutil import dbutil modulenotfounderror modul name pyspark dbutil relev code section run multipl notebook def spark session pyspark sql import sparksess return sparksess builder getorcr def dbutil self spark try spark spark spark pyspark dbutil import dbutil error line dbutil dbutil spark error line importerror import ipython dbutil ipython ipython user dbutil return dbutil def submitnotebook notebook print run notebook notebook path spark spark session dbutil dbutil spark tri option tri http stackoverflow com question modulenotfounderror modul name pyspark dbutil work suggest fix mention error run job cluster code work fine insid databrick local notebook run outsid travisci isn work requir pipelin autom",
        "Question_preprocessed_content":"modulenotfounderror modul name run job cluster databrick work travisci databrick environ sit git master branch detect chang file get updat run command run file databrick environ mlproject file look follow workflow follow travisci detect chang master build run command spin job cluster databrick run file repo work fine file tri run multipl notebook dbutil throw relev code section tri option tri work suggest fix error run job cluster code work fine insid databrick local notebook run outsid travisci isn work requir pipelin autom",
        "Question_gpt_summary_original":"The user is encountering a \"ModuleNotFoundError: No module named 'pyspark.dbutils'\" error while trying to run multiple notebooks using dbutils in a Databricks environment. The error occurs when running a .py file from outside using TravisCI and MLFlow, but the code works fine inside Databricks local notebook. The user has tried various solutions but none have worked so far.",
        "Question_gpt_summary":"user encount modulenotfounderror modul name pyspark dbutil error try run multipl notebook dbutil databrick environ error occur run file outsid travisci code work fine insid databrick local notebook user tri solut work far"
    },
    {
        "Question_title":"Can I add OpenAPI specification to a webservice deployed with AzureML in AKS?",
        "Question_body":"<p>I'd like to deploy a machine learning service using AzureML on AKS. I also need to add some OpenAPI specification for it.    <\/p>\n<p>Features in <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python<\/a> are neat, but that of having API docs\/swagger for the webservice seems missing.    <\/p>\n<p>Having some documentation is useful especially if the model takes in input several features of different type.    <\/p>\n<p>To overcome this, I currently get models trained in AzureML and include them in Docker containers that use the python FastAPI library to build the API and OpenAPI\/Swagger specs, and those are deployed on some host.     <\/p>\n<p>Can I do something equivalent to this with AKS in AzureML instead? If so, how?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1600897231890,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p><a href=\"\/users\/na\/?userid=9ced4628-b03a-4169-99b4-e42b0955c045\">@Davide Fiocco  <\/a> The deployments of Azure ML provide a swagger specification URI that can be used directly. The documentation of this is available <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.akswebservice?view=azure-ml-py\">here<\/a>. You can print your <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service\">swagger_uri<\/a> of the web service and check if it confirms with the specifications you are creating currently.     <\/p>\n<p>If the above response helps, please accept the response as answer. Thanks!!    <\/p>\n",
        "Answer_comment_count":0.0,
        "Question_closed_time":1600930445547,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/105437\/can-i-add-openapi-specification-to-a-webservice-de",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"add openapi specif webservic deploi ak like deploi machin learn servic ak need add openapi specif featur http learn microsoft com azur machin learn deploi azur kubernet servic tab python neat have api doc swagger webservic miss have document us especi model take input featur differ type overcom current model train includ docker contain us python fastapi librari build api openapi swagger spec deploi host equival ak instead",
        "Question_preprocessed_content":"add openapi specif webservic deploi ak like deploi machin learn servic ak need add openapi specif featur neat have api webservic miss have document us especi model take input featur differ type overcom current model train includ docker contain us python fastapi librari build api spec deploi host equival ak instead",
        "Question_gpt_summary_original":"The user wants to deploy a machine learning service using AzureML on AKS and add OpenAPI specification for it. The user finds that the API docs\/swagger feature is missing in AzureML on AKS and currently uses Docker containers with FastAPI library to build the API and OpenAPI\/Swagger specs. The user is seeking guidance on how to add OpenAPI specification to a webservice deployed with AzureML in AKS.",
        "Question_gpt_summary":"user want deploi machin learn servic ak add openapi specif user find api doc swagger featur miss ak current us docker contain fastapi librari build api openapi swagger spec user seek guidanc add openapi specif webservic deploi ak"
    },
    {
        "Question_title":"Kedro-mlflow usage - when to use it from notebooks, and when from kedro pipeline?",
        "Question_body":"<p>I'm a bit confused - what is the common practice for kedro-mlflow usage? It's seems slightly uncomfortable to use it only from kedro pipelines, but kedro intention is fully reproducible research.<\/p>\n<p>At the same time rather rare tutorials on kedro-mlflow usage describe experiments creation from Jupiter notebooks, which seems natural, but then full reproducibility without the full pipeline is broken.<\/p>\n<p>Question - what are common patterns on kedro-mlflow usage, as well as subexperiments creation (in scope of CrossValidation or HyperOptimisation)? When kedro pipelines should be used, and when code (and which code) should be placed in notebooks?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_created_time":1621081977757,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":160.0,
        "Poster_created_time":1514895810992,
        "Poster_location":"Riga, Latvia",
        "Poster_reputation_count":11.0,
        "Poster_view_count":7.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67546769",
        "Tool":"Kedro",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"usag us notebook pipelin bit confus common practic usag slightli uncomfort us pipelin intent fulli reproduc research time rare tutori usag experi creation jupit notebook natur reproduc pipelin broken question common pattern usag subexperi creation scope crossvalid hyperoptimis pipelin code code place notebook",
        "Question_preprocessed_content":"usag us notebook pipelin bit confus common practic usag slightli uncomfort us pipelin intent fulli reproduc research time rare tutori usag experi creation jupit notebook natur reproduc pipelin broken question common pattern usag subexperi creation pipelin code place notebook",
        "Question_gpt_summary_original":"The user is confused about the common practice for using kedro-mlflow. They are unsure whether to use it only from kedro pipelines for fully reproducible research or to create experiments from Jupiter notebooks, which seems natural but may break full reproducibility without the full pipeline. The user is seeking advice on common patterns for kedro-mlflow usage and when to use kedro pipelines versus notebooks for subexperiments creation.",
        "Question_gpt_summary":"user confus common practic unsur us pipelin fulli reproduc research creat experi jupit notebook natur break reproduc pipelin user seek advic common pattern usag us pipelin versu notebook subexperi creation"
    },
    {
        "Question_title":"In Kedro, how to pick up intermediate dataset in a pipeline?",
        "Question_body":"<p>I'm working on my pipeline and testing it manually on jupyter notebook.<\/p>\n<p>Here is my situation.<\/p>\n<p>I wanted to pick up <code>example_train<\/code> and <code>example_valid<\/code> from it,  so I wrote like this.<\/p>\n<p><code>context.pipeline.to_outputs(&quot;example_train&quot;, &quot;example_valid&quot;)<\/code><\/p>\n<p>and passed this another pipeline into SequencialRunner, and I got them.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/LPNim.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LPNim.png\" alt=\"example of pipeline\" \/><\/a><\/p>\n<p>And I also wanted <code>total_steps<\/code> so I changed the line like this.<\/p>\n<p><code>context.pipeline.to_outputs(&quot;example_train&quot;, &quot;example_valid&quot;, &quot;total_steps&quot;)<\/code><\/p>\n<p>but, the result did not contain <code>exampe_train<\/code>.\nYes, I understood <code>example_train<\/code> wasn't the output of this modified pipeline, so it didn't contain.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/mrDM0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mrDM0.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Is there a way to pick up an intermediate dataset like this situation?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1598539659947,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":275.0,
        "Poster_created_time":1545209959320,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":7.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63618590",
        "Tool":"Kedro",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"pick intermedi dataset pipelin work pipelin test manual jupyt notebook situat want pick exampl train exampl valid wrote like context pipelin output exampl train exampl valid pass pipelin sequencialrunn got want total step chang line like context pipelin output exampl train exampl valid total step result contain examp train ye understood exampl train wasn output modifi pipelin contain wai pick intermedi dataset like situat",
        "Question_preprocessed_content":"pick intermedi dataset pipelin work pipelin test manual jupyt notebook situat want pick wrote like pass pipelin sequencialrunn got want chang line like result contain ye understood wasn output modifi pipelin contain wai pick intermedi dataset like situat",
        "Question_gpt_summary_original":"The user is facing a challenge in picking up an intermediate dataset in a pipeline while testing it manually on Jupyter Notebook in Kedro. They were able to pick up 'example_train' and 'example_valid' using 'context.pipeline.to_outputs', but when they added 'total_steps' to the line, 'example_train' was not included in the result. The user is seeking a solution to pick up an intermediate dataset in this situation.",
        "Question_gpt_summary":"user face challeng pick intermedi dataset pipelin test manual jupyt notebook abl pick exampl train exampl valid context pipelin output ad total step line exampl train includ result user seek solut pick intermedi dataset situat"
    },
    {
        "Question_title":"Why do I encounter \"INVALID_PARAMETER VALUE\" error when opening \"Models\" tab in MLFlow UI?",
        "Question_body":"<p>I installed mlflow via pip and opened it in the browser through the terminal. The tab Experiments displays normally; however, when I switch to Models tab, the app crashes due to the following error:<\/p>\n<pre><code>INVALID_PARAMETER_VALUE: Model registry functionality is unavailable; \ngot unsupported URI '.\/mlruns' for model registry data storage. \nSupported URI schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. \nSee https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#storage for how to run \nan MLflow server against one of the supported backend storage locations.\n<\/code><\/pre>\n<p>I would like to log model data locally without connecting to external servers or databases. I thank you in advance for any help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1659957616140,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1660218063030,
        "Question_score_count":1.0,
        "Question_view_count":39.0,
        "Poster_created_time":1652711404248,
        "Poster_location":null,
        "Poster_reputation_count":51.0,
        "Poster_view_count":4.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73277043",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"encount invalid paramet valu error open model tab instal pip open browser termin tab experi displai normal switch model tab app crash follow error invalid paramet valu model registri function unavail got unsupport uri mlrun model registri data storag support uri scheme postgresql mysql sqlite mssql http org doc latest track html storag run server support backend storag locat like log model data local connect extern server databas thank advanc help",
        "Question_preprocessed_content":"encount valu error open model tab instal pip open browser termin tab experi displai normal switch model tab app crash follow error like log model data local connect extern server databas thank advanc help",
        "Question_gpt_summary_original":"The user encounters an \"INVALID_PARAMETER_VALUE\" error when trying to open the \"Models\" tab in MLFlow UI. The error message suggests that the model registry functionality is unavailable due to an unsupported URI for model registry data storage. The user wants to log model data locally without connecting to external servers or databases.",
        "Question_gpt_summary":"user encount invalid paramet valu error try open model tab error messag suggest model registri function unavail unsupport uri model registri data storag user want log model data local connect extern server databas"
    },
    {
        "Question_title":"updating data in dvc registry from other projects",
        "Question_body":"<p>I have a couple of projects that are using and updating the same data sources. I recently learned about <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\" rel=\"nofollow noreferrer\">dvc's data registries<\/a>, which sound like a great way of versioning data across these different projects (e.g. scrapers, computational pipelines).<\/p>\n<p>I have put all of the relevant data into <code>data-registry<\/code> and then I imported the relevant files into the scraper project with:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ poetry run dvc import https:\/\/github.com\/username\/data-registry raw\n<\/code><\/pre>\n<p>where <code>raw<\/code> is a directory that stores the scraped data. This seems to have worked properly, but then when I went to build <a href=\"https:\/\/dvc.org\/doc\/start\/data-pipelines\" rel=\"nofollow noreferrer\">a dvc pipeline<\/a> that <em>outputted<\/em> data into a file that was already tracked by dvc, I got an error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc run -n menu_items -d src\/ -o raw\/menu_items\/restaurant.jsonl scrapy crawl restaurant\nERROR: Paths for outs:                                                \n'raw'('raw.dvc')\n'raw\/menu_items\/restaurant.jsonl'('menu_items')\noverlap. To avoid unpredictable behaviour, rerun command with non overlapping outs paths.\n<\/code><\/pre>\n<p>Can someone help me understand what is going on here? <strong>What is the best way to use data registries to share and update data across projects?<\/strong><\/p>\n<p>I would ideally like to update the data-registry with new data from the scraper project and then allow other dependent projects to update their data when they are ready to do so.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1614516713937,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1614699218992,
        "Question_score_count":3.0,
        "Question_view_count":388.0,
        "Poster_created_time":1294268936687,
        "Poster_location":"Chicago, IL",
        "Poster_reputation_count":2893.0,
        "Poster_view_count":168.0,
        "Answer_body":"<p>When you <code>import<\/code> (or <code>add<\/code>) something into your project, a .dvc file is created with that lists that something (in this case the <code>raw\/<\/code> dir) as an &quot;output&quot;.<\/p>\n<p>DVC doesn't allow overlapping outputs among .dvc files or dvc.yaml stages, meaning that your &quot;menu_items&quot; stage shouldn't write to <code>raw\/<\/code> since it's already under the control of <code>raw.dvc<\/code>.<\/p>\n<p>Can you make a separate directory for the pipeline outputs? E.g. use <code>processed\/menu_items\/restaurant.jsonl<\/code><\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1614537291720,
        "Answer_last_edit_time":1614698988012,
        "Answer_score_count":1.0,
        "Answerer_created_time":1305851487736,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66409283",
        "Tool":"DVC",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"updat data registri project coupl project updat data sourc recent learn data registri sound like great wai version data differ project scraper comput pipelin relev data data registri import relev file scraper project poetri run import http github com usernam data registri raw raw directori store scrape data work properli went build pipelin output data file track got error run menu item src raw menu item restaur jsonl scrapi crawl restaur error path out raw raw raw menu item restaur jsonl menu item overlap avoid unpredict behaviour rerun command non overlap out path help understand go best wai us data registri share updat data project ideal like updat data registri new data scraper project allow depend project updat data readi",
        "Question_preprocessed_content":"updat data registri project coupl project updat data sourc recent learn data registri sound like great wai version data differ project relev data import relev file scraper project directori store scrape data work properli went build pipelin output data file track got error help understand go best wai us data registri share updat data project ideal like updat new data scraper project allow depend project updat data readi",
        "Question_gpt_summary_original":"The user is trying to use dvc's data registries to version data across different projects. They imported relevant files into the scraper project, but when they tried to build a dvc pipeline that outputted data into a file already tracked by dvc, they received an error message. The user is seeking help to understand what is going on and the best way to use data registries to share and update data across projects.",
        "Question_gpt_summary":"user try us data registri version data differ project import relev file scraper project tri build pipelin output data file track receiv error messag user seek help understand go best wai us data registri share updat data project"
    }
]