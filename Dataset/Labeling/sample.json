[
    {
        "Question_id":56771758.0,
        "Question_title":"On-Premises Hosting of Trained DeepAR Model built on AWS SageMaker",
        "Question_body":"<p>I have started working with <strong>AWS SageMaker<\/strong> recently with the examples provided by AWS. I used this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/deepar_electricity\/DeepAR-Electricity.ipynb\" rel=\"nofollow noreferrer\">example<\/a> (<strong>DeepAR<\/strong> Model) in order to forecast a time series. After training, a model artifacts file has been created in my S3 bucket. <\/p>\n\n<p><strong>My question:<\/strong> Is there a way to host that trained model in a own hosting environment? (client premises)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1561548485583,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":379.0,
        "Answer_body":"<p>Except SageMaker XGBoost, SageMaker built-in algorithms are not designed to be used out of Amazon. That does not mean that it's impossible, for example you can find here and there snippets peeking inside model artifacts (eg for <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations\/\" rel=\"nofollow noreferrer\">Factorization Machines<\/a> and <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/ntm_20newsgroups_topic_modeling\/ntm_20newsgroups_topic_model.ipynb\" rel=\"nofollow noreferrer\">Neural Topic Model<\/a>) but these things can be hacky and are usually not part of official service features. Regarding DeepAR specifically, the model was open-sourced couple weeks ago as part of <code>gluon-ts<\/code> python package (<a href=\"https:\/\/aws.amazon.com\/blogs\/opensource\/gluon-time-series-open-source-time-series-modeling-toolkit\/\" rel=\"nofollow noreferrer\">blog post<\/a>, <a href=\"https:\/\/gluon-ts.mxnet.io\/\" rel=\"nofollow noreferrer\">code<\/a>) so if you develop a model specifically for your own hosting environment I'd recommend to use that gluon-ts code in the MXNet container, so that you'll be able to open and read the artifact out of SageMaker.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56771758",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1561921541280,
        "Question_original_content":"premis host train deepar model built start work recent exampl provid aw exampl deepar model order forecast time seri train model artifact file creat bucket question wai host train model host environ client premis",
        "Question_preprocessed_content":"host train deepar model built start work recent exampl provid aw exampl order forecast time seri train model artifact file creat bucket question wai host train model host environ",
        "Question_gpt_summary_original":"The user is facing a challenge of finding a way to host a trained DeepAR model, created using AWS SageMaker, on their own hosting environment instead of using AWS services.",
        "Question_gpt_summary":"user face challeng find wai host train deepar model creat host environ instead aw servic",
        "Answer_original_content":"xgboost built algorithm design amazon mean imposs exampl snippet peek insid model artifact factor machin neural topic model thing hacki usual offici servic featur deepar specif model open sourc coupl week ago gluon python packag blog post code develop model specif host environ recommend us gluon code mxnet contain abl open read artifact",
        "Answer_preprocessed_content":"xgboost algorithm design amazon mean imposs exampl snippet peek insid model artifact thing hacki usual offici servic featur deepar specif model coupl week ago python packag develop model specif host environ recommend us code mxnet contain abl open read artifact",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n- xgboost can be used out of amazon.\n- snippets can be found to peek inside model artifacts for some built-in algorithms, but they can be hacky and unofficial.\n- for hosting a trained deepar model specifically, the gluon-ts python package can be used in the mxnet container to open and read the artifact.",
        "Answer_gpt_summary":"possibl solut answer xgboost amazon snippet peek insid model artifact built algorithm hacki unoffici host train deepar model specif gluon python packag mxnet contain open read artifact"
    },
    {
        "Question_id":66817781.0,
        "Question_title":"What are the differences between AWS sagemaker and sagemaker_pyspark?",
        "Question_body":"<p>I'm currently running a quick Machine Learning proof of concept on AWS with SageMaker, and I've come across two libraries: <code>sagemaker<\/code> and <code>sagemaker_pyspark<\/code>. I would like to work with distributed data. My questions are:<\/p>\n<ol>\n<li><p>Is using <code>sagemaker<\/code> the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented <code>sagemaker_pyspark<\/code>? Based on this assumption, I do not understand what it would offer regarding using <code>scikit-learn<\/code> on a SageMaker notebook (in terms of computing capabilities).<\/p>\n<\/li>\n<li><p>Is it normal for something like <code>model = xgboost_estimator.fit(training_data)<\/code> to take 4 minutes to run with <code>sagemaker_pyspark<\/code> for a small set of test data? I see that what it does below is to train the model and also create an Endpoint to be able to offer its predictive services, and I assume that this endpoint is deployed on an EC2 instance that is created and started at the moment. Correct me if I'm wrong. I assume this from how the estimator is defined:<\/p>\n<\/li>\n<\/ol>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator\n\n\nxgboost_estimator = XGBoostSageMakerEstimator (\n    trainingInstanceType = &quot;ml.m4.xlarge&quot;,\n    trainingInstanceCount = 1,\n    endpointInstanceType = &quot;ml.m4.xlarge&quot;,\n    endpointInitialInstanceCount = 1,\n    sagemakerRole = IAMRole(get_execution_role())\n)\n\nxgboost_estimator.setNumRound(1)\n<\/code><\/pre>\n<p>If so, is there a way to reuse the same endpoint with different training jobs so that I don't have to wait for a new endpoint to be created each time?<\/p>\n<ol start=\"3\">\n<li><p>Does <code>sagemaker_pyspark<\/code> support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/p>\n<\/li>\n<li><p>Do you know if <code>sagemaker_pyspark<\/code> can perform hyperparameter optimization? From what I see, <code>sagemaker<\/code> offers the <code>HyperparameterTuner<\/code> class, but I can't find anything like it in <code>sagemaker_pyspark<\/code>. I suppose it is a more recent library and there is still a lot of functionality to implement.<\/p>\n<\/li>\n<li><p>I am a bit confused about the concept of <code>entry_point<\/code> and <code>container<\/code>\/<code>image_name<\/code> (both possible input arguments for the <code>Estimator<\/code> object from the <code>sagemaker<\/code> library): can you deploy models with and without containers? why would you use model containers? Do you always need to define the model externally with the <code>entry_point<\/code> script? It is also confusing that the class <code>AlgorithmEstimator<\/code> allows the input argument <code>algorithm_arn<\/code>; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/li>\n<li><p>I see the <code>sagemaker<\/code> library offers SageMaker Pipelines, which seem to be very handy for deploying properly structured ML workflows. However, I don't think this is available with <code>sagemaker_pyspark<\/code>, so in that case, I would rather create my workflows with a combination of Step Functions (to orchestrate the entire thing), Glue processes (for ETL, preprocessing and feature\/target engineering) and SageMaker processes using <code>sagemaker_pyspark<\/code>.<\/p>\n<\/li>\n<li><p>I also found out that <code>sagemaker<\/code> has the <code>sagemaker.sparkml.model.SparkMLModel<\/code> object. What is the difference between this and what <code>sagemaker_pyspark<\/code> offers?<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616764992750,
        "Question_favorite_count":null,
        "Question_last_edit_time":1618398204670,
        "Question_score":1.0,
        "Question_view_count":242.0,
        "Answer_body":"<p><code>sagemaker<\/code> is the SageMaker Python SDK. It calls SageMaker-related AWS service APIs on your behalf. You don't need to use it, but it can make life easier<\/p>\n<blockquote>\n<ol>\n<li>Is using sagemaker the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented sagemaker_pyspark?<\/li>\n<\/ol>\n<\/blockquote>\n<p>No. You can run distributed training jobs using <code>sagemaker<\/code> (see <code>instance_count<\/code> parameter)<\/p>\n<p><code>sagemaker_pyspark<\/code> facilitates calling SageMaker-related AWS service APIs from Spark. Use it if you want to use SageMaker services from Spark<\/p>\n<blockquote>\n<ol start=\"2\">\n<li>Is it normal for something like model = xgboost_estimator.fit(training_data) to take 4 minutes to run with sagemaker_pyspark for a small set of test data?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, it takes a few minutes for an EC2 instance to spin-up. Use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a> if you want to iterate more quickly locally. Note: Local Mode won't work with SageMaker built-in algorithms, but you can prototype with (non AWS) XGBoost\/SciKit-Learn<\/p>\n<blockquote>\n<ol start=\"3\">\n<li>Does sagemaker_pyspark support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, but you'd probably want to extend <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/api.html#sagemakerestimator\" rel=\"nofollow noreferrer\">SageMakerEstimator<\/a>. Here you can provide the <code>trainingImage<\/code> URI<\/p>\n<blockquote>\n<ol start=\"4\">\n<li>Do you know if sagemaker_pyspark can perform hyperparameter optimization?<\/li>\n<\/ol>\n<\/blockquote>\n<p>It does not appear so. It'd probably be easier just to do this from SageMaker itself though<\/p>\n<blockquote>\n<p>can you deploy models with and without containers?<\/p>\n<\/blockquote>\n<p>You can certainly host your own models any way you want. But if you want to use SageMaker model inference hosting, then containers are required<\/p>\n<blockquote>\n<p>why would you use model containers?<\/p>\n<\/blockquote>\n<blockquote>\n<p>Do you always need to define the model externally with the entry_point script?<\/p>\n<\/blockquote>\n<p>The whole Docker thing makes bundling dependencies easier, and also makes things language\/runtime-neutral. SageMaker doesn't care if your algorithm is in Python or Java or Fortran. But it needs to know how to &quot;run&quot; it, so you tell it a working directory and a command to run. This is the entry point<\/p>\n<blockquote>\n<p>It is also confusing that the class AlgorithmEstimator allows the input argument algorithm_arn; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/blockquote>\n<p>Please clarify which &quot;three&quot; you are referring to<\/p>\n<p>6 is not a question, so no answer required :)<\/p>\n<blockquote>\n<ol start=\"7\">\n<li>What is the difference between this and what sagemaker_pyspark offers?<\/li>\n<\/ol>\n<\/blockquote>\n<p>sagemaker_pyspark lets you call SageMaker services from Spark, whereas SparkML Serving lets you use Spark ML services from SageMaker<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1634230982412,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66817781",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1617119678127,
        "Question_original_content":"differ pyspark current run quick machin learn proof concept aw come librari pyspark like work distribut data question equival run train job take advantag distribut comput capabl aw assum implement pyspark base assumpt understand offer scikit learn notebook term comput capabl normal like model xgboost estim fit train data minut run pyspark small set test data train model creat endpoint abl offer predict servic assum endpoint deploi instanc creat start moment correct wrong assum estim defin import execut role pyspark algorithm import xgboostestim xgboost estim xgboostestim traininginstancetyp xlarg traininginstancecount endpointinstancetyp xlarg endpointinitialinstancecount role iamrol execut role xgboost estim setnumround wai reus endpoint differ train job wait new endpoint creat time pyspark support custom algorithm allow us predefin on librari know pyspark perform hyperparamet optim offer hyperparametertun class like pyspark suppos recent librari lot function implement bit confus concept entri point contain imag possibl input argument estim object librari deploi model contain us model contain need defin model extern entri point script confus class algorithmestim allow input argument algorithm arn differ wai pass model input better librari offer pipelin handi deploi properli structur workflow think avail pyspark case creat workflow combin step function orchestr entir thing glue process etl preprocess featur target engin process pyspark sparkml model sparkmlmodel object differ pyspark offer",
        "Question_preprocessed_content":"differ current run quick machin learn proof concept aw come librari like work distribut data question equival run train job take advantag distribut comput capabl aw assum implement base assumpt understand offer notebook normal like minut run small set test data train model creat endpoint abl offer predict servic assum endpoint deploi instanc creat start moment correct wrong assum estim defin wai reus endpoint differ train job wait new endpoint creat time support custom algorithm allow us predefin on librari know perform hyperparamet optim offer class like suppos recent librari lot function implement bit confus concept deploi model contain us model contain need defin model extern script confus class allow input argument differ wai pass model input better librari offer pipelin handi deploi properli structur workflow think avail case creat workflow combin step function glue process process object differ offer",
        "Question_gpt_summary_original":"The user is encountering several challenges while working with AWS SageMaker. They are confused about the differences between the sagemaker and sagemaker_pyspark libraries, and whether sagemaker_pyspark offers distributed computing capabilities. They are also experiencing slow training times and wondering if there is a way to reuse the same endpoint with different training jobs. The user is unsure if sagemaker_pyspark supports custom algorithms and hyperparameter optimization. They are also confused about the concept of entry_point and container\/image_name and why model containers are used. Finally, the user is considering using a combination of Step Functions, Glue processes, and sagemaker_pyspark for their ML workflows and is curious about the difference between sagemaker.sparkml.model.SparkMLModel and sagemaker_pyspark.",
        "Question_gpt_summary":"user encount challeng work confus differ pyspark librari pyspark offer distribut comput capabl experienc slow train time wonder wai reus endpoint differ train job user unsur pyspark support custom algorithm hyperparamet optim confus concept entri point contain imag model contain final user consid combin step function glue process pyspark workflow curiou differ sparkml model sparkmlmodel pyspark",
        "Answer_original_content":"python sdk call relat aw servic api behalf need us life easier equival run train job take advantag distribut comput capabl aw assum implement pyspark run distribut train job instanc count paramet pyspark facilit call relat aw servic api spark us want us servic spark normal like model xgboost estim fit train data minut run pyspark small set test data ye take minut instanc spin us local mode want iter quickli local note local mode won work built algorithm prototyp non aw xgboost scikit learn pyspark support custom algorithm allow us predefin on librari ye probabl want extend estim provid trainingimag uri know pyspark perform hyperparamet optim appear probabl easier deploi model contain certainli host model wai want want us model infer host contain requir us model contain need defin model extern entri point script docker thing make bundl depend easier make thing languag runtim neutral care algorithm python java fortran need know run tell work directori command run entri point confus class algorithmestim allow input argument algorithm arn differ wai pass model input better clarifi refer question answer requir differ pyspark offer pyspark let servic spark sparkml serv let us spark servic",
        "Answer_preprocessed_content":"python sdk call relat aw servic api behalf need us life easier equival run train job take advantag distribut comput capabl aw assum implement run distribut train job facilit call relat aw servic api spark us want us servic spark normal like model minut run small set test data ye take minut instanc us local mode want iter quickli local note local mode won work algorithm prototyp support custom algorithm allow us predefin on librari ye probabl want extend estim provid uri know perform hyperparamet optim appear probabl easier deploi model contain certainli host model wai want want us model infer host contain requir us model contain need defin model extern script docker thing make bundl depend easier make thing care algorithm python java fortran need know run tell work directori command run entri point confus class algorithmestim allow input argument differ wai pass model input better clarifi refer question answer requir differ offer let servic spark sparkml serv let us spark servic",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are:\n\n- use _pyspark to run distributed training jobs and call -related aws service apis from spark.\n- use local mode if you want to iterate more quickly locally, but note that it won't work with built-in algorithms.\n- extend estimator to support custom algorithms and provide the training image uri.\n- use model containers for model inference hosting, as they make bundling dependencies easier and language\/runtime-neutral.\n- define the model externally with the entry_point script, which specifies the working directory and command to run.\n- use sparkml serving to use spark ml services from spark.",
        "Answer_gpt_summary":"possibl solut extract answer us pyspark run distribut train job relat aw servic api spark us local mode want iter quickli local note won work built algorithm extend estim support custom algorithm provid train imag uri us model contain model infer host bundl depend easier languag runtim neutral defin model extern entri point script specifi work directori command run us sparkml serv us spark servic spark"
    },
    {
        "Question_id":64170759.0,
        "Question_title":"Pyathena is super slow compared to querying from Athena",
        "Question_body":"<p>I run a query from AWS <strong>Athena console<\/strong> and takes 10s.\nThe same query run from <strong>Sagemaker<\/strong> using <strong>PyAthena<\/strong> takes 155s.\nIs PyAthena slowing it down or is the data transfer from Athena to sagemaker so time consuming?<\/p>\n<p>What could I do to speed this up?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601638106830,
        "Question_favorite_count":null,
        "Question_last_edit_time":1601641259400,
        "Question_score":3.0,
        "Question_view_count":3188.0,
        "Answer_body":"<p>Just figure out a way of boosting the queries:<\/p>\n<p>Before I was trying:<\/p>\n<pre><code>import pandas as pd\nfrom pyathena import connect\n\nconn = connect(s3_staging_dir=STAGIN_DIR,\n             region_name=REGION)\npd.read_sql(QUERY, conn)\n# takes 160s\n<\/code><\/pre>\n<p>Figured out that using a <em>PandasCursor<\/em> instead of a <em>connection<\/em> is way faster<\/p>\n<pre><code>import pandas as pd\npyathena import connect\nfrom pyathena.pandas.cursor import PandasCursor\n\ncursor = connect(s3_staging_dir=STAGIN_DIR,\n                 region_name=REGION,\n                 cursor_class=PandasCursor).cursor()\ndf = cursor.execute(QUERY).as_pandas()\n# takes 12s\n<\/code><\/pre>\n<p>Ref: <a href=\"https:\/\/github.com\/laughingman7743\/PyAthena\/issues\/46\" rel=\"noreferrer\">https:\/\/github.com\/laughingman7743\/PyAthena\/issues\/46<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1629380324768,
        "Answer_score":13.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64170759",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1601639323903,
        "Question_original_content":"pyathena super slow compar queri athena run queri aw athena consol take queri run pyathena take pyathena slow data transfer athena time consum speed",
        "Question_preprocessed_content":"pyathena super slow compar queri athena run queri aw athena consol take queri run pyathena take pyathena slow data transfer athena time consum speed",
        "Question_gpt_summary_original":"The user is facing challenges with slow query performance when using PyAthena to query data from Athena compared to querying directly from the Athena console. The user is unsure whether PyAthena is causing the slowdown or if the data transfer from Athena to Sagemaker is the issue. The user is seeking advice on how to improve query performance.",
        "Question_gpt_summary":"user face challeng slow queri perform pyathena queri data athena compar queri directli athena consol user unsur pyathena caus slowdown data transfer athena issu user seek advic improv queri perform",
        "Answer_original_content":"figur wai boost queri try import panda pyathena import connect conn connect stage dir stagin dir region region read sql queri conn take figur pandascursor instead connect wai faster import panda pyathena import connect pyathena panda cursor import pandascursor cursor connect stage dir stagin dir region region cursor class pandascursor cursor cursor execut queri panda take ref http github com laughingman pyathena issu",
        "Answer_preprocessed_content":"figur wai boost queri try figur pandascursor instead connect wai faster ref",
        "Answer_gpt_summary_original":"the answer suggests using a pandas cursor instead of a connection to speed up the query process when using pyathena. the user can modify their code to use the pandas cursor and execute the query, which should significantly reduce the query time.",
        "Answer_gpt_summary":"answer suggest panda cursor instead connect speed queri process pyathena user modifi code us panda cursor execut queri significantli reduc queri time"
    },
    {
        "Question_id":null,
        "Question_title":"Project in Data Labeling not working, getting only \"Loading project details\"",
        "Question_body":"Project in Azure Machine Learning Studio in Data Labeling was working, we did label it every day. One day we just could not open it, it showed only Loading project details.. for hours.\nSAS token is working, it also work for our admin account, but not for Labellers.\nDo you have any idea?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647676305203,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/778679\/project-in-data-labeling-not-working-getting-only.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-21T16:36:47.9Z",
                "Answer_score":0,
                "Answer_body":"@ika-8686 Thanks for the question. Please share details of your experiment and issue from the ml.azure.com portal for a service engineer to lookup the issue from the back-end? This option is available from the top right hand corner of the portal by clicking the smiley face, Please select the option Microsoft can email you about the feedback along with a screen shot so our service team can lookup and advise through email.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"project data label work get load project detail project studio data label work label dai dai open show load project detail hour sa token work work admin account label idea",
        "Question_preprocessed_content":"project data label work get load project detail project studio data label work label dai dai open show load project hour sa token work work admin account label idea",
        "Question_gpt_summary_original":"The user is facing challenges with their project in Azure Machine Learning Studio's Data Labeling. They were able to label the project every day, but one day they were unable to open it and it only showed \"Loading project details\" for hours. The SAS token is working for the admin account but not for the labellers, and the user is seeking assistance to resolve the issue.",
        "Question_gpt_summary":"user face challeng project studio data label abl label project dai dai unabl open show load project detail hour sa token work admin account label user seek assist resolv issu",
        "Answer_original_content":"ika thank question share detail experi issu azur com portal servic engin lookup issu end option avail right hand corner portal click smilei face select option microsoft email feedback screen shot servic team lookup advis email",
        "Answer_preprocessed_content":"thank question share detail experi issu portal servic engin lookup issu option avail right hand corner portal click smilei face select option microsoft email feedback screen shot servic team lookup advis email",
        "Answer_gpt_summary_original":"the answer suggests that the user should share details of their experiment and issue from the ml.azure.com portal for a service engineer to lookup the issue from the back-end. the user can do this by clicking the smiley face in the top right-hand corner of the portal and selecting the option for microsoft to email them about the feedback along with a screenshot. the service team will then be able to advise the user through email.",
        "Answer_gpt_summary":"answer suggest user share detail experi issu azur com portal servic engin lookup issu end user click smilei face right hand corner portal select option microsoft email feedback screenshot servic team abl advis user email"
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker GroundTruth Interface - option to skip a task and then return",
        "Question_body":"Customer wants to configure the SageMaker Ground Truth interface seen by the workers such that the labeler can navigate to previous or next tasks. For example, if one is labelling images, they could skip the current image, label the next one, and then return to the skipped image. The Ground Truth interface does not seem to have this capability. Is there an option for it that I missed? I could not find anything about it here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-labeling.html.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596055479000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":68.0,
        "Answer_body":"Currently, there is no functionality to skip a task and go back to it later. However, you could add a field like\n\n[ ] this task was skipped\n\nwhere the annotator could check the box for those items to be reviewed and processed at another time.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_S8ylg4UQdKh76o3zp3dWQ\/sage-maker-ground-truth-interface-option-to-skip-a-task-and-then-return",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-29T21:28:39.000Z",
                "Answer_score":0,
                "Answer_body":"Currently, there is no functionality to skip a task and go back to it later. However, you could add a field like\n\n[ ] this task was skipped\n\nwhere the annotator could check the box for those items to be reviewed and processed at another time.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1596058119000,
        "Question_original_content":"groundtruth interfac option skip task return custom want configur ground truth interfac seen worker label navig previou task exampl label imag skip current imag label return skip imag ground truth interfac capabl option miss http doc aw amazon com latest sm data label html",
        "Question_preprocessed_content":"groundtruth interfac option skip task return custom want configur ground truth interfac seen worker label navig previou task exampl label imag skip current imag label return skip imag ground truth interfac capabl option miss",
        "Question_gpt_summary_original":"The user is facing a challenge with the SageMaker Ground Truth interface as it does not provide an option for labelers to navigate to previous or next tasks. The user wants to configure the interface to allow labelers to skip a task and return to it later, but cannot find any information about this option in the documentation.",
        "Question_gpt_summary":"user face challeng ground truth interfac provid option label navig previou task user want configur interfac allow label skip task return later inform option document",
        "Answer_original_content":"current function skip task later add field like task skip annot check box item review process time",
        "Answer_preprocessed_content":"current function skip task later add field like task skip annot check box item review process time",
        "Answer_gpt_summary_original":"possible solution: add a checkbox field labeled \"this task was skipped\" for annotators to mark items that need to be reviewed and processed at a later time. however, there is currently no built-in functionality to skip a task and return to it later in the ground truth interface.",
        "Answer_gpt_summary":"possibl solut add checkbox field label task skip annot mark item need review process later time current built function skip task return later ground truth interfac"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning Compute Cost - Detailed information for each compute",
        "Question_body":"Hi all,\n\nI need to have the cost detail for each compute (compute instance, compute clusters) configured within Azure Machine Learning.\nRight now from portal I can only retrieve an aggregate cost per compute type, but I need to have the detail per individual compute.\n\nHow can I get this information? Not necessarily from portal, but it's fine via SDK, Rest API, querying logs, etc. Just having this information is enough.\n\nThanks!\n\nG",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1654075947357,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/872840\/azure-machine-learning-compute-cost-detailed-infor.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-06T13:58:23.183Z",
                "Answer_score":1,
                "Answer_body":"Hi all,\n\nshould anyone else be interested in tracking the actual costs of individual computes within AML, here is an Azure CLI command that is a great place to start. Within the \"additionalProperties\" field is the cluster detail information.\n\n(az consumption usage list --subscription $subscription --start-date $start_date --end-date $end_date --query \"[?contains(consumedService, 'Microsoft.MachineLearningServices') && contains(instanceName, $resource_group) && contains(product, 'Virtual Machine')]\" --include-additional-properties | ConvertFrom-Json) | select additionalProperties, pretaxCost, product, usageStart, usageEnd\n\n\n\n\nThis command can be used to retrieve the cost detail of all resources on Azure.\n\nGCocci",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"comput cost detail inform comput need cost comput comput instanc comput cluster configur right portal retriev aggreg cost comput type need individu comput inform necessarili portal fine sdk rest api queri log have inform thank",
        "Question_preprocessed_content":"comput cost detail inform comput need cost comput configur right portal retriev aggreg cost comput type need individu comput inform necessarili portal fine sdk rest api queri log have inform thank",
        "Question_gpt_summary_original":"The user is facing a challenge in obtaining detailed cost information for each compute instance and cluster configured within Azure Machine Learning. The user is currently only able to retrieve an aggregate cost per compute type and is seeking a way to obtain this information per individual compute, either through the portal, SDK, Rest API, or querying logs.",
        "Question_gpt_summary":"user face challeng obtain detail cost inform comput instanc cluster configur user current abl retriev aggreg cost comput type seek wai obtain inform individu comput portal sdk rest api queri log",
        "Answer_original_content":"interest track actual cost individu comput aml azur cli command great place start additionalproperti field cluster inform consumpt usag list subscript subscript start date start date end date end date queri contain consumedservic microsoft machinelearningservic contain instancenam resourc group contain product virtual machin includ addit properti convertfrom json select additionalproperti pretaxcost product usagestart usageend command retriev cost resourc azur gcocci",
        "Answer_preprocessed_content":"interest track actual cost individu comput aml azur cli command great place start additionalproperti field cluster inform consumpt usag list subscript contain contain contain select additionalproperti pretaxcost product usagestart usageend command retriev cost resourc azur gcocci",
        "Answer_gpt_summary_original":"the answer suggests using an azure cli command to retrieve cost information for individual compute instances and clusters within an aml environment. the command can be used to retrieve cost details for all resources on azure.",
        "Answer_gpt_summary":"answer suggest azur cli command retriev cost inform individu comput instanc cluster aml environ command retriev cost detail resourc azur"
    },
    {
        "Question_id":null,
        "Question_title":"Using R model in SageMaker ML pipelines",
        "Question_body":"Hi there,\n\nIs it possible to use R model training and serving in SageMaker ML Pipelines? Looked in examples here. And it doesn't look that R is fully supported currently by ML Pipelines. Any examples and success stories are very welcome.\n\nThanks.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643230196748,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":97.0,
        "Answer_body":"In general it is possible to use the SageMaker python SDK and boto3 using the reticulate package in R, However do not have direct examples of SageMaker Pipelines using R.\n\nIt is possible to orchestrate the production pipeline using the R Containers for training and serving and setting up the DAG can be done with reticulate and SageMaker Python SDK and can be achieved using the AWS Step Functions. Please refer to the following example for reference.\n\nhttps:\/\/github.com\/aws-samples\/reinvent2020-aim404-productionize-r-using-amazon-sagemaker https:\/\/www.youtube.com\/watch?v=Zpp0nfvqDCA",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU17aS4s7uSRqmiLuveuchBw\/using-r-model-in-sage-maker-ml-pipelines",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-28T21:07:43.709Z",
                "Answer_score":1,
                "Answer_body":"In general it is possible to use the SageMaker python SDK and boto3 using the reticulate package in R, However do not have direct examples of SageMaker Pipelines using R.\n\nIt is possible to orchestrate the production pipeline using the R Containers for training and serving and setting up the DAG can be done with reticulate and SageMaker Python SDK and can be achieved using the AWS Step Functions. Please refer to the following example for reference.\n\nhttps:\/\/github.com\/aws-samples\/reinvent2020-aim404-productionize-r-using-amazon-sagemaker https:\/\/www.youtube.com\/watch?v=Zpp0nfvqDCA",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1643404063708,
        "Question_original_content":"model pipelin possibl us model train serv pipelin look exampl look fulli support current pipelin exampl success stori welcom thank",
        "Question_preprocessed_content":"model pipelin possibl us model train serv pipelin look exampl look fulli support current pipelin exampl success stori welcom thank",
        "Question_gpt_summary_original":"The user is facing challenges in using R model training and serving in SageMaker ML Pipelines as it does not seem to be fully supported currently. They are seeking examples and success stories to overcome this challenge.",
        "Question_gpt_summary":"user face challeng model train serv pipelin fulli support current seek exampl success stori overcom challeng",
        "Answer_original_content":"gener possibl us python sdk boto reticul packag direct exampl pipelin possibl orchestr product pipelin contain train serv set dag reticul python sdk achiev aw step function refer follow exampl refer http github com aw sampl reinvent aim production amazon http youtub com watch zppnfvqdca",
        "Answer_preprocessed_content":"gener possibl us python sdk boto reticul packag direct exampl pipelin possibl orchestr product pipelin contain train serv set dag reticul python sdk achiev aw step function refer follow exampl refer",
        "Answer_gpt_summary_original":"possible solutions for using r models in ml pipelines include using the python sdk and boto3 with the reticulate package in r, orchestrating the production pipeline using r containers for training and serving, and setting up the dag using reticulate and the python sdk. aws step functions can also be used to achieve this. the provided github example and youtube video can be used as references.",
        "Answer_gpt_summary":"possibl solut model pipelin includ python sdk boto reticul packag orchestr product pipelin contain train serv set dag reticul python sdk aw step function achiev provid github exampl youtub video refer"
    },
    {
        "Question_id":null,
        "Question_title":"Academic account",
        "Question_body":"<p>Hi, I am facing an issue with an academic account. I added the email where the domain is <span class=\"mention\">@diag.uniroma1.it<\/span>, as any domain ending with uniroma1.it is from different faculties in Sapienza. DIAG is the department of computer engineering and automation.<\/p>\n<p>However the account seems to still be considered non-academic. Is there any step necessary from my side?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1676943452575,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":12.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/academic-account\/3921",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-21T18:31:13.616Z",
                "Answer_body":"<p>Hi Pere-Lluis!<\/p>\n<p>What is the full email you are trying to mark as academic?<\/p>\n<p>Cheers,<br>\nArtsiom<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"academ account face issu academ account ad email domain diag uniroma domain end uniroma differ faculti sapienza diag depart engin autom account consid non academ step necessari",
        "Question_preprocessed_content":"academ account face issu academ account ad email domain domain end differ faculti sapienza diag depart engin autom account consid step necessari",
        "Question_gpt_summary_original":"The user is facing an issue with their academic account as it is not being recognized as an academic account despite adding the correct email domain for their department. They are seeking guidance on any necessary steps to resolve the issue.",
        "Question_gpt_summary":"user face issu academ account recogn academ account despit ad correct email domain depart seek guidanc necessari step resolv issu",
        "Answer_original_content":"pere llui email try mark academ cheer artsiom",
        "Answer_preprocessed_content":"email try mark academ cheer artsiom",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer to the user's difficulty with their academic account associated with the domain @diag.uniroma1.it. the responder is simply asking for more information about the email in question.",
        "Answer_gpt_summary":"solut provid answer user difficulti academ account associ domain diag uniroma respond simpli ask inform email question"
    },
    {
        "Question_id":null,
        "Question_title":"Cloud Translation Permission",
        "Question_body":"So I'm pulling my hair out over this and reaching out here for help. I'm trying to set up a service account with Cloud Translation, and Text-to-speech enabled, but we keep getting this response:I have confirmed that the service account has the \"cloudtranslate.generalModels.predict\" permission, and showing the \"Cloud Translation API User\" role. We've also confirmed that it works with a different Service account that my colleague set up in his personal Google console profile. But, we need this setup with an account through our org. I did verify that the service account has the permission from the https:\/\/console.cloud.google.com\/iam-admin\/troubleshooter so and that my organization's admin sees that the service account is granted access through ancestor policies.  So what else can we check? ",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1668497880000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":53.0,
        "Answer_body":"Ok, turned out we had a hard-coded value for resource location, which was set to the wrong project. So of course it was coming back as permission denied.\u00a0\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Translation-Permission\/td-p\/489632\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-15T08:30:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Ok, turned out we had a hard-coded value for resource location, which was set to the wrong project. So of course it was coming back as permission denied.\u00a0\n\nView solution in original post"
            },
            {
                "Answer_creation_time":"2022-11-15T08:30:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Ok, turned out we had a hard-coded value for resource location, which was set to the wrong project. So of course it was coming back as permission denied."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1668501000000,
        "Question_original_content":"cloud translat permiss pull hair reach help try set servic account cloud translat text speech enabl get respons confirm servic account cloudtransl generalmodel predict permiss show cloud translat api user role confirm work differ servic account colleagu set person googl consol profil need setup account org verifi servic account permiss http consol cloud googl com iam admin troubleshoot organ admin see servic account grant access ancestor polici check",
        "Question_preprocessed_content":"cloud translat permiss pull hair reach help try set servic account cloud translat enabl get respons confirm servic account permiss show cloud translat api user role confirm work differ servic account colleagu set person googl consol profil need setup account org verifi servic account permiss organ admin see servic account grant access ancestor polici check",
        "Question_gpt_summary_original":"The user is facing challenges while setting up a service account with Cloud Translation and Text-to-speech enabled. Despite having the necessary permissions and roles, the user is receiving an error message. The user has confirmed that the service account works with a different account but needs to set it up with an account through their organization. The user has verified that the service account has the required permission and access through ancestor policies. The user is seeking help to identify the issue.",
        "Question_gpt_summary":"user face challeng set servic account cloud translat text speech enabl despit have necessari permiss role user receiv error messag user confirm servic account work differ account need set account organ user verifi servic account requir permiss access ancestor polici user seek help identifi issu",
        "Answer_original_content":"turn hard code valu resourc locat set wrong project cours come permiss deni view solut origin post",
        "Answer_preprocessed_content":"turn valu resourc locat set wrong project cours come permiss deni view solut origin post",
        "Answer_gpt_summary_original":"the solution to the difficulty in setting up a service account with cloud translation and text-to-speech enabled was to check the hard-coded value for resource location and ensure that it is set to the correct project.",
        "Answer_gpt_summary":"solut difficulti set servic account cloud translat text speech enabl check hard code valu resourc locat ensur set correct project"
    },
    {
        "Question_id":null,
        "Question_title":"how to configure ideal value for MaxConcurrentTransforms in setting up a sagemaker batch transform ?",
        "Question_body":"based on the documentation , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html, it states that \" The ideal value for MaxConcurrentTransforms is equal to the number of compute workers in the batch transform job.\" how to figure out what the number of compute workers is , i assume this depends on the instance type. also what about the instance count parameter we can set , do we have to take that into account as well?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649205572690,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":152.0,
        "Answer_body":"The ideal value for MaxConcurrentTransforms varies based on instance type as well as based on your specific model.\n\nit could make sense to increase MaxConcurrentTransforms up to the core count of the instance you are using (for cpu based transform), however, you should also take into account the memory utilisation by your model.\n\nThe ultimate answer is it \"depends\" and I would recommend that you experiment with increasing this number gradually from 1 up to instance core count, while monitoring RAM\/cpu utilisation to find the optimal.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKcUBF0wPQSyerTP2IK53hQ\/how-to-configure-ideal-value-for-max-concurrent-transforms-in-setting-up-a-sagemaker-batch-transform",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-06T12:08:37.948Z",
                "Answer_score":0,
                "Answer_body":"The ideal value for MaxConcurrentTransforms varies based on instance type as well as based on your specific model.\n\nit could make sense to increase MaxConcurrentTransforms up to the core count of the instance you are using (for cpu based transform), however, you should also take into account the memory utilisation by your model.\n\nThe ultimate answer is it \"depends\" and I would recommend that you experiment with increasing this number gradually from 1 up to instance core count, while monitoring RAM\/cpu utilisation to find the optimal.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1649246917948,
        "Question_original_content":"configur ideal valu maxconcurrenttransform set batch transform base document http doc aw amazon com latest batch transform html state ideal valu maxconcurrenttransform equal number comput worker batch transform job figur number comput worker assum depend instanc type instanc count paramet set account",
        "Question_preprocessed_content":"configur ideal valu maxconcurrenttransform set batch transform base document state ideal valu maxconcurrenttransform equal number comput worker batch transform figur number comput worker assum depend instanc type instanc count paramet set account",
        "Question_gpt_summary_original":"The user is facing challenges in configuring the ideal value for MaxConcurrentTransforms while setting up a Sagemaker batch transform. The documentation suggests that the ideal value for MaxConcurrentTransforms is equal to the number of compute workers in the batch transform job, but the user is unsure how to determine the number of compute workers, which may depend on the instance type. The user is also unsure if the instance count parameter needs to be taken into account.",
        "Question_gpt_summary":"user face challeng configur ideal valu maxconcurrenttransform set batch transform document suggest ideal valu maxconcurrenttransform equal number comput worker batch transform job user unsur determin number comput worker depend instanc type user unsur instanc count paramet need taken account",
        "Answer_original_content":"ideal valu maxconcurrenttransform vari base instanc type base specif model sens increas maxconcurrenttransform core count instanc cpu base transform account memori utilis model ultim answer depend recommend experi increas number gradual instanc core count monitor ram cpu utilis optim",
        "Answer_preprocessed_content":"ideal valu maxconcurrenttransform vari base instanc type base specif model sens increas maxconcurrenttransform core count instanc account memori utilis model ultim answer depend recommend experi increas number gradual instanc core count monitor utilis optim",
        "Answer_gpt_summary_original":"possible solutions to configuring the ideal value for maxconcurrenttransforms when setting up a batch transform are: \n1. increase maxconcurrenttransforms up to the core count of the instance being used (for cpu-based transform).\n2. take into account the memory utilization by the model.\n3. experiment with increasing the number gradually from 1 up to instance core count while monitoring ram\/cpu utilization to find the optimal value.",
        "Answer_gpt_summary":"possibl solut configur ideal valu maxconcurrenttransform set batch transform increas maxconcurrenttransform core count instanc cpu base transform account memori util model experi increas number gradual instanc core count monitor ram cpu util optim valu"
    },
    {
        "Question_id":null,
        "Question_title":"How do I create a resource group when creating a workspace?",
        "Question_body":"I am going through the Azure AI training and need to create a workspace under machine learning. When it asks me to select a resource group there are no options. When I want to create a new resource group it says I dont have permissions under my subscription. What do I need to do?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638162185497,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/643801\/how-do-i-create-a-resource-group-when-creating-a-w.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-29T05:11:17.963Z",
                "Answer_score":1,
                "Answer_body":"I believe you need to be a subscription-level owner or contributor.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"creat resourc group creat workspac go azur train need creat workspac machin learn ask select resourc group option want creat new resourc group sai dont permiss subscript need",
        "Question_preprocessed_content":"creat resourc group creat workspac go azur train need creat workspac machin learn ask select resourc group option want creat new resourc group sai dont permiss subscript need",
        "Question_gpt_summary_original":"The user is facing challenges in creating a workspace under machine learning in Azure AI training. They are unable to select a resource group and cannot create a new one due to lack of permissions under their subscription.",
        "Question_gpt_summary":"user face challeng creat workspac machin learn azur train unabl select resourc group creat new lack permiss subscript",
        "Answer_original_content":"believ need subscript level owner contributor",
        "Answer_preprocessed_content":"believ need owner contributor",
        "Answer_gpt_summary_original":"possible solutions to the difficulty encountered when creating a resource group for a workspace in azure ai training are to ensure that the user has subscription-level owner or contributor access.",
        "Answer_gpt_summary":"possibl solut difficulti encount creat resourc group workspac azur train ensur user subscript level owner contributor access"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML workspace- compute prices",
        "Question_body":"I was trying to create a compute instance on Azure ml workspace and found that few of the virtual machine sizes show blank in thee price column. Wondering if they are free to use or its a bug. I sorted by prices. So, you can see first few rows showing blanks under price column",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1606098165613,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/171550\/azure-ml-workspace-compute-prices.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-30T08:49:53.507Z",
                "Answer_score":0,
                "Answer_body":"@rawwar Thanks for the feedback. It's bug over free AzureML compute, We will update once the team fix this issue.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"workspac comput price try creat comput instanc workspac virtual machin size blank thee price column wonder free us bug sort price row show blank price column",
        "Question_preprocessed_content":"workspac comput price try creat comput instanc workspac virtual machin size blank thee price column wonder free us bug sort price row show blank price column",
        "Question_gpt_summary_original":"The user encountered challenges while creating a compute instance on Azure ML workspace as some virtual machine sizes showed blank in the price column, leaving them unsure if they were free to use or if it was a bug. The user sorted by prices and noticed that the first few rows showed blanks under the price column.",
        "Question_gpt_summary":"user encount challeng creat comput instanc workspac virtual machin size show blank price column leav unsur free us bug user sort price notic row show blank price column",
        "Answer_original_content":"rawwar thank feedback bug free comput updat team fix issu",
        "Answer_preprocessed_content":"thank feedback bug free comput updat team fix issu",
        "Answer_gpt_summary_original":"the answer suggests that the blank virtual machine sizes in the price column are a bug and the team is working on fixing it. there are no immediate solutions provided in the answer.",
        "Answer_gpt_summary":"answer suggest blank virtual machin size price column bug team work fix immedi solut provid answer"
    },
    {
        "Question_id":null,
        "Question_title":"Wandb not compatible with Torch Script",
        "Question_body":"<p>I\u2019m using wandb logging in conjunction with pytorch + pytorch lightning, and it seems like some of the code in wandb makes it so I cannot JIT my model into torchscript. Here\u2019s the error<\/p>\n<pre><code class=\"lang-auto\">torch.jit.frontend.UnsupportedNodeError: Set aren't supported:\n  File \"\/home\/peter\/catkin_ws\/src\/venv\/lib\/python3.8\/site-packages\/wandb\/wandb_torch.py\", line 355\n        \n            # hook has been processed\n            self._graph_hooks -= {id(module)}\n                                 ~ &lt;--- HERE\n        \n            if not self._graph_hooks:\n<\/code><\/pre>\n<p>is there a workaround for this?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1646168187010,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":109.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-not-compatible-with-torch-script\/1997",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-02T20:33:21.182Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/petermitrano\">@petermitrano<\/a>,<\/p>\n<p>I\u2019m sorry you are facing this issue. Would it be possible for you to share a minimal reproduction of this issue? It will help us debug this for you.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-02T22:41:06.790Z",
                "Answer_body":"<p>This demonstrates the problem:<\/p>\n<pre><code class=\"lang-auto\">#!\/usr\/bin\/env python\n\nimport torch\nfrom torch import nn\n\nimport pytorch_lightning as pl\n\nfrom pytorch_lightning.loggers import WandbLogger\n\nclass LitMLP(pl.LightningModule):\n\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(10, 10)\n\n    def forward(self, x):\n        return self.layer_1(x)\n\n\nwandb_logger = WandbLogger(project=\"torchscript_debugging\")\n\ntrainer = pl.Trainer(logger=wandb_logger)\n\nmodel = LitMLP()\n\n# comment in the line below and the model will fail to compile\n# wandb_logger.watch(model)\n\nscript = model.to_torchscript()\n\nprint(script(torch.ones(10)))\n\n<\/code><\/pre>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-08T21:48:05.926Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/petermitrano\">@petermitrano<\/a>,<\/p>\n<p>Thanks for the reproduction! This is a known issue and there is already an internal ticket tracking this bug - I\u2019m going to bump the priority of this issue for you and will let you know as soon as I have some news on the progress of this issue.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-07T21:48:09.312Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-18T06:01:03.378Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/petermitrano\">@petermitrano<\/a>,<\/p>\n<p>This feature has been implemented and should be released in the next version of <code>wandb<\/code> CLI!<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"compat torch script log conjunct pytorch pytorch lightn like code make jit model torchscript here error torch jit frontend unsupportednodeerror set aren support file home peter catkin src venv lib python site packag torch line hook process self graph hook modul self graph hook workaround",
        "Question_preprocessed_content":"compat torch script log conjunct pytorch pytorch lightn like code make jit model torchscript here error workaround",
        "Question_gpt_summary_original":"The user is facing a challenge with using wandb logging with pytorch and pytorch lightning as some of the code in wandb is preventing them from JITing their model into torchscript, resulting in an error related to unsupported nodes. The user is seeking a workaround for this issue.",
        "Question_gpt_summary":"user face challeng log pytorch pytorch lightn code prevent jite model torchscript result error relat unsupport node user seek workaround issu",
        "Answer_original_content":"hei petermitrano sorri face issu possibl share minim reproduct issu help debug thank ramit demonstr problem usr bin env python import torch torch import import pytorch lightn pytorch lightn logger import logger class litmlp lightningmodul def init self super init self layer linear def forward self return self layer logger logger project torchscript debug trainer trainer logger logger model litmlp comment line model fail compil logger watch model script model torchscript print script torch on petermitrano thank reproduct known issu intern ticket track bug go bump prioriti issu let know soon new progress issu thank ramit topic automat close dai repli new repli longer allow petermitrano featur implement releas version cli thank ramit",
        "Answer_preprocessed_content":"hei sorri face issu possibl share minim reproduct issu help debug thank ramit demonstr problem thank reproduct known issu intern ticket track bug go bump prioriti issu let know soon new progress issu thank ramit topic automat close dai repli new repli longer allow featur implement releas version cli thank ramit",
        "Answer_gpt_summary_original":"there is an issue with logging in pytorch and pytorch lightning that is preventing the user from jit-ing their model into torchscript. the answer suggests sharing a minimal reproduction of the issue to help debug it. the issue is a known bug and there is an internal ticket tracking it. the feature to fix the bug has been implemented and will be released in the next version of cli.",
        "Answer_gpt_summary":"issu log pytorch pytorch lightn prevent user jit ing model torchscript answer suggest share minim reproduct issu help debug issu known bug intern ticket track featur fix bug implement releas version cli"
    },
    {
        "Question_id":null,
        "Question_title":"Partial download data",
        "Question_body":"<p>I pushed N images and meta info to remote repository, then remove all data locally.<\/p>\n<p>After some time my colleague want download this dataset, but not fully - he filter meta info and he knows paths to necessary images.<\/p>\n<p>Is there a way download part of data from remote repository?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1582013387314,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":598.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/partial-download-data\/322",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-02-20T22:57:03.138Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/miraina\">@miraina<\/a>!<\/p>\n<blockquote>\n<p>Is there a way download part of data<\/p>\n<\/blockquote>\n<p>Yes! Per <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/3002\">https:\/\/github.com\/iterative\/dvc\/pull\/3002<\/a> and <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/3309\">https:\/\/github.com\/iterative\/dvc\/pull\/3309<\/a> you should now be able to specify paths to pull\/import. Our docs are currently still outdated about this though, it seems\u2026 But feel free to take a look at those PRs and try it, and please find us in dvc.or\/chat if you have specific usage questions \u2013 it may be a little faster to get answers over there.<\/p>",
                "Answer_score":67.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"partial download data push imag meta info remot repositori remov data local time colleagu want download dataset fulli filter meta info know path necessari imag wai download data remot repositori",
        "Question_preprocessed_content":"partial download data push imag meta info remot repositori remov data local time colleagu want download dataset fulli filter meta info know path necessari imag wai download data remot repositori",
        "Question_gpt_summary_original":"The user uploaded images and meta info to a remote repository, then deleted the data locally. A colleague now needs to download a portion of the data based on filtered meta info and specific image paths. The user is seeking a way to download only the necessary data from the remote repository.",
        "Question_gpt_summary":"user upload imag meta info remot repositori delet data local colleagu need download portion data base filter meta info specif imag path user seek wai download necessari data remot repositori",
        "Answer_original_content":"miraina wai download data ye http github com iter pull http github com iter pull abl specifi path pull import doc current outdat feel free look pr try chat specif usag question littl faster answer",
        "Answer_preprocessed_content":"wai download data ye abl specifi path doc current outdat feel free look pr try specif usag question littl faster answer",
        "Answer_gpt_summary_original":"possible solutions to partially download data from a remote repository that has been previously pushed with images and meta info, but then removed locally are to specify paths to pull\/import. the user can refer to the links provided in the answer and try it out. they can also seek help in .or\/chat if they have specific usage questions.",
        "Answer_gpt_summary":"possibl solut partial download data remot repositori previous push imag meta info remov local specifi path pull import user refer link provid answer try seek help chat specif usag question"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML - Is there a way to prevent registering an Environment when deploying locally?",
        "Question_body":"Hi, we're trying to separate environment registration from deployment scripts so we have created two py files. This is a very simple example of the first one, which is actually registering the environment as desired.\n\n import argparse\n import logging\n    \n from azureml.core import Workspace\n from azureml.core.environment import Environment\n    \n import azure_config\n    \n '''\n This script registers an environment in Azure ML Studio.\n Usage:\n     # Deploy a local webservice for testing\n     python register_env-rm.py -m department\n '''\n    \n # Set up logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger()\n logger.setLevel(logging.INFO)\n    \n def register_env(test_name: str):\n     '''\n     Registers an environment if one of the same version OR specs doesnt already exist.\n     '''\n     # Create instance of workspace\n     ws = Workspace.get(\n         name=azure_config.WORKSPACE_NAME,\n         subscription_id=azure_config.SUBSCRIPTION_ID,\n         resource_group=azure_config.RESOURCE_GROUP\n     )\n        \n     # Create an instance of the Environment based on the given specs\n     myenv = Environment(name=test_name)\n    \n     # Register Environment\n     myenv.register(workspace=ws)\n     myenv.python.conda_dependencies = CondaDependencies.create(\n             pip_packages=[\"fasttext==0.9.2\"]\n             )\n     logger.info(\"New env registered\")\n    \n if __name__ == \"__main__\":\n     parser = argparse.ArgumentParser(description='Registers an environment in Azure ML Studio')\n     parser.add_argument(\n         \"-m\",\n         \"--test-name\",\n         help=\"Name of test\",\n         required=True,\n         default=None,\n         type=str)\n        \n     args = parser.parse_args()\n     test_name = args.test_name\n    \n     register_env(test_name)\n\nThen, here's another simple example of the script that will use to deploy our model. Since we're detecting that a new environment is being registered when deploying locally after the one that we've explicitly registered (running the script above), I'm just including that part of the deployment.\n\n import argparse\n import json\n import logging\n    \n from azureml.core import Workspace\n from azureml.core.environment import Environment\n from azureml.core.model import InferenceConfig, Model\n from azureml.core.webservice import LocalWebservice\n    \n import azure_config\n    \n '''\n This script reproduces the bug where Model.deploy registers an environment.\n '''\n    \n # Set up logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger()\n logger.setLevel(logging.INFO)\n    \n def test_dup_envs(test_name: str):\n    \n     # Create instance of workspace\n     ws = Workspace.get(\n         name=azure_config.WORKSPACE_NAME,\n         subscription_id=azure_config.SUBSCRIPTION_ID,\n         resource_group=azure_config.RESOURCE_GROUP\n     )\n        \n     # Load model if exists\n     model = Model(\n         ws,\n         name='mymodel', \n         version=1\n     )\n    \n     # Get a newly registered env\n     env = Environment.get(\n             workspace=ws, \n             name=test_name, \n             version='1')\n        \n     logger.info(f\"Env object: {env}\")\n    \n     # Create inference config\n     inference_config = InferenceConfig(\n         entry_script='score.py', \n         source_directory='mysourcedir',\n         environment=env)\n    \n     # Deploy service locally\n     local_dep_config = LocalWebservice.deploy_configuration(port=6789)\n    \n     service = Model.deploy(\n         workspace=ws, \n         name=test_name, \n         models=[model], \n         inference_config=inference_config, \n         deployment_config=local_dep_config\n         )\n    \n     service.wait_for_deployment(show_output=True)\n     logger.info(service.get_logs())\n    \n    \n if __name__ == \"__main__\":\n     parser = argparse.ArgumentParser(description='Reproduces the duplicate env bug')\n     parser.add_argument(\n         \"-m\",\n         \"--test-name\",\n         help=\"Name of test to deploy\",\n         required=True,\n         default=None,\n         type=str)\n    \n     args = parser.parse_args()\n        \n     test_name = args.test_name\n    \n     test_dup_envs(test_name)\n\nIs there a way to prevent that? Although we believe this might no impact predictions, we wouldn't like to have many versions of the same environment being registered.\n\nThanks",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1665427554937,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1042597\/azure-ml-is-there-a-way-to-prevent-registering-an.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-18T14:36:30.37Z",
                "Answer_score":0,
                "Answer_body":"@RodrigoMaldonado-2494 Thanks for the details. we can simply register it to be able to use it when necessary.my_spark_env.register(workspace). To get this environment we can use Environment.get from azureml-sdk as well, so we can get the frozen environment to reuse it when it's being defined the Inference Config in the deployment process.\n\n\n\n  from azureml.core.environment import Environment\n     from azureml.core.model import InferenceConfig\n        \n     my_spark_env = Environment.get(name='spark-env-custom', workspace=workspace)\n        \n     inference_config = InferenceConfig(entry_script=\"<YOUR-ENTRY-SCRIPT>\", environment=my_spark_env)",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"wai prevent regist environ deploi local try separ environ registr deploy script creat file simpl exampl actual regist environ desir import argpars import log core import workspac core environ import environ import azur config script regist environ studio usag deploi local webservic test python regist env depart set log log basicconfig level log info logger log getlogg logger setlevel log info def regist env test str regist environ version spec doesnt exist creat instanc workspac workspac azur config workspac subscript azur config subscript resourc group azur config resourc group creat instanc environ base given spec myenv environ test regist environ myenv regist workspac myenv python conda depend condadepend creat pip packag fasttext logger info new env regist main parser argpars argumentpars descript regist environ studio parser add argument test help test requir true default type str arg parser pars arg test arg test regist env test simpl exampl script us deploi model detect new environ regist deploi local explicitli regist run script includ deploy import argpars import json import log core import workspac core environ import environ core model import inferenceconfig model core webservic import localwebservic import azur config script reproduc bug model deploi regist environ set log log basicconfig level log info logger log getlogg logger setlevel log info def test dup env test str creat instanc workspac workspac azur config workspac subscript azur config subscript resourc group azur config resourc group load model exist model model mymodel version newli regist env env environ workspac test version logger info env object env creat infer config infer config inferenceconfig entri script score sourc directori mysourcedir environ env deploi servic local local dep config localwebservic deploi configur port servic model deploi workspac test model model infer config infer config deploy config local dep config servic wait deploy output true logger info servic log main parser argpars argumentpars descript reproduc duplic env bug parser add argument test help test deploi requir true default type str arg parser pars arg test arg test test dup env test wai prevent believ impact predict wouldn like version environ regist thank",
        "Question_preprocessed_content":"wai prevent regist environ deploi local try separ environ registr deploy script creat file simpl exampl actual regist environ desir import argpars import log core import workspac import environ import script regist environ studio usag deploi local webservic test python depart set log logger def str regist environ version spec doesnt exist creat instanc workspac creat instanc environ base given spec myenv regist environ env regist parser environ studio help test requir true default type str arg simpl exampl script us deploi model detect new environ regist deploi local explicitli regist includ deploy import argpars import json import log core import workspac import environ import inferenceconfig model import localwebservic import script reproduc bug regist environ set log logger def str creat instanc workspac load model exist model model newli regist env env workspac version object creat infer config inferenceconfig deploi servic local servic workspac model model parser duplic env bug help test deploi requir true default type str arg wai prevent believ impact predict wouldn like version environ regist thank",
        "Question_gpt_summary_original":"The user is trying to separate environment registration from deployment scripts in Azure ML Studio by creating two Python files. The first file registers the environment, while the second file deploys the model. However, when deploying locally, a new environment is being registered after the one that was explicitly registered, causing duplicate environments to be registered. The user is seeking a way to prevent this from happening to avoid having multiple versions of the same environment being registered.",
        "Question_gpt_summary":"user try separ environ registr deploy script studio creat python file file regist environ second file deploi model deploi local new environ regist explicitli regist caus duplic environ regist user seek wai prevent happen avoid have multipl version environ regist",
        "Answer_original_content":"rodrigomaldonado thank detail simpli regist abl us necessari spark env regist workspac environ us environ sdk frozen environ reus defin infer config deploy process core environ import environ core model import inferenceconfig spark env environ spark env custom workspac workspac infer config inferenceconfig entri script environ spark env",
        "Answer_preprocessed_content":"thank detail simpli regist abl us environ us sdk frozen environ reus defin infer config deploy process import environ import inferenceconfig workspac workspac",
        "Answer_gpt_summary_original":"the answer suggests using the `register` method to register the environment and prevent duplicate environments. it also recommends using the `get` method from the sdk to retrieve the frozen environment and reuse it when defining the inference config in the deployment process.",
        "Answer_gpt_summary":"answer suggest regist method regist environ prevent duplic environ recommend method sdk retriev frozen environ reus defin infer config deploy process"
    },
    {
        "Question_id":null,
        "Question_title":"Register Data Sets in Azure ML",
        "Question_body":"Hello dear Microsoft Q&A Team,\n\nI'm new to Azure ML and have the following problem: I can't create data assets using a datastore. I did the following:\n\nI created a datastore which points on my adls gen2 datalake using a service principal. I assigend all Storage Blob Data rolls (of the ressource group of the datalake) to this service principal. But when I try to create a dataset using this datastore the storage path does not load:\n\nAny idea what I'm doing wrong?",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1665561633607,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1044629\/register-data-sets-in-azure-ml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-21T07:48:19.023Z",
                "Answer_score":0,
                "Answer_body":"Hello @romungi-MSFT ,\n\nno worries. I was able to create a data set, but had to enter the file path manually:\n\n\n\n\n\nFor me it is fine, but I think it still not working the way it should be.\n\nCheers\n\nLukas",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"regist data set hello dear microsoft team new follow problem creat data asset datastor follow creat datastor point adl gen datalak servic princip assigend storag blob data roll ressourc group datalak servic princip try creat dataset datastor storag path load idea wrong",
        "Question_preprocessed_content":"regist data set hello dear microsoft team new follow problem creat data asset datastor follow creat datastor point adl gen datalak servic princip assigend storag blob data roll servic princip try creat dataset datastor storag path load idea wrong",
        "Question_gpt_summary_original":"The user is facing challenges in creating data assets using a datastore in Azure ML. Despite creating a datastore that points to their ADLS Gen2 datalake using a service principal and assigning all Storage Blob Data roles to the service principal, the storage path does not load when attempting to create a dataset using the datastore. The user is seeking assistance in identifying the issue.",
        "Question_gpt_summary":"user face challeng creat data asset datastor despit creat datastor point adl gen datalak servic princip assign storag blob data role servic princip storag path load attempt creat dataset datastor user seek assist identifi issu",
        "Answer_original_content":"hello romungi msft worri abl creat data set enter file path manual fine think work wai cheer luka",
        "Answer_preprocessed_content":"hello worri abl creat data set enter file path manual fine think work wai cheer luka",
        "Answer_gpt_summary_original":"possible solutions: \n- manually enter the file path to create a data set.",
        "Answer_gpt_summary":"possibl solut manual enter file path creat data set"
    },
    {
        "Question_id":null,
        "Question_title":"Cannot apply the first exp after new commit",
        "Question_body":"<p>Hello,<\/p>\n<p>Iv been trying to use dvc to manage my experiments and like it very much so for but I encountered rather unexpected behavior.<\/p>\n<p>Anytime I commit the code, the very first experiment which I performs afterwards cannot be applied to workspace. The command \u201cdvc exp run\u201d will proceed without errors and environment is modified accordingly. But when I change the environment and then try to \u201cdvc exp apply\u201d it back, no changes to the environment are made (again, no errors or warnings). Applying any other experiments works as expected.<\/p>\n<p>The minimal example is the following:<\/p>\n<p><strong>dvc.yaml:<\/strong><br>\nstages:<br>\nmain:<br>\ncmd: Rscript src\/main.R<br>\nparams:<br>\n- par.yaml:<br>\nmetrics:<br>\n- out.yaml:<br>\ncache: false<\/p>\n<p><strong>par.yaml:<\/strong><br>\na: 1<br>\nb: 1<br>\nc: 1<br>\nd: 1<\/p>\n<p><strong>main.R<\/strong><br>\nlibrary(yaml)<br>\npar \u2190 yaml.load_file(\u201cpar.yaml\u201d)<br>\nout \u2190 list(<br>\nperf = par$a * 1 + par$b * 0.1 + par$c * 0.01 + par$d * 0.001<br>\n)<br>\nprint(out)<br>\nwrite_yaml(out, \u201cout.yaml\u201d)<\/p>\n<p>However, I believe that the issue may not be with the dvc.yaml but rather in my workflow as the same happens also when creating the environment with \u201cdvc exp init\u201d and when cloning the official example <a href=\"https:\/\/github.com\/iterative\/example-dvc-experiments\" rel=\"noopener nofollow ugc\">repo<\/a>. Em I supposed to call some other command after new commit to prime the environment for new experiments? I had not found anything in the reference manual.<\/p>\n<p>Right now I\u2019m sidestepping the issue by running the first experiment twice and disregarding the first one but it is cumbersome for computationally costly experiments.<\/p>\n<p>Thank you very much for your help.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1672649058557,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":86.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cannot-apply-the-first-exp-after-new-commit\/1444",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-03T02:13:55.634Z",
                "Answer_body":"<blockquote>\n<p>But when I change the environment and then try to \u201cdvc exp apply\u201d it back<\/p>\n<\/blockquote>\n<p>Could you please clarify what you are modifying when you \u201cchange the environment\u201d before using <code>dvc exp apply<\/code>?<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-03T14:26:24.834Z",
                "Answer_body":"<p>Of course. I by that meant modifying the par.yaml either manually or via <code>-S<\/code> option and reruning the model to regenerate outputs. In particular, for the following (slightly modified) setup<\/p>\n<p><strong>dvc.yaml:<\/strong><\/p>\n<pre><code class=\"lang-auto\">stages:\n  main:\n    cmd: Rscript src\/main.R\n    deps:\n    - src\n    params:\n    - par.yaml:\n    metrics:\n    - out.yaml:\n        cache: false\n<\/code><\/pre>\n<p><strong>main.R:<\/strong><\/p>\n<pre><code class=\"lang-auto\">library(yaml)\npar &lt;- yaml.load_file(\"par.yaml\")\nb &lt;- 9\nout &lt;- list(\n    perf = par$a * 1 + b * 0.1\n)\nprint(out)\nwrite_yaml(out, \"out.yaml\")\n<\/code><\/pre>\n<p><strong>par.yaml:<\/strong><br>\n<code>a: 9<\/code><\/p>\n<p>I do the following:<\/p>\n<ul>\n<li>change values of <code>a<\/code> and <code>b<\/code> to 1 to simulate the process of working on both the code and parameters<\/li>\n<li>commit all<\/li>\n<li>run <code>dvc exp run -n exp_a<\/code> and observe that perf=1.1 is created as expected<\/li>\n<li>run <code>dvc exp run -n exp_b -S \"par.yaml:a=2\"<\/code> and observe that perf=2.1 is created as expected<\/li>\n<li>run <code>dvc exp run -n exp_c -S \"par.yaml:a=3\"<\/code> and observe that perf=3.1 is created as expected<\/li>\n<li>Now I want return to exp_a. I call <code>dvc exp apply exp_a<\/code>. However, <code>a<\/code> in par.yaml stays at its current value, in this case 3. Interestingly though, the out.yaml gets restored to the correct value of 1.1.<\/li>\n<li>If I try to apply any other exp, say exp_b or c, everything works as expected and both par.yaml and out.yaml are restored.<\/li>\n<\/ul>\n<p>I also tried to perform another experiment (say exp_0) right before the commit (so that the commit is published with the correct value of out.yaml) In this case, neither par.yaml nor out.yaml is restored when calling <code>dvc exp apply exp_a<\/code>.<\/p>\n<p>Em I using it right? Thank you.<\/p>",
                "Answer_score":71.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-05T05:39:01.342Z",
                "Answer_body":"<p>I\u2019ve confirmed that I can reproduce the issue, this is a bug in DVC. To clarify, the changes from the first exp are applied except for files where the file in the experiment result is unchanged from the file in the initial git commit. So in your case, the changes for the output <code>out.yaml<\/code> are applied, but since <code>par.yaml<\/code> is unchanged (both the original commit and the experiment result contain <code>par.yaml:a=2<\/code>) <code>exp apply<\/code> is essentially ignoring the params file and does not modify it.<\/p>\n<p>I\u2019ve opened a github issue for this bug, you can subscribe to it for further updates: <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/8764\" class=\"inline-onebox\">exp apply: git-tracked files which are unchanged from HEAD are not reset on apply \u00b7 Issue #8764 \u00b7 iterative\/dvc \u00b7 GitHub<\/a><\/p>",
                "Answer_score":11.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-05T09:26:49.318Z",
                "Answer_body":"<p>Great, thank you for opening the issue.<br>\nKeep up with the great work, it is pleasure to use DVC.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"appli exp new commit hello try us manag experi like encount unexpect behavior anytim commit code experi perform appli workspac command exp run proce error environ modifi accordingli chang environ try exp appli chang environ error warn appli experi work expect minim exampl follow yaml stage main cmd rscript src main param par yaml metric yaml cach fals par yaml main librari yaml par yaml load file par yaml list perf par par par par print write yaml yaml believ issu yaml workflow happen creat environ exp init clone offici exampl repo suppos command new commit prime environ new experi refer manual right sidestep issu run experi twice disregard cumbersom computation costli experi thank help",
        "Question_preprocessed_content":"appli exp new commit hello try us manag experi like encount unexpect behavior anytim commit code experi perform appli workspac command exp run proce error environ modifi accordingli chang environ try exp appli chang environ appli experi work expect minim exampl follow yaml stage main cmd rscript param metric cach fals librari par list print believ issu yaml workflow happen creat environ exp init clone offici exampl repo suppos command new commit prime environ new experi refer manual right sidestep issu run experi twice disregard cumbersom computation costli experi thank help",
        "Question_gpt_summary_original":"The user is encountering an issue with DVC where the first experiment after committing code cannot be applied to the workspace. The \"dvc exp run\" command proceeds without errors and modifies the environment, but when the user tries to apply the changes back using \"dvc exp apply\", no changes are made. This issue occurs even when creating the environment with \"dvc exp init\" or cloning the official example repository. The user is unsure if they need to call another command after committing to prime the environment for new experiments. The user is currently running the first experiment twice and disregarding the first one, which is cumbersome for computationally costly experiments.",
        "Question_gpt_summary":"user encount issu experi commit code appli workspac exp run command proce error modifi environ user tri appli chang exp appli chang issu occur creat environ exp init clone offici exampl repositori user unsur need command commit prime environ new experi user current run experi twice disregard cumbersom computation costli experi",
        "Answer_original_content":"chang environ try exp appli clarifi modifi chang environ exp appli cours meant modifi par yaml manual option rerun model regener output particular follow slightli modifi setup yaml stage main cmd rscript src main dep src param par yaml metric yaml cach fals main librari yaml par yaml load file par yaml list perf par print write yaml yaml par yaml follow chang valu simul process work code paramet commit run exp run exp observ perf creat expect run exp run exp par yaml observ perf creat expect run exp run exp par yaml observ perf creat expect want return exp exp appli exp par yaml stai current valu case interestingli yaml get restor correct valu try appli exp exp work expect par yaml yaml restor tri perform experi exp right commit commit publish correct valu yaml case par yaml yaml restor call exp appli exp right thank iv confirm reproduc issu bug clarifi chang exp appli file file experi result unchang file initi git commit case chang output yaml appli par yaml unchang origin commit experi result contain par yaml exp appli essenti ignor param file modifi iv open github issu bug subscrib updat exp appli git track file unchang head reset appli issu iter github great thank open issu great work pleasur us",
        "Answer_preprocessed_content":"chang environ try exp appli clarifi modifi chang environ cours meant modifi manual option rerun model regener output particular follow setup yaml follow chang valu simul process work code paramet commit run observ creat expect run observ creat expect run observ creat expect want return stai current valu case interestingli get restor correct valu try appli exp work expect restor tri perform experi right commit case restor call right thank iv confirm reproduc issu bug clarifi chang exp appli file file experi result unchang file initi git commit case chang output appli unchang essenti ignor param file modifi iv open github issu bug subscrib updat exp appli file unchang head reset appli issu iter github great thank open issu great work pleasur us",
        "Answer_gpt_summary_original":"the user is encountering an issue where the first experiment they run after a new commit cannot be applied to the workspace, despite no errors or warnings being thrown. one possible solution is to modify the par.yaml either manually or via -s option and rerun the model to regenerate outputs. however, there seems to be a bug in exp apply where git-tracked files which are unchanged from head are not reset on apply. the user can subscribe to the github issue for further updates.",
        "Answer_gpt_summary":"user encount issu experi run new commit appli workspac despit error warn thrown possibl solut modifi par yaml manual option rerun model regener output bug exp appli git track file unchang head reset appli user subscrib github issu updat"
    },
    {
        "Question_id":39251701.0,
        "Question_title":"AzureML: experiment working for a subset and not for the whole dataset",
        "Question_body":"<p>some times ago I had written a code in AzureML meeting \"out of memory\" issues. So I tried to split the code in three different codes and that partially worked. It remains a part that (I think) is affected by memory issues too.<\/p>\n\n<p>I have created an experiment that I have published in this <a href=\"http:\/\/gallery.cortanaintelligence.com\/Experiment\/TextMining-sample-NA-v1-1\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n\n<p>There is a module that considers only a sample of my dataset, and it does work. This means that the code is supposed to work correctly. If you remove the sampling code (the second module starting from the top) <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Cbzhj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Cbzhj.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>and you connect directly the original dataset you have the following situation<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/XOo8e.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XOo8e.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>producing the following error:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/mRSSQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mRSSQ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Does someone have some way to understand where Azure crashes?<\/p>\n\n<p>Thanks you,<\/p>\n\n<p>Andrea<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1472651915230,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":102.0,
        "Answer_body":"<p>Thanks so much for publishing the example -- this really helped to understand the issue. I suspect that you want to modify the <code>gsub()<\/code> calls in your script by adding the argument \"<code>fixed=TRUE<\/code>\" to each. (The documentation for this function is <a href=\"https:\/\/stat.ethz.ch\/R-manual\/R-devel\/library\/base\/html\/grep.html\" rel=\"nofollow\">here<\/a>.)<\/p>\n\n<p>What appears to have happened is that somewhere in your full dataset -- but not in the subsampled dataset -- there is some text that winds up being included in <code>df[i, \"names\"]<\/code> as \"<code>(art.<\/code>\".  Your script pads this into \"<code>\\\\b(art.\\\\b<\/code>\". The <code>gsub()<\/code> function tries to interpret this as a regular expression instead of a simple string, then throws an error because it is not a valid regular expression: it contains an opening parenthesis but no closing parenthesis. I believe that you actually did not want <code>gsub()<\/code> to interpret the input as a regular expression in the first place, and specifying <code>gsub(..., fixed=TRUE)<\/code> will correct that.<\/p>\n\n<p>I believe the reason why this error disappears when you add the sample\/partition module is because, by chance, the problematic input value was dropped on subsampling. I do not think it is an issue of available resources on Azure ML. (Caveat: I cannot confirm the fix works yet; I made the suggested update and started running the experiment, but it has not yet completed successfully.)<\/p>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/39251701",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1472676812496,
        "Question_original_content":"experi work subset dataset time ago written code meet memori issu tri split code differ code partial work remain think affect memori issu creat experi publish link modul consid sampl dataset work mean code suppos work correctli remov sampl code second modul start connect directli origin dataset follow situat produc follow error wai understand azur crash thank andrea",
        "Question_preprocessed_content":"experi work subset dataset time ago written code meet memori issu tri split code differ code partial work remain affect memori issu creat experi publish link modul consid sampl dataset work mean code suppos work correctli remov sampl code connect directli origin dataset follow situat produc follow error wai understand azur crash thank andrea",
        "Question_gpt_summary_original":"The user has encountered memory issues while working with AzureML. They tried to split the code into three different codes, which partially worked, but there is still a part that is affected by memory issues. The experiment created by the user works for a subset of the dataset, but not for the whole dataset, resulting in an error. The user is seeking help to understand where Azure crashes.",
        "Question_gpt_summary":"user encount memori issu work tri split code differ code partial work affect memori issu experi creat user work subset dataset dataset result error user seek help understand azur crash",
        "Answer_original_content":"thank publish exampl help understand issu suspect want modifi gsub call script ad argument fix true document function appear happen dataset subsampl dataset text wind includ name art script pad art gsub function tri interpret regular express instead simpl string throw error valid regular express contain open parenthesi close parenthesi believ actual want gsub interpret input regular express place specifi gsub fix true correct believ reason error disappear add sampl partit modul chanc problemat input valu drop subsampl think issu avail resourc caveat confirm fix work suggest updat start run experi complet successfulli",
        "Answer_preprocessed_content":"thank publish exampl help understand issu suspect want modifi call script ad argument appear happen dataset subsampl dataset text wind includ script pad function tri interpret regular express instead simpl string throw error valid regular express contain open parenthesi close parenthesi believ actual want interpret input regular express place specifi correct believ reason error disappear add modul chanc problemat input valu drop subsampl think issu avail resourc",
        "Answer_gpt_summary_original":"the solution to the problem of encountering an error while running an experiment on the full dataset but not on the subsampled dataset is to modify the gsub() calls in the script by adding the argument \"fixed=true\" to each. this will prevent gsub() from interpreting the input as a regular expression and correct the error. the reason why the error disappears when adding the sample\/partition module is that the problematic input value was dropped on subsampling.",
        "Answer_gpt_summary":"solut problem encount error run experi dataset subsampl dataset modifi gsub call script ad argument fix true prevent gsub interpret input regular express correct error reason error disappear ad sampl partit modul problemat input valu drop subsampl"
    },
    {
        "Question_id":null,
        "Question_title":"Deploy ML model to Kubernetes + overwrite previous endpoint",
        "Question_body":"I'm building a CI\/CD pipeline in Azure DevOps for the deployment of my Machine Learning model to Azure Kubernetes Service. I have the following task in my YAML pipeline file (replaced some of the values with '...'):\n\n - task: AzureCLI@1\n   displayName: \"Deploy to AKS\"\n   inputs:\n     azureSubscription: '...'\n     scriptLocation: inlineScript\n     workingDirectory: $(Build.SourcesDirectory)\/score\n     inlineScript: |\n       set -e # fail on error\n             \n       az ml model deploy --name 'aks-deploy-test' --model '$(MODEL_NAME):$(get_model.MODEL_VERSION)' \\\n       --compute-target $(AKS_COMPUTE_NAME) \\\n       --ic inference_config.yml \\\n       --dc deployment_config_aks.yml \\\n       -g ... --workspace-name ... \\\n       --overwrite -v\n\n\n\nWhen I run the pipeline the first time, it successfully deployed the ML model and I can see the Endpoint in the Azure ML workspace. However, when I try to run the pipeline a second time (to deploy a newer version of the model), I get the error:\n\n Error:\n {\n   \"code\": \"KubernetesError\",\n   \"statusCode\": 400,\n   \"message\": \"Kubernetes Deployment Error\",\n   \"details\": [\n     {\n       \"code\": \"Unschedulable\",\n       \"message\": \"0\/6 nodes are available: 4 Insufficient cpu, 6 Insufficient memory.\"\n     },\n     {\n       \"code\": \"DeploymentFailed\",\n       \"message\": \"Couldn't schedule because the kubernetes cluster didn't have available resources after trying for 00:05:00.\\nYou can address this error by either adding more nodes, changing the SKU of your nodes or changing the resource requirements of your service.\\nPlease refer to https:\/\/aka.ms\/debugimage#container-cannot-be-scheduled for more information.\"\n     }\n   ]\n }\n\n\n\nIsn't the --overwrite option in the az ml model deploy command supposed to completely overwrite the current deployment of the model? If so, why am I still getting this error, or is there a better way to deploy a newer version of the ML model to the same AKS cluster?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1598425965377,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/77362\/deploy-ml-model-to-kubernetes-overwrite-previous-e.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-08-28T04:02:16.747Z",
                "Answer_score":0,
                "Answer_body":"@AxelVandevelde Thanks for the question. The error message literally indicates the issue, none of the nodes have CPUs available.\nCan you please add more details about the sku's. If Possible please share the link to the tutorial\/documentation you were following.\n\nEnable AppInsights flag will start flowing the logs (for Application monitoring).\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-enable-app-insights\n\n\n\n\nFor cluster monitoring you can use log analytics via AKS.\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/aks\/view-master-logs",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"deploi model kubernet overwrit previou endpoint build pipelin azur devop deploy machin learn model azur kubernet servic follow task yaml pipelin file replac valu task azurecli displaynam deploi ak input azuresubscript scriptloc inlinescript workingdirectori build sourcesdirectori score inlinescript set fail error model deploi ak deploi test model model model model version comput target ak comput infer config yml deploy config ak yml workspac overwrit run pipelin time successfulli deploi model endpoint workspac try run pipelin second time deploi newer version model error error code kuberneteserror statuscod messag kubernet deploy error detail code unschedul messag node avail insuffici cpu insuffici memori code deploymentfail messag couldn schedul kubernet cluster avail resourc try nyou address error ad node chang sku node chang resourc requir servic npleas refer http aka debugimag contain schedul inform isn overwrit option model deploi command suppos complet overwrit current deploy model get error better wai deploi newer version model ak cluster",
        "Question_preprocessed_content":"deploi model kubernet overwrit previou endpoint build pipelin azur devop deploy machin learn model azur kubernet servic follow task yaml pipelin file task azurecli displaynam deploi ak input azuresubscript scriptloc inlinescript workingdirectori inlinescript set fail error model deploi run pipelin time successfulli deploi model endpoint workspac try run pipelin second time error error isn option model deploi command suppos complet overwrit current deploy model get error better wai deploi newer version model ak cluster",
        "Question_gpt_summary_original":"The user is facing challenges while deploying a newer version of their Machine Learning model to Azure Kubernetes Service using Azure DevOps. The first deployment was successful, but the second deployment failed with a Kubernetes Deployment Error due to insufficient resources. The user is questioning the effectiveness of the --overwrite option in the az ml model deploy command and seeking a better way to deploy a newer version of the ML model to the same AKS cluster.",
        "Question_gpt_summary":"user face challeng deploi newer version machin learn model azur kubernet servic azur devop deploy success second deploy fail kubernet deploy error insuffici resourc user question effect overwrit option model deploi command seek better wai deploi newer version model ak cluster",
        "Answer_original_content":"axelvandeveld thank question error messag liter indic issu node cpu avail add detail sku possibl share link tutori document follow enabl appinsight flag start flow log applic monitor http doc microsoft com azur machin learn servic enabl app insight cluster monitor us log analyt ak http doc microsoft com azur ak view master log",
        "Answer_preprocessed_content":"thank question error messag liter indic issu node cpu avail add detail sku possibl share link follow enabl appinsight flag start flow log cluster monitor us log analyt ak",
        "Answer_gpt_summary_original":"possible solutions from the answer include checking the available cpus on the nodes, providing more details about the sku's, enabling app insights for application monitoring, and using log analytics via aks for cluster monitoring.",
        "Answer_gpt_summary":"possibl solut answer includ check avail cpu node provid detail sku enabl app insight applic monitor log analyt ak cluster monitor"
    },
    {
        "Question_id":27987910.0,
        "Question_title":"Azure Machine Learning - CORS",
        "Question_body":"<p>I've searched for hours for this and can't find a single thing that answers the question. I've created and published a new Azure Machine Learning service, and have created an endpoint. I can call the service using the Postman REST CLient, but accessing it via a JavaScript webpage returns a console log saying that CORS is enabled for the service. Now, for the life of me, I can't figure out how to disable CORS for Azure Machine Learning services. Any help would be much appreciated, thanks!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":4,
        "Question_creation_time":1421424031317,
        "Question_favorite_count":null,
        "Question_last_edit_time":1454343355232,
        "Question_score":12.0,
        "Question_view_count":3242.0,
        "Answer_body":"<p>Currently, we don't support disabling CORS on API side but you can either use the above option or you can use the API management service to disable CORS. The links below should help you with this<\/p>\n\n<p>Here are the links: <a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/api-management-get-started\/\" rel=\"noreferrer\">step by step<\/a> guide, also this <a href=\"http:\/\/channel9.msdn.com\/Blogs\/AzureApiMgmt\/Last-mile-Security\" rel=\"noreferrer\">video<\/a> on setting headers, and <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn894084.aspx#JSONP\" rel=\"noreferrer\">this doc<\/a> on policies.<\/p>\n\n<p>API Management service allow CORS by enabling it in the API configuration page<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/27987910",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1454387492627,
        "Question_original_content":"cor search hour singl thing answer question creat publish new servic creat endpoint servic postman rest client access javascript webpag return consol log sai cor enabl servic life figur disabl cor servic help appreci thank",
        "Question_preprocessed_content":"cor search hour singl thing answer question creat publish new servic creat endpoint servic postman rest client access javascript webpag return consol log sai cor enabl servic life figur disabl cor servic help appreci thank",
        "Question_gpt_summary_original":"The user is facing challenges in disabling CORS for their Azure Machine Learning service, which is preventing them from accessing the service via a JavaScript webpage. They have tried searching for a solution but have not found any helpful information.",
        "Question_gpt_summary":"user face challeng disabl cor servic prevent access servic javascript webpag tri search solut help inform",
        "Answer_original_content":"current support disabl cor api us option us api manag servic disabl cor link help link step step guid video set header doc polici api manag servic allow cor enabl api configur page",
        "Answer_preprocessed_content":"current support disabl cor api us option us api manag servic disabl cor link help link step step guid video set header doc polici api manag servic allow cor enabl api configur page",
        "Answer_gpt_summary_original":"possible solutions to disabling cors for a newly created and published service include using the api management service to disable cors or using the options provided in the links provided, such as a step-by-step guide, a video on setting headers, and a document on policies. enabling cors can be done by enabling it in the api configuration page of the api management service.",
        "Answer_gpt_summary":"possibl solut disabl cor newli creat publish servic includ api manag servic disabl cor option provid link provid step step guid video set header document polici enabl cor enabl api configur page api manag servic"
    },
    {
        "Question_id":null,
        "Question_title":"(UserError) Error when parsing request; unable to deserialize request body",
        "Question_body":"Hi, getting this error when i run an azureml experiment with custom_docker_image (basegpu image of mcr) - can anybody help me understand this? Have tested this in local compute and it works, not sure why this does not work on a training cluster vm?\n\n\n\n\n    azureml._restclient.exceptions.ServiceException: ServiceException:\n         Code: 400\n         Message: (UserError) Error when parsing request; unable to deserialize request body\n         Details:\n        \n         Headers: {\n             \"Date\": \"Mon, 08 Jun 2020 11:03:52 GMT\",\n             \"Content-Type\": \"application\/json; charset=utf-8\",\n             \"Transfer-Encoding\": \"chunked\",\n             \"Connection\": \"keep-alive\",\n             \"Request-Context\": \"appId=cid-v1:6a27ce65-5555-41a3-85f7-b7a1ce31fd6b\",\n             \"x-ms-response-type\": \"error\",\n             \"Strict-Transport-Security\": \"max-age=15724800; includeSubDomains; preload\"\n         }\n         InnerException: {\n         \"additional_properties\": {},\n         \"error\": {\n             \"additional_properties\": {},\n             \"code\": \"UserError\",\n             \"message\": \"Error when parsing request; unable to deserialize request body\",\n             \"details_uri\": null,\n             \"target\": null,\n             \"details\": [],\n             \"inner_error\": null,\n             \"debug_info\": null,\n             \"message_format\": null,\n             \"message_parameters\": null,\n             \"reference_code\": null\n         },\n         \"correlation\": {\n             \"operation\": \"e96d6285280f5849a4a5e3f172d65d36\",\n             \"request\": \"1beee8ecb7180147\"\n         },\n         \"environment\": \"westeurope\",\n         \"location\": \"westeurope\",\n         \"time\": {}\n     }",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1591616365497,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"My bad, after giving it some days and looking at the code, I noticed i had forgotten to add the parameters for the estimator configuration. Here is the estimator configuration that works for me:\n\n\n\n estimator = Estimator(source_directory=experiment_folder,\n                       compute_target=compute_target,\n                       script_params=script_params,\n                       entry_script='rps_efn_b0.py',\n                       node_count=1,        \n                       conda_packages=['ipykernel'],\n                       pip_packages = ['azureml-sdk',\n                                       'pyarrow',\n                                       'pyspark',\n                                       'azureml-mlflow',\n                                       'joblib',\n                                       'matplotlib',\n                                       'Pillow',\n                                       'tensorflow==2.2',\n                                       'tensorflow-datasets',\n                                       'tensorflow-hub',\n                                       'azureml-defaults',\n                                       'azureml-dataprep[fuse,pandas]'],\n                       custom_docker_image='mcr.microsoft.com\/azureml\/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04')",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/33313\/usererror-error-when-parsing-request-unable-to-des.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-11T14:05:05.637Z",
                "Answer_score":1,
                "Answer_body":"My bad, after giving it some days and looking at the code, I noticed i had forgotten to add the parameters for the estimator configuration. Here is the estimator configuration that works for me:\n\n\n\n estimator = Estimator(source_directory=experiment_folder,\n                       compute_target=compute_target,\n                       script_params=script_params,\n                       entry_script='rps_efn_b0.py',\n                       node_count=1,        \n                       conda_packages=['ipykernel'],\n                       pip_packages = ['azureml-sdk',\n                                       'pyarrow',\n                                       'pyspark',\n                                       'azureml-mlflow',\n                                       'joblib',\n                                       'matplotlib',\n                                       'Pillow',\n                                       'tensorflow==2.2',\n                                       'tensorflow-datasets',\n                                       'tensorflow-hub',\n                                       'azureml-defaults',\n                                       'azureml-dataprep[fuse,pandas]'],\n                       custom_docker_image='mcr.microsoft.com\/azureml\/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04')",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2020-06-09T14:44:45.27Z",
                "Answer_score":0,
                "Answer_body":"@sadeghHosseinpoor-9324 Apologies for not responding on the previous thread as we are having platform issues on providing response to some threads.\n\nIn this case having access to the backend details of your experiment run could help to suggest what changes are required in your current experiment setup. This can be addressed by our support team if you can raise a support ticket against this resource from the azure portal.\n\nMeanwhile, I have tried to lookup similar error messages faced by other users and based on your scenario are you installing any package in your environment with package name and version which is failing to read the request json? I found one user who corrected this error during environment setup by correcting the request for spark package installation in their environment.\n\nBefore:\n\n environment.spark.packages=[SparkPackage(group='com.microsoft.azure', artifact='azure:azure-sqldb-spark', version='1.0.2')]\n\n\n\nAfter:\n\n environment.spark.packages=[{'group':'com.microsoft.azure', 'artifact': 'azure-sqldb-spark', 'version':'1.0.2'}]\n\n\n\n\nIs there any similar requests in your case?",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":3.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1591884305636,
        "Question_original_content":"usererror error pars request unabl deseri request bodi get error run experi custom docker imag basegpu imag mcr anybodi help understand test local comput work sure work train cluster restclient except serviceexcept serviceexcept code messag usererror error pars request unabl deseri request bodi detail header date mon jun gmt content type applic json charset utf transfer encod chunk connect aliv request context appid cid ac bacefdb respons type error strict transport secur max ag includesubdomain preload innerexcept addit properti error addit properti code usererror messag error pars request unabl deseri request bodi detail uri null target null detail inner error null debug info null messag format null messag paramet null refer code null correl oper edfaaefdd request becb environ westeurop locat westeurop time",
        "Question_preprocessed_content":"error pars request unabl deseri request bodi get error run experi anybodi help understand test local comput work sure work train cluster serviceexcept code messag error pars request unabl deseri request bodi detail header innerexcept error code usererror messag error pars request unabl deseri request bodi null target null detail null null null null null correl environ westeurop locat westeurop time",
        "Question_gpt_summary_original":"The user encountered an error when running an AzureML experiment with a custom Docker image on a training cluster VM. The error message indicates that there was an issue parsing the request and deserializing the request body, resulting in a 400 error. The user is seeking help to understand why this error occurred.",
        "Question_gpt_summary":"user encount error run experi custom docker imag train cluster error messag indic issu pars request deseri request bodi result error user seek help understand error occur",
        "Answer_original_content":"bad give dai look code notic forgotten add paramet estim configur estim configur work estim estim sourc directori experi folder comput target comput target script param script param entri script rp efn node count conda packag ipykernel pip packag sdk pyarrow pyspark joblib matplotlib pillow tensorflow tensorflow dataset tensorflow hub default dataprep fuse panda custom docker imag mcr microsoft com base gpu openmpi cuda cudnn ubuntu",
        "Answer_preprocessed_content":"bad give dai look code notic forgotten add paramet estim configur estim configur work estim",
        "Answer_gpt_summary_original":"the answer provides a possible solution to an error encountered when running an experiment with a custom docker image on a training cluster vm. the solution involves adding parameters for the estimator configuration, including specifying the source directory, compute target, script parameters, entry script, node count, conda packages, pip packages, and custom docker image.",
        "Answer_gpt_summary":"answer provid possibl solut error encount run experi custom docker imag train cluster solut involv ad paramet estim configur includ specifi sourc directori comput target script paramet entri script node count conda packag pip packag custom docker imag"
    },
    {
        "Question_id":null,
        "Question_title":"Idle shutdown for user-managed notebook (vertex-AI)",
        "Question_body":"There are two types of notebooks in Vertex-AI1) managed notebook: https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/managed\/introduction2) user-managed notebook: https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/introductionI see that the former has a useful function called \"idle shutdown\" that help manage costs: managed notebooks instances shut down after being idle for a specific time period by default.Why we didn't make it available for user-managed notebook as well? Thanks",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654153800000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":188.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Idle-shutdown-for-user-managed-notebook-vertex-AI\/td-p\/428171\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-02T22:36:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Thanks for the feedback here. We are aware of the request and this is further to prioritize this work. Happy to get back when we have a concrete plan for this feature."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"idl shutdown user manag notebook vertex type notebook vertex manag notebook http cloud googl com vertex doc workbench manag introduct user manag notebook http cloud googl com vertex doc workbench user manag introductioni us function call idl shutdown help manag cost manag notebook instanc shut idl specif time period default avail user manag notebook thank",
        "Question_preprocessed_content":"idl shutdown notebook type notebook manag notebook notebook us function call idl shutdown help manag cost manag notebook instanc shut idl specif time period avail notebook thank",
        "Question_gpt_summary_original":"The user is facing a challenge with the lack of an \"idle shutdown\" function in the user-managed notebook in Vertex-AI, which is available in the managed notebook. This function helps manage costs by shutting down instances after a specific idle time period. The user is questioning why this function is not available for the user-managed notebook as well.",
        "Question_gpt_summary":"user face challeng lack idl shutdown function user manag notebook vertex avail manag notebook function help manag cost shut instanc specif idl time period user question function avail user manag notebook",
        "Answer_original_content":"thank feedback awar request priorit work happi concret plan featur",
        "Answer_preprocessed_content":"thank feedback awar request priorit work happi concret plan featur",
        "Answer_gpt_summary_original":"there is currently no \"idle shutdown\" feature for user-managed notebooks in vertex-ai. however, the team is aware of the request and is working to prioritize this feature. they will provide a concrete plan for this feature in the future.",
        "Answer_gpt_summary":"current idl shutdown featur user manag notebook vertex team awar request work priorit featur provid concret plan featur futur"
    },
    {
        "Question_id":72203674.0,
        "Question_title":"Deploy AWS SageMaker pipeline using the cloud development kit (CDK)",
        "Question_body":"<p>I'm looking to deploy the SageMaker pipeline using CDK (<a href=\"https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html<\/a>) but could not find any code examples. Any pointers?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1652282728443,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":237.0,
        "Answer_body":"<p>CDK L1 Constructs correspond 1:1 to a CloudFormation resource of the same name. The construct props match the resouce properties.  The go-to source is therefore the CloudFormation docs.<\/p>\n<p>The <code>AWS::SageMaker::Pipeline<\/code> <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html#aws-resource-sagemaker-pipeline--examples\" rel=\"nofollow noreferrer\">docs have a more complete example<\/a>.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72203674",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1652283812907,
        "Question_original_content":"deploi pipelin cloud develop kit cdk look deploi pipelin cdk http doc aw amazon com cdk api doc aw cdk lib aw cfnpipelin html code exampl pointer",
        "Question_preprocessed_content":"deploi pipelin cloud develop kit look deploi pipelin cdk code exampl pointer",
        "Question_gpt_summary_original":"The user is facing challenges in finding code examples to deploy the SageMaker pipeline using CDK.",
        "Question_gpt_summary":"user face challeng find code exampl deploi pipelin cdk",
        "Answer_original_content":"cdk construct correspond cloudform resourc construct prop match resouc properti sourc cloudform doc aw pipelin doc complet exampl",
        "Answer_preprocessed_content":"cdk construct correspond cloudform resourc construct prop match resouc properti sourc cloudform doc doc complet exampl",
        "Answer_gpt_summary_original":"possible solutions to the user's difficulty in finding code examples for deploying a pipeline using cdk are: \n1. refer to the cloudformation documentation as cdk l1 constructs correspond 1:1 to a cloudformation resource of the same name and the construct props match the resource properties.\n2. check out the aws::::pipeline documentation for a more complete example.",
        "Answer_gpt_summary":"possibl solut user difficulti find code exampl deploi pipelin cdk refer cloudform document cdk construct correspond cloudform resourc construct prop match resourc properti check aw pipelin document complet exampl"
    },
    {
        "Question_id":null,
        "Question_title":"Deploy problems",
        "Question_body":"Hi guys!\n\n\nI'm new with MLFlow and I would like to deploy a simple model on SageMaker but I got some problems.\n\n\n\nI have trained a model locally and successfully generated a docker image and uploaded it to AWS ECR but, when I called deploy method I got the following error:\u00a0MlflowException: Run '6b33f54c1b2541da8ec95152ab5f566b' not found\n\n\nI'm using MLFlow\u00a00.9.1 and I'm following this tutorial:\u00a0https:\/\/docs.databricks.com\/_static\/notebooks\/mlflow\/mlflow-quick-start-deployment-aws.html\n\n\nCould someone help me please =D S2\u00a0\n\n\nThanks a lot!",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1557760602000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":27.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/mZCOZ5FCQaM",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-05-13T16:17:17",
                "Answer_body":"Hi Rafael.\u00a0\n\n\nThe exception you got signals that MLflow was not able to access the run at the time you called the deploy. What is your mlflow set up? Do you run against a remote tracking server or the default file based one? The local file-based store will by default store the data on a relative path, so you need to make sure that you logged to the same location you are trying to read from when you deploy. You can look for mlruns directory and see if it has run you are trying to deploy. You can set location of the store data by setting MLFLOW_TRACKING_URI environment variable.\n\n\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/68f260a8-ea05-4690-af08-b0796065d255%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2019-05-16T12:27:48",
                "Answer_body":"Hi Tomas\n\nI finally got to put the trained model on sagemaker, I was actually running the commands in the wrong folder, but there were some issues with the AWS configuration as well. Thanks a lot for\u00a0your help!\n\n\nNow that I got it I have different questions like how can I \"integrate\" my Jupyter Notebook experiments with MLFlow but I'll check the mail list history for that.\n\n\nThanks!\nAtenciosamente,\nRafael J. R. Novello\n\n\nSkype: rafael.novello\nBlog:\u00a0http:\/\/rafanovello.blogspot.com.br\/\n\n\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"deploi problem gui new like deploi simpl model got problem train model local successfulli gener docker imag upload aw ecr call deploi method got follow error except run bfcbdaecabfb follow tutori http doc databrick com static notebook quick start deploy aw html help thank lot",
        "Question_preprocessed_content":"deploi problem gui new like deploi simpl model got problem train model local successfulli gener docker imag upload aw ecr call deploi method got follow error except run follow help thank lot",
        "Question_gpt_summary_original":"The user is facing challenges while trying to deploy a simple ML model on SageMaker using MLFlow. They have successfully generated a docker image and uploaded it to AWS ECR, but when they called the deploy method, they received an error stating that the run was not found. The user is using MLFlow 0.9.1 and following a tutorial, but they require assistance to resolve the issue.",
        "Question_gpt_summary":"user face challeng try deploi simpl model successfulli gener docker imag upload aw ecr call deploi method receiv error state run user follow tutori requir assist resolv issu",
        "Answer_original_content":"rafael except got signal abl access run time call deploi set run remot track server default file base local file base store default store data rel path need sure log locat try read deploi look mlrun directori run try deploi set locat store data set track uri environ variabl receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user googlegroup com option visit http group googl com optout toma final got train model actual run command wrong folder issu aw configur thank lot foryour help got differ question like integr jupyt notebook experi check mail list histori thank atenciosament rafael novello skype rafael novello blog http rafanovello blogspot com",
        "Answer_preprocessed_content":"rafael except got signal abl access run time call deploi set run remot track server default file base local store default store data rel path need sure log locat try read deploi look mlrun directori run try deploi set locat store data set environ variabl receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit toma final got train model actual run command wrong folder issu aw configur thank lot foryour help got differ question like integr jupyt notebook experi check mail list histori thank atenciosament rafael novello skype",
        "Answer_gpt_summary_original":"Solutions provided: \n- Check the MLflow set up and make sure that the data is stored in the same location that is being read from when deploying. \n- Set the location of the store data by setting the MLFLOW_TRACKING_URI environment variable. \n\nNo solution provided for integrating Jupyter Notebook experiments with MLFlow.",
        "Answer_gpt_summary":"solut provid check set sure data store locat read deploi set locat store data set track uri environ variabl solut provid integr jupyt notebook experi"
    },
    {
        "Question_id":null,
        "Question_title":"DVC with git worktree",
        "Question_body":"<p>Hi all,<\/p>\n<p>I\u2019ve recently started using git worktree to practice parallel development. It comes very handy when I want to work on two branches in the same repo at the same time.<\/p>\n<p>However, I\u2019m came across issue with adding data to DVC on a tree other than the original dir.<\/p>\n<p>I wonder if anyone has experience on this?<\/p>\n<p>Many thanks,<br>\nMaggie<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1602863664967,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":371.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-with-git-worktree\/532",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-16T23:02:12.031Z",
                "Answer_body":"<p>Hi Maggie,<\/p>\n<p>I don\u2019t think DVC has been tested with <code>git worktree<\/code> and I personally don\u2019t have experience on it but it seems interesting.<\/p>\n<p>Can you please share the full list of commands you use and what the issue is so we can try it? It\u2019s hard to give you an answer without these specifics.<\/p>\n<p>Thanks<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"git worktre iv recent start git worktre practic parallel develop come handi want work branch repo time came issu ad data tree origin dir wonder experi thank maggi",
        "Question_preprocessed_content":"git worktre iv recent start git worktre practic parallel develop come handi want work branch repo time came issu ad data tree origin dir wonder experi thank maggi",
        "Question_gpt_summary_original":"The user is experiencing challenges with adding data to DVC on a tree other than the original directory while using git worktree for parallel development. They are seeking advice and experiences from others who may have encountered similar issues.",
        "Question_gpt_summary":"user experienc challeng ad data tree origin directori git worktre parallel develop seek advic experi encount similar issu",
        "Answer_original_content":"maggi dont think test git worktre person dont experi interest share list command us issu try hard answer specif thank",
        "Answer_preprocessed_content":"maggi dont think test person dont experi interest share list command us issu try hard answer specif thank",
        "Answer_gpt_summary_original":"there are no specific solutions provided in the answer. the responder is asking for more information about the issue and the commands used by maggie to add data to a git worktree other than the original directory. once they have more information, they can try to help maggie with her issue.",
        "Answer_gpt_summary":"specif solut provid answer respond ask inform issu command maggi add data git worktre origin directori inform try help maggi issu"
    },
    {
        "Question_id":null,
        "Question_title":"Not able to read variable in custom RStudio open-source application , Azure ML",
        "Question_body":"In Azure ML we have created compute instance by using setup shell script and we also installed custom RStudio open-source application using below MS docs\nhttps:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=azure-studio\n\nIn setup shell script we define environment variable\n\nCreate .Renviron file\n\nr_env_file=\"\/home\/${aml_user}\/.Renviron\"\n\necho \"Creating R environment variables\"\nsudo tee \"${r_env_file}\" > \/dev\/null <<EOF\nAZURE_TENANT_ID=${AZURE_TENANT_ID}\nAZURE_CLIENT_ID=${AZURE_CLIENT_ID}\nAZURE_CLIENT_SECRET=${AZURE_CLIENT_SECRET}\nDEFAULT_ADLS_ACCOUNT=${DEFAULT_ADLS_ACCOUNT}\nDEFAULT_ADLS_CONTAINER=${DEFAULT_ADLS_CONTAINER}\nEOF\n\nsudo chown \"${aml_user}\" \"${r_env_file}\"\nsudo chmod 600 \"${r_env_file}\"\n\nIn terminal we are able to read above variable\n\n\nHowever, we are not able to read above variable in custom RStudio open-source application.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1665035652217,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1037115\/not-able-to-read-variable-in-custom-rstudio-open-s.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-07T03:08:25.81Z",
                "Answer_score":0,
                "Answer_body":"@SHAIKHAlifAbdul-9052 Thanks for the question. Can you please add more details about the usecase that you are trying?. The Azure Machine Learning SDK for R was deprecated at the end of 2021 to make way for an improved R training and deployment experience using Azure Machine Learning CLI 2.0. See the samples repository below to get started with the 2.0 CLI.\n\nThis directory provides sample code to run R using the Azure ML CLI (v2).",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"abl read variabl custom rstudio open sourc applic creat comput instanc setup shell script instal custom rstudio open sourc applic doc http learn microsoft com azur machin learn creat manag comput instanc tab azur studio setup shell script defin environ variabl creat renviron file env file home aml user renviron echo creat environ variabl sudo tee env file dev null eof azur tenant azur tenant azur client azur client azur client secret azur client secret default adl account default adl account default adl contain default adl contain eof sudo chown aml user env file sudo chmod env file termin abl read variabl abl read variabl custom rstudio open sourc applic",
        "Question_preprocessed_content":"abl read variabl custom rstudio applic creat comput instanc setup shell script instal custom rstudio applic doc setup shell script defin environ variabl creat renviron file echo creat environ variabl sudo tee eof eof sudo chown sudo chmod termin abl read variabl abl read variabl custom rstudio applic",
        "Question_gpt_summary_original":"The user has encountered a challenge where they are unable to read environment variables defined in a setup shell script in a custom RStudio open-source application created in Azure ML, despite being able to read them in the terminal.",
        "Question_gpt_summary":"user encount challeng unabl read environ variabl defin setup shell script custom rstudio open sourc applic creat despit abl read termin",
        "Answer_original_content":"shaikhalifabdul thank question add detail usecas try sdk deprec end wai improv train deploy experi cli sampl repositori start cli directori provid sampl code run cli",
        "Answer_preprocessed_content":"thank question add detail usecas try sdk deprec end wai improv train deploy experi cli sampl repositori start cli directori provid sampl code run cli",
        "Answer_gpt_summary_original":"there are no specific solutions provided in the answer for the challenge of reading environment variables in a custom rstudio open-source application. however, the answer suggests that the user should provide more details about their use case. additionally, the answer mentions that the sdk for r was deprecated at the end of 2021 and suggests using the cli 2.0 for improved r training and deployment experience. the answer also provides a link to a samples repository to get started with the cli 2.0.",
        "Answer_gpt_summary":"specif solut provid answer challeng read environ variabl custom rstudio open sourc applic answer suggest user provid detail us case addition answer mention sdk deprec end suggest cli improv train deploy experi answer provid link sampl repositori start cli"
    },
    {
        "Question_id":61520346.0,
        "Question_title":"No such file or directory: 'docker': 'docker' when running sagemaker studio in local mode",
        "Question_body":"<p>I try to train a pytorch model on amazon sagemaker studio.<\/p>\n\n<p>It's working when I use an EC2 for training with:<\/p>\n\n<pre><code>estimator = PyTorch(entry_point='train_script.py',\n                role=role,\n                sagemaker_session = sess,\n                train_instance_count=1,\n                train_instance_type='ml.c5.xlarge',\n                framework_version='1.4.0', \n                source_dir='.',\n                git_config=git_config, \n               )\nestimator.fit({'stockdata': data_path})\n<\/code><\/pre>\n\n<p>and it's work on local mode in classic sagemaker notebook (non studio) with:<\/p>\n\n<pre><code> estimator = PyTorch(entry_point='train_script.py',\n                role=role,\n                train_instance_count=1,\n                train_instance_type='local',\n                framework_version='1.4.0', \n                source_dir='.',\n                git_config=git_config, \n               )\nestimator.fit({'stockdata': data_path})\n<\/code><\/pre>\n\n<p>But when I use it the same code (with train_instance_type='local') on sagemaker studio it doesn't work and I have the following error: No such file or directory: 'docker': 'docker'<\/p>\n\n<p>I tried to install docker with pip install but the docker command is not found if use it in terminal<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1588239469173,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":1884.0,
        "Answer_body":"<p>This indicates that there is a problem finding the Docker service.<\/p>\n<p>By default, the Docker is not installed in the SageMaker Studio  (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/656#issuecomment-632170943\" rel=\"nofollow noreferrer\">confirming github ticket response<\/a>).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1620410809823,
        "Answer_score":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61520346",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1596561017227,
        "Question_original_content":"file directori docker docker run studio local mode try train pytorch model studio work us train estim pytorch entri point train script role role session sess train instanc count train instanc type xlarg framework version sourc dir git config git config estim fit stockdata data path work local mode classic notebook non studio estim pytorch entri point train script role role train instanc count train instanc type local framework version sourc dir git config git config estim fit stockdata data path us code train instanc type local studio work follow error file directori docker docker tri instal docker pip instal docker command us termin",
        "Question_preprocessed_content":"file directori docker docker run studio local mode try train pytorch model studio work us train work local mode classic notebook us code studio work follow error file directori docker docker tri instal docker pip instal docker command us termin",
        "Question_gpt_summary_original":"The user is encountering an error when trying to train a PyTorch model on Amazon SageMaker Studio in local mode. The same code works on an EC2 instance and in classic SageMaker notebook in local mode, but not in SageMaker Studio. The error message states \"No such file or directory: 'docker': 'docker'\" and the user has attempted to install docker with pip but the command is not found in the terminal.",
        "Question_gpt_summary":"user encount error try train pytorch model studio local mode code work instanc classic notebook local mode studio error messag state file directori docker docker user attempt instal docker pip command termin",
        "Answer_original_content":"indic problem find docker servic default docker instal studio confirm github ticket respons",
        "Answer_preprocessed_content":"indic problem find docker servic default docker instal studio",
        "Answer_gpt_summary_original":"summary: the user is facing an error message while training a pytorch model on studio in local mode. the error message indicates that the docker service is not found. the solution is to confirm that docker is not installed by default in studio, as per the github ticket response.",
        "Answer_gpt_summary":"summari user face error messag train pytorch model studio local mode error messag indic docker servic solut confirm docker instal default studio github ticket respons"
    },
    {
        "Question_id":71185505.0,
        "Question_title":"XGBClassifer, when de-serialized, gives 'XGBModel' object has no attribute 'enable_categorical'",
        "Question_body":"<p>I have a serialized XGBClassifier object, trained and generated using xgboost=1.5.2.<\/p>\n<pre><code>XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n              colsample_bynode=1, colsample_bytree=0.30140958911801474,\n              eval_metric='logloss', gamma=0.1203484640861413, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_bin=368, max_delta_step=0, max_depth=6,\n              min_child_weight=1, missing=nan, monotone_constraints='()',\n              n_estimators=100, n_jobs=6, num_parallel_tree=1, random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n              single_precision_histogram=True, subsample=0.976171515775659,\n              tree_method='gpu_hist', use_label_encoder=False,\n              validate_parameters=1, verbosity=None)\n<\/code><\/pre>\n<p>I load the object using:<\/p>\n<pre><code>clf_model = joblib.load(model_path)\n<\/code><\/pre>\n<p>I want to use the object to predict on some data I am using Azure environment which also has xgboost=1.5.2. but it gives error:<\/p>\n<pre><code>File &quot;score.py&quot;, line 78, in score_execution\n[stderr]    clf_preds = clf_model.predict(clf_data_transformed)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 1284, in predict\n[stderr]    class_probs = super().predict(\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 879, in predict\n[stderr]    if self._can_use_inplace_predict():\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 811, in _can_use_inplace_predict\n[stderr]    predictor = self.get_params().get(&quot;predictor&quot;, None)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 505, in get_params\n[stderr]    params.update(cp.__class__.get_params(cp, deep))\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 502, in get_params\n[stderr]    params = super().get_params(deep)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/sklearn\/base.py&quot;, line 210, in get_params\n[stderr]    value = getattr(self, key)\n[stderr]AttributeError: 'XGBModel' object has no attribute 'enable_categorical'\n<\/code><\/pre>\n<p>We have same version in pipelines that produce\/serialize the model and in the pipeline that deserialize the model to predict on new data.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645277798680,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":358.0,
        "Answer_body":"<p>Here are some possible solutions :<\/p>\n<ul>\n<li>Save the model in some other way, e.g. the JSON specified here <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html<\/a><\/li>\n<li>Limit the allowed range of xgboost versions to those that are known to work with our model. This could lead to issues in the future, for example if the aging version of xgboost we require is no longer supported by newer versions of Python.<\/li>\n<li>Using <code>save_model<\/code> to save in JSON is worth a shot to try.<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71185505",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1645427212643,
        "Question_original_content":"xgbclassif serial give xgbmodel object attribut enabl categor serial xgbclassifi object train gener xgboost xgbclassifi base score booster gbtree colsampl bylevel colsampl bynod colsampl bytre eval metric logloss gamma gpu import type gain interact constraint learn rate max bin max delta step max depth min child weight miss nan monoton constraint estim job num parallel tree random state reg alpha reg lambda scale po weight singl precis histogram true subsampl tree method gpu hist us label encod fals valid paramet verbos load object clf model joblib load model path want us object predict data azur environ xgboost give error file score line score execut stderr clf pred clf model predict clf data transform stderr file opt miniconda lib python site packag xgboost sklearn line predict stderr class prob super predict stderr file opt miniconda lib python site packag xgboost sklearn line predict stderr self us inplac predict stderr file opt miniconda lib python site packag xgboost sklearn line us inplac predict stderr predictor self param predictor stderr file opt miniconda lib python site packag xgboost sklearn line param stderr param updat class param deep stderr file opt miniconda lib python site packag xgboost sklearn line param stderr param super param deep stderr file opt miniconda lib python site packag sklearn base line param stderr valu getattr self kei stderr attributeerror xgbmodel object attribut enabl categor version pipelin produc serial model pipelin deseri model predict new data",
        "Question_preprocessed_content":"xgbclassif give xgbmodel object attribut serial xgbclassifi object train gener load object want us object predict data azur environ give error version pipelin model pipelin deseri model predict new data",
        "Question_gpt_summary_original":"The user has encountered an error while trying to use a serialized XGBClassifier object for prediction on new data. The error message indicates that the 'XGBModel' object has no attribute 'enable_categorical'. The user has confirmed that the same version of xgboost is being used in both the pipelines that produce\/serialize the model and in the pipeline that deserializes the model to predict on new data.",
        "Question_gpt_summary":"user encount error try us serial xgbclassifi object predict new data error messag indic xgbmodel object attribut enabl categor user confirm version xgboost pipelin produc serial model pipelin deseri model predict new data",
        "Answer_original_content":"possibl solut save model wai json specifi http xgboost readthedoc latest tutori save model html limit allow rang xgboost version known work model lead issu futur exampl ag version xgboost requir longer support newer version python save model save json worth shot try",
        "Answer_preprocessed_content":"possibl solut save model wai json specifi limit allow rang xgboost version known work model lead issu futur exampl ag version xgboost requir longer support newer version python save json worth shot try",
        "Answer_gpt_summary_original":"possible solutions to the challenge of de-serializing an xgbclassifier object include saving the model in a different way, such as using the json format specified in the xgboost documentation, or limiting the allowed range of xgboost versions to those known to work with the model. another solution is to try using the save_model function to save the model in json format.",
        "Answer_gpt_summary":"possibl solut challeng serial xgbclassifi object includ save model differ wai json format specifi xgboost document limit allow rang xgboost version known work model solut try save model function save model json format"
    },
    {
        "Question_id":58018893.0,
        "Question_title":"How do S3 file download and estimator.fit() work in this blog post?",
        "Question_body":"<h1>Difficulty in understanding<\/h1>\n<p>Q2) How to download a file from S3?<\/p>\n<p><strong>From<\/strong>  <a href=\"https:\/\/medium.com\/akeneo-labs\/machine-learning-workflow-with-sagemaker-b83b293337ff\" rel=\"nofollow noreferrer\">The Machine Learning Workflow with SageMaker<\/a><\/p>\n<p>And also why are we using this piece of code?<\/p>\n<p><code>estimator.fit(train_data_location)<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1568926981497,
        "Question_favorite_count":null,
        "Question_last_edit_time":1592644375060,
        "Question_score":1.0,
        "Question_view_count":220.0,
        "Answer_body":"<h2>Downloading a file from S3:<\/h2>\n\n<p>This code block in the Q2 section defines the function that downloads a file from S3. The user instantiates an S3 client, and then passes the S3 URL along to the <code>s3.Bucket.download_file()<\/code> method.<\/p>\n\n<pre><code>def download_from_s3(url):\n    \"\"\"ex: url = s3:\/\/sagemakerbucketname\/data\/validation.tfrecords\"\"\"\n    url_parts = url.split(\"\/\")  # =&gt; ['s3:', '', 'sagemakerbucketname', 'data', ...\n    bucket_name = url_parts[2]\n    key = os.path.join(*url_parts[3:])\n    filename = url_parts[-1]\n    if not os.path.exists(filename):\n        try:\n            # Create an S3 client\n            s3 = boto3.resource('s3')\n            print('Downloading {} to {}'.format(url, filename))\n            s3.Bucket(bucket_name).download_file(key, filename)\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] == \"404\":\n                print('The object {} does not exist in bucket {}'.format(\n                    key, bucket_name))\n            else:\n                raise\n<\/code><\/pre>\n\n<h2>Estimator.fit() explanation:<\/h2>\n\n<p>The <code>estimator.fit(train_data_location)<\/code> line is what initiates the training process with SageMaker. When run, SageMaker will provision the necessary infrastructure, fetch the data from the location the user designated (here, <code>train_data_location<\/code> which is a path to Amazon S3) and distribute it amongst the training cluster, carry out the training process, return the resulting model, and tear down the training infrastructure. <\/p>\n\n<p>You can find the result of this training job in the SageMaker console.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58018893",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1568928522076,
        "Question_original_content":"file download estim fit work blog post difficulti understand download file machin learn workflow piec code estim fit train data locat",
        "Question_preprocessed_content":"file download work blog post difficulti understand download file machin learn workflow piec code",
        "Question_gpt_summary_original":"The user is facing difficulty in understanding how to download a file from S3 and why the code \"estimator.fit(train_data_location)\" is being used in the context of the blog post \"The Machine Learning Workflow with SageMaker\".",
        "Question_gpt_summary":"user face difficulti understand download file code estim fit train data locat context blog post machin learn workflow",
        "Answer_original_content":"download file code block section defin function download file user instanti client pass url bucket download file method def download url url bucketnam data valid tfrecord url part url split bucketnam data bucket url part kei path join url part filenam url part path exist filenam try creat client boto resourc print download format url filenam bucket bucket download file kei filenam botocor except clienterror respons error code print object exist bucket format kei bucket rais estim fit explan estim fit train data locat line initi train process run provis necessari infrastructur fetch data locat user design train data locat path amazon distribut train cluster carri train process return result model tear train infrastructur result train job consol",
        "Answer_preprocessed_content":"download file code block section defin function download file user instanti client pass url method explan line initi train process run provis necessari infrastructur fetch data locat user design distribut train cluster carri train process return result model tear train infrastructur result train job consol",
        "Answer_gpt_summary_original":"the answer provides two possible solutions. the first solution is to use the provided code block in the q2 section to define a function that downloads a file from s3. the second solution is to use the code \"estimator.fit(train_data_location)\" to initiate the training process, which will provision the necessary infrastructure, fetch the data from the designated location, distribute it amongst the training cluster, carry out the training process, return the resulting model, and tear down the training infrastructure. the result of this training job can be found in the console.",
        "Answer_gpt_summary":"answer provid possibl solut solut us provid code block section defin function download file second solut us code estim fit train data locat initi train process provis necessari infrastructur fetch data design locat distribut train cluster carri train process return result model tear train infrastructur result train job consol"
    },
    {
        "Question_id":null,
        "Question_title":"Runs log stops at 50",
        "Question_body":"<p>Hello, i am running wandb locally in my computer. I start a sweep and it runs smoothly, but when it reaches 50 runs it stops. Although kernel seems to be running it does not show any runs in the wandb site nor locally files in my computer. Does anyone know what seems to be the problem? I can provide any logs if requested, i don\u2019t know what to post and be helpful.<\/p>",
        "Question_answer_count":9,
        "Question_comment_count":0,
        "Question_creation_time":1656936147013,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":192.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/runs-log-stops-at-50\/2696",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-07T00:31:30.034Z",
                "Answer_body":"<p>Hi John, can you give me more information on how you set up your Sweep? For example, are you using grid search? If so, can you change to random search and see if you are still running into the same problem with it crashing at 50 runs? This might be due to a specific parameter configuration crashing the sweeps.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-09T13:23:13.763Z",
                "Answer_body":"<p>Hello, i have the same issue with another computer, this one stopped at 32 runs. The same code in google colab runs just fine. Could be a hardware issue? It is a grid search.<\/p>\n<p>sweep_config = {<br>\n\u2018method\u2019: \u2018grid\u2019<br>\n}<\/p>\n<p>metric = {<br>\n\u2018name\u2019: \u2018loss\u2019,<br>\n\u2018goal\u2019: \u2018minimize\u2019<br>\n}<\/p>\n<p>sweep_config[\u2018metric\u2019] = metric<\/p>\n<p>parameters_dict = {<br>\n\u2018learning-rate\u2019:{<br>\n\u2018values\u2019: [0.001, 0.0001, 0.002]<br>\n},<br>\n\u2018conv1\u2019: {<br>\n\u2018values\u2019: [32, 48, 64]<br>\n},<br>\n\u2018conv2\u2019: {<br>\n\u2018values\u2019: [48, 64, 128]<br>\n},<br>\n\u2018conv3\u2019: {<br>\n\u2018values\u2019: [64, 128, 256]<br>\n},<br>\n\u2018dropout\u2019: {<br>\n\u2018values\u2019: [0.2, 0.3]<br>\n},<br>\n\u2018batch_size\u2019: {<br>\n\u2018values\u2019: [64, 128, 256]<br>\n},<br>\n}<\/p>\n<p>sweep_config[\u2018parameters\u2019] = parameters_dict<br>\nparameters_dict.update({<br>\n\u2018epochs\u2019: {<br>\n\u2018value\u2019: 10}<br>\n})<\/p>\n<p>pprint.pprint(sweep_config)<\/p>\n<p>Last log:<br>\n2022-07-08T17:17:49.759534 Sending commands to agent c6swh3i1: [{\u201crun_id\u201d:\u201ckbabazlh\u201d,\u201cprogram\u201d:\u201c\u201d,\u201ctype\u201d:\u201crun\u201d,\u201cargs\u201d:{\u201cbatch_size\u201d:{\u201cvalue\u201d:64},\u201cconv1\u201d:{\u201cvalue\u201d:32},\u201cconv2\u201d:{\u201cvalue\u201d:64},\u201cconv3\u201d:{\u201cvalue\u201d:256},\u201cdropout\u201d:{\u201cvalue\u201d:0.2},\u201cepochs\u201d:{\u201cvalue\u201d:10},\u201clearning-rate\u201d:{\u201cvalue\u201d:0.001}},\u201crunqueue_item_id\u201d:\u201cUnVuUXVldWVJdGVtOjEyNTk3MDUzNQ==\u201d,\u201clogs\u201d:<span class=\"chcklst-box fa fa-square-o fa-fw\"><\/span>,\u201crun_storage_id\u201d:\u201cUnVuOnYxOmtiYWJhemxoOkVNR18xOnBvZGlrYWtvcw==\u201d}]<\/p>\n<p>2022-07-08T17:19:21.436651 Launched new run 113o3dnp (decent-sweep-32)<\/p>\n<p>2022-07-08T17:19:21.465885 Sending commands to agent c6swh3i1: [{\u201crun_id\u201d:\u201c113o3dnp\u201d,\u201cprogram\u201d:\u201c\u201d,\u201ctype\u201d:\u201crun\u201d,\u201cargs\u201d:{\u201cbatch_size\u201d:{\u201cvalue\u201d:64},\u201cconv1\u201d:{\u201cvalue\u201d:32},\u201cconv2\u201d:{\u201cvalue\u201d:64},\u201cconv3\u201d:{\u201cvalue\u201d:256},\u201cdropout\u201d:{\u201cvalue\u201d:0.2},\u201cepochs\u201d:{\u201cvalue\u201d:10},\u201clearning-rate\u201d:{\u201cvalue\u201d:0.0001}},\u201crunqueue_item_id\u201d:\u201cUnVuUXVldWVJdGVtOjEyNTk3MDUzNg==\u201d,\u201clogs\u201d:<span class=\"chcklst-box fa fa-square-o fa-fw\"><\/span>,\u201crun_storage_id\u201d:\u201cUnVuOnYxOjExM28zZG5wOkVNR18xOnBvZGlrYWtvcw==\u201d}]<\/p>\n<p>2022-07-08T17:35:19.290112 Agent c6swh3i1 state changed from ERROR to RUNNING<\/p>\n<p>2022-07-08T17:55:11.671631 Agent c6swh3i1 state changed from ERROR to RUNNING<\/p>\n<p>2022-07-08T18:15:13.199534 Agent c6swh3i1 state changed from ERROR to RUNNING<\/p>\n<p>2022-07-08T18:35:13.31471 Agent c6swh3i1 state changed from ERROR to RUNNING<\/p>\n<p>2022-07-08T18:55:15.914243 Agent c6swh3i1 state changed from ERROR to RUNNING<\/p>\n<p>2022-07-08T19:15:11.71383 Agent c6swh3i1 state changed from ERROR to RUNNING<\/p>\n<p>2022-07-08T19:35:17.42472 Agent c6swh3i1 state changed from ERROR to RUNNING<\/p>\n<p>2022-07-08T19:55:16.585164 Agent c6swh3i1 state changed from ERROR to RUNNING<\/p>\n<p>2022-07-08T20:15:17.665501 Agent c6swh3i1 state changed from ERROR to RUNNING<\/p>\n<p>2022-07-08T20:35:13.42555 Agent c6swh3i1 state changed from ERROR to RUNNING<\/p>\n<p>2022-07-08T20:55:16.863404 Agent c6swh3i1 state changed from ERROR to RUNNING<\/p>\n<p>2022-07-08T21:15:12.686781 Agent c6swh3i1 state changed from ERROR to RUNNING<\/p>\n<p>2022-07-08T21:35:11.491317 Agent c6swh3i1 state changed from ERROR to RUNNING<\/p>\n<p>2022-07-08T21:55:11.980649 Agent c6swh3i1 state changed from ERROR to RUNNING<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-12T16:31:15.460Z",
                "Answer_body":"<p>Hi John, is there a reason why you need to use grid search? Because of the configuration that is not working properly, it would be better to use random search in this circumstance.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-15T19:41:14.016Z",
                "Answer_body":"<p>Hi John,<\/p>\n<p>Do you still need help here?<\/p>\n<p>Warmly,<br>\nLeslie<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-17T21:51:30.158Z",
                "Answer_body":"<p>Hello,<\/p>\n<p>this configuration runs properly in google colab. The reason i use grid search is because i need to explore all possible configurations. Why do i get this error? Any ideas? Is grid search problematic with any particular version of python or tensorflow?<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-28T19:49:49.416Z",
                "Answer_body":"<p>That\u2019s interesting, if it works in one place, it should work in the other. Can you send me the debug logs of when you ran it in your terminal please?<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-05T15:13:09.352Z",
                "Answer_body":"<p>Do you still need help here John?<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-10T17:28:59.862Z",
                "Answer_body":"<p>Hi John, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-15T21:52:23.432Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"run log stop hello run local start sweep run smoothli reach run stop kernel run run site local file know problem provid log request dont know post help",
        "Question_preprocessed_content":"run log stop hello run local start sweep run smoothli reach run stop kernel run run site local file know problem provid log request dont know post help",
        "Question_gpt_summary_original":"The user is encountering a challenge where their wandb log stops at 50 runs, despite the kernel appearing to be running. They are unsure of the cause and are willing to provide logs for assistance.",
        "Question_gpt_summary":"user encount challeng log stop run despit kernel appear run unsur caus will provid log assist",
        "Answer_original_content":"john inform set sweep exampl grid search chang random search run problem crash run specif paramet configur crash sweep hello issu stop run code googl colab run fine hardwar issu grid search sweep config method grid metric loss goal minim sweep config metric metric paramet dict learn rate valu conv valu conv valu conv valu dropout valu batch size valu sweep config paramet paramet dict paramet dict updat epoch valu pprint pprint sweep config log send command agent cswhi run kbabazlh program type run arg batch size valu conv valu conv valu conv valu dropout valu epoch valu learn rate valu runqueu item unvuuxvldwvjdgvtojeyntkmduznq log run storag unvuonyxomtiywjhemxookvnrxonbvzglrywtvcw launch new run odnp decent sweep send command agent cswhi run odnp program type run arg batch size valu conv valu conv valu conv valu dropout valu epoch valu learn rate valu runqueu item unvuuxvldwvjdgvtojeyntkmduzng log run storag unvuonyxojexmzzgwokvnrxonbvzglrywtvcw agent cswhi state chang error run agent cswhi state chang error run agent cswhi state chang error run agent cswhi state chang error run agent cswhi state chang error run agent cswhi state chang error run agent cswhi state chang error run agent cswhi state chang error run agent cswhi state chang error run agent cswhi state chang error run agent cswhi state chang error run agent cswhi state chang error run agent cswhi state chang error run agent cswhi state chang error run john reason need us grid search configur work properli better us random search circumst john need help warmli lesli hello configur run properli googl colab reason us grid search need explor possibl configur error idea grid search problemat particular version python tensorflow that interest work place work send debug log ran termin need help john john heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"john inform set sweep exampl grid search chang random search run problem crash run specif paramet configur crash sweep hello issu stop run code googl colab run fine hardwar issu grid search metric metric conv conv conv dropout epoch log send command agent swh launch new run dnp send command agent swh agent swh state chang error run agent swh state chang error run agent swh state chang error run agent swh state chang error run agent swh state chang error run agent swh state chang error run agent swh state chang error run agent swh state chang error run agent swh state chang error run agent swh state chang error run agent swh state chang error run agent swh state chang error run agent swh state chang error run agent swh state chang error run john reason need us grid search configur work properli better us random search circumst john need help warmli lesli hello configur run properli googl colab reason us grid search need explor possibl configur error idea grid search problemat particular version python tensorflow that interest work place work send debug log ran termin need help john john heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"possible solutions to the issue of the runs log stopping at 50 include changing from grid search to random search to avoid a specific parameter configuration crashing the sweeps, and checking for hardware issues. the user is advised to send debug logs of when they ran the code in their terminal and consider using random search instead of grid search. it is also suggested that the user explore all possible configurations and check if grid search is problematic with any particular version of python or tensorflow.",
        "Answer_gpt_summary":"possibl solut issu run log stop includ chang grid search random search avoid specif paramet configur crash sweep check hardwar issu user advis send debug log ran code termin consid random search instead grid search suggest user explor possibl configur check grid search problemat particular version python tensorflow"
    },
    {
        "Question_id":null,
        "Question_title":"runtime timeout?",
        "Question_body":"Hi all,\nIs there a way to set timeout value when mlflow.project.run(...) is called?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1537200910000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":12.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/JN9X_yUJ1bY",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-09-17T16:30:16",
                "Answer_body":"Hi,\n\nCurrently there's no way to explicitly pass a timeout to mlflow.project.run(...). However, you can launch your run asynchronously e.g. via mlflow.projects.run(..., block=False). The projects.run() API then returns a SubmittedRun\u00a0that exposes a cancel() method for cancelling the run, so you could write your own logic to wait for some time period\u00a0 & cancel the run.\n\n\nOn Mon, Sep 17, 2018 at 1:15 PM, MrAsanjar <afsa...@gmail.com> wrote:\n\nHi all,\nIs there a way to set timeout value when mlflow.project.run(...) is called?\u00a0\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users+unsubscribe@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/0c2991e3-0acd-4788-bfee-7bffab7432b4%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2018-09-17T20:39:45",
                "Answer_body":"thanks for the info. however, I believe that would be a critical feature to have.\nWould you consider a contribution to add this feature?\n\n\n\n\nOn Monday, September 17, 2018 at 3:30:16 PM UTC-5, Siddharth Murching wrote:\nHi,\n\nCurrently there's no way to explicitly pass a timeout to mlflow.project.run(...). However, you can launch your run asynchronously e.g. via mlflow.projects.run(..., block=False). The projects.run() API then returns a SubmittedRun\u00a0that exposes a cancel() method for cancelling the run, so you could write your own logic to wait for some time period\u00a0 & cancel the run.\nOn Mon, Sep 17, 2018 at 1:15 PM, MrAsanjar <afsa...@gmail.com> wrote:\n\nHi all,\nIs there a way to set timeout value when mlflow.project.run(...) is called?\u00a0\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"runtim timeout wai set timeout valu project run call",
        "Question_preprocessed_content":"runtim timeout wai set timeout valu call",
        "Question_gpt_summary_original":"The user is facing a challenge with setting a timeout value when using mlflow.project.run().",
        "Question_gpt_summary":"user face challeng set timeout valu project run",
        "Answer_original_content":"current wai explicitli pass timeout project run launch run asynchron project run block fals project run api return submittedrunthat expos cancel method cancel run write logic wait time period cancel run mon sep mrasanjar wrote wai set timeout valu project run call receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user unsubscrib googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user acd bfee bffabb googlegroup com option visit http group googl com optout thank info believ critic featur consid contribut add featur mondai septemb utc siddharth murch wrote current wai explicitli pass timeout project run launch run asynchron project run block fals project run api return submittedrunthat expos cancel method cancel run write logic wait time period cancel run mon sep mrasanjar wrote wai set timeout valu project run call receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com",
        "Answer_preprocessed_content":"current wai explicitli pass timeout launch run asynchron block fals api return submittedrunthat expos cancel method cancel run write logic wait time period cancel run mon sep mrasanjar wrote wai set timeout valu call receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit thank info believ critic featur consid contribut add featur mondai septemb siddharth murch wrote current wai explicitli pass timeout launch run asynchron block fals api return submittedrunthat expos cancel method cancel run write logic wait time period cancel run mon sep mrasanjar wrote wai set timeout valu call receiv messag subscrib googl group group unsubscrib group stop receiv email send email",
        "Answer_gpt_summary_original":"Solution: \n- Currently, there is no way to explicitly pass a timeout to mlflow.project.run(...).\n- However, you can launch your run asynchronously via mlflow.projects.run(..., block=False).\n- The projects.run() API then returns a SubmittedRun that exposes a cancel() method for cancelling the run, so you could write your own logic to wait for some time period & cancel the run.",
        "Answer_gpt_summary":"solut current wai explicitli pass timeout project run launch run asynchron project run block fals project run api return submittedrun expos cancel method cancel run write logic wait time period cancel run"
    },
    {
        "Question_id":null,
        "Question_title":"statsmodels",
        "Question_body":"Hello:\n\n\nThe MLFlow documentation shows this list of built-in flavors.\n\n\n\nPython Function (python_function)\n\nR Function (crate)\n\nH2O (h2o)\n\nKeras (keras)\n\nMLeap (mleap)\n\nPyTorch (pytorch)\n\nScikit-learn (sklearn)\n\nSpark MLlib (spark)\n\nTensorFlow (tensorflow)\n\nONNX (onnx)\n\nMXNet Gluon (gluon)\n\nXGBoost (xgboost)\n\nLightGBM (lightgbm)\n\nI need to deploy a statsmodels.Logit model, which is not built-in.\u00a0 Has somebody already added statsmodels to MLFlow?\u00a0 What would be the best way to accomplish this?\n\n\nFor reference:\u00a0https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.discrete.discrete_model.Logit.html#statsmodels.discrete.discrete_model.Logit\n\n\nThanks,\n\n\nRay",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1595248118000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":24.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/5jPXLRBZ6sY",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-20T13:08:34",
                "Answer_body":"You should be able to implement this with a custom Python model:\nhttps:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.pyfunc.html#pyfunc-create-custom\n\nYou might also consider just fitting a logistic regression with scikit instead.\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/42041a1d-09d6-406d-9dca-6d881e43381ao%40googlegroups.com."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"statsmodel hello document show list built flavor python function python function function crate kera kera mleap mleap pytorch pytorch scikit learn sklearn spark mllib spark tensorflow tensorflow onnx onnx mxnet gluon gluon xgboost xgboost lightgbm lightgbm need deploi statsmodel logit model built somebodi ad statsmodel best wai accomplish refer http statsmodel org stabl gener statsmodel discret discret model logit html statsmodel discret discret model logit thank rai",
        "Question_preprocessed_content":"statsmodel hello document show list flavor python function function kera mleap pytorch spark mllib tensorflow onnx mxnet gluon xgboost lightgbm need deploi model somebodi ad statsmodel best wai accomplish thank rai",
        "Question_gpt_summary_original":"The user is facing a challenge in deploying a statsmodels.Logit model in MLFlow as it is not a built-in flavor. They are seeking advice on whether someone has already added statsmodels to MLFlow and the best way to accomplish this.",
        "Question_gpt_summary":"user face challeng deploi statsmodel logit model built flavor seek advic ad statsmodel best wai accomplish",
        "Answer_original_content":"abl implement custom python model http org doc latest python api pyfunc html pyfunc creat custom consid fit logist regress scikit instead receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user dca deao googlegroup com",
        "Answer_preprocessed_content":"abl implement custom python model consid fit logist regress scikit instead receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include implementing a custom Python model using mlflow.pyfunc or fitting a logistic regression with scikit.",
        "Answer_gpt_summary":"possibl solut mention discuss includ implement custom python model pyfunc fit logist regress scikit"
    },
    {
        "Question_id":null,
        "Question_title":"Copy instead of moving runs to team",
        "Question_body":"<p>Hello,<\/p>\n<p>I know how to move runs to a team, but my problem is that the runs are then removed from my profile.<\/p>\n<p>Is there a way to copy the runs, keeping them in my profile and in the team ?<\/p>\n<p>A better solution would be to link them to a team project and if we add things to the run in the user project the changes should  also be  reported in the team project. Basically, both projects would point to the same unique run and if a user deletes a run in his project the run would still be present in team\u2019s project (only the link would be removed). If a run as no links attached to it, it should be erased.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1652890834786,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":66.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/copy-instead-of-moving-runs-to-team\/2442",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-17T16:21:35.467Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"copi instead move run team hello know run team problem run remov profil wai copi run keep profil team better solut link team project add thing run user project chang report team project basic project point uniqu run user delet run project run present team project link remov run link attach eras",
        "Question_preprocessed_content":"copi instead move run team hello know run team problem run remov profil wai copi run keep profil team better solut link team project add thing run user project chang report team project basic project point uniqu run user delet run project run present team project run link attach eras",
        "Question_gpt_summary_original":"The user is facing a challenge in moving runs to a team as it removes the runs from their profile. They are looking for a way to copy the runs and keep them in both their profile and the team. They suggest a solution to link the runs to a team project so that any changes made in the user project are also reported in the team project.",
        "Question_gpt_summary":"user face challeng move run team remov run profil look wai copi run profil team suggest solut link run team project chang user project report team project",
        "Answer_original_content":"topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer as it only states that the topic has been closed.",
        "Answer_gpt_summary":"solut provid answer state topic close"
    },
    {
        "Question_id":null,
        "Question_title":"how to add autoscaling policy to an sagemaker endpoint via terraform?",
        "Question_body":"based on the documentation here, https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/async-inference\/Async-Inference-Walkthrough.ipynb, an autoscaling policy could be added to an sagemaker async endpoint . I want to set this via terraform and have this so far (sample below) , but i'm not how to set Dimensions attribute in TargetTrackingScalingPolicyConfiguration in terraform?\n\n  TargetTrackingScalingPolicyConfiguration={\n        \"TargetValue\": 5.0, SageMakerVariantInvocationsPerInstance\n        \"CustomizedMetricSpecification\": {\n            \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n            \"Namespace\": \"AWS\/SageMaker\",\n            \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n            \"Statistic\": \"Average\",\n        },\n\nresource \"aws_appautoscaling_target\" \"sagemaker_target\" {\n  max_capacity       =  3\n  min_capacity       = 1\n  resource_id        = \"myendpoint\"\n  scalable_dimension = \"sagemaker:variant:DesiredInstanceCount\"\n  service_namespace  = \"sagemaker\"\n}\n\nresource \"aws_appautoscaling_policy\" \"sagemaker_policy\" {\n  name               = \"somepolicy\"\n  policy_type        = \"TargetTrackingScaling\"\n  resource_id        = aws_appautoscaling_target.sagemaker_target.resource_id\n  scalable_dimension = aws_appautoscaling_target.sagemaker_target.scalable_dimension\n  service_namespace  = aws_appautoscaling_target.sagemaker_target.service_namespace\n\n  target_tracking_scaling_policy_configuration {\n    customized_metric_specification{\n      metric_name = \"ApproximateBacklogSizePerInstance\"\n      namespace =  \"AWS\/SageMaker\"\n      Dimensions = ....?????\n      statistic =  \"Average\"\n    }\n    target_value       = 3\n    scale_in_cooldown  =300\n    scale_out_cooldown = 600\n  }\n}",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1666661848109,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":29.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvlNBp88AT7GtMkUbWIIZCw\/how-to-add-autoscaling-policy-to-an-sagemaker-endpoint-via-terraform",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-30T05:14:51.808Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nThis Dimensions refer to the CloudWatch dimensions of metric ApproximateBacklogSizePerInstance. It's the same dimension when you first define the metric as described in doc here",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"add autosc polici endpoint terraform base document http github com aw amazon exampl blob main async infer async infer walkthrough ipynb autosc polici ad async endpoint want set terraform far sampl set dimens attribut targettrackingscalingpolicyconfigur terraform targettrackingscalingpolicyconfigur targetvalu variantinvocationsperinst customizedmetricspecif metricnam approximatebacklogsizeperinst namespac aw dimens endpointnam valu endpoint statist averag resourc aw appautosc target target max capac min capac resourc myendpoint scalabl dimens variant desiredinstancecount servic namespac resourc aw appautosc polici polici somepolici polici type targettrackingsc resourc aw appautosc target target resourc scalabl dimens aw appautosc target target scalabl dimens servic namespac aw appautosc target target servic namespac target track scale polici configur custom metric specif metric approximatebacklogsizeperinst namespac aw dimens statist averag target valu scale cooldown scale cooldown",
        "Question_preprocessed_content":"add autosc polici endpoint terraform base document autosc polici ad async endpoint want set terraform far set dimens attribut targettrackingscalingpolicyconfigur terraform targettrackingscalingpolicyconfigur targetvalu variantinvocationsperinst customizedmetricspecif statist averag resourc resourc",
        "Question_gpt_summary_original":"The user is trying to add an autoscaling policy to a SageMaker endpoint via Terraform, but is unsure how to set the Dimensions attribute in the TargetTrackingScalingPolicyConfiguration. The user has provided a sample code for reference and is seeking assistance in setting the Dimensions attribute.",
        "Question_gpt_summary":"user try add autosc polici endpoint terraform unsur set dimens attribut targettrackingscalingpolicyconfigur user provid sampl code refer seek assist set dimens attribut",
        "Answer_original_content":"dimens refer cloudwatch dimens metric approximatebacklogsizeperinst dimens defin metric describ doc",
        "Answer_preprocessed_content":"dimens refer cloudwatch dimens metric approximatebacklogsizeperinst dimens defin metric describ doc",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer to the user's question about adding an autoscaling policy to an existing asynchronous endpoint via terraform. the answer simply clarifies the dimensions of a metric called \"approximatebacklogsizeperinstance\" in cloudwatch.",
        "Answer_gpt_summary":"solut provid answer user question ad autosc polici exist asynchron endpoint terraform answer simpli clarifi dimens metric call approximatebacklogsizeperinst cloudwatch"
    },
    {
        "Question_id":60072981.0,
        "Question_title":"How to open a model tarfile stored in S3 bucket in sagemaker notebook?",
        "Question_body":"<p>I know that loading a .csv file into sagemaker notebook from S3 bucket is pretty straightforward but I want to load a model.tar.gz file stored in S3 bucket. I tried to do the following<\/p>\n\n<pre><code>import botocore \nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.predictor import csv_serializer\nimport boto3\n\nsm_client = boto3.client(service_name='sagemaker')\nruntime_sm_client = boto3.client(service_name='sagemaker-runtime')\n\ns3 = boto3.resource('s3')\ns3_client = boto3.client('s3')\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\nACCOUNT_ID  = boto3.client('sts').get_caller_identity()['Account']\nREGION      = boto3.Session().region_name\nBUCKET      = 'sagemaker.prismade.net'\ndata_key    = 'DEMO_MME_ANN\/multi_model_artifacts\/axel.tar.gz'\nloc = 's3:\/\/{}\/{}'.format(BUCKET, data_key)\nprint(loc)\nwith tarfile.open(loc) as tar:\n    tar.extractall(path='.')\n<\/code><\/pre>\n\n<p>I get the following error:<\/p>\n\n<pre><code>--------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-215-bfdddac71b95&gt; in &lt;module&gt;()\n     20 loc = 's3:\/\/{}\/{}'.format(BUCKET, data_key)\n     21 print(loc)\n---&gt; 22 with tarfile.open(loc) as tar:\n     23     tar.extractall(path='.')\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/tarfile.py in open(cls, name, mode, fileobj, bufsize, **kwargs)\n   1567                     saved_pos = fileobj.tell()\n   1568                 try:\n-&gt; 1569                     return func(name, \"r\", fileobj, **kwargs)\n   1570                 except (ReadError, CompressionError):\n   1571                     if fileobj is not None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/tarfile.py in gzopen(cls, name, mode, fileobj, compresslevel, **kwargs)\n   1632 \n   1633         try:\n-&gt; 1634             fileobj = gzip.GzipFile(name, mode + \"b\", compresslevel, fileobj)\n   1635         except OSError:\n   1636             if fileobj is not None and mode == 'r':\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/gzip.py in __init__(self, filename, mode, compresslevel, fileobj, mtime)\n    161             mode += 'b'\n    162         if fileobj is None:\n--&gt; 163             fileobj = self.myfileobj = builtins.open(filename, mode or 'rb')\n    164         if filename is None:\n    165             filename = getattr(fileobj, 'name', '')\n\nFileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/sagemaker.prismade.net\/DEMO_MME_ANN\/multi_model_artifacts\/axel.tar.gz'\n<\/code><\/pre>\n\n<p>What is the mistake here and how can I accomplish this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1580895955387,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":2984.0,
        "Answer_body":"<p>Not every python library that is designed to work with a file system (tarfile.open, in this example) knows how to read an object from S3 as a file. <\/p>\n\n<p>The simple way to solve it is to first copy the object into the local file system as a file.<\/p>\n\n<pre><code>import boto3\n\ns3 = boto3.client('s3')\ns3.download_file('BUCKET_NAME', 'OBJECT_NAME', 'FILE_NAME')\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60072981",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1581004771980,
        "Question_original_content":"open model tarfil store bucket notebook know load csv file notebook bucket pretti straightforward want load model tar file store bucket tri follow import botocor import import execut role predictor import csv serial import boto client boto client servic runtim client boto client servic runtim boto resourc client boto client session session role execut role account boto client st caller ident account region boto session region bucket prismad net data kei demo mme ann multi model artifact axel tar loc format bucket data kei print loc tarfil open loc tar tar extractal path follow error filenotfounderror traceback recent loc format bucket data kei print loc tarfil open loc tar tar extractal path anaconda env python lib python tarfil open cl mode fileobj bufsiz kwarg save po fileobj tell try return func fileobj kwarg readerror compressionerror fileobj anaconda env python lib python tarfil gzopen cl mode fileobj compresslevel kwarg try fileobj gzip gzipfil mode compresslevel fileobj oserror fileobj mode anaconda env python lib python gzip init self filenam mode compresslevel fileobj mtime mode fileobj fileobj self myfileobj builtin open filenam mode filenam filenam getattr fileobj filenotfounderror errno file directori prismad net demo mme ann multi model artifact axel tar mistak accomplish",
        "Question_preprocessed_content":"open model tarfil store bucket notebook know load csv file notebook bucket pretti straightforward want load file store bucket tri follow follow error mistak accomplish",
        "Question_gpt_summary_original":"The user is trying to load a model.tar.gz file stored in an S3 bucket into a SageMaker notebook. They have attempted to use the tarfile module to extract the file but encountered a FileNotFoundError. The user is seeking assistance in identifying the mistake and finding a solution to load the model file.",
        "Question_gpt_summary":"user try load model tar file store bucket notebook attempt us tarfil modul extract file encount filenotfounderror user seek assist identifi mistak find solut load model file",
        "Answer_original_content":"python librari design work file tarfil open exampl know read object file simpl wai solv copi object local file file import boto boto client download file bucket object file",
        "Answer_preprocessed_content":"python librari design work file know read object file simpl wai solv copi object local file file",
        "Answer_gpt_summary_original":"the solution to the challenge of opening a model tarfile stored in an s3 bucket in a notebook is to first copy the object into the local file system as a file using the boto3 library. this is because not every python library that is designed to work with a file system knows how to read an object from s3 as a file.",
        "Answer_gpt_summary":"solut challeng open model tarfil store bucket notebook copi object local file file boto librari python librari design work file know read object file"
    },
    {
        "Question_id":51391639.0,
        "Question_title":"Is it possible to predict in sagemaker without using s3",
        "Question_body":"<p>I have a .pkl which I would like to put into production. I would like to do a daily query of my SQL server and do a prediction on about 1000 rows. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">documentation<\/a> implies I have to load the daily data into s3. Is there a way around this? It should be able to fit in memory no problem. <\/p>\n\n<p>The answer to \" <a href=\"https:\/\/stackoverflow.com\/questions\/48319893\/is-there-some-kind-of-persistent-local-storage-in-aws-sagemaker-model-training\">is there some kind of persistent local storage in aws sagemaker model training?<\/a> \" says that \"<em>The notebook instance is coming with a local EBS (5GB) that you can use to copy some data into it and run the fast development iterations without copying the data every time from S3.<\/em>\" The 5GB could be enough but I am not sure you can actually run from a notebook in this manner. If I set up a VPN could I just query using pyodbc?<\/p>\n\n<p>Is there sagemaker integration with AWS Lambda? That in combination with a docker container would suit my needs.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1531871216253,
        "Question_favorite_count":null,
        "Question_last_edit_time":1531893078607,
        "Question_score":2.0,
        "Question_view_count":1576.0,
        "Answer_body":"<p>While you need to to specify a s3 \"folder\" as input, this folder can contain only a dummy file. \nAlso if you bring your own docker container for training like in <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">this example<\/a>, you can do pretty much everthing in it. So you could do your daily query inside your docker container, because they have access to the internet. <\/p>\n\n<p>Also inside this container you have access to all the other aws services. Your access is defined by the role you're passing to your training job.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51391639",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1531919252070,
        "Question_original_content":"possibl predict pkl like product like daili queri sql server predict row document impli load daili data wai abl fit memori problem answer kind persist local storag model train sai notebook instanc come local eb us copi data run fast develop iter copi data time sure actual run notebook manner set vpn queri pyodbc integr aw lambda combin docker contain suit need",
        "Question_preprocessed_content":"possibl predict pkl like product like daili queri sql server predict row document impli load daili data wai abl fit memori problem answer kind persist local storag model train sai notebook instanc come local eb us copi data run fast develop iter copi data time sure actual run notebook manner set vpn queri pyodbc integr aw lambda combin docker contain suit need",
        "Question_gpt_summary_original":"The user wants to put a .pkl file into production and do a daily query of their SQL server to predict on about 1000 rows. However, the documentation implies that the daily data needs to be loaded into s3, which the user wants to avoid. They are unsure if they can run from a notebook with local EBS storage or query using pyodbc with a VPN. They are also wondering if there is sagemaker integration with AWS Lambda, which would suit their needs.",
        "Question_gpt_summary":"user want pkl file product daili queri sql server predict row document impli daili data need load user want avoid unsur run notebook local eb storag queri pyodbc vpn wonder integr aw lambda suit need",
        "Answer_original_content":"need specifi folder input folder contain dummi file bring docker contain train like exampl pretti everth daili queri insid docker contain access internet insid contain access aw servic access defin role pass train job",
        "Answer_preprocessed_content":"need specifi folder input folder contain dummi file bring docker contain train like exampl pretti everth daili queri insid docker contain access internet insid contain access aw servic access defin role pass train job",
        "Answer_gpt_summary_original":"possible solutions to the challenge of predicting on data from a sql server without using s3 include using a notebook instance with a local ebs, setting up a vpn, or using aws lambda with a docker container. it is also possible to specify a dummy file in an s3 folder as input and bring your own docker container for training, which allows for access to the internet and all other aws services. access is defined by the role passed to the training job.",
        "Answer_gpt_summary":"possibl solut challeng predict data sql server includ notebook instanc local eb set vpn aw lambda docker contain possibl specifi dummi file folder input bring docker contain train allow access internet aw servic access defin role pass train job"
    },
    {
        "Question_id":null,
        "Question_title":"how to set up autoscaling for async sagemaker endpoint?",
        "Question_body":"working with an example documented here -> https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/async-inference\/Async-Inference-Walkthrough.ipynb. I was able to set up the sagemaker model, config and aync endpoint via lambda, now I'm trying to re-create the stack via terraform. based on the documentation on terraform, i was able to set up the model, config and the endpoint but couldn't find how to go about setting up the auto scaling ( sample code below). is this possible?\n\nclient = boto3.client(    \"application-autoscaling\") \nresource_id = (    \"endpoint\/\" + endpoint_name + \"\/variant\/\" + \"variant1\")  \nresponse = client.register_scalable_target(\n    ServiceNamespace=\"sagemaker\",\n    ResourceId=resource_id,\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n    MinCapacity=0,\n    MaxCapacity=5,\n)\nresponse = client.put_scaling_policy(\n    PolicyName=\"Invocations-ScalingPolicy\",\n    ServiceNamespace=\"sagemaker\", \n    ResourceId=resource_id,  # Endpoint name\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  \n    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling'|'TargetTrackingScaling'\n    TargetTrackingScalingPolicyConfiguration={\n        \"TargetValue\": 5.0,  \nSageMakerVariantInvocationsPerInstance\n        \"CustomizedMetricSpecification\": {\n            \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n            \"Namespace\": \"AWS\/SageMaker\",\n            \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n            \"Statistic\": \"Average\",\n        },\n        \"ScaleInCooldown\": 600,\n   ....\n    },\n)\n\n\nclean up\n\nresponse = client.deregister_scalable_target(\n    ServiceNamespace='sagemaker',\n    ResourceId='resource_id',\n    ScalableDimension='sagemaker:variant:DesiredInstanceCount'\n)",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646861290947,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":256.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjjLx7h0TR0q1KwubwQjU9A\/how-to-set-up-autoscaling-for-async-sagemaker-endpoint",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-11T17:05:39.186Z",
                "Answer_score":1,
                "Answer_body":"You will using the regular autoscaling config outlined in the doc here to configure it for the SageMaker Async endpoint. There are no specifics for SageMaker.\n\nFirst, you define the \"aws_appautoscaling_target\" with minimum and maximum capacities. Then go ahead and define your \"TargetTrackingScaling\" in the autoscaling policy",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"set autosc async endpoint work exampl document http github com aw amazon exampl blob main async infer async infer walkthrough ipynb abl set model config aync endpoint lambda try creat stack terraform base document terraform abl set model config endpoint couldn set auto scale sampl code possibl client boto client applic autosc resourc endpoint endpoint variant variant respons client regist scalabl target servicenamespac resourceid resourc scalabledimens variant desiredinstancecount mincapac maxcapac respons client scale polici policynam invoc scalingpolici servicenamespac resourceid resourc endpoint scalabledimens variant desiredinstancecount policytyp targettrackingsc stepscal targettrackingsc targettrackingscalingpolicyconfigur targetvalu variantinvocationsperinst customizedmetricspecif metricnam approximatebacklogsizeperinst namespac aw dimens endpointnam valu endpoint statist averag scaleincooldown clean respons client deregist scalabl target servicenamespac resourceid resourc scalabledimens variant desiredinstancecount",
        "Question_preprocessed_content":"set autosc async endpoint work exampl document abl set model config aync endpoint lambda try stack terraform base document terraform abl set model config endpoint couldn set auto scale possibl client respons servicenamespac scalabledimens variant desiredinstancecount mincapac maxcapac respons servicenamespac endpoint scalabledimens variant desiredinstancecount policytyp targettrackingsc stepscal targettrackingsc targettrackingscalingpolicyconfigur targetvalu variantinvocationsperinst customizedmetricspecif statist averag scaleincooldown clean respons servicenamespac scalabledimens variant desiredinstancecount",
        "Question_gpt_summary_original":"The user is trying to set up autoscaling for an async SageMaker endpoint using Terraform. They were able to set up the model, config, and endpoint but are having trouble finding information on how to set up autoscaling. They provided sample code for autoscaling using boto3 but are unsure if it is possible to do so with Terraform.",
        "Question_gpt_summary":"user try set autosc async endpoint terraform abl set model config endpoint have troubl find inform set autosc provid sampl code autosc boto unsur possibl terraform",
        "Answer_original_content":"regular autosc config outlin doc configur async endpoint specif defin aw appautosc target minimum maximum capac ahead defin targettrackingsc autosc polici",
        "Answer_preprocessed_content":"regular autosc config outlin doc configur async endpoint specif defin minimum maximum capac ahead defin targettrackingsc autosc polici",
        "Answer_gpt_summary_original":"summary: to set up autoscaling for an async endpoint using terraform, the user can use the regular autoscaling config outlined in the documentation. they need to define the \"aws_appautoscaling_target\" with minimum and maximum capacities, and then define the \"targettrackingscaling\" in the autoscaling policy.",
        "Answer_gpt_summary":"summari set autosc async endpoint terraform user us regular autosc config outlin document need defin aw appautosc target minimum maximum capac defin targettrackingsc autosc polici"
    },
    {
        "Question_id":63667022.0,
        "Question_title":"Getting KeyError : 'callable_inputs' when trying to save a TF model in S3 bucket",
        "Question_body":"<p>I'm using sagemaker 2.5.1 and tensorflow 2.3.0\nThe weird part is that the same code worked before, the only change that I could think of is the new release of the two libraries<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1598861056290,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":174.0,
        "Answer_body":"<p>The problem is actually coming from smdebug version 0.9.1\nDowngrading to 0.8.1 solves the issue<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63667022",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1599662565023,
        "Question_original_content":"get keyerror callabl input try save model bucket tensorflow weird code work chang think new releas librari",
        "Question_preprocessed_content":"get keyerror try save model bucket tensorflow weird code work chang think new releas librari",
        "Question_gpt_summary_original":"The user is encountering a KeyError when attempting to save a TensorFlow model in an S3 bucket using SageMaker 2.5.1 and TensorFlow 2.3.0. The issue is unexpected as the same code worked previously, and the only change made was the update of the two libraries.",
        "Question_gpt_summary":"user encount keyerror attempt save tensorflow model bucket tensorflow issu unexpect code work previous chang updat librari",
        "Answer_original_content":"problem actual come smdebug version downgrad solv issu",
        "Answer_preprocessed_content":"problem actual come smdebug version downgrad solv issu",
        "Answer_gpt_summary_original":"the solution to the keyerror encountered when saving a tensorflow model in an s3 bucket is to downgrade the smdebug version from 0.9.1 to 0.8.1.",
        "Answer_gpt_summary":"solut keyerror encount save tensorflow model bucket downgrad smdebug version"
    },
    {
        "Question_id":64302986.0,
        "Question_title":"How to highlight custom extractions using a2i's crowd-textract-analyze-document?",
        "Question_body":"<p>I would like to create a human review loop for images that undergone OCR using Amazon Textract and Entity Extraction using Amazon Comprehend.<\/p>\n<p>My process is:<\/p>\n<ol>\n<li>send image to Textract to extract the text<\/li>\n<li>send text to Comprehend to extract entities<\/li>\n<li>find the Block IDs in Textract's output of the entities extracted by Comprehend<\/li>\n<li>add new Blocks of type <code>KEY_VALUE_SET<\/code> to textract's JSON output <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/a2i-crowd-textract-detection.html\" rel=\"nofollow noreferrer\">per the docs<\/a><\/li>\n<li>create a Human Task with <code>crowd-textract-analyze-document<\/code> element in the template and feed it the modified textract output<\/li>\n<\/ol>\n<p>What fails to work in this process is step 5. My custom entities are not rendered properly. By &quot;fails to work&quot; I mean that the entities are not highlighted on the image when I click them on the sidebar. There is no error in the browser's console.<\/p>\n<p>Has anyone tried such a thing?<\/p>\n<p><em>Sorry for not including examples. I will remove secrets\/PII from my files and attach them to the question<\/em><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1602412144293,
        "Question_favorite_count":null,
        "Question_last_edit_time":1624091301367,
        "Question_score":0.0,
        "Question_view_count":252.0,
        "Answer_body":"<p>I used the AWS documentation of the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/a2i-crowd-textract-detection.html\" rel=\"nofollow noreferrer\">a2i-crowd-textract-detection human task element<\/a> to generate the value of the <code>initialValue<\/code> attribute. It appears the doc for that attribute is incorrect. While the the doc shows that the value should be in the same format as the output of Textract, namely:<\/p>\n<pre><code>[\n        {\n            &quot;BlockType&quot;: &quot;KEY_VALUE_SET&quot;,\n            &quot;Confidence&quot;: 38.43309020996094,\n            &quot;Geometry&quot;: { ... }\n            &quot;Id&quot;: &quot;8c97b240-0969-4678-834a-646c95da9cf4&quot;,\n            &quot;Relationships&quot;: [\n                { &quot;Type&quot;: &quot;CHILD&quot;, &quot;Ids&quot;: [...]},\n                { &quot;Type&quot;: &quot;VALUE&quot;, &quot;Ids&quot;: [...]}\n            ],\n            &quot;EntityTypes&quot;: [&quot;KEY&quot;],\n            &quot;Text&quot;: &quot;Foo bar&quot;\n        },\n]\n<\/code><\/pre>\n<p>the <code>a2i-crowd-textract-detection<\/code> expects the input to have lowerCamelCase attribute names (rather than UpperCamelCase). For example:<\/p>\n<pre><code>[\n        {\n            &quot;blockType&quot;: &quot;KEY_VALUE_SET&quot;,\n            &quot;confidence&quot;: 38.43309020996094,\n            &quot;geometry&quot;: { ... }\n            &quot;id&quot;: &quot;8c97b240-0969-4678-834a-646c95da9cf4&quot;,\n            &quot;relationships&quot;: [\n                { &quot;Type&quot;: &quot;CHILD&quot;, &quot;ids&quot;: [...]},\n                { &quot;Type&quot;: &quot;VALUE&quot;, &quot;ids&quot;: [...]}\n            ],\n            &quot;entityTypes&quot;: [&quot;KEY&quot;],\n            &quot;text&quot;: &quot;Foo bar&quot;\n        },\n]\n<\/code><\/pre>\n<p>I opened a support case about this documentation error to AWS.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64302986",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1603012533232,
        "Question_original_content":"highlight custom extract crowd textract analyz document like creat human review loop imag undergon ocr amazon textract entiti extract amazon comprehend process send imag textract extract text send text comprehend extract entiti block id textract output entiti extract comprehend add new block type kei valu set textract json output doc creat human task crowd textract analyz document element templat feed modifi textract output fail work process step custom entiti render properli fail work mean entiti highlight imag click sidebar error browser consol tri thing sorri includ exampl remov secret pii file attach question",
        "Question_preprocessed_content":"highlight custom extract like creat human review loop imag undergon ocr amazon textract entiti extract amazon comprehend process send imag textract extract text send text comprehend extract entiti block id textract output entiti extract comprehend add new block type textract json output doc creat human task element templat feed modifi textract output fail work process step custom entiti render properli fail work mean entiti highlight imag click sidebar error browser consol tri thing sorri includ exampl remov file attach question",
        "Question_gpt_summary_original":"The user is encountering challenges in creating a human review loop for images that have undergone OCR using Amazon Textract and Entity Extraction using Amazon Comprehend. Specifically, the user is having difficulty highlighting custom extractions using a2i's crowd-textract-analyze-document. The custom entities are not being rendered properly and are not highlighted on the image when clicked on the sidebar. The user is seeking advice from others who may have tried a similar process.",
        "Question_gpt_summary":"user encount challeng creat human review loop imag undergon ocr amazon textract entiti extract amazon comprehend specif user have difficulti highlight custom extract crowd textract analyz document custom entiti render properli highlight imag click sidebar user seek advic tri similar process",
        "Answer_original_content":"aw document crowd textract detect human task element gener valu initialvalu attribut appear doc attribut incorrect doc show valu format output textract blocktyp kei valu set confid geometri cdacf relationship type child id type valu id entitytyp kei text foo bar crowd textract detect expect input lowercamelcas attribut name uppercamelcas exampl blocktyp kei valu set confid geometri cdacf relationship type child id type valu id entitytyp kei text foo bar open support case document error aw",
        "Answer_preprocessed_content":"aw document human task element gener valu attribut appear doc attribut incorrect doc show valu format output textract expect input lowercamelcas attribut name exampl open support case document error aw",
        "Answer_gpt_summary_original":"the answer provides a solution to a documentation error in the aws a2i-crowd-textract-detection human task element. the user can generate the value of the initialvalue attribute using the aws documentation and ensure that the input has lowercamelcase attribute names. the user also opened a support case about the documentation error to aws.",
        "Answer_gpt_summary":"answer provid solut document error aw crowd textract detect human task element user gener valu initialvalu attribut aw document ensur input lowercamelcas attribut name user open support case document error aw"
    },
    {
        "Question_id":null,
        "Question_title":"GC overhead limit exceeded",
        "Question_body":"I have a modest size dataset, and I am running Jupyter Notebook in Sagemaker (instance type ml.c5.xlarge with 200G instance size). I receive the error message \" GC overhead limit exceeded\" Everything ran fine with small data size. BTW, I need to go through the dataframe one row at a time using df.collect(), which seems t be an expensive operation... Would you suggest another way of accomplishing this? I would appreciate your kind help.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654390632353,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":87.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDc_WeDqcTjitN1bkIJRosg\/gc-overhead-limit-exceeded",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-08T07:40:20.941Z",
                "Answer_score":0,
                "Answer_body":"The GC overhead limit exceeded error indicates that the JVM spent a lot of time on garbage collection but recovered very little memory, so it throws this error to let you know that your program is not making much progress but wasting time on doing useless garbage collection task. Iterating through the dataframe might be the problem, because you might be creating a lot of temporary objects when you go through each line, and they couldn't be garbage collected. What is the framework that you are using? And what are you trying to do by going through the dataframe row-by-row? Maybe you can think about processing multiple lines in a batch? For example using some vectorization or matrix operation as georgios_s suggested in the comment.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"overhead limit exceed modest size dataset run jupyt notebook instanc type xlarg instanc size receiv error messag overhead limit exceed ran fine small data size btw need datafram row time collect expens oper suggest wai accomplish appreci kind help",
        "Question_preprocessed_content":"overhead limit exceed modest size dataset run jupyt notebook receiv error messag overhead limit exceed ran fine small data size btw need datafram row time expens suggest wai accomplish appreci kind help",
        "Question_gpt_summary_original":"The user is encountering the \"GC overhead limit exceeded\" error while running Jupyter Notebook in Sagemaker with a modest size dataset. The error occurs when going through the dataframe one row at a time using df.collect(), which is an expensive operation. The user is seeking suggestions for alternative ways to accomplish this task.",
        "Question_gpt_summary":"user encount overhead limit exceed error run jupyt notebook modest size dataset error occur go datafram row time collect expens oper user seek suggest altern wai accomplish task",
        "Answer_original_content":"overhead limit exceed error indic jvm spent lot time garbag collect recov littl memori throw error let know program make progress wast time useless garbag collect task iter datafram problem creat lot temporari object line couldn garbag collect framework try go datafram row row mayb think process multipl line batch exampl vector matrix oper georgio suggest comment",
        "Answer_preprocessed_content":"overhead limit exceed error indic jvm spent lot time garbag collect recov littl memori throw error let know program make progress wast time useless garbag collect task iter datafram problem creat lot temporari object line couldn garbag collect framework try go datafram mayb think process multipl line batch exampl vector matrix oper suggest comment",
        "Answer_gpt_summary_original":"possible solutions to the \"gc overhead limit exceeded\" error while running jupyter notebook on an ml.c5.xlarge instance with 200g instance size are to avoid iterating through the dataframe one row at a time using df.collect() and to consider processing multiple lines in a batch using vectorization or matrix operation. it is also suggested to provide more context about the framework being used and the purpose of going through the dataframe row-by-row.",
        "Answer_gpt_summary":"possibl solut overhead limit exceed error run jupyt notebook xlarg instanc instanc size avoid iter datafram row time collect consid process multipl line batch vector matrix oper suggest provid context framework purpos go datafram row row"
    },
    {
        "Question_id":null,
        "Question_title":"Save audio file from speech to text stream",
        "Question_body":"I am using @Google-cloud\/speech for streaming audio from the browser to my nodejs backend.\nI would like to save the recorded audio.\nI see no option to do so. Any suggestions? Thanks.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646292900000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":57.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Save-audio-file-from-speech-to-text-stream\/td-p\/398993\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-08T13:17:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hey,\u00a0\n\nYou shall probably use other packages for recording such as recordrtc as mentioned at [1].\u00a0\u00a0\n\n[1]\u00a0https:\/\/www.leeboonstra.dev\/chatbots\/building-your-own-voice-ai-3\/"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"save audio file speech text stream googl cloud speech stream audio browser nodej backend like save record audio option suggest thank",
        "Question_preprocessed_content":"save audio file speech text stream stream audio browser nodej backend like save record audio option suggest thank",
        "Question_gpt_summary_original":"The user is facing a challenge in saving recorded audio from a speech to text stream using @Google-cloud\/speech in their nodejs backend as they cannot find an option to do so. They are seeking suggestions to overcome this challenge.",
        "Question_gpt_summary":"user face challeng save record audio speech text stream googl cloud speech nodej backend option seek suggest overcom challeng",
        "Answer_original_content":"hei shall probabl us packag record recordrtc mention http leeboonstra dev chatbot build voic",
        "Answer_preprocessed_content":"hei shall probabl us packag record recordrtc mention",
        "Answer_gpt_summary_original":"possible solution: the user can use other packages such as recordrtc for recording audio files from a speech to text stream in their node.js backend.",
        "Answer_gpt_summary":"possibl solut user us packag recordrtc record audio file speech text stream node backend"
    },
    {
        "Question_id":null,
        "Question_title":"Error running Experiment script (Azure Notebook - Python SDK)",
        "Question_body":"Hello,\n\nI am running the Azure ML Python example from here, https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py#run\n\nSpecifically this section of the example;\n\n from azureml.core.experiment import Experiment\n from azureml.core import ScriptRunConfig\n    \n script_run_config = ScriptRunConfig(source_directory=os.getcwd(), script=\"train.ipynb\", run_config=compute_config)\n experiment = Experiment(workspace=ws, name=\"compute_target_test\")\n run = experiment.submit(config=script_run_config)\n run.wait_for_completion(show_output=True)\n\n\n\nWhen I execute the above code I receive the error;\nmessage': \"User program failed with NameError: name 'true' is not defined\"\n\nThe log file gives the following details;\n\nFile \"train.ipynb\", line 67, in <module>\n\"notebookHasBeenCompleted\": true\nNameError: name 'true' is not defined\n\nThe problem is the train.ipynb file does not contain 67 lines.\n\nCould anyone provide some assistance as to how to resolve this.\n\nThanks.",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1648360151510,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/788720\/error-running-experiment-script-azure-notebook-pyt.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-28T14:31:37.3Z",
                "Answer_score":0,
                "Answer_body":"@GrahamBenson-6517 Thanks for the question. We are able to run successfully with out an error as shown below.\n\n\nHere is link to the sample for using the environments.\n\n\n\n\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/tree\/master\/how-to-use-azureml\/training\/using-environments",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-30T09:21:36.737Z",
                "Answer_score":1,
                "Answer_body":"With the help of this post, https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/674712\/nameerror-when-trying-to-run-an-scriptrunconfig-in.html\nI have been able to resolve the problem.\n\nThe issue is the script parameter in the ScriptRunConfig class must be a .py file and not a .ipynb file. After changing the extension to *.py and downloading the Notebook the following JSON object will be visible;\n\n \"microsoft\": {\n           \"host\": {\n             \"AzureML\": {\n               \"notebookHasBeenCompleted\": true\n             }\n           }\n         },\n\n\n\nAfter changing the \"true\" value to \"True\" the script will execute successfully.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-02T07:27:31.687Z",
                "Answer_score":0,
                "Answer_body":"@ramr-msft\n\nHaving solved the above issue, I am now receiving the warning;\n\n\"Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies....\"\n\nI am using the curated environment, AzureML-sklearn-0.24-ubuntu18.04-py37-cpu which contains the packages;\n\nscikit-learn==0.24.1\n\n\nnumpy>=1.16.0\n\n\npandas~=1.1.x\n\nFollowing is the Notebook code;\n\n     from azureml.core import Workspace, Environment    \n     from azureml.core import ScriptRunConfig, Experiment\n     ws = Workspace.from_config()\n     myenv = Environment.get(workspace=ws, name=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\")   \n       \n        \n     exp = Experiment(name=\"myexp\", workspace = ws)\n     # Instantiate environment\n     myenv = Environment(name=\"myenv\")\n        \n     # Add training script to run config\n     runconfig = ScriptRunConfig(source_directory=\".\", script=\"train.py\")\n        \n     # Attach compute target to run config\n     runconfig.run_config.target = \"local\"\n        \n     # Attach environment to run config\n     runconfig.run_config.environment = myenv\n        \n     # Submit run \n     run = exp.submit(runconfig)\n\n\n\nThe train.py script is taken from the example, here https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py#run\n\n ------------------------------ train.py script ----------------------------------\n # train.py\n    \n from sklearn import svm\n import numpy as np\n import joblib\n import os\n    \n # customer ages\n X_train = np.array([50, 17, 35, 23, 28, 40, 31, 29, 19, 62])\n X_train = X_train.reshape(-1, 1)\n # churn y\/n\n y_train = [\"yes\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"yes\"]\n    \n clf = svm.SVC(gamma=0.001, C=100.)\n clf.fit(X_train, y_train)\n    \n os.makedirs(\"outputs\", exist_ok=True)\n joblib.dump(value=clf, filename=\"outputs\/churn-model.pkl\")\n    \n ------------------------------ train.py script ----------------------------------\n\nCan you please assist with the above. Why would I receive the error message when the environment being used is a Microsoft curated environment?\n\nThank you",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error run experi script azur notebook python sdk hello run python exampl http doc microsoft com python api overview azur view azur run specif section exampl core experi import experi core import scriptrunconfig script run config scriptrunconfig sourc directori getcwd script train ipynb run config comput config experi experi workspac comput target test run experi submit config script run config run wait complet output true execut code receiv error messag user program fail nameerror true defin log file give follow detail file train ipynb line notebookhasbeencomplet true nameerror true defin problem train ipynb file contain line provid assist resolv thank",
        "Question_preprocessed_content":"error run experi script hello run python exampl specif section exampl import experi core import scriptrunconfig experi experi run execut code receiv error messag user program fail nameerror true defin log file give follow detail file line notebookhasbeencomplet true nameerror true defin problem file contain line provid assist resolv thank",
        "Question_gpt_summary_original":"The user encountered an error while running an Azure ML Python example script. The error message states that \"User program failed with NameError: name 'true' is not defined\". The log file indicates that the error occurred on line 67 of the \"train.ipynb\" file, but the file does not contain 67 lines. The user is seeking assistance to resolve the issue.",
        "Question_gpt_summary":"user encount error run python exampl script error messag state user program fail nameerror true defin log file indic error occur line train ipynb file file contain line user seek assist resolv issu",
        "Answer_original_content":"grahambenson thank question abl run successfulli error shown link sampl environ http github com azur machinelearningnotebook tree master us train environ help post http doc microsoft com answer question nameerror try run scriptrunconfig html abl resolv problem issu script paramet scriptrunconfig class file ipynb file chang extens download notebook follow json object visibl microsoft host notebookhasbeencomplet true chang true valu true script execut successfulli ramr msft have solv issu receiv warn warn pip instal depend environ file list pip conda depend curat environ sklearn ubuntu cpu contain packag scikit learn numpi panda follow notebook code core import workspac environ core import scriptrunconfig experi workspac config myenv environ workspac sklearn ubuntu cpu exp experi myexp workspac instanti environ myenv environ myenv add train script run config runconfig scriptrunconfig sourc directori script train attach comput target run config runconfig run config target local attach environ run config runconfig run config environ myenv submit run run exp submit runconfig train script taken exampl http doc microsoft com python api overview azur view azur run train script train sklearn import svm import numpi import joblib import custom ag train arrai train train reshap churn train ye ye ye ye ye clf svm svc gamma clf fit train train makedir output exist true joblib dump valu clf filenam output churn model pkl train script assist receiv error messag environ microsoft curat environ thank",
        "Answer_preprocessed_content":"thank question abl run successfulli error shown link sampl environ help post abl resolv problem issu script paramet scriptrunconfig class file ipynb file chang extens download notebook follow json object visibl microsoft chang true valu true script execut successfulli have solv issu receiv warn warn depend environ file list pip conda curat environ contain packag follow notebook code core import workspac environ core import scriptrunconfig experi myenv exp experi instanti environ myenv environ add train script run config runconfig attach comput target run config local attach environ run config myenv submit run run script taken exampl script sklearn import svm import numpi import joblib import custom ag churn clf script assist receiv error messag environ microsoft curat environ thank",
        "Answer_gpt_summary_original":"the answer provides a solution to the nameerror encountered when running an experiment script in azure notebook. the issue was caused by the script parameter in the scriptrunconfig class being a .ipynb file instead of a .py file. changing the extension to *.py and downloading the notebook resolved the problem. the answer also mentions a warning about pip-installed dependencies in the environment file and provides a notebook code example.",
        "Answer_gpt_summary":"answer provid solut nameerror encount run experi script azur notebook issu caus script paramet scriptrunconfig class ipynb file instead file chang extens download notebook resolv problem answer mention warn pip instal depend environ file provid notebook code exampl"
    },
    {
        "Question_id":42017727.0,
        "Question_title":"Upload Saved ML Model in R (local) to Azure Machine Learning Studio",
        "Question_body":"<p>I am trying to reduce my development headaches for creating a ML Webservice on Azure ML Studio. One of the things that stuck me was can we just upload .rda files in the workbench and load it via an RScript (like in the figure below). <\/p>\n\n<p><img src=\"https:\/\/raw.githubusercontent.com\/pratos\/pratos.github.io\/master\/images\/stackb1model.png\" alt=\"Do\"><\/p>\n\n<p>But can't connect directly to the R Script block. There's another way to do it (works to upload packages that aren't available in Azure's R directories) -- using zip. But there isn't really any resource out there that I found to access the .rda file in .zip.<\/p>\n\n<p>I have 2 options here, make the .zip work or any other work around where I can directly use my .rda model. If someone could guide me about how to go forward it would appreciate it.<\/p>\n\n<p>Note: Currently, I'm creating models via the \"Create RModel\" block, training them and saving it, so that I can use it to make a predictive web service. But for models like Random Forest, not sure how the randomness might create models (local versions and Azure versions are different, the setting of seed also isn't very helpful). A bit tight on schedule, Azure ML seems boxed for creating iterations and automating the ML workflow (or maybe I'm doing it wrong).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1486101642917,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":730.0,
        "Answer_body":"<p>Here is an example of uploading a .rda file for scoring:\n<a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Womens-Health-Risk-Assessment-using-the-XGBoost-classification-algorithm-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Womens-Health-Risk-Assessment-using-the-XGBoost-classification-algorithm-1<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42017727",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1486104439200,
        "Question_original_content":"upload save model local studio try reduc develop headach creat webservic studio thing stuck upload rda file workbench load rscript like figur connect directli script block wai work upload packag aren avail azur directori zip isn resourc access rda file zip option zip work work directli us rda model guid forward appreci note current creat model creat rmodel block train save us predict web servic model like random forest sure random creat model local version azur version differ set seed isn help bit tight schedul box creat iter autom workflow mayb wrong",
        "Question_preprocessed_content":"upload save model studio try reduc develop headach creat webservic studio thing stuck upload rda file workbench load rscript connect directli script block wai zip isn resourc access rda file zip option zip work work directli us rda model guid forward appreci note current creat model creat rmodel block train save us predict web servic model like random forest sure random creat model bit tight schedul box creat iter autom workflow",
        "Question_gpt_summary_original":"The user is facing challenges in uploading a saved ML model in R (local) to Azure Machine Learning Studio. They are unable to directly connect to the R Script block and are looking for a workaround to use their .rda model. They have tried using zip but are unable to access the .rda file in .zip. The user is also unsure about how the randomness in models like Random Forest might create differences between local and Azure versions. They are looking for guidance on how to proceed and are feeling constrained by the limitations of Azure ML for automating the ML workflow.",
        "Question_gpt_summary":"user face challeng upload save model local studio unabl directli connect script block look workaround us rda model tri zip unabl access rda file zip user unsur random model like random forest creat differ local azur version look guidanc proce feel constrain limit autom workflow",
        "Answer_original_content":"exampl upload rda file score http galleri cortanaintellig com experi women health risk assess xgboost classif algorithm",
        "Answer_preprocessed_content":"exampl upload rda file score",
        "Answer_gpt_summary_original":"the answer provides a solution to the user's challenge of uploading a saved ml model in r to azure studio by sharing a link to an example of uploading a .rda file for scoring.",
        "Answer_gpt_summary":"answer provid solut user challeng upload save model azur studio share link exampl upload rda file score"
    },
    {
        "Question_id":60637170.0,
        "Question_title":"How to pass arguments to scoring file when deploying a Model in AzureML",
        "Question_body":"<p>I am deploying a trained model to an ACI endpoint on Azure Machine Learning, using the Python SDK.\nI have created my score.py file, but I would like that file to be called with an argument being passed (just like with a training file) that I can interpret using <code>argparse<\/code>.\nHowever, I don't seem to find how I can pass arguments\nThis is the code I have to create the InferenceConfig environment and which obviously does not work.  Should I fall back on using the extra Docker file steps or so?<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.environment import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment('my_hosted_environment')\nenv.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['scikit-learn'],\n    pip_packages=['azureml-defaults'])\nscoring_script = 'score.py --model_name ' + model_name\ninference_config = InferenceConfig(entry_script=scoring_script, environment=env)\n<\/code><\/pre>\n\n<p>Adding the score.py for reference on how I'd love to use the arguments in that script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#removed imports\nimport argparse\n\ndef init():\n    global model\n\n    parser = argparse.ArgumentParser(description=\"Load sklearn model\")\n    parser.add_argument('--model_name', dest=\"model_name\", required=True)\n    args, _ = parser.parse_known_args()\n\n    model_path = Model.get_model_path(model_name=args.model_name)\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    try:\n        data = json.loads(raw_data)['data']\n        data = np.array(data)\n        result = model.predict(data)\n        return result.tolist()\n\n    except Exception as e:\n        result = str(e)\n        return result\n<\/code><\/pre>\n\n<p>Interested to hear your thoughts<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_time":1583933260433,
        "Question_favorite_count":null,
        "Question_last_edit_time":1584005920356,
        "Question_score":4.0,
        "Question_view_count":1681.0,
        "Answer_body":"<p>How to deploy using environments can be found here <a href=\"https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2FAzure%2FMachineLearningNotebooks%2Fblob%2Fmaster%2Fhow-to-use-azureml%2Fdeployment%2Fdeploy-to-cloud%2Fmodel-register-and-deploy.ipynb&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7Ce06d310b0447416ab46b08d7bc836a81%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637185146436156499&amp;sdata=uQo332dpjuiNqWFCguvs3Kgg7UUMN8MBEzLxTPyH4MM%3D&amp;reserved=0\" rel=\"nofollow noreferrer\">model-register-and-deploy.ipynb<\/a> .  InferenceConfig class accepts  source_directory and entry_script <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py#parameters\" rel=\"nofollow noreferrer\">parameters<\/a>, where source_directory  is a path to the folder that contains all files(score.py and any other additional files) to create the image. <\/p>\n\n<p>This <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-multi-model\/multi-model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">multi-model-register-and-deploy.ipynb<\/a> has code snippets on how to create InferenceConfig with source_directory and entry_script.<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\n\nmyenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\ninference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n\nservice = Model.deploy(workspace=ws,\n                       name='sklearn-mnist-svc',\n                       models=[model], \n                       inference_config=inference_config,\n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n\nprint(service.scoring_uri)\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1584011988083,
        "Answer_score":-2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60637170",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1584005785480,
        "Question_original_content":"pass argument score file deploi model deploi train model aci endpoint python sdk creat score file like file call argument pass like train file interpret argpars pass argument code creat inferenceconfig environ obvious work fall extra docker file step core conda depend import condadepend core environ import environ core model import inferenceconfig env environ host environ env python conda depend condadepend creat conda packag scikit learn pip packag default score script score model model infer config inferenceconfig entri script score script environ env ad score refer love us argument script remov import import argpars def init global model parser argpars argumentpars descript load sklearn model parser add argument model dest model requir true arg parser pars known arg model path model model path model arg model model joblib load model path def run raw data try data json load raw data data data arrai data result model predict data return result tolist except result str return result interest hear thought",
        "Question_preprocessed_content":"pass argument score file deploi model deploi train model aci endpoint python sdk creat file like file call argument pass interpret pass argument code creat inferenceconfig environ obvious work fall extra docker file step ad refer love us argument script interest hear thought",
        "Question_gpt_summary_original":"The user is facing a challenge in passing arguments to the score.py file when deploying a trained model to an ACI endpoint on Azure Machine Learning using the Python SDK. They have created the score.py file and want it to be called with an argument that can be interpreted using argparse, but they are unable to find a way to pass arguments. The user has shared their code for creating the InferenceConfig environment and the score.py file for reference.",
        "Question_gpt_summary":"user face challeng pass argument score file deploi train model aci endpoint python sdk creat score file want call argument interpret argpars unabl wai pass argument user share code creat inferenceconfig environ score file refer",
        "Answer_original_content":"deploi environ model regist deploi ipynb inferenceconfig class accept sourc directori entri script paramet sourc directori path folder contain file score addit file creat imag multi model regist deploi ipynb code snippet creat inferenceconfig sourc directori entri script core webservic import webservic core model import inferenceconfig core environ import environ myenv environ conda specif myenv file path myenv yml infer config inferenceconfig entri script score environ myenv servic model deploi workspac sklearn mnist svc model model infer config infer config deploy config aciconfig servic wait deploy output true print servic score uri",
        "Answer_preprocessed_content":"deploi environ inferenceconfig class accept paramet path folder contain addit file creat imag code snippet creat inferenceconfig",
        "Answer_gpt_summary_original":"the answer suggests that the user can deploy using environments and provides a link to a notebook with instructions. it also mentions the use of the inferenceconfig class, which accepts source_directory and entry_script parameters. the answer provides a link to another notebook with code snippets on how to create inferenceconfig with source_directory and entry_script. additionally, the answer includes code snippets for creating an environment and deploying the model.",
        "Answer_gpt_summary":"answer suggest user deploi environ provid link notebook instruct mention us inferenceconfig class accept sourc directori entri script paramet answer provid link notebook code snippet creat inferenceconfig sourc directori entri script addition answer includ code snippet creat environ deploi model"
    },
    {
        "Question_id":null,
        "Question_title":"AutoML tables - sample size of an average",
        "Question_body":"Hi everyonesI'm new to google automl tables and have a basic question about which data is worthwhile including in the training of my model.I have a dataset of golfers and will be looking at the averages of scores over different periods. For example, average over the past 3 months, 6 months, 1 year etc.My question is, is it worthwhile also including the sample size for each date range for each player. For example, over the past 3 months, some players will have a sample size of 28 while some will only have 2. Those players that have 28 rounds will have more accurate averages than those with 2. However, I didn't know whether google automl tables would pick up this link automatically, whether I could create a different weighting\/reliability variable, or whether there's a way to specify a link between columns? Or if this automated type of automl isn't really suitable or just leave out that sample size variable?Thanks in advance",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645771440000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":66.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-tables-sample-size-of-an-average\/td-p\/397276\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-11T11:17:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hello,\n\nGoogle Groups are reserved for general product discussion, StackOverflow for technical questions whereas Issue Tracker for product bugs (unexpected behaviors) and feature requests. To get a better support you should post to the relevant forum, thus please read the Community Support article for better understanding."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"automl tabl sampl size averag everyonesi new googl automl tabl basic question data worthwhil includ train model dataset golfer look averag score differ period exampl averag past month month year question worthwhil includ sampl size date rang player exampl past month player sampl size player round accur averag know googl automl tabl pick link automat creat differ weight reliabl variabl wai specifi link column autom type automl isn suitabl leav sampl size variabl thank advanc",
        "Question_preprocessed_content":"automl tabl sampl size averag everyonesi new googl automl tabl basic question data worthwhil includ train dataset golfer look averag score differ period exampl averag past month month year question worthwhil includ sampl size date rang player exampl past month player sampl size player round accur averag know googl automl tabl pick link automat creat differ variabl wai specifi link column autom type automl isn suitabl leav sampl size variabl thank advanc",
        "Question_gpt_summary_original":"The user is new to Google AutoML Tables and is unsure whether to include the sample size for each date range for each player in their dataset of golfers. They are concerned that some players with a smaller sample size may have less accurate averages and are unsure if AutoML Tables will automatically pick up this link or if they should create a different weighting\/reliability variable. They are seeking advice on whether to include the sample size variable or leave it out.",
        "Question_gpt_summary":"user new googl automl tabl unsur includ sampl size date rang player dataset golfer concern player smaller sampl size accur averag unsur automl tabl automat pick link creat differ weight reliabl variabl seek advic includ sampl size variabl leav",
        "Answer_original_content":"hello googl group reserv gener product discuss stackoverflow technic question issu tracker product bug unexpect behavior featur request better support post relev forum read commun support articl better understand",
        "Answer_preprocessed_content":"hello googl group reserv gener product discuss stackoverflow technic question issu tracker product bug featur request better support post relev forum read commun support articl better understand",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the answer suggests that the user should post their question to the relevant forum for better support. the answer also provides information on which forum to use for different types of questions.",
        "Answer_gpt_summary":"solut provid answer answer suggest user post question relev forum better support answer provid inform forum us differ type question"
    },
    {
        "Question_id":null,
        "Question_title":"Private endpoint in hub-and-spoke architecture (Try to access a storage account in my different vnets)",
        "Question_body":"Hello,\n\nI'm trying to register a dataset in my different Azure Machine Learning workspaces (of each vnet spoke) but for that I need to connect from my ML workspaces to my storage account (dev or prod) located in the vnet 'spoke-pdata'. (screenshot)\n\nThe datastore (refer to storage account) is already register with Access key.\nI have already checked:\n- peering between my hub and my different spoke\n- private dns zone (privatelink.blob.core.windows.net) in my hub vnet with record set for storage\n- all virtual network links between hub and each spoke (privatelink blob)\n- private dns zone (privatelink.api.azureml.ms) in my hub vnet with record set for each azure machine learning\n- all virtual network links between hub and each spoke (privatelink azureml)\n- storage account roles for each azure ML\n- contributor role for each azure ml in storage account (to be sure it's not role issue)\n- NSG off for storage account\n\n\n\n\nWhen I set ''All network'' in network of my storage account, Azure ML is connected to my storage account and I can register my dataset... So it's a network issue.\nI tried nslookup in AML and I can get the private ip of my storage account:\n\n\nWhat did I forget? Maybe I misunderstood the concept of private endpoint... From my point of view, I just need a private endpoint connected to a private dns. And in case there are two different vnet, I need to connect them with a private link and a vnet peering.\nI read the documentation but did not find a similar case... I am still confused.\n\nError:\n\n\nScriptExecutionException was caused by StreamAccessException.\nStreamAccessException was caused by AuthenticationException.\nAuthentication failed for 'AzureBlob GetReference' operation at '[REDACTED]' with '403: AuthorizationFailure'. Please make sure the SAS token or the account key is correct.\nFailed due to inner exception of type: StorageException",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644509273567,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/730824\/private-endpoint-in-hub-and-spoke-architecture-try.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-11T03:19:40.743Z",
                "Answer_score":1,
                "Answer_body":"Hello @mhajji\n\nThanks for reaching out to us, one of the possible reason for this issue may be firewall is not enabled.\n\nCould you please validate if your storage account is firewall enabled ?\n\nPlease follow this steps to do so:\nAzure Portal -> Storage Account -> Networking -> Check Allow Access From (All Networks \/ Selected Networks)\nIf it is \"Selected Networks\" - It means the storage account is firewall enabled.\n\nIf the storage account is firewall enabled , check your CORS is setting correctly as below:\n\nFor me, I am using Allowed origins as below:\nhttps:\/\/mlworkspace.azure.ai,https:\/\/ml.azure.com,https:\/\/*.ml.azure.com,https:\/\/mlworkspacecanary.azure.ai,https:\/\/mlworkspace.azureml-test.net\n\nLet us know if the above steps helps out in resolving the issue , please don't forget to Upvote and Accept the Answer\n\nRegards,\nYutong",
                "Answer_comment_count":5,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":22.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"privat endpoint hub spoke architectur try access storag account differ vnet hello try regist dataset differ workspac vnet spoke need connect workspac storag account dev prod locat vnet spoke pdata screenshot datastor refer storag account regist access kei check peer hub differ spoke privat dn zone privatelink blob core window net hub vnet record set storag virtual network link hub spoke privatelink blob privat dn zone privatelink api hub vnet record set virtual network link hub spoke privatelink storag account role contributor role storag account sure role issu nsg storag account set network network storag account connect storag account regist dataset network issu tri nslookup aml privat storag account forget mayb misunderstood concept privat endpoint point view need privat endpoint connect privat dn case differ vnet need connect privat link vnet peer read document similar case confus error scriptexecutionexcept caus streamaccessexcept streamaccessexcept caus authenticationexcept authent fail azureblob getrefer oper redact authorizationfailur sure sa token account kei correct fail inner except type storageexcept",
        "Question_preprocessed_content":"privat endpoint architectur hello try regist dataset differ workspac need connect workspac storag account locat vnet datastor regist access kei check peer hub differ spoke privat dn zone hub vnet record set storag virtual network link hub spoke privat dn zone hub vnet record set virtual network link hub spoke storag account role contributor role storag account nsg storag account set network network storag account connect storag account regist network issu tri nslookup aml privat storag account forget mayb misunderstood concept privat point view need privat endpoint connect privat dn case differ vnet need connect privat link vnet peer read document similar confus error scriptexecutionexcept caus streamaccessexcept streamaccessexcept caus authenticationexcept authent fail azureblob getrefer oper authorizationfailur sure sa token account kei correct fail inner except type storageexcept",
        "Question_gpt_summary_original":"The user is facing challenges in accessing a storage account located in a different vnet spoke from their Azure Machine Learning workspaces. They have tried various solutions such as peering between hub and spoke, private DNS zones, virtual network links, storage account roles, and contributor roles, but they are still unable to connect. The user suspects that they may have misunderstood the concept of private endpoint and is seeking help to resolve the issue. The error message indicates an authentication failure for the 'AzureBlob GetReference' operation due to an incorrect SAS token or account key.",
        "Question_gpt_summary":"user face challeng access storag account locat differ vnet spoke workspac tri solut peer hub spoke privat dn zone virtual network link storag account role contributor role unabl connect user suspect misunderstood concept privat endpoint seek help resolv issu error messag indic authent failur azureblob getrefer oper incorrect sa token account kei",
        "Answer_original_content":"hello mhajji thank reach possibl reason issu firewal enabl valid storag account firewal enabl follow step azur portal storag account network check allow access network select network select network mean storag account firewal enabl storag account firewal enabl check cor set correctli allow origin http mlworkspac azur http azur com http azur com http mlworkspacecanari azur http mlworkspac test net let know step help resolv issu forget upvot accept answer regard yutong",
        "Answer_preprocessed_content":"hello thank reach possibl reason issu firewal enabl valid storag account firewal enabl follow step azur portal storag account network check allow access select network mean storag account firewal enabl storag account firewal enabl check cor set correctli allow origin let know step help resolv issu forget upvot accept answer regard yutong",
        "Answer_gpt_summary_original":"the possible solution to the user's challenge in accessing a storage account in a hub-and-spoke architecture is to check if the storage account is firewall enabled and to validate the cors settings. the user can follow the steps provided to check if the storage account is firewall enabled and ensure that the cors settings are correctly set.",
        "Answer_gpt_summary":"possibl solut user challeng access storag account hub spoke architectur check storag account firewal enabl valid cor set user follow step provid check storag account firewal enabl ensur cor set correctli set"
    },
    {
        "Question_id":68812238.0,
        "Question_title":"How to export a MLFlow Model from Azure Databricks as an Azure DevOps Artifacts for CD phase?",
        "Question_body":"<p>I am trying to create an MLOps Pipeline using Azure DevOps and Azure Databricks. From Azure DevOps, I am submitting a Databricks job to a cluster, which trains a Machine Learning Model and saves it into MLFlow Model Registry with a custom flavour (using PyFunc Custom Model).<\/p>\n<p>Now after the job gets over, I want to export this MLFlow Object (with all dependencies - Conda dependencies, two model files - one <code>.pkl<\/code> and one <code>.h5<\/code>, the Python Class with <code>load_context()<\/code> and <code>predict()<\/code> functions defined so that after exporting I can import it and call predict as we do with MLFlow Models).<\/p>\n<p>How do I export this entire MLFlow Model and save it as an AzureDevOps Artifact to be used in the CD phase (where I will deploy it to an AKS cluster with a custom base image)?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1629179063687,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":575.0,
        "Answer_body":"<p>There is no official way to export a Databricks MLflow run from one workspace to another. However, there is an &quot;unofficial&quot; tool that does most of the job with the main limitation being that notebook revisions linked to a run cannot be exported due to lack of a REST API endpoint for this.<\/p>\n<p><a href=\"https:\/\/github.com\/amesar\/mlflow-export-import\" rel=\"nofollow noreferrer\">https:\/\/github.com\/amesar\/mlflow-export-import<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68812238",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1629787454223,
        "Question_original_content":"export model azur databrick azur devop artifact phase try creat mlop pipelin azur devop azur databrick azur devop submit databrick job cluster train machin learn model save model registri custom flavour pyfunc custom model job get want export object depend conda depend model file pkl python class load context predict function defin export import predict model export entir model save azuredevop artifact phase deploi ak cluster custom base imag",
        "Question_preprocessed_content":"export model azur databrick azur devop artifact phase try creat mlop pipelin azur devop azur databrick azur devop submit databrick job cluster train machin learn model save model registri custom flavour job get want export object export entir model save azuredevop artifact phase",
        "Question_gpt_summary_original":"The user is facing challenges in exporting an MLFlow Model from Azure Databricks as an Azure DevOps Artifacts for the CD phase. They have successfully trained a Machine Learning Model and saved it into the MLFlow Model Registry with a custom flavour, but are struggling to export the entire MLFlow Model with all dependencies, including Conda dependencies, two model files, and the Python Class with load_context() and predict() functions defined. The user wants to save the exported MLFlow Model as an Azure DevOps Artifact to be used in the CD phase for deployment to an AKS cluster with a custom base image.",
        "Question_gpt_summary":"user face challeng export model azur databrick azur devop artifact phase successfulli train machin learn model save model registri custom flavour struggl export entir model depend includ conda depend model file python class load context predict function defin user want save export model azur devop artifact phase deploy ak cluster custom base imag",
        "Answer_original_content":"offici wai export databrick run workspac unoffici tool job main limit notebook revis link run export lack rest api endpoint http github com amesar export import",
        "Answer_preprocessed_content":"offici wai export databrick run workspac unoffici tool job main limit notebook revis link run export lack rest api endpoint",
        "Answer_gpt_summary_original":"possible solution: use an unofficial tool called \"export-import\" from github to export a databricks run from one workspace to another. however, notebook revisions linked to a run cannot be exported due to the lack of a rest api endpoint for this.",
        "Answer_gpt_summary":"possibl solut us unoffici tool call export import github export databrick run workspac notebook revis link run export lack rest api endpoint"
    },
    {
        "Question_id":55551617.0,
        "Question_title":"Azure ML and r scripts",
        "Question_body":"<p>I have dataframe with columns <\/p>\n\n<pre><code>date    open    high    low     close   adjclose    volume\n<\/code><\/pre>\n\n<p>I want to add one more column named \"result\"(1 if close > open, 0 if close &lt; open)<\/p>\n\n<p>I do<\/p>\n\n<pre><code># Map 1-based optional input ports to variables\ndata &lt;- maml.mapInputPort(1) # class: data.frame\n\n\n\n# calculate pass\/fail\ndata$result &lt;- as.factor(sapply(data$close,function(res) \n    if (res - data$open &gt;= 0) '1' else '0'))\n\n# Select data.frame to be sent to the output Dataset port\nmaml.mapOutputPort(\"data\");\n<\/code><\/pre>\n\n<p>But I have only 1 in result. Where is the problem?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1554571406233,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":77.0,
        "Answer_body":"<p>The <code>if\/else<\/code> can return only a single TRUE\/FALSE and is not vectorized for length > 1.  It may be suitable to use <code>ifelse<\/code> (but that is also not required and would be less efficient compared to direct coersion of logical vector to binary (<code>as.integer<\/code>).   In the OP's code, the 'close' column elements are looped  (<code>sapply<\/code>) and subtracted from the whole 'open' column.  The intention might be to do elementwise subtraction.  In that case, <code>-<\/code> between the columns is much cleaner and efficient (as these operations are vectorized)<\/p>\n\n<pre><code>data$result &lt;- with(data, factor(as.integer((close - open) &gt;= 0)))\n<\/code><\/pre>\n\n<p>In the above, we get the difference between the columns ('close', 'open'), check if it is greater than or equal to 0 (returns logical vector), convert it to binary (<code>as.integer<\/code> - TRUE -> 1, FALSE -> 0) and then change it to <code>factor<\/code> type (if needed)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55551617",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1554572339496,
        "Question_original_content":"script datafram column date open high low close adjclos volum want add column name result close open close select data frame sent output dataset port maml mapoutputport data result problem",
        "Question_preprocessed_content":"script datafram column want add column name result close open close open result problem",
        "Question_gpt_summary_original":"The user is facing a challenge in adding a new column named \"result\" to a dataframe using Azure ML and R scripts. The column should have a value of 1 if the \"close\" value is greater than the \"open\" value, and 0 if the \"close\" value is less than the \"open\" value. However, the user is only getting a value of 1 in the \"result\" column and is seeking help to identify the problem.",
        "Question_gpt_summary":"user face challeng ad new column name result datafram script column valu close valu greater open valu close valu open valu user get valu result column seek help identifi problem",
        "Answer_original_content":"return singl true fals vector length suitabl us ifels requir effici compar direct coersion logic vector binari integ code close column element loop sappli subtract open column intent elementwis subtract case column cleaner effici oper vector data result differ column close open check greater equal return logic vector convert binari integ true fals chang factor type need",
        "Answer_preprocessed_content":"return singl vector length suitabl us code close column element loop subtract open column intent elementwis subtract case column cleaner effici differ column check greater equal convert binari chang type",
        "Answer_gpt_summary_original":"the answer suggests using vectorized operations instead of loops to create a new column in a dataframe based on the values of other columns. it recommends using the '-' operator to subtract columns element-wise and then converting the resulting logical vector to binary using 'as.integer'. the answer also mentions that 'ifelse' can be used instead of 'if\/else' but may be less efficient.",
        "Answer_gpt_summary":"answer suggest vector oper instead loop creat new column datafram base valu column recommend oper subtract column element wise convert result logic vector binari integ answer mention ifels instead effici"
    },
    {
        "Question_id":70287087.0,
        "Question_title":"How to deploy sagemaker.workflow.pipeline.Pipeline?",
        "Question_body":"<p>I have a <code>sagemaker.workflow.pipeline.Pipeline<\/code> which contains multiple <code>sagemaker.workflow.steps.ProcessingStep<\/code> and each <code>ProcessingStep<\/code> contains <code>sagemaker.processing.ScriptProcessor<\/code>.<\/p>\n<p>The current pipeline graph look like the below shown image. It will take data from multiple sources from S3, process it and create a final dataset using the data from previous steps.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6XImq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6XImq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As the <code>Pipeline<\/code> object doesn't support <code>.deploy<\/code> method, how to deploy this pipeline?<\/p>\n<p>While inference\/scoring, When we receive a raw data(single row for each source), how to trigger the pipeline?<\/p>\n<p>or Sagemaker Pipeline is designed for only data processing and model training on huge\/batch data? Not for the inference with the single data point?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639040387083,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":313.0,
        "Answer_body":"<blockquote>\n<p>As the Pipeline object doesn't support .deploy method, how to deploy this pipeline?<\/p>\n<\/blockquote>\n<p>Pipeline does not have a <code>.deploy()<\/code> method, no<\/p>\n<p>Use <code>pipeline.upsert(role_arn='...')<\/code> to create\/update the pipeline definition to SageMaker, then call <code>pipeline.start()<\/code> . Docs <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#pipeline\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<blockquote>\n<p>While inference\/scoring, When we receive a raw data(single row for each source), how to trigger the pipeline?<\/p>\n<\/blockquote>\n<p>There are actually two types of pipelines in SageMaker. Model Building Pipelines (which you have in your question), and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>, which are used for Inference. AWS definitely should have called the former &quot;workflows&quot;<\/p>\n<p>You can use a model building pipeline to setup a serial inference pipeline<\/p>\n<p>To do pre-processing in a serial inference pipeline, you want to train an encoder\/estimator (such as SKLearn) and save its model. Then train a learning algorithm, and save its model, then create a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/pipeline.html\" rel=\"nofollow noreferrer\">PipelineModel<\/a> using both models<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70287087",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1639073178300,
        "Question_original_content":"deploi workflow pipelin pipelin workflow pipelin pipelin contain multipl workflow step processingstep processingstep contain process scriptprocessor current pipelin graph look like shown imag data multipl sourc process creat final dataset data previou step pipelin object support deploi method deploi pipelin infer score receiv raw data singl row sourc trigger pipelin pipelin design data process model train huge batch data infer singl data point",
        "Question_preprocessed_content":"deploi contain multipl contain current pipelin graph look like shown imag data multipl sourc process creat final dataset data previou step object support method deploi pipelin receiv raw data trigger pipelin pipelin design data process model train data infer singl data point",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a sagemaker.workflow.pipeline.Pipeline that contains multiple sagemaker.workflow.steps.ProcessingStep, each with sagemaker.processing.ScriptProcessor. The user is unsure how to trigger the pipeline during inference\/scoring with a single data point and is questioning whether Sagemaker Pipeline is only designed for data processing and model training on batch data.",
        "Question_gpt_summary":"user face challeng deploi workflow pipelin pipelin contain multipl workflow step processingstep process scriptprocessor user unsur trigger pipelin infer score singl data point question pipelin design data process model train batch data",
        "Answer_original_content":"pipelin object support deploi method deploi pipelin pipelin deploi method us pipelin upsert role arn creat updat pipelin definit pipelin start doc infer score receiv raw data singl row sourc trigger pipelin actual type pipelin model build pipelin question serial infer pipelin infer aw definit call workflow us model build pipelin setup serial infer pipelin pre process serial infer pipelin want train encod estim sklearn save model train learn algorithm save model creat pipelinemodel model",
        "Answer_preprocessed_content":"pipelin object support deploi method deploi pipelin pipelin method us pipelin definit doc receiv raw data trigger pipelin actual type pipelin model build pipelin serial infer pipelin infer aw definit call workflow us model build pipelin setup serial infer pipelin serial infer pipelin want train save model train learn algorithm save model creat pipelinemodel model",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- use pipeline.upsert(role_arn='...') to create\/update the pipeline definition, then call pipeline.start() to deploy the pipeline.\n- to trigger the pipeline for inference\/scoring with a single data point, use a model building pipeline to setup a serial inference pipeline to do pre-processing. train an encoder\/estimator and save its model, then train a learning algorithm and save its model, then create a pipelinemodel using both models.",
        "Answer_gpt_summary":"possibl solut answer us pipelin upsert role arn creat updat pipelin definit pipelin start deploi pipelin trigger pipelin infer score singl data point us model build pipelin setup serial infer pipelin pre process train encod estim save model train learn algorithm save model creat pipelinemodel model"
    },
    {
        "Question_id":73621446.0,
        "Question_title":"Log Any Type of Model in MLflow",
        "Question_body":"<p>I am trying to create a wrapper function that allows my Data Scientists to log their models in MLflow.<\/p>\n<p>This is what the function looks like,<\/p>\n<pre><code>def log_model(self, params, metrics, model, run_name, artifact_path, artifacts=None):\n\n    with mlflow.start_run(run_name=run_name):\n        run_id = mlflow.active_run().info.run_id\n        mlflow.log_params(params)\n        mlflow.log_metrics(metrics)\n\n        if model:\n            mlflow.lightgbm.log_model(model, artifact_path=artifact_path)\n\n        if artifacts:\n            for artifact in artifacts:\n                mlflow.log_artifact(artifact, artifact_path=artifact_path)\n\n    return run_id\n<\/code><\/pre>\n<p>It can be seen here that the model is being logged as a <code>lightgbm<\/code> model, however, the <code>model<\/code> parameter that is passed into this function can be of any type.<\/p>\n<p>How can I update this function, so that it will be able to log any kind of model?<\/p>\n<p>As far as I know, there is no <code>log_model<\/code> function that comes with <code>mlflow<\/code>. It's always <code>mlflow.&lt;model_type&gt;.log_model<\/code>.<\/p>\n<p>How can I go about handling this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662464447387,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":39.0,
        "Answer_body":"<p>I was able to solve this using the following approach,<\/p>\n<pre><code>def log_model(model, artifact_path):\n    model_class = get_model_class(model).split('.')[0]\n\n    try:\n        log_model = getattr(mlflow, model_class).log_model\n        log_model(model, artifact_path)\n    except AttributeError:\n        logger.info('The log_model function is not available as expected!')\n\ndef get_model_class(model):\n    klass = model.__class__\n    module = klass.__module__\n\n    if module == 'builtins':\n        return klass.__qualname__\n    return module + '.' + klass.__qualname__\n<\/code><\/pre>\n<p>From what I have seen, this will be able to handle most cases. The <code>get_model_class()<\/code> method will return the class used to develop the model and based on this, we can use the <code>getattr()<\/code> method to extract the relevant <code>log_model()<\/code> method.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73621446",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1663636318523,
        "Question_original_content":"log type model try creat wrapper function allow data scientist log model function look like def log model self param metric model run artifact path artifact start run run run run activ run info run log param param log metric metric model lightgbm log model model artifact path artifact path artifact artifact artifact log artifact artifact artifact path artifact path return run seen model log lightgbm model model paramet pass function type updat function abl log kind model far know log model function come log model handl",
        "Question_preprocessed_content":"log type model try creat wrapper function allow data scientist log model function look like seen model log model paramet pass function type updat function abl log kind model far know function come handl",
        "Question_gpt_summary_original":"The user is trying to create a wrapper function that allows Data Scientists to log their models in MLflow. The current function logs the model as a lightgbm model, but the model parameter passed into the function can be of any type. The user is seeking a way to update the function to log any kind of model, as there is no log_model function that comes with mlflow.",
        "Question_gpt_summary":"user try creat wrapper function allow data scientist log model current function log model lightgbm model model paramet pass function type user seek wai updat function log kind model log model function come",
        "Answer_original_content":"abl solv follow approach def log model model artifact path model class model class model split try log model getattr model class log model log model model artifact path attributeerror logger info log model function avail expect def model class model klass model class modul klass modul modul builtin return klass qualnam return modul klass qualnam seen abl handl case model class method return class develop model base us getattr method extract relev log model method",
        "Answer_preprocessed_content":"abl solv follow approach seen abl handl case method return class develop model base us method extract relev method",
        "Answer_gpt_summary_original":"the answer suggests a possible solution to the challenge of creating a wrapper function to log any type of model. the solution involves defining a new function called log_model that takes in the model and artifact path as parameters. the function uses the get_model_class() method to determine the class used to develop the model and then uses the getattr() method to extract the relevant log_model() method. this approach should be able to handle most cases.",
        "Answer_gpt_summary":"answer suggest possibl solut challeng creat wrapper function log type model solut involv defin new function call log model take model artifact path paramet function us model class method determin class develop model us getattr method extract relev log model method approach abl handl case"
    },
    {
        "Question_id":32451243.0,
        "Question_title":"How to load images faster from Azure Blob?",
        "Question_body":"<p>I've been trying to upload some images to azure blob and then using <strong>ImageReader<\/strong> in <strong>Azure ML studio<\/strong> to read them from the blob. The problem is that ImageReader takes a lot of time to load images and I need it in real time. <br>\nI also tried making a <strong>csv<\/strong> of <strong>4 images (four rows)<\/strong> containing 800x600 pixels as columns <strong>(500,000 cols. approx)<\/strong> and tried simple <strong>Reader<\/strong>. Reader took <strong>31 mins<\/strong> to read the file from the blob.<br>\nI want to know the alternate methods of loading and reading images in Azure ML studio. If anyone know any other method or can share a helpful and relevant link.<br>\nPlease share if i can speed up ImageReader by any means.\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1441695780200,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":803.0,
        "Answer_body":"<p>Look at the Azure CDN <a href=\"http:\/\/azure.microsoft.com\/en-us\/services\/cdn\/\" rel=\"nofollow\">http:\/\/azure.microsoft.com\/en-us\/services\/cdn\/<\/a> , after which the blobs will get an alternative url. My blob downloads became about 4 times faster after switching.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32451243",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1441739836496,
        "Question_original_content":"load imag faster azur blob try upload imag azur blob imageread studio read blob problem imageread take lot time load imag need real time tri make csv imag row contain pixel column col approx tri simpl reader reader took min read file blob want know altern method load read imag studio know method share help relev link share speed imageread mean thank",
        "Question_preprocessed_content":"load imag faster azur blob try upload imag azur blob imageread studio read blob problem imageread take lot time load imag need real time tri make csv imag contain pixel column tri simpl reader reader took min read file blob want know altern method load read imag studio know method share help relev link share speed imageread mean thank",
        "Question_gpt_summary_original":"The user is facing challenges with loading images quickly from Azure Blob using ImageReader in Azure ML studio. They have tried creating a csv file with 4 images and using a simple Reader, but it took 31 minutes to read the file from the blob. The user is seeking alternate methods to load and read images in Azure ML studio and is looking for ways to speed up ImageReader.",
        "Question_gpt_summary":"user face challeng load imag quickli azur blob imageread studio tri creat csv file imag simpl reader took minut read file blob user seek altern method load read imag studio look wai speed imageread",
        "Answer_original_content":"look azur cdn http azur microsoft com servic cdn blob altern url blob download time faster switch",
        "Answer_preprocessed_content":"look azur cdn blob altern url blob download time faster switch",
        "Answer_gpt_summary_original":"the solution to the slow loading and reading of images in studio from azure blob is to consider using azure cdn. by switching to azure cdn, the blobs will have an alternative url, and the blob downloads can become about four times faster.",
        "Answer_gpt_summary":"solut slow load read imag studio azur blob consid azur cdn switch azur cdn blob altern url blob download time faster"
    },
    {
        "Question_id":null,
        "Question_title":"How to deal with artifact.wait() when running in mode \"DISABLED\"",
        "Question_body":"<p>During the preparation for a training (in <code>prepare_data<\/code> in pytorch lightning) I either create or update local data (download, prepare different encodings). I then create a W&amp;B artifact and wait for the upload to be complete. Later in the code (in <code>setup()<\/code> in pytorch lightning) I use the data. Strictly speaking, this is not necessary, because I have the files locally, but I want to track the usage of the data (and the IDs of the data used for training, validation, \u2026). I added the <code>wait()<\/code> statement, because wandb would download the previous version (v=n-1) of the data \/without the enoding just added). In mode <code>ONLINE<\/code> this works nicely. However, in mode <code>DISABLED<\/code> I get this error: <code>ValueError: Cannot call wait on an artifact before it has been logged or in offline mode<\/code>. How am I supposed to handle <code>wait()<\/code>in order to have it work in all modes? (it would be nice if <code>wait()<\/code> would do it).<\/p>\n<p>This is the sample code:<\/p>\n<pre><code class=\"lang-python\"># Upload the data\nartifact = wandb.Artifact(name=..., type=...)\nartifact.description = ...\nartifact.metadata = ...\nartifact.add_file(local_path=...)\nwandb.run.log_artifact(artifact)\nartifact.save()  # I think I don't need this, playing around because of this issue\nartifact.wait()\n<\/code><\/pre>\n<pre><code class=\"lang-python\"># Use (Download) the data\nartifact = wandb.run.use_artifact(artifact_or_name=... + \":latest\")\nartifact_entry = artifact.get_path(...)\nartifact_entry.download(root=...)\n<\/code><\/pre>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1655133484499,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":84.0,
        "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/hogru\">@hogru<\/a>, sorry about the late response. Runs have a <code>disabled<\/code> attribute. Here is a code snippet you can use:<\/p>\n<pre><code class=\"lang-auto\">run = wandb.init(mode=\"disabled\")\nif run.disabled:\n    \/\/ your code\n<\/code><\/pre>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-deal-with-artifact-wait-when-running-in-mode-disabled\/2607",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-14T06:40:31.480Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/hogru\">@hogru<\/a>, \u201cdisabled\u201d mode returns mocked objects and prevents all network communication. So when you call the log_artifact function, nothing will be logged which causes the error message when calling artifact.wait().<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-14T07:37:51.854Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/armanharutyunyan\">@armanharutyunyan<\/a>, I understand what\u2019s going on and the technical reason for it, but what is your suggested way to deal with it? I need <code>artifact.wait()<\/code> when online, but need the code to work when offline. Is there a simple way to check the mode of wandb or see whether there are pending uploads or\u2026? My question is how to handle the situation properly.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-17T10:48:43.780Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/hogru\">@hogru<\/a>, sorry about the late response. Runs have a <code>disabled<\/code> attribute. Here is a code snippet you can use:<\/p>\n<pre><code class=\"lang-auto\">run = wandb.init(mode=\"disabled\")\nif run.disabled:\n    \/\/ your code\n<\/code><\/pre>",
                "Answer_score":5.8,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-06-17T15:25:46.147Z",
                "Answer_body":"<p>Ah, that definitely helps, thank you. And apparently there\u2019s also <code>run.offline<\/code>. Thanks!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-16T15:25:54.451Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1655462923780,
        "Question_original_content":"deal artifact wait run mode disabl prepar train prepar data pytorch lightn creat updat local data download prepar differ encod creat artifact wait upload complet later code setup pytorch lightn us data strictli speak necessari file local want track usag data id data train valid ad wait statement download previou version data enod ad mode onlin work nice mode disabl error valueerror wait artifact log offlin mode suppos handl wait order work mode nice wait sampl code upload data artifact artifact type artifact descript artifact metadata artifact add file local path run log artifact artifact artifact save think need plai issu artifact wait us download data artifact run us artifact artifact latest artifact entri artifact path artifact entri download root",
        "Question_preprocessed_content":"deal run mode disabl prepar train creat updat local data creat artifact wait upload complet later code us data strictli speak necessari file local want track usag data ad statement download previou version data enod ad mode work nice mode error suppos handl order work mode sampl code",
        "Question_gpt_summary_original":"The user is facing a challenge with the use of the `wait()` statement in the `DISABLED` mode of pytorch lightning. The user wants to track the usage of data and its IDs during training and validation by creating a W&B artifact and waiting for its upload to be complete. However, in the `DISABLED` mode, the user encounters a `ValueError` stating that `wait()` cannot be called on an artifact before it has been logged or in offline mode. The user is seeking guidance on how to handle `wait()` to make it work in all modes.",
        "Question_gpt_summary":"user face challeng us wait statement disabl mode pytorch lightn user want track usag data id train valid creat artifact wait upload complet disabl mode user encount valueerror state wait call artifact log offlin mode user seek guidanc handl wait work mode",
        "Answer_original_content":"hei hogru sorri late respons run disabl attribut code snippet us run init mode disabl run disabl code",
        "Answer_preprocessed_content":"hei sorri late respons run attribut code snippet us",
        "Answer_gpt_summary_original":"possible solutions to the challenge of using the artifact.wait() function in \"disabled\" mode are provided in the answer. the user can use the code snippet provided to initialize the run in \"disabled\" mode and then add their code within the if statement.",
        "Answer_gpt_summary":"possibl solut challeng artifact wait function disabl mode provid answer user us code snippet provid initi run disabl mode add code statement"
    },
    {
        "Question_id":68764644.0,
        "Question_title":"Training Google-Cloud-Automl Model on multiple datasets",
        "Question_body":"<p>I would like to train an automl model on gcp's vertex ai using multiple datasets.  I would like to keep the datasets separate, since they come from different sources, want to train on them separately, etc.  Is that possible?  Or will I need to create a dataset containing both datasets? It looks like I can only select one dataset in the web UI.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628805774297,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":223.0,
        "Answer_body":"<p>It is possible via the Vertex AI API as long as your sources are in Google Cloud Storage, just provide a list of training data which are in JSON or CSV format that qualifies with the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-image\" rel=\"nofollow noreferrer\">best practices for formatting of training data<\/a>.<\/p>\n<p>See code for creating and importing datasets. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/create-dataset-api#create-dataset\" rel=\"nofollow noreferrer\">documentation<\/a> for code reference and further details.<\/p>\n<pre><code>from typing import List, Union\nfrom google.cloud import aiplatform\n\n    def create_and_import_dataset_image_sample(\n        project: str,\n        location: str,\n        display_name: str,\n        src_uris: Union[str, List[str]], \/\/ example: [&quot;gs:\/\/bucket\/file1.csv&quot;, &quot;gs:\/\/bucket\/file2.csv&quot;]\n        sync: bool = True,\n    ):\n        aiplatform.init(project=project, location=location)\n    \n        ds = aiplatform.ImageDataset.create(\n            display_name=display_name,\n            gcs_source=src_uris,\n            import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n            sync=sync,\n        )\n    \n        ds.wait()\n    \n        print(ds.display_name)\n        print(ds.resource_name)\n        return ds\n<\/code><\/pre>\n<p>NOTE: The links provided are for Vertex AI AutoML Image. If you access the links there are options for other AutoML products like Text, Tabular and Video.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68764644",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1628828205167,
        "Question_original_content":"train googl cloud automl model multipl dataset like train automl model gcp multipl dataset like dataset separ come differ sourc want train separ possibl need creat dataset contain dataset look like select dataset web",
        "Question_preprocessed_content":"train model multipl dataset like train automl model gcp multipl dataset like dataset separ come differ sourc want train separ possibl need creat dataset contain dataset look like select dataset web",
        "Question_gpt_summary_original":"The user wants to train a Google-Cloud-Automl model on multiple datasets, but is facing the challenge of not being able to select more than one dataset in the web UI. They want to keep the datasets separate due to different sources and training requirements. The user is unsure if they need to create a new dataset containing both datasets or if it is possible to train on them separately.",
        "Question_gpt_summary":"user want train googl cloud automl model multipl dataset face challeng abl select dataset web want dataset separ differ sourc train requir user unsur need creat new dataset contain dataset possibl train separ",
        "Answer_original_content":"possibl api long sourc googl cloud storag provid list train data json csv format qualifi best practic format train data code creat import dataset document code refer detail type import list union googl cloud import aiplatform def creat import dataset imag sampl project str locat str displai str src uri union str list str exampl bucket file csv bucket file csv sync bool true aiplatform init project project locat locat aiplatform imagedataset creat displai displai gc sourc src uri import schema uri aiplatform schema dataset ioformat imag singl label classif sync sync wait print displai print resourc return note link provid automl imag access link option automl product like text tabular video",
        "Answer_preprocessed_content":"possibl api long sourc googl cloud storag provid list train data json csv format qualifi best practic format train data code creat import dataset document code refer detail note link provid automl imag access link option automl product like text tabular video",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- it is possible to train a google cloud automl model on multiple datasets without combining them.\n- the datasets need to be in google cloud storage and in json or csv format that follows the best practices for formatting of training data.\n- the user can use the provided code for creating and importing datasets.\n- the user can refer to the documentation for code reference and further details.\n- the links provided are for automl image, but there are options for other automl products like text, tabular, and video.",
        "Answer_gpt_summary":"possibl solut answer possibl train googl cloud automl model multipl dataset combin dataset need googl cloud storag json csv format follow best practic format train data user us provid code creat import dataset user refer document code refer detail link provid automl imag option automl product like text tabular video"
    },
    {
        "Question_id":71023918.0,
        "Question_title":"ModuleNotFound Error - Azure ML with prebuilt docker image",
        "Question_body":"<p>I have developed a module which works perfectly when executed locally.<\/p>\n<p>I have created an environment on azure using a prebuilt docker image found here:\n<strong>&quot;azureml\/minimal-ubuntu18.04-py37-cpu-inference&quot;<\/strong> <a href=\"https:\/\/mcr.microsoft.com\/v2\/_catalog\" rel=\"nofollow noreferrer\">https:\/\/mcr.microsoft.com\/v2\/_catalog<\/a>\n. Also, Using pythonScriptStep, to run a pipeline. Here is how the step looks<\/p>\n<pre><code>StepPreprocessing = PythonScriptStep(\n    name=&quot;Preprocessing&quot;,\n    script_name=e.preprocess_script_path,\n    arguments=[\n        &quot;--config_path&quot;, e.preprocess_config_path,\n        &quot;--task&quot;, e.preprocess_task,\n    ],\n    inputs=None,\n    compute_target=aml_compute,\n    runconfig=run_config,\n    source_directory=e.sources_directory,\n    allow_reuse=False\n)\nprint(&quot;Step Preprocessing created&quot;)\n<\/code><\/pre>\n<p>This results in error:<\/p>\n<pre><code>Traceback (most recent call last):\n[stderr]  File &quot;Pipeline\/custom_pipeline.py&quot;, line 4, in &lt;module&gt;\n[stderr]    from Preprocess.logger import logger\n[stderr]ModuleNotFoundError: No module named 'Preprocess'\n<\/code><\/pre>\n<p>in the 1st line of entry script (<strong>custom_pipeline.py<\/strong>):<\/p>\n<pre><code>import sys\nsys.path.append(&quot;.&quot;) \nfrom Preprocess.logger import logger\n<\/code><\/pre>\n<p>The folder structure is as:<\/p>\n<pre><code>-Preprocess\n  -__init__.py\n  - Module1\n    -__init__.py\n    -somefile.py\n  - Module2\n    -__init__.py\n    -someOtherfile.py\n  - Pipeline\n    -__init__.py\n    -custom_pipeline.py\n  - logger\n    -__init__.py\n    -logger.py\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644260100200,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":109.0,
        "Answer_body":"<p>I found out that the python script step copies everything inside the source_dir and therefore in my case it was copying the modules and not the root folder. So I had to put the dir Preprocess inside another dir and mention the new dir as source_dir.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71023918",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1644604906183,
        "Question_original_content":"modulenotfound error prebuilt docker imag develop modul work perfectli execut local creat environ azur prebuilt docker imag minim ubuntu cpu infer http mcr microsoft com catalog pythonscriptstep run pipelin step look steppreprocess pythonscriptstep preprocess script preprocess script path argument config path preprocess config path task preprocess task input comput target aml comput runconfig run config sourc directori sourc directori allow reus fals print step preprocess creat result error traceback recent stderr file pipelin custom pipelin line stderr preprocess logger import logger stderr modulenotfounderror modul name preprocess line entri script custom pipelin import sy sy path append preprocess logger import logger folder structur preprocess init modul init somefil modul init someotherfil pipelin init custom pipelin logger init logger",
        "Question_preprocessed_content":"modulenotfound error prebuilt docker imag develop modul work perfectli execut local creat environ azur prebuilt docker imag pythonscriptstep run pipelin step look result error line entri script folder structur",
        "Question_gpt_summary_original":"The user encountered a ModuleNotFound error when running a pipeline on Azure ML using a prebuilt docker image. The error occurred in the entry script, custom_pipeline.py, when importing a module named 'Preprocess'. The folder structure includes the 'Preprocess' folder containing subfolders and files, including the 'logger' module.",
        "Question_gpt_summary":"user encount modulenotfound error run pipelin prebuilt docker imag error occur entri script custom pipelin import modul name preprocess folder structur includ preprocess folder contain subfold file includ logger modul",
        "Answer_original_content":"python script step copi insid sourc dir case copi modul root folder dir preprocess insid dir mention new dir sourc dir",
        "Answer_preprocessed_content":"python script step copi insid case copi modul root folder dir preprocess insid dir mention new dir",
        "Answer_gpt_summary_original":"the solution to the \"modulenotfounderror\" error when running a pipeline on azure using a prebuilt docker image is to ensure that the root folder is copied instead of the modules. this can be achieved by placing the directory containing the modules inside another directory and specifying the new directory as the source directory.",
        "Answer_gpt_summary":"solut modulenotfounderror error run pipelin azur prebuilt docker imag ensur root folder copi instead modul achiev place directori contain modul insid directori specifi new directori sourc directori"
    },
    {
        "Question_id":null,
        "Question_title":"Problems accessing web interface: \"rate limit exceeded\"",
        "Question_body":"<p>Hi,<\/p>\n<p>I am having trouble accessing the <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a> web interface. I very often, yet not always, get error messages like \u201crate limit exceeded\u201d or \u201cThere was a problem rendering these panels\u201d.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b365941060b6ae0dc38310d95e97379e66323316.png\" data-download-href=\"\/uploads\/short-url\/pB164ekBeFRf2vXOzJZpRNGSnBQ.png?dl=1\" title=\"Screenshot_google-chrome_20220308104954_crop\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b365941060b6ae0dc38310d95e97379e66323316_2_690x254.png\" alt=\"Screenshot_google-chrome_20220308104954_crop\" data-base62-sha1=\"pB164ekBeFRf2vXOzJZpRNGSnBQ\" width=\"690\" height=\"254\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b365941060b6ae0dc38310d95e97379e66323316_2_690x254.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b365941060b6ae0dc38310d95e97379e66323316.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b365941060b6ae0dc38310d95e97379e66323316.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b365941060b6ae0dc38310d95e97379e66323316_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot_google-chrome_20220308104954_crop<\/span><span class=\"informations\">790\u00d7291 10.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Yesterday I had no problems at all. What\u2019s the issue?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1646737169636,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":102.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/problems-accessing-web-interface-rate-limit-exceeded\/2041",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-08T20:03:14.936Z",
                "Answer_body":"<p>Hi Florian, can you tell me how many parallel processes you are doing? Has it increased from yesterday? I will increase your rate limit from my end.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-14T12:36:17.028Z",
                "Answer_body":"<p>Hi Florian,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-17T14:06:51.529Z",
                "Answer_body":"<p>Hi Florian, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-07T11:00:10.508Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"problem access web interfac rate limit exceed have troubl access web interfac error messag like rate limit exceed problem render panel screenshot googl chrome crop yesterdai problem what issu thank",
        "Question_preprocessed_content":"problem access web interfac rate limit exceed have troubl access web interfac error messag like rate limit exceed problem render panel yesterdai problem what issu thank",
        "Question_gpt_summary_original":"The user is having trouble accessing the wandb.ai web interface and is receiving error messages such as \"rate limit exceeded\" and \"There was a problem rendering these panels\". The issue is not consistent and the user did not have any problems accessing the interface the day before.",
        "Question_gpt_summary":"user have troubl access web interfac receiv error messag rate limit exceed problem render panel issu consist user problem access interfac dai",
        "Answer_original_content":"florian tell parallel process increas yesterdai increas rate limit end florian want follow support request heard let know assist issu resolv best florian heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"florian tell parallel process increas yesterdai increas rate limit end florian want follow support request heard let know assist issu resolv best florian heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"possible solutions from the answer include increasing the user's rate limit and following up with the user to see if their issue has been resolved or if further assistance is needed. additionally, the user is informed that if they do not respond within 60 days, the conversation will be automatically closed.",
        "Answer_gpt_summary":"possibl solut answer includ increas user rate limit follow user issu resolv assist need addition user inform respond dai convers automat close"
    },
    {
        "Question_id":null,
        "Question_title":"Consume Azure ML Web Service with Postman: how to pass arguments?",
        "Question_body":"Is it possible to pass parameters to an Azure ML Web Service with Postman? I created an R web service endpoint that runs in an Azure Container Instance. My run function has one argument (\"data\"). I can call the web service using Azure ML R SDK (using invoke_webservice()) and the input parameter is read successfully from the request content. The input is constructed like:\n\n toJSON(data.frame(data=\"This is my test string\"))\n\n\n\nResult:\n\n [{\"data\": \"This is my test string\"}]\n\n\n\nIf I create a Postman request and copy the input to the request body, the input parameter is not passed to the web service. The web service can return a static output to Postman but the variable data is always empty. Is this a property of the ML Web Service? If not, how can I set up the request body so that the argument is read successfully? I have tried many variations, but none have worked.\n\nI have set content-type header to application\/json. I don't have authentication in the web service, since it is just a test instance.\n\nUltimately, we need to call the web service with C# from Azure Function. I know that we can use the C# template from documentation and it can probably pass the parameter to the web service, but it would be nice to be able to test the web service with Postman.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603965300457,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Try this in postman.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/144230\/consume-azure-ml-web-service-with-postman-how-to-p.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-29T12:55:26.077Z",
                "Answer_score":1,
                "Answer_body":"Try this in postman.",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1603976126076,
        "Question_original_content":"consum web servic postman pass argument possibl pass paramet web servic postman creat web servic endpoint run azur contain instanc run function argument data web servic sdk invok webservic input paramet read successfulli request content input construct like tojson data frame data test string result data test string creat postman request copi input request bodi input paramet pass web servic web servic return static output postman variabl data properti web servic set request bodi argument read successfulli tri variat work set content type header applic json authent web servic test instanc ultim need web servic azur function know us templat document probabl pass paramet web servic nice abl test web servic postman",
        "Question_preprocessed_content":"consum web servic postman pass argument possibl pass paramet web servic postman creat web servic endpoint run azur contain instanc run function argument web servic sdk input paramet read successfulli request content input construct like test string result creat postman request copi input request bodi input paramet pass web servic web servic return static output postman variabl data properti web servic set request bodi argument read successfulli tri variat work set header authent web servic test instanc ultim need web servic azur function know us templat document probabl pass paramet web servic nice abl test web servic postman",
        "Question_gpt_summary_original":"The user is facing challenges in passing parameters to an Azure ML Web Service with Postman. Despite successfully calling the web service using Azure ML R SDK, the input parameter is not passed to the web service when using Postman. The user has tried various methods, including setting the content-type header to application\/json, but none have worked. The user is seeking a solution to successfully pass the parameter to the web service using Postman and ultimately needs to call the web service with C# from Azure Function.",
        "Question_gpt_summary":"user face challeng pass paramet web servic postman despit successfulli call web servic sdk input paramet pass web servic postman user tri method includ set content type header applic json work user seek solut successfulli pass paramet web servic postman ultim need web servic azur function",
        "Answer_original_content":"try postman",
        "Answer_preprocessed_content":"try postman",
        "Answer_gpt_summary_original":"possible solutions were not provided in the answer. the responder simply suggested trying something in postman.",
        "Answer_gpt_summary":"possibl solut provid answer respond simpli suggest try postman"
    },
    {
        "Question_id":61763206.0,
        "Question_title":"Is there a way to pass arguments to multiple jobs in optuna?",
        "Question_body":"<p>I am trying to use optuna for searching hyper parameter spaces.<\/p>\n\n<p>In one particular scenario I train a model on a machine with a few GPUs.\nThe model and batch size allows me to run 1 training per 1 GPU.\nSo, ideally I would like to let optuna spread all trials across the available GPUs\nso that there is always 1 trial running on each GPU.<\/p>\n\n<p>In the <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#how-can-i-use-two-gpus-for-evaluating-two-trials-simultaneously\" rel=\"nofollow noreferrer\">docs<\/a> it says, I should just start one process per GPU in a separate terminal like:<\/p>\n\n<pre><code>CUDA_VISIBLE_DEVICES=0 optuna study optimize foo.py objective --study foo --storage sqlite:\/\/\/example.db\n<\/code><\/pre>\n\n<p>I want to avoid that because the whole hyper parameter search continues in multiple rounds after that. I don't want to always manually start a process per GPU, check when all are finished, then start the next round.<\/p>\n\n<p>I saw <code>study.optimize<\/code> has a <code>n_jobs<\/code> argument.\nAt first glance this seems to be perfect.\n<em>E.g.<\/em> I could do this:<\/p>\n\n<pre><code>import optuna\n\ndef objective(trial):\n    # the actual model would be trained here\n    # the trainer here would need to know which GPU\n    # it should be using\n    best_val_loss = trainer(**trial.params)\n    return best_val_loss\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100, n_jobs=8)\n<\/code><\/pre>\n\n<p>This starts multiple threads each starting a training.\nHowever, the trainer within <code>objective<\/code> somehow needs to know which GPU it should be using.\nIs there a trick to accomplish that?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1589323176827,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":2684.0,
        "Answer_body":"<p>After a few mental breakdowns I figured out that I can do what I want using a <code>multiprocessing.Queue<\/code>. To get it into the objective function I need to define it as a lambda function or as a class (I guess partial also works). <em>E.g.<\/em><\/p>\n\n<pre><code>from contextlib import contextmanager\nimport multiprocessing\nN_GPUS = 2\n\nclass GpuQueue:\n\n    def __init__(self):\n        self.queue = multiprocessing.Manager().Queue()\n        all_idxs = list(range(N_GPUS)) if N_GPUS &gt; 0 else [None]\n        for idx in all_idxs:\n            self.queue.put(idx)\n\n    @contextmanager\n    def one_gpu_per_process(self):\n        current_idx = self.queue.get()\n        yield current_idx\n        self.queue.put(current_idx)\n\n\nclass Objective:\n\n    def __init__(self, gpu_queue: GpuQueue):\n        self.gpu_queue = gpu_queue\n\n    def __call__(self, trial: Trial):\n        with self.gpu_queue.one_gpu_per_process() as gpu_i:\n            best_val_loss = trainer(**trial.params, gpu=gpu_i)\n            return best_val_loss\n\nif __name__ == '__main__':\n    study = optuna.create_study()\n    study.optimize(Objective(GpuQueue()), n_trials=100, n_jobs=8)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61763206",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1589464897583,
        "Question_original_content":"wai pass argument multipl job try us search hyper paramet space particular scenario train model machin gpu model batch size allow run train gpu ideal like let spread trial avail gpu trial run gpu doc sai start process gpu separ termin like cuda visibl devic studi optim foo object studi foo storag sqlite exampl want avoid hyper paramet search continu multipl round want manual start process gpu check finish start round saw studi optim job argument glanc perfect import def object trial actual model train trainer need know gpu best val loss trainer trial param return best val loss studi creat studi studi optim object trial job start multipl thread start train trainer object need know gpu trick accomplish",
        "Question_preprocessed_content":"wai pass argument multipl job try us search hyper paramet space particular scenario train model machin gpu model batch size allow run train gpu ideal like let spread trial avail gpu trial run gpu doc sai start process gpu separ termin like want avoid hyper paramet search continu multipl round want manual start process gpu check finish start round saw argument glanc perfect start multipl thread start train trainer need know gpu trick accomplish",
        "Question_gpt_summary_original":"The user is trying to use optuna for hyperparameter search and wants to spread all trials across available GPUs. The documentation suggests starting one process per GPU in a separate terminal, but the user wants to avoid that. The user has tried using the `n_jobs` argument in `study.optimize` to start multiple threads, but the trainer within `objective` needs to know which GPU to use. The user is seeking a solution to this challenge.",
        "Question_gpt_summary":"user try us hyperparamet search want spread trial avail gpu document suggest start process gpu separ termin user want avoid user tri job argument studi optim start multipl thread trainer object need know gpu us user seek solut challeng",
        "Answer_original_content":"mental breakdown figur want multiprocess queue object function need defin lambda function class guess partial work contextlib import contextmanag import multiprocess gpu class gpuqueu def init self self queue multiprocess manag queue idx list rang gpu gpu idx idx self queue idx contextmanag def gpu process self current idx self queue yield current idx self queue current idx class object def init self gpu queue gpuqueu self gpu queue gpu queue def self trial trial self gpu queue gpu process gpu best val loss trainer trial param gpu gpu return best val loss main studi creat studi studi optim object gpuqueu trial job",
        "Answer_preprocessed_content":"mental breakdown figur want object function need defin lambda function class",
        "Answer_gpt_summary_original":"Possible solutions to the challenge of passing arguments for multiple jobs in a hyperparameter search include using a multiprocessing.Queue and defining it as a lambda function or a class. The code example provided shows how to use a GpuQueue class to manage GPU resources and optimize hyperparameters using Optuna.",
        "Answer_gpt_summary":"possibl solut challeng pass argument multipl job hyperparamet search includ multiprocess queue defin lambda function class code exampl provid show us gpuqueu class manag gpu resourc optim hyperparamet"
    },
    {
        "Question_id":67639665.0,
        "Question_title":"Azure ML not able to create conda environment (exit code: -15)",
        "Question_body":"<p>When I try to run the experiment defined in <a href=\"https:\/\/github.com\/MicrosoftLearning\/mslearn-dp100\/blob\/main\/06%20-%20Work%20with%20Data.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a> in  notebook, I encountered an error when it is creating the conda env. The error occurs when the below cell is executed:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Experiment, ScriptRunConfig, Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.widgets import RunDetails\n\n\n# Create a Python environment for the experiment\nsklearn_env = Environment(&quot;sklearn-env&quot;)\n\n# Ensure the required packages are installed (we need scikit-learn, Azure ML defaults, and Azure ML dataprep)\npackages = CondaDependencies.create(conda_packages=['scikit-learn','pip'],\n                                    pip_packages=['azureml-defaults','azureml-dataprep[pandas]'])\nsklearn_env.python.conda_dependencies = packages\n\n# Get the training dataset\ndiabetes_ds = ws.datasets.get(&quot;diabetes dataset&quot;)\n\n# Create a script config\nscript_config = ScriptRunConfig(source_directory=experiment_folder,\n                              script='diabetes_training.py',\n                              arguments = ['--regularization', 0.1, # Regularizaton rate parameter\n                                           '--input-data', diabetes_ds.as_named_input('training_data')], # Reference to dataset\n                              environment=sklearn_env)\n\n# submit the experiment\nexperiment_name = 'mslearn-train-diabetes'\nexperiment = Experiment(workspace=ws, name=experiment_name)\nrun = experiment.submit(config=script_config)\nRunDetails(run).show()\nrun.wait_for_completion() \n<\/code><\/pre>\n<p>Everytime I run this, I always faced the issue of creating the conda env as below:<\/p>\n<pre><code>Creating conda environment...\nRunning: ['conda', 'env', 'create', '-p', '\/home\/azureuser\/.azureml\/envs\/azureml_000000000000', '-f', 'azureml-environment-setup\/mutated_conda_dependencies.yml']\nCollecting package metadata (repodata.json): ...working... done\nSolving environment: ...working... done\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\n\nInstalling pip dependencies: ...working... \n\nAttempting to clean up partially built conda environment: \/home\/azureuser\/.azureml\/envs\/azureml_000000000000\nRemove all packages in environment \/home\/azureuser\/.azureml\/envs\/azureml_000000000000:\nCreating conda environment failed with exit code: -15\n<\/code><\/pre>\n<p>I could not find anything useful on the internet and this is not the only script where it fail. When I am try to run other experiments I have sometimes faced this issue. One solution which worked in the above case is I moved the pandas from pip to conda and it was able to create the coonda env. Example below:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Ensure the required packages are installed (we need scikit-learn, Azure ML defaults, and Azure ML dataprep)\npackages = CondaDependencies.create(conda_packages=['scikit-learn','pip'],\n                                    pip_packages=['azureml-defaults','azureml-dataprep[pandas]'])\n\n<\/code><\/pre>\n<pre class=\"lang-py prettyprint-override\"><code># Ensure the required packages are installed (we need scikit-learn, Azure ML defaults, and Azure ML dataprep)\npackages = CondaDependencies.create(conda_packages=['scikit-learn','pip','pandas'],\n                                    pip_packages=['azureml-defaults','azureml-dataprep'])\n\n<\/code><\/pre>\n\n<p>The error message (or the logs from Azure) is also not much help. Would apprecite if a proper solution is available.<\/p>\n<p>Edit: I have recently started learning to use Azure for Machine learning and so if I am not sure if I am missing something? I assume the example notebooks should work as is hence raised this question.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1621610368967,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1621615281776,
        "Question_score":4.0,
        "Question_view_count":2373.0,
        "Answer_body":"<h2>short answer<\/h2>\n<p>Totally been in your shoes before. This code sample seems a smidge out of date. Using <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/datasets-tutorial\/train-with-datasets\/train-with-datasets.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a> as a reference, can you try the following?<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>packages = CondaDependencies.create(\n    pip_packages=['azureml-defaults','scikit-learn']\n)\n<\/code><\/pre>\n<h2>longer  answer<\/h2>\n<p><a href=\"https:\/\/www.anaconda.com\/blog\/using-pip-in-a-conda-environment\" rel=\"nofollow noreferrer\">Using pip with Conda<\/a> is not always smooth sailing. In this instance, conda isn't reporting up the issue that pip is having. The solution is to create and test this environment locally where we can get more information, which will at least will give you a more informative error message.<\/p>\n<ol>\n<li>Install anaconda  or miniconda (or use an Azure ML Compute Instance which has conda pre-installed)<\/li>\n<li>Make a  file called environment.yml that looks like this<\/li>\n<\/ol>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: aml_env\ndependencies:\n - python=3.8\n - pip=21.0.1\n - pip:\n    - azureml-defaults\n    - azureml-dataprep[pandas]\n    - scikit-learn==0.24.1\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Create this environment with the command <code>conda env create -f environment.yml<\/code>.<\/li>\n<li>respond to any discovered error message<\/li>\n<li>If there' no error, use this new <code>environment.yml<\/code> with Azure ML like so<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>sklearn_env = Environment.from_conda_specification(name = 'sklearn-env', file_path = '.\/environment.yml')\n<\/code><\/pre>\n<h2>more context<\/h2>\n<p>the error I'm guessing that's happening is when you reference a pip requirements file from a conda environment file. In this scenario, conda calls <code>pip install -r  requirements.txt<\/code> and if that command errors out, conda can't report the error.<\/p>\n<h3><code>requirements.txt<\/code><\/h3>\n<pre><code>scikit-learn==0.24.1\nazureml-dataprep[pandas]\n<\/code><\/pre>\n<h3><code>environment.yml<\/code><\/h3>\n<pre><code>name: aml_env\ndependencies:\n - python=3.8\n - pip=21.0.1\n - pip:\n    - -rrequirements.txt\n<\/code><\/pre>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1621619024732,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67639665",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1621615054950,
        "Question_original_content":"abl creat conda environ exit code try run experi defin notebook notebook encount error creat conda env error occur cell execut core import experi scriptrunconfig environ core conda depend import condadepend widget import rundetail creat python environ experi sklearn env environ sklearn env ensur requir packag instal need scikit learn default dataprep packag condadepend creat conda packag scikit learn pip pip packag default dataprep panda sklearn env python conda depend packag train dataset diabet dataset diabet dataset creat script config script config scriptrunconfig sourc directori experi folder script diabet train argument regular regularizaton rate paramet input data diabet name input train data refer dataset environ sklearn env submit experi experi mslearn train diabet experi experi workspac experi run experi submit config script config rundetail run run wait complet everytim run face issu creat conda env creat conda environ run conda env creat home azureus env environ setup mutat conda depend yml collect packag metadata repodata json work solv environ work prepar transact work verifi transact work execut transact work instal pip depend work attempt clean partial built conda environ home azureus env remov packag environ home azureus env creat conda environ fail exit code us internet script fail try run experi face issu solut work case move panda pip conda abl creat coonda env exampl ensur requir packag instal need scikit learn default dataprep packag condadepend creat conda packag scikit learn pip pip packag default dataprep panda ensur requir packag instal need scikit learn default dataprep packag condadepend creat conda packag scikit learn pip panda pip packag default dataprep error messag log azur help apprecit proper solut avail edit recent start learn us azur machin learn sure miss assum exampl notebook work rais question",
        "Question_preprocessed_content":"abl creat conda environ try run experi defin notebook notebook encount error creat conda env error occur cell execut everytim run face issu creat conda env us internet script fail try run experi face issu solut work case move panda pip conda abl creat coonda env exampl error messag help apprecit proper solut avail edit recent start learn us azur machin learn sure miss assum exampl notebook work rais question",
        "Question_gpt_summary_original":"The user encountered an error when creating a conda environment in Azure ML while running an experiment defined in a notebook. The error occurred when executing a specific cell of code, and the error message was not helpful in finding a solution. The user found a workaround by moving pandas from pip to conda in the required packages. The user is unsure if they are missing something and expects the example notebooks to work as is.",
        "Question_gpt_summary":"user encount error creat conda environ run experi defin notebook error occur execut specif cell code error messag help find solut user workaround move panda pip conda requir packag user unsur miss expect exampl notebook work",
        "Answer_original_content":"short answer total shoe code sampl smidg date notebook refer try follow packag condadepend creat pip packag default scikit learn longer answer pip conda smooth sail instanc conda isn report issu pip have solut creat test environ local inform inform error messag instal anaconda miniconda us comput instanc conda pre instal file call environ yml look like aml env depend python pip pip default dataprep panda scikit learn creat environ command conda env creat environ yml respond discov error messag error us new environ yml like sklearn env environ conda specif sklearn env file path environ yml context error guess happen refer pip requir file conda environ file scenario conda call pip instal requir txt command error conda report error requir txt scikit learn dataprep panda environ yml aml env depend python pip pip rrequir txt",
        "Answer_preprocessed_content":"short answer total shoe code sampl smidg date notebook refer try follow longer answer pip conda smooth sail instanc conda isn report issu pip have solut creat test environ local inform inform error messag instal anaconda miniconda file call look like creat environ command respond discov error messag error us new like context error guess happen refer pip requir file conda environ file scenario conda call command error conda report error",
        "Answer_gpt_summary_original":"the answer suggests creating and testing the environment locally to get more information about the error message. the solution involves installing anaconda or miniconda, creating an environment.yml file, and using the command \"conda env create -f environment.yml\" to create the environment. the user should respond to any discovered error message, and if there is no error, use the new environment.yml file. the error may be happening when referencing a pip requirements file from a conda environment file.",
        "Answer_gpt_summary":"answer suggest creat test environ local inform error messag solut involv instal anaconda miniconda creat environ yml file command conda env creat environ yml creat environ user respond discov error messag error us new environ yml file error happen referenc pip requir file conda environ file"
    },
    {
        "Question_id":null,
        "Question_title":"AzureML pipeline not working",
        "Question_body":"I have designed a pipeline that makes predictions and saves the results to a blob container.\nThe pipeline works fine after submitting the experiment. However, after I publish it and call it via its REST endpoint, it does not work (I don't get my results). The portal shows that the job has been completed, without any error.\nCan someone enlighten me on how to use a publish pipeline?",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1654438612767,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/877179\/azureml-pipeline-not-working.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-07T12:23:00.923Z",
                "Answer_score":0,
                "Answer_body":"Thanks for the details. All published pipelines have a REST endpoint. With the pipeline endpoint, you can trigger a run of the pipeline from external systems, such as non-Python clients. For information about how to authenticate when calling REST endpoints, see https:\/\/aka.ms\/pl-restep-auth.\n\nUsing the endpoint enables \"managed repeatability\" in batch scoring and retraining scenarios, for example. For more information, see https:\/\/aka.ms\/pl-first-pipeline.\n\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.graph.publishedpipeline?view=azure-ml-py\n\nHere is the sample and document for publish.",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"pipelin work design pipelin make predict save result blob contain pipelin work fine submit experi publish rest endpoint work result portal show job complet error enlighten us publish pipelin",
        "Question_preprocessed_content":"pipelin work design pipelin make predict save result blob contain pipelin work fine submit experi publish rest endpoint work portal show job complet error enlighten us publish pipelin",
        "Question_gpt_summary_original":"The user has designed an AzureML pipeline that works fine after submitting the experiment, but after publishing it and calling it via its REST endpoint, it does not work and the user does not receive any results. The portal shows that the job has been completed without any error. The user is seeking guidance on how to use a publish pipeline.",
        "Question_gpt_summary":"user design pipelin work fine submit experi publish call rest endpoint work user receiv result portal show job complet error user seek guidanc us publish pipelin",
        "Answer_original_content":"thank detail publish pipelin rest endpoint pipelin endpoint trigger run pipelin extern system non python client inform authent call rest endpoint http aka restep auth endpoint enabl manag repeat batch score retrain scenario exampl inform http aka pipelin http doc microsoft com python api pipelin core pipelin core graph publishedpipelin view azur sampl document publish",
        "Answer_preprocessed_content":"thank detail publish pipelin rest endpoint pipelin endpoint trigger run pipelin extern system client inform authent call rest endpoint endpoint enabl manag repeat batch score retrain scenario exampl inform sampl document publish",
        "Answer_gpt_summary_original":"possible solutions from the answer include: \n- checking the pipeline endpoint to trigger a run of the pipeline from external systems\n- reviewing the authentication process when calling rest endpoints\n- utilizing the endpoint for \"managed repeatability\" in batch scoring and retraining scenarios\n- referencing the provided sample and document for publishing pipelines.",
        "Answer_gpt_summary":"possibl solut answer includ check pipelin endpoint trigger run pipelin extern system review authent process call rest endpoint util endpoint manag repeat batch score retrain scenario referenc provid sampl document publish pipelin"
    },
    {
        "Question_id":null,
        "Question_title":"Hide projects on googling",
        "Question_body":"<p>Hi, I\u2019m a user of wandb.<br>\nI\u2019m in several projects now, but others can search some of them on google despite I already locked them.<br>\nHow to prevent it to come out?<\/p>\n<p>Sincerely<\/p>",
        "Question_answer_count":10,
        "Question_comment_count":0,
        "Question_creation_time":1652170278096,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":222.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/hide-projects-on-googling\/2388",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-11T08:40:26.262Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/wooshik-m\">@wooshik-m<\/a>, they shouldn\u2019t be able to access the projects. Can you share what you are seeing when searching in google?<\/p>",
                "Answer_score":7.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-12T03:36:01.643Z",
                "Answer_body":"<p>Thank you for your reply.<\/p>\n<p>I checked others cannot access the project but it still is exposed simply on the web as below.<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/9c408106b216bb1d54921a9b82562de4e1629da9.png\" alt=\"image\" data-base62-sha1=\"migGrz8ukVJogJ8HmfYHFxBKrbj\" width=\"649\" height=\"133\"><\/p>\n<p>Can I hide this?<\/p>",
                "Answer_score":2.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-12T15:40:23.105Z",
                "Answer_body":"<p>Hi,<br>\nThis should disappear in an hour or two. It sometimes happens when googles crawler gets to it while the project is public, and takes a little time to remove it when it\u2019s become private.<br>\nWe made a removal request so that should expedite the process.<\/p>",
                "Answer_score":6.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-13T07:11:49.352Z",
                "Answer_body":"<p>I tried today again, but it still can be searched\u2026<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/a6c48a57595ef909574aa768df472f87738f8048.png\" data-download-href=\"\/uploads\/short-url\/nNilrPszcOGYxWQutS8qJAL9RkQ.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a6c48a57595ef909574aa768df472f87738f8048_2_690x159.png\" alt=\"image\" data-base62-sha1=\"nNilrPszcOGYxWQutS8qJAL9RkQ\" width=\"690\" height=\"159\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a6c48a57595ef909574aa768df472f87738f8048_2_690x159.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a6c48a57595ef909574aa768df472f87738f8048_2_1035x238.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/a6c48a57595ef909574aa768df472f87738f8048.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a6c48a57595ef909574aa768df472f87738f8048_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1362\u00d7314 40.1 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-17T18:32:24.248Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/wooshik-m\">@wooshik-m<\/a>, sorry about all the back and forth. Could you check again? It takes a while to update.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-18T00:37:50.597Z",
                "Answer_body":"<p>I still can search our project  on guest.<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/948ac6be412703c5fd120a891f7ac6dfcc65d8b3.png\" alt=\"image\" data-base62-sha1=\"lc3ZxpZy7WvCYnO7vmAQDVUtUUb\" width=\"659\" height=\"258\"><\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-19T19:16:08.612Z",
                "Answer_body":"<p>Thanks, I am checking on this. Sorry about all the back and forth.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-02T13:38:59.858Z",
                "Answer_body":"<p>Hey there, we have made some updates on our end. Can you please double-check?<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-06T03:50:43.528Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/wooshik-m\">@wooshik-m<\/a>,\u200b I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>\u200b<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-05T03:51:20.487Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"hide project googl user project search googl despit lock prevent come sincer",
        "Question_preprocessed_content":"hide project googl user project search googl despit lock prevent come sincer",
        "Question_gpt_summary_original":"The user is facing a challenge where some of their projects on wandb are appearing on Google search results despite being locked, and they are seeking advice on how to prevent this from happening.",
        "Question_gpt_summary":"user face challeng project appear googl search result despit lock seek advic prevent happen",
        "Answer_original_content":"hei wooshik shouldnt abl access project share see search googl thank repli check access project expos simpli web hide disappear hour happen googl crawler get project public take littl time remov privat remov request expedit process tri todai search imag hei wooshik sorri forth check take updat search project guest thank check sorri forth hei updat end doubl check hei wooshik want follow request let know assist issu resolv topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"hei shouldnt abl access project share see search googl thank repli check access project expos simpli web hide disappear hour happen googl crawler get project public take littl time remov privat remov request expedit process tri todai search imag hei sorri forth check take updat search project guest thank check sorri forth hei updat end hei want follow request let know assist issu resolv topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the user is facing a challenge of preventing their projects from appearing on google searches despite having locked them. the solution provided is that the project should disappear in an hour or two. it sometimes happens when google's crawler gets to it while the project is public, and takes a little time to remove it when it's become private. the team made a removal request so that should expedite the process. the user is asked to check again as it takes a while to update. the team has made some updates on their end and the user is asked to double-check.",
        "Answer_gpt_summary":"user face challeng prevent project appear googl search despit have lock solut provid project disappear hour happen googl crawler get project public take littl time remov privat team remov request expedit process user ask check take updat team updat end user ask doubl check"
    },
    {
        "Question_id":null,
        "Question_title":"How to track file",
        "Question_body":"<p>hello,<\/p>\n<p>\u0131 couldn\u2019t  track my folder. What am I doing wrong ?<\/p>\n<p>My folder hiearchy<\/p>\n<p>Folder<br>\n-01<\/p>\n<p>\u0131 m using \u201cdvc add Folder\u201d comment.<br>\nthen \u0131 added to git.<br>\ngit tag -a \u201cv1\u201d -m \u201cmessage\u201d<\/p>\n<p>then \u0131 added other file to Folder.<br>\nNew data hiearchy is<\/p>\n<p>Folder<br>\n-01<br>\n-02<br>\n-03<\/p>\n<p>\u0131 repeated same things.<\/p>\n<p>dvc add Folder<br>\ngit add Folder.dvc<br>\ngit tag -a \u201cv1.1\u201d -m \u201cmessage\u201d<\/p>\n<p>then \u0131 check the tag v1 \u0131 want to see just 01 file in folder but \u0131 saw 01 02 and 03 file. Why? What am I doing wrong ?<\/p>",
        "Question_answer_count":13,
        "Question_comment_count":0,
        "Question_creation_time":1619613397408,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":423.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-track-file\/739",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-28T12:43:38.356Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/merve\">@merve<\/a> !<\/p>\n<p>Did you run <code>dvc checkout<\/code> after <code>git checkout<\/code>?<\/p>",
                "Answer_score":7.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-28T12:57:00.365Z",
                "Answer_body":"<p>yes, \u0131 forgot to mention,sorry.  Also I push to git repo.<br>\nI changed my branch and tag then \u0131 writing dvc checkout but it is not change .<\/p>",
                "Answer_score":7.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-28T12:58:27.793Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"merve\" data-post=\"1\" data-topic=\"739\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/m\/f07891\/40.png\" class=\"avatar\"> merve:<\/div>\n<blockquote>\n<p>Folder.dvc<\/p>\n<\/blockquote>\n<\/aside>\n<p>When you git checkout to a different version, does the content of  Folder.dvc change?<\/p>",
                "Answer_score":7.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-28T13:04:08.763Z",
                "Answer_body":"<p>yes when \u0131 git checkout a different version and then dvc checkout my folder.dvc is changing<\/p>",
                "Answer_score":7.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-28T15:56:24.672Z",
                "Answer_body":"<p>And if you delete <code>folder<\/code> itself and run <code>dvc checkout<\/code>, does it restore your folder as you expect? Also, please show <code>dvc doctor<\/code> output.<\/p>",
                "Answer_score":2.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-30T05:51:55.776Z",
                "Answer_body":"<p>when I deleted folder itself and then dvc checkout, folder is restore but it is empty <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/frowning.png?v=9\" title=\":frowning:\" class=\"emoji\" alt=\":frowning:\"><\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-30T18:17:33.741Z",
                "Answer_body":"<p>Looks like there is some misunderstanding here. Would you be able to create a minimal reproducible example for this issue?<\/p>",
                "Answer_score":2.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-24T07:04:21.973Z",
                "Answer_body":"<p>My folder hierarhy is comletely below.<\/p>\n<p>-DATASET<br>\n-plate<br>\n-01<br>\n-images<br>\n- 1.jpg<br>\n- 2.jpg<br>\n-labels<br>\n- 1.txt<br>\n- 2.txt<\/p>\n<p>then my steps;<\/p>\n<p>git init<br>\ngit remote add origin \u201c\u201d<br>\ndvc remote add -d dvcremote \u201c\u201d<br>\ndvc add plate<br>\ngit add plate.dvc<br>\ngit commit -m \u201cplate data added.\u201d<br>\ndvc push<br>\ngit push -u origin master<\/p>\n<p>then, \u0131 have added to folder 02 under the plate folder. 02 folder hierarcy is,<br>\n-02<br>\n-images<br>\n- 1.jpg<br>\n- 2.jpg<br>\n-labels<br>\n- 1.txt<br>\n- 2.txt<\/p>\n<p>then \u0131 repeated steps.<\/p>\n<p>dvc add plate<br>\ngit add plate.dvc<br>\ngit commit -m \u201cplate dataset updated.\u201d<br>\ndvc push<br>\ngit push -u origin master<\/p>\n<p>then, onother person want to pull the data according to commit.<br>\nhe checked the commit and go to plate dataset addded commit<\/p>\n<p>git checkout xxxxxx<br>\ndvc checkout<\/p>\n<p>he want to see just 01 folder. Okay there is no problem.<br>\nthen he checked another commit plate dataset updated.<\/p>\n<p>git checkout wwwww<br>\ndvc checkout<\/p>\n<p>and he saw the 01 and 02 folder. Okay, also there is no problem.<\/p>\n<p>then he want to check again first plate dataset added commit<\/p>\n<p>git checkout xxxxxx<br>\ndvc checkout<\/p>\n<p>but he saw the 01 and 02 folders again, but he want see the just 01 folder under the plate folder. Right ?<\/p>\n<p>What is the reason? what is the wrong step is here ?<\/p>",
                "Answer_score":31.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-24T08:07:02.498Z",
                "Answer_body":"<p>Can you share the logs for the <code>dvc checkout<\/code>. As well as the existence of <code>02.dvc<\/code> file.<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-24T09:20:00.877Z",
                "Answer_body":"<p>when I wrote a dvc checkout \u0131 couldnt see any log.<br>\nIt occurs plate.dvc everytime, because \u0131 tracked this plate folder. then \u0131 checked plate.dvc files according to differents commit. these plate.dvc files are different eachother.<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-24T18:36:54.683Z",
                "Answer_body":"<aside class=\"quote no-group quote-modified\" data-username=\"merve\" data-post=\"9\" data-topic=\"739\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/m\/f07891\/40.png\" class=\"avatar\"> merve:<\/div>\n<blockquote>\n<p>he saw the 01 and 02 folders again, but he want see the just 01 folder under the plate folder.<br>\nWhat is the reason? what is the wrong step is here ?<\/p>\n<\/blockquote>\n<\/aside>\n<p><a class=\"mention\" href=\"\/u\/merve\">@merve<\/a> I just tried this and indeed both directories are left in there, but <code>plate\/02\/<\/code> would be empty. Can you confirm this is what you see too?<\/p>\n<p>My last steps:<\/p>\n<pre><code class=\"lang-auto\">$ tree\n.\n\u251c\u2500\u2500 plate\n\u2502   \u251c\u2500\u2500 01\n\u2502   \u2502   \u2514\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 02\n\u2502       \u2514\u2500\u2500 data\n\u2514\u2500\u2500 plate.dvc\n$  git checkout HEAD^\n$ dvc checkout\nM       plate\/\n$  tree\n.\n\u251c\u2500\u2500 plate\n\u2502   \u251c\u2500\u2500 01\n\u2502   \u2502   \u2514\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 02\n\u2514\u2500\u2500 plate.dvc\n<\/code><\/pre>\n<p>TBH I\u2019m not sure whether DVC should be removing that directory. <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a> <a class=\"mention\" href=\"\/u\/isidentical\">@isidentical<\/a> ?<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-24T20:10:38.278Z",
                "Answer_body":"<p>yes now i noticed. but why the plate\/ 02 directory is not removed? This causes confusion. I shouldn\u2019t always check if the folder is empty, right?<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-24T20:13:54.050Z",
                "Answer_body":"<p>Hi guys,<\/p>\n<p>That checkout behavior is a known bug <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2397\" class=\"inline-onebox\">dvc checkout will leave empty folders \u00b7 Issue #2397 \u00b7 iterative\/dvc \u00b7 GitHub<\/a><\/p>",
                "Answer_score":16.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"track file hello track folder wrong folder hiearchi folder add folder comment ad git git tag messag ad file folder new data hiearchi folder repeat thing add folder git add folder git tag messag check tag want file folder saw file wrong",
        "Question_preprocessed_content":"track file hello track folder wrong folder hiearchi folder add folder comment ad git git tag messag ad file folder new data hiearchi folder repeat thing add folder git add folder git tag messag check tag want file folder saw file wrong",
        "Question_gpt_summary_original":"The user is facing challenges in tracking their folder and is unsure of what they are doing wrong. They have added files to the folder using \"dvc add Folder\" and then added it to git with a tag. However, when they added more files to the folder and repeated the process, they found that all the files in the folder were being displayed instead of just the specific file they wanted to see. The user is seeking help to understand why this is happening and what they can do to fix it.",
        "Question_gpt_summary":"user face challeng track folder unsur wrong ad file folder add folder ad git tag ad file folder repeat process file folder displai instead specif file want user seek help understand happen fix",
        "Answer_original_content":"merv run checkout git checkout ye forgot mention sorri push git repo chang branch tag write checkout chang merv folder git checkout differ version content folder chang ye git checkout differ version checkout folder chang delet folder run checkout restor folder expect doctor output delet folder checkout folder restor look like misunderstand abl creat minim reproduc exampl issu folder hierarhi comlet dataset plate imag jpg jpg label txt txt step git init git remot add origin remot add remot add plate git add plate git commit plate data ad push git push origin master ad folder plate folder folder hierarci imag jpg jpg label txt txt repeat step add plate git add plate git commit plate dataset updat push git push origin master onoth person want pull data accord commit check commit plate dataset a commit git checkout checkout want folder okai problem check commit plate dataset updat git checkout checkout saw folder okai problem want check plate dataset ad commit git checkout checkout saw folder want folder plate folder right reason wrong step share log checkout exist file wrote checkout log occur plate everytim track plate folder check plate file accord differ commit plate file differ eachoth merv saw folder want folder plate folder reason wrong step merv tri directori left plate confirm step tree plate data data plate git checkout head checkout plate tree plate data plate tbh sure remov directori kuprus isident ye notic plate directori remov caus confus shouldnt check folder right gui checkout behavior known bug checkout leav folder issu iter github",
        "Answer_preprocessed_content":"run ye forgot mention sorri push git repo chang branch tag write checkout chang merv folder git checkout differ version content folder chang ye git checkout differ version checkout folder chang delet run restor folder expect output delet folder checkout folder restor look like misunderstand abl creat minim reproduc exampl issu folder hierarhi comlet dataset plate imag label step git init git remot add origin remot add remot add plate git add plate git commit plate data ad push git push origin master ad folder plate folder folder hierarci imag label repeat step add plate git add plate git commit plate dataset updat push git push origin master onoth person want pull data accord commit check commit plate dataset a commit git checkout checkout want folder okai problem check commit plate dataset updat git checkout checkout saw folder okai problem want check plate dataset ad commit git checkout checkout saw folder want folder plate folder right reason wrong step share log exist file wrote checkout log occur plate everytim track plate folder check plate file accord differ commit plate file differ eachoth merv saw folder want folder plate folder reason wrong step tri directori left confirm step tbh sure remov directori ye notic plate directori remov caus confus shouldnt check folder right gui checkout behavior known bug checkout leav folder issu iter github",
        "Answer_gpt_summary_original":"the user is encountering challenges with tracking their folder and understanding why they are seeing more files than expected when checking the tag v1. the possible solutions include creating a minimal reproducible example for the issue, checking the logs for the checkout, and confirming that the plate\/02\/ folder is empty. there is also a known bug where checkout will leave empty folders, causing confusion.",
        "Answer_gpt_summary":"user encount challeng track folder understand see file expect check tag possibl solut includ creat minim reproduc exampl issu check log checkout confirm plate folder known bug checkout leav folder caus confus"
    },
    {
        "Question_id":null,
        "Question_title":"Why PyTorch is using only one GPU ?",
        "Question_body":"Azure does not use the two GPUs of my node with PyTorch (and Hugging Face). The monitoring tool of Azure shows the GPU usage is stuck at 50%.\nIts a Standard_NC12, so it has two K80s.\n\n\n\n\nI tried this way :\nhttps:\/\/azure.github.io\/azureml-cheatsheets\/docs\/cheatsheets\/python\/v1\/distributed-training\/#distributeddataparallel-per-process-launch\nand it looked like this in my notebook :\n\n\n\n\n\nI copied the docker file from the curated environments and added the libraries I needed successfully :\n\n FROM mcr.microsoft.com\/azureml\/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20220329.v1\n    \n ENV AZUREML_CONDA_ENVIRONMENT_PATH \/azureml-envs\/pytorch-1.10\n    \n # Create conda environment\n RUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\n     python=3.8 \\\n     pip=20.2.4 \\\n     pytorch=1.10.0 \\\n     torchvision=0.11.1 \\\n     torchaudio=0.10.0 \\\n     cudatoolkit=11.1.1 \\\n     nvidia-apex=0.1.0 \\\n     gxx_linux-64 \\\n     -c anaconda -c pytorch -c conda-forge\n    \n # Prepend path to AzureML conda environment\n ENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/bin:$PATH\n    \n # Install pip dependencies\n RUN pip install 'matplotlib>=3.3,<3.4' \\\n                 'psutil>=5.8,<5.9' \\\n                 'tqdm>=4.59,<4.63' \\\n                 'pandas>=1.3,<1.4' \\\n                 'scipy>=1.5,<1.8' \\\n                 'numpy>=1.10,<1.22' \\\n                 'ipykernel~=6.0' \\\n                 'azureml-core==1.40.0' \\\n                 'azureml-defaults==1.40.0' \\\n                 'azureml-mlflow==1.40.0' \\\n                 'azureml-telemetry==1.40.0' \\\n                 'tensorboard==2.6.0' \\\n                 'tensorflow-gpu==2.6.0' \\\n                 'onnxruntime-gpu>=1.7,<1.10' \\\n                 'horovod==0.23' \\\n                 'future==0.18.2' \\\n                 'wandb' \\\n                 'transformers' \\\n                 'einops' \\\n                 'torch-tb-profiler==0.3.1'\n    \n    \n # This is needed for mpi to locate libpython\n ENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/lib:$LD_LIBRARY_PATH\n    \n RUN export CUDA_VISIBLE_DEVICES=0,1\n\n\n\nI tried everything, I even added the CUDA_VISIBLE_DEVICES=0,1 inside the docker file.\n\nMy cluster is correctly configured because my colleague can use another tool (Detr with Lightning) and use 100% of the computing power.\nI copied his docker file and the result was the same, so our guess is that his tool is automatically managing all GPUs for him.\n\nDoes anyone know why the cluster is using only one GPU ?",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1653503234337,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/864175\/why-pytorch-is-using-only-one-gpu.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-27T15:37:03Z",
                "Answer_score":0,
                "Answer_body":"That's interesting because it was written :\nVirtual machine size\nStandard_NC12 (12 cores, 112 GB RAM, 680 GB disk)\nProcessing unit\nGPU - 2 x NVIDIA Tesla K80\nThen I guess I did not understand it properly and I am stuck using 50% of 1 K80.\n\nprint(torch.cuda.device_count()) gives :\n2\n\nnode_count = 2 leads to :\nRequested 2 nodes but AzureMLCompute cluster only has 1 maximum nodes.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-01T08:42:04.183Z",
                "Answer_score":0,
                "Answer_body":"(I also realized in the job's properties raw json that gpuCount is 0 in the compute and computeRequest sections)",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-02T12:19:04.443Z",
                "Answer_score":1,
                "Answer_body":"model = nn.DataParallel(model)\ndid the job.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"pytorch gpu azur us gpu node pytorch hug face monitor tool azur show gpu usag stuck standard tri wai http azur github cheatsheet doc cheatsheet python distribut train distributeddataparallel process launch look like notebook copi docker file curat environ ad librari need successfulli mcr microsoft com openmpi cuda cudnn ubuntu env conda environ path env pytorch creat conda environ run conda creat conda environ path python pip pytorch torchvis torchaudio cudatoolkit nvidia apex gxx linux anaconda pytorch conda forg prepend path conda environ env path conda environ path bin path instal pip depend run pip instal matplotlib horovod futur transform einop torch profil need mpi locat libpython env librari path conda environ path lib librari path run export cuda visibl devic tri ad cuda visibl devic insid docker file cluster correctli configur colleagu us tool detr lightn us comput power copi docker file result guess tool automat manag gpu know cluster gpu",
        "Question_preprocessed_content":"pytorch gpu azur us gpu node pytorch monitor tool azur show gpu usag stuck tri wai look like notebook copi docker file curat environ ad librari need successfulli env creat conda environ run conda creat anaconda pytorch prepend path conda environ env path instal pip depend run pip instal transform einop need mpi locat libpython env run export tri ad insid docker file cluster correctli configur colleagu us tool us comput power copi docker file result guess tool automat manag gpu know cluster gpu",
        "Question_gpt_summary_original":"The user is facing challenges with PyTorch and Hugging Face on Azure, as the monitoring tool shows that GPU usage is stuck at 50% and only one of the two K80s is being used. The user has tried various solutions, including modifying the docker file and adding the CUDA_VISIBLE_DEVICES parameter, but the issue persists. The user's colleague is able to use 100% of the computing power with a different tool, leading the user to suspect that the issue may be related to PyTorch's management of GPUs.",
        "Question_gpt_summary":"user face challeng pytorch hug face azur monitor tool show gpu usag stuck user tri solut includ modifi docker file ad cuda visibl devic paramet issu persist user colleagu abl us comput power differ tool lead user suspect issu relat pytorch manag gpu",
        "Answer_original_content":"interest written virtual machin size standard core ram disk process unit gpu nvidia tesla guess understand properli stuck print torch cuda devic count give node count lead request node comput cluster maximum node realiz job properti raw json gpucount comput computerequest section model dataparallel model job",
        "Answer_preprocessed_content":"interest written virtual machin size process unit gpu nvidia tesla guess understand properli stuck give lead request node comput cluster maximum node realiz job properti raw json gpucount comput computerequest section model job",
        "Answer_gpt_summary_original":"the solution to the challenge of pytorch not utilizing both gpus of the node is to use the nn.dataparallel(model) function. the user had attempted to configure the environment correctly but was only able to use 50% of 1 k80 gpu. the virtual machine size was standard_nc12 with 2 x nvidia tesla k80 gpus, but the user did not understand it properly. the print(torch.cuda.device_count()) function gave 2, but the node_count was only 1. the requested 2 nodes were not available in the compute cluster. the job's properties raw json showed that gpucount was 0 in the compute and computerequest sections.",
        "Answer_gpt_summary":"solut challeng pytorch util gpu node us dataparallel model function user attempt configur environ correctli abl us gpu virtual machin size standard nvidia tesla gpu user understand properli print torch cuda devic count function gave node count request node avail comput cluster job properti raw json show gpucount comput computerequest section"
    },
    {
        "Question_id":null,
        "Question_title":"Parallel batch trial pipeline",
        "Question_body":"<p>In a batched trial pipeline see <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/196\" rel=\"nofollow noopener\">196<\/a>, how can we run as fast as possible parallelizing everything we can ?<\/p>\n<p>Example Pipeline:<\/p>\n<pre><code>operation_1: # has no dependencies\n\noperation_2: \n    requires:\n        - operation: operation_1\n    flags-dest: globals\n    flags-import:\n        - some_param\n\noperation_3: \n    requires:\n        - operation: operation_2\n    flags-dest: globals\n    flags-import:\n        - some_other_param\n\npipeline:\n  steps:\n    - run: operation_1\n    - run: operation_2\n      flags:\n        some_param: [a, b]\n    - run: operation_3\n      flags:\n        some_other_param: [1, 2, 3]\n<\/code><\/pre>\n<pre><code># create 6 queues as we have 6 batch trials that can be done in parallel (a1, a2, a3, b1, b2, b3, c1, c2, c3)\n# run this command 6 times\nguild run queue --background \n\nguild run pipeline --stage\n<\/code><\/pre>\n<p>For some reason this leads to out of sequence events happening, like operation 2 being run before operation 1 resulting in an error. Is this the correct way to do this?<\/p>\n<p>Further, is there a good way of timing this to sanity check parallel works faster i.e time batched trials run?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591810114957,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":360.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/parallel-batch-trial-pipeline\/142",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-10T23:26:07.272Z",
                "Answer_body":"<p>Don\u2019t use staged runs for this. Just run your pipelines in the background. At some point Guild will be optimized for parallel runs and you won\u2019t have to think about this (as you say - we want to run everything as fast as possible). But at the moment, you need to use parallel OS processes to manage parallel runs.<\/p>",
                "Answer_score":3.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"parallel batch trial pipelin batch trial pipelin run fast possibl parallel exampl pipelin oper depend oper requir oper oper flag dest global flag import param oper requir oper oper flag dest global flag import param pipelin step run oper run oper flag param run oper flag param creat queue batch trial parallel run command time run queue background run pipelin stage reason lead sequenc event happen like oper run oper result error correct wai good wai time saniti check parallel work faster time batch trial run",
        "Question_preprocessed_content":"parallel batch trial pipelin batch trial pipelin run fast possibl parallel exampl pipelin reason lead sequenc event happen like oper run oper result error correct wai good wai time saniti check parallel work faster time batch trial run",
        "Question_gpt_summary_original":"The user is facing challenges in running a batched trial pipeline in parallel as fast as possible. They are experiencing out of sequence events, such as operation 2 being run before operation 1, resulting in errors. The user is also seeking a way to time the batched trials to ensure that parallel processing is faster.",
        "Question_gpt_summary":"user face challeng run batch trial pipelin parallel fast possibl experienc sequenc event oper run oper result error user seek wai time batch trial ensur parallel process faster",
        "Answer_original_content":"dont us stage run run pipelin background point optim parallel run wont think want run fast possibl moment need us parallel process manag parallel run",
        "Answer_preprocessed_content":"dont us stage run run pipelin background point optim parallel run wont think moment need us parallel process manag parallel run",
        "Answer_gpt_summary_original":"possible solutions to the challenge of running a parallel batch trial pipeline include avoiding staged runs and running pipelines in the background. it is recommended to use parallel os processes to manage parallel runs until guild is optimized for parallel runs.",
        "Answer_gpt_summary":"possibl solut challeng run parallel batch trial pipelin includ avoid stage run run pipelin background recommend us parallel process manag parallel run optim parallel run"
    },
    {
        "Question_id":null,
        "Question_title":"How to resolve error - Creating conda environment failed with exit code: 1?",
        "Question_body":"I am getting this error when I run script job_submit.py. I do not know how to debug this issue, would appreciate help to solve this.\n\n\n\n\nPS: I have just started learning azure so I am not sure what I am missing.\n\n\n\n\nscript_to_run.py\nfrom azureml.core import Workspace, Dataset, Run\nws = Workspace.from_config()\naz_dataset = Dataset.get_by_name(workspace=ws, name='titanic-dataset')\n# Get the context of the experiment\nnew_run = Run.get_context()\ndf = az_dataset.to_pandas_dataframe()\n### count the observations\ntotal_obs = len(df)\n### get the gender count\ngender_count = df['Sex'].value_counts()\n# log the metrics to workspace\nnew_run.log(name = \"Total observations\", value = total_obs)\n### Log the gender data values\nfor val in df['Sex'].unique():\n    new_run.log(name = val, value = gender_count[val])\n# complete an experiment run\nnew_run.complete()\n\n\n\njob_submit.py\nfrom azureml.core import Workspace, Datastore, Dataset, Experiment, ScriptRunConfig, Environment\n# Access workspace\nws = Workspace.from_config()\n# create an experiment object\nexp = Experiment(workspace=ws, name = \"Titanic_exp\")\n# create custom env - myenv\nmyenv = Environment(name = 'MyEnvironment')\n# to install dependencies\nfrom azureml.core.environment import CondaDependencies\n# from CondaDependencies class we need to create an object which will have all the required dependencies\n# create the dependencies object\npackages = CondaDependencies.create(conda_packages=['pandas', 'scikit-learn']) # this will have list of all packages we will need\nmyenv.python.conda_dependencies = packages # this will tell to install the packages\n# register environment to workspace so that we have access to it\nmyenv.register(ws)\n# create a script configuration for custom env\nscript_config = ScriptRunConfig(source_directory = '.', script = \"script_to_run.py\", environment = myenv)\nnew_run = exp.submit(config = script_config)",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1629557858217,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/522968\/how-to-resolve-error-creating-conda-environment-fa.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-24T01:12:50.137Z",
                "Answer_score":0,
                "Answer_body":"Hi @ramr-msft thanks for replying.\nI am following this link to learn azureml: 4-azure-ml-experiment\n\nI did the following steps:\n1. Created the workspace, downloaded the config.json file and uploaded it in the .azureml folder created by me.\n2. I was able to successfully run this script using experiment.start_logging() function as given in the module. \n3. The problem comes when i try to run the script as an experiment using Run.get_context()125769-60-control-log.txt\n\n4. I am attaching the 60_control_log.txt for reference.\n\nPlease let me know if this answers your question and helps you to figure out the issue.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-07T11:52:30.913Z",
                "Answer_score":0,
                "Answer_body":"Hey, I was having the same issue. resolved in two steps, however i think you can skip step 1\n\nstep 1 (not sure if this actually had any impact but did not retest with default setup)\nI have modified the environment.yml fiIe and specified a python version as 3.8.10\n\n\nstep 2 (I think this did the trick)\nCreated new compute instance and rerun\n\nHope same works for you",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"resolv error creat conda environ fail exit code get error run script job submit know debug issu appreci help solv start learn azur sure miss script run core import workspac dataset run workspac config dataset dataset workspac titan dataset context experi new run run context dataset panda datafram count observ total ob len gender count gender count sex valu count log metric workspac new run log total observ valu total ob log gender data valu val sex uniqu new run log val valu gender count val complet experi run new run complet job submit core import workspac datastor dataset experi scriptrunconfig environ access workspac workspac config creat experi object exp experi workspac titan exp creat custom env myenv myenv environ myenviron instal depend core environ import condadepend condadepend class need creat object requir depend creat depend object packag condadepend creat conda packag panda scikit learn list packag need myenv python conda depend packag tell instal packag regist environ workspac access myenv regist creat script configur custom env script config scriptrunconfig sourc directori script script run environ myenv new run exp submit config script config",
        "Question_preprocessed_content":"resolv error creat conda environ fail exit code get error run script know debug issu appreci help solv start learn azur sure miss core import workspac dataset run context experi count observ len gender count log metric workspac total observ valu log gender data valu val val valu complet experi run core import workspac datastor dataset experi scriptrunconfig environ access workspac creat experi object exp experi creat custom env myenv myenv environ instal depend import condadepend condadepend class need creat object requir depend creat depend object packag list packag need packag tell instal packag regist environ workspac access creat script configur custom env script environ myenv",
        "Question_gpt_summary_original":"The user is encountering an error when running the script job_submit.py, which is related to creating a conda environment. The user is unsure how to debug the issue and is seeking help to resolve it. The user has just started learning Azure and is not sure what they are missing.",
        "Question_gpt_summary":"user encount error run script job submit relat creat conda environ user unsur debug issu seek help resolv user start learn azur sure miss",
        "Answer_original_content":"ramr msft thank repli follow link learn azur experi follow step creat workspac download config json file upload folder creat abl successfulli run script experi start log function given modul problem come try run script experi run context control log txt attach control log txt refer let know answer question help figur issu hei have issu resolv step think skip step step sure actual impact retest default setup modifi environ yml fiie specifi python version step think trick creat new comput instanc rerun hope work",
        "Answer_preprocessed_content":"thank repli follow link learn follow step creat workspac download file upload folder creat abl successfulli run script function given modul problem come try run script experi attach refer let know answer question help figur issu hei have issu resolv step think skip step step modifi fiie specifi python version step creat new comput instanc rerun hope work",
        "Answer_gpt_summary_original":"possible solutions to the error encountered when running the script job_submit.py are: \n1. modify the environment.yml file and specify a python version as 3.8.10.\n2. create a new compute instance and rerun the script.",
        "Answer_gpt_summary":"possibl solut error encount run script job submit modifi environ yml file specifi python version creat new comput instanc rerun script"
    },
    {
        "Question_id":null,
        "Question_title":"Spot instances for inference and sagemaker?",
        "Question_body":"Is it possible to deploy spot inf1 instances on sagemaker? We run an API 24\/7, and it's costly to keep it up, considering we only have 2 hours of peak performance a day.\n\nWe don't shut off those machines because we might have random bursts of traffic during the day that CPU instances can't hold. Alternatively, we could deploy spot EC2 inf machines; however, I'm unsure how I would invoke them from gateway and lambda. Does anybody have a tip or recommendation for our case?\n\nThanks!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649350121825,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":184.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcQU2DOmNQdyI8HWeIMzzdg\/spot-instances-for-inference-and-sagemaker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-07T23:39:13.030Z",
                "Answer_score":0,
                "Answer_body":"You could possibly integrate EC2 Spot instance fleet with Application Auto Scaling service to spin up or down spot instances when you receive traffic. To scale it down to 0 instances, you will need to configure a queue to hold the requests while you spin up from 0 instances to 1 or more. Then your application would insert the requests in the SQS queue and wait for an instance to be available. Take a look at this link for more information on how to configure application autoscaling with Spot instances: https:\/\/docs.aws.amazon.com\/autoscaling\/application\/userguide\/services-that-can-integrate-ec2.html\n\nTo configure your policy for the autoscaling, you can look at SQS queue length metric. Here is how you can set a target tracking policy for the application autoscaling: https:\/\/docs.aws.amazon.com\/autoscaling\/application\/userguide\/create-target-tracking-policy-cli.html",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"spot instanc infer possibl deploi spot inf instanc run api costli consid hour peak perform dai shut machin random burst traffic dai cpu instanc hold altern deploi spot inf machin unsur invok gatewai lambda anybodi tip recommend case thank",
        "Question_preprocessed_content":"spot instanc infer possibl deploi spot inf instanc run api costli consid hour peak perform dai shut machin random burst traffic dai cpu instanc hold altern deploi spot inf machin unsur invok gatewai lambda anybodi tip recommend case thank",
        "Question_gpt_summary_original":"The user is facing challenges with the cost of running an API 24\/7 on CPU instances and is considering using spot EC2 inf machines. However, they are unsure how to invoke them from gateway and lambda. They are seeking tips or recommendations for their case.",
        "Question_gpt_summary":"user face challeng cost run api cpu instanc consid spot inf machin unsur invok gatewai lambda seek tip recommend case",
        "Answer_original_content":"possibl integr spot instanc fleet applic auto scale servic spin spot instanc receiv traffic scale instanc need configur queue hold request spin instanc applic insert request sq queue wait instanc avail look link inform configur applic autosc spot instanc http doc aw amazon com autosc applic userguid servic integr html configur polici autosc look sq queue length metric set target track polici applic autosc http doc aw amazon com autosc applic userguid creat target track polici cli html",
        "Answer_preprocessed_content":"possibl integr spot instanc fleet applic auto scale servic spin spot instanc receiv traffic scale instanc need configur queue hold request spin instanc applic insert request sq queue wait instanc avail look link inform configur applic autosc spot instanc configur polici autosc look sq queue length metric set target track polici applic autosc",
        "Answer_gpt_summary_original":"possible solutions to keep the api running 24\/7 in a cost-effective way include integrating ec2 spot instance fleet with application auto scaling service to spin up or down spot instances when traffic is received. to scale down to 0 instances, a queue needs to be configured to hold requests while instances are spun up. the policy for autoscaling can be configured using the sqs queue length metric and a target tracking policy can be set for application autoscaling. more information on how to configure application autoscaling with spot instances can be found in the provided link.",
        "Answer_gpt_summary":"possibl solut api run cost effect wai includ integr spot instanc fleet applic auto scale servic spin spot instanc traffic receiv scale instanc queue need configur hold request instanc spun polici autosc configur sq queue length metric target track polici set applic autosc inform configur applic autosc spot instanc provid link"
    },
    {
        "Question_id":null,
        "Question_title":"Predicting H2O-models stored as pyfunc-models",
        "Question_body":"Hi,\n\n\u00a0\n\nMe and my team are trying to run H2O models (stored as pyfunc-models) on our Hadoop-cluster using spark.\nBasically not much more than this: \u00a0\n\nimport mlflow\n\nimport mlflow.pyfunc\n\n\u00a0\n\nfeatures = spark.read.parquet(\u201c\u2026\/features.parquet\u201d) \\\n\npredict_udf = mlflow.pyfunc.spark_udf(spark,\"..\/mlruns\/0\/f4ff13d1d93f4baab67dea545effca9b\/artifacts\/model_pyfunc\/\")\n\n\u00a0\n\npredictions = features.withColumn(\"prediction\", predict_udf('FML_Saved_vs_AUM_Ratio','TOT_IVT_AM','IND_Saved_vs_AUM_Ratio','SOL_SMT_CD','FML_PSN_FUND_AM')\n\npredictions.show(100,False)\n\n\u00a0\n\n\u00a0\n\nWe get the same prediction for each row.\n\n\n\n\nAfter taking a closer look at what happens during the prediction I see that the column names on which the data is trained are not recognized within the dataset that is being predicted.\n\n\n\n\/hadoop\/sdj1\/yarn\/local\/usercache\/jd05953\/appcache\/application_1557817794126_12227\/container_e312_1557817794126_12227_01_000002\/env\/mlflow_h2o\/lib\/python2.7\/site-packages\/h2o\/job.py:69: UserWarning: Test\/Validation dataset is missing column 'FML_Saved_vs_AUM_Ratio': substituting in a column of NaN\n\n\u00a0 warnings.warn(w)\n\n\/hadoop\/sdj1\/yarn\/local\/usercache\/jd05953\/appcache\/application_1557817794126_12227\/container_e312_1557817794126_12227_01_000002\/env\/mlflow_h2o\/lib\/python2.7\/site-packages\/h2o\/job.py:69: UserWarning: Test\/Validation dataset is missing column 'TOT_IVT_AM': substituting in a column of NaN\n\n\u00a0 warnings.warn(w)\n\n\/hadoop\/sdj1\/yarn\/local\/usercache\/jd05953\/appcache\/application_1557817794126_12227\/container_e312_1557817794126_12227_01_000002\/env\/mlflow_h2o\/lib\/python2.7\/site-packages\/h2o\/job.py:69: UserWarning: Test\/Validation dataset is missing column 'IND_Saved_vs_AUM_Ratio': substituting in a column of NaN\n\n\u00a0 warnings.warn(w)\n\n\/hadoop\/sdj1\/yarn\/local\/usercache\/jd05953\/appcache\/application_1557817794126_12227\/container_e312_1557817794126_12227_01_000002\/env\/mlflow_h2o\/lib\/python2.7\/site-packages\/h2o\/job.py:69: UserWarning: Test\/Validation dataset is missing column 'SOL_SMT_CD': substituting in a column of NaN\n\n\u00a0 warnings.warn(w)\n\n\/hadoop\/sdj1\/yarn\/local\/usercache\/jd05953\/appcache\/application_1557817794126_12227\/container_e312_1557817794126_12227_01_000002\/env\/mlflow_h2o\/lib\/python2.7\/site-packages\/h2o\/job.py:69: UserWarning: Test\/Validation dataset is missing column 'FML_PSN_FUND_AM': substituting in a column of NaN\n\n\u00a0 warnings.warn(w)\n\n\u00a0\n\n\u00a0\n\nWhen I take a look at the dataframe which is actually being used for the prediction (generated within the pandas UDF of the pyfunc-module)\u00a0 \u2026\n\n\u00a0\u00a0\u00a0 def predict(*args):\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 model = SparkModelCache.get_or_load(archive_path)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 schema = {str(i): arg for i, arg in enumerate(args)}\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 # Explicitly pass order of columns to avoid lexicographic ordering (i.e., 10 < 2)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 columns = [str(i) for i, _ in enumerate(args)]\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 pdf = pandas.DataFrame(schema, columns=columns)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 result = model.predict(pdf)\n\n\u00a0\n\n\u2026 I do indeed see that the column names are replaced by numeric values:\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4\u00a0\u00a0\u00a0\u00a0\u00a0\n\n0\u00a0\u00a0\u00a0\u00a0 0.826852\u00a0\u00a0\u00a0 2891.36\u00a0 0.825862\u00a0\u00a0\u00a0\u00a0\u00a0 123.62\u00a0\u00a0 0.00\u00a0\u00a0\u00a0\n\n1\u00a0\u00a0\u00a0\u00a0 0.656996\u00a0\u00a0\u00a0 7227.10\u00a0 0.000000\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.00\u00a0\u00a0\u00a0 0.00\u00a0\u00a0\u00a0\u00a0\u00a0\n\n2\u00a0\u00a0\u00a0\u00a0 0.011852\u00a0\u00a0\u00a0 1498.00\u00a0 0.000000\u00a0 \u00a0\u00a0\u00a065.25\u00a0\u00a0\u00a0\u00a0 0.00\n\n\u00a0\n\n\u00a0\n\nSo it seems I\u2019m getting the same prediction because everything is predicted on NaN-values.\n\n\u00a0\n\nIn Scikit-learn the order of the column is used so this should not be a problem. However, in H2O, the column names are used.\n\nSo I don\u2019t see how I can score my H2O-model properly? Am I missing something?\u00a0\n\n\n\n\nkind regards\n\nTomasz",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1557995579000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":29.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/y8hhL4ecb7k",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-05-24T07:22:01",
                "Answer_body":"Anyone with any thoughts?\n\n\nDid someone ever try running an H2O model with the pyfunc.predict?\u00a0\nBecause I don't see how this feature can work..\n\n\ngreets\nTomasz\n\nOp donderdag 16 mei 2019 14:32:59 UTC+2 schreef tomasz2:\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-05-24T13:37:08",
                "Answer_body":"Hey Tomasz. Yes this is a known limitation of spark_udf. It relies on pandas udf under the hood and that means the column names are lost. This should be fixed soon-ish with updates coming to spark but for now you would need to do some workaround. The easiest solution is to use a custom pyfunc model wrapper which would set column names to expected values before calling h2o.\n\n\nYou can do something like this:\n\n\n```\nclass H2OPyFuncWrapper(mlflow.pyfunc.PythonModel):\n\n\u00a0 \u00a0 def __init__(self, column_names):\n\u00a0 \u00a0 \u00a0 \u00a0 self.column_names = column_names\n\n\u00a0 \u00a0 def load_context(self, context):\n\u00a0 \u00a0 \u00a0 \u00a0 self.wrapped_model = mlflow.pyfunc.load_pyfunc(context[\"wrapped_model\"])\n\n\n\u00a0 \u00a0 def predict(self, context, model_input):\n\u00a0 \u00a0 \u00a0 \u00a0 pandas_df = pandas_df.copy(deep=False)\n\u00a0 \u00a0 \u00a0 \u00a0 pandas_df.columns = list(self.column_names)\n\u00a0 \u00a0 \u00a0 \u00a0 return self.wrapped_model.predict(pandas_df)\n\n\ndef log_model(path_to_h2o_model_artifact, env = mlflow.h2o.DEFAULT_CONDA_ENV):\n\u00a0 \u00a0 mlflow.pyfunc.log_model(artifact_path=\"wrapped_h2o_model\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python_model=H2OModelPyFuncWrapper(column_names = column_names),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 artifacts={\"wrapped_model\": path_to_model_artifact},\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 conda_env=env)\n\n```\n\n\n\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/3649ddcc-30a7-45f0-9fb7-f2bd8f7f68e2%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2019-05-28T04:34:58",
                "Answer_body":"Hi Tomas,\n\n\nthanks for the feedback!\n\n\nCurious for the spark updates. In the meanwhile we'll be using the work-around.\u00a0\n\n\n\n\n\ngreets\nTomasz\n\n\nOp vrijdag 24 mei 2019 19:37:08 UTC+2 schreef Tomas Nykodym:\n\ue5d3\n\ue5d3\n\ue5d3\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow...@googlegroups.com.\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"predict model store pyfunc model team try run model store pyfunc model hadoop cluster spark basic import import pyfunc featur spark read parquet featur parquet predict udf pyfunc spark udf spark mlrun fffddfbaabdeaeffcab artifact model pyfunc predict featur withcolumn predict predict udf fml save aum ratio tot ivt ind save aum ratio sol smt fml psn fund predict fals predict row take closer look happen predict column name data train recogn dataset predict hadoop sdj yarn local usercach appcach applic contain env lib python site packag job userwarn test valid dataset miss column fml save aum ratio substitut column nan warn warn hadoop sdj yarn local usercach appcach applic contain env lib python site packag job userwarn test valid dataset miss column tot ivt substitut column nan warn warn hadoop sdj yarn local usercach appcach applic contain env lib python site packag job userwarn test valid dataset miss column ind save aum ratio substitut column nan warn warn hadoop sdj yarn local usercach appcach applic contain env lib python site packag job userwarn test valid dataset miss column sol smt substitut column nan warn warn hadoop sdj yarn local usercach appcach applic contain env lib python site packag job userwarn test valid dataset miss column fml psn fund substitut column nan warn warn look datafram actual predict gener panda udf pyfunc modul def predict arg model sparkmodelcach load archiv path schema str arg arg enumer arg explicitli pass order column avoid lexicograph order column str enumer arg pdf panda datafram schema column column result model predict pdf column name replac numer valu get predict predict nan valu scikit learn order column problem column name dont score model properli miss kind regard tomasz",
        "Question_preprocessed_content":"predict store team try run model spark basic import import pyfunc featur predict predict row take closer look happen predict column name data train recogn dataset predict userwarn dataset miss column substitut column nan userwarn dataset miss column substitut column nan userwarn dataset miss column substitut column nan userwarn dataset miss column substitut column nan userwarn dataset miss column substitut column nan look datafram actual predict def predict model schema explicitli pass order column avoid lexicograph order column pdf column column result column name replac numer valu get predict predict order column problem column name dont score properli miss kind regard tomasz",
        "Question_gpt_summary_original":"The user is facing challenges in running H2O models stored as pyfunc-models on a Hadoop-cluster using spark. The user is getting the same prediction for each row, and upon closer inspection, the column names on which the data is trained are not recognized within the dataset that is being predicted. The user also noticed that the column names are replaced by numeric values, which is causing everything to be predicted on NaN-values. The user is unsure how to score the H2O-model properly and is seeking assistance.",
        "Question_gpt_summary":"user face challeng run model store pyfunc model hadoop cluster spark user get predict row closer inspect column name data train recogn dataset predict user notic column name replac numer valu caus predict nan valu user unsur score model properli seek assist",
        "Answer_original_content":"thought try run model pyfunc predict featur work greet tomasz donderdag mei utc schreef tomasz hei tomasz ye known limit spark udf reli panda udf hood mean column name lost fix soon ish updat come spark need workaround easiest solut us custom pyfunc model wrapper set column name expect valu call like class hopyfuncwrapp pyfunc pythonmodel def init self column name self column name column name def load context self context self wrap model pyfunc load pyfunc context wrap model def predict self context model input panda panda copi deep fals panda column list self column name return self wrap model predict panda def log model path model artifact env default conda env pyfunc log model artifact path wrap model python model homodelpyfuncwrapp column name column name artifact wrap model path model artifact conda env env receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user ddcc fbdffe googlegroup com option visit http group googl com optout toma thank feedback curiou spark updat work greet tomasz vrijdag mei utc schreef toma nykodym unsubscrib group stop receiv email send email googlegroup com",
        "Answer_preprocessed_content":"thought try run model featur greet tomasz donderdag mei utc schreef tomasz hei tomasz ye known limit reli panda udf hood mean column name lost fix updat come spark need workaround easiest solut us custom pyfunc model wrapper set column name expect valu call like receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit toma thank feedback curiou spark updat greet tomasz vrijdag mei utc schreef toma nykodym unsubscrib group stop receiv email send email",
        "Answer_gpt_summary_original":"Solution:\n- A workaround solution is provided by using a custom pyfunc model wrapper which would set column names to expected values before calling h2o. The code for the wrapper is provided in the discussion. \n- The issue is a known limitation of spark_udf and should be fixed soon with updates coming to spark.",
        "Answer_gpt_summary":"solut workaround solut provid custom pyfunc model wrapper set column name expect valu call code wrapper provid discuss issu known limit spark udf fix soon updat come spark"
    },
    {
        "Question_id":59711677.0,
        "Question_title":"Why am I getting error while importing SparkContext library in sagemaker notebook?",
        "Question_body":"<p>I am using SageMaker Notebook in AWS Glue for ETL development. <\/p>\n\n<p>On importing the SparkContext library I am getting the below error. I have tried to restart the kernel but did not heled. Can some one explain me the point \"a\". <\/p>\n\n<blockquote>\n  <p>The code failed because of a fatal error: Error sending http request\n  and maximum retry encountered..*<\/p>\n  \n  <p>Some things to try: <\/p>\n  \n  <p>a. Make sure Spark has enough available resources for Jupyter to\n  create a Spark context. <\/p>\n  \n  <p>b. Contact your Jupyter administrator to make sure the Spark magics\n  library is configured correctly. <\/p>\n  \n  <p>c. Restart the kernel.<\/p>\n<\/blockquote>\n\n<p>Following points to be noted:<\/p>\n\n<ol>\n<li><p>I am creating the sagemaker notebook from AWS Console > AWS Glue > Dev Endpoint > Notebooks.<\/p><\/li>\n<li><p>VPC, Subnet and Security group of the dev endpoint created is same as the RDS to which connection is\nsupposed to be made. While creating dev endpoint, in the networking\npage I choose an existing connection from the list of connections\navailable in the drop down so that VPC, subnet and security group\nare automatically chosen.<\/p><\/li>\n<li>I had increased the DPU from 5 to 10\nbut still getting this error. <\/li>\n<li>Not able reach the step where I can\ncreate connection to RDS because getting error while calling the\nlibrary. <\/li>\n<li>If I skip the networking info while creating the dev end\npoint I am successfully able to call all the relevant libraries\n(screenshot attached). (which is not suggested when connecting to\nRDS as it would not work).<\/li>\n<\/ol>\n\n<p>So, this error (\"The code failed because...\") is coming only when providing a connection.<\/p>\n\n<p>Would be helpful if some one could help out in resolving this issue. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/JmonV.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JmonV.jpg\" alt=\"When adding connections, getting this error\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ewRKh.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ewRKh.jpg\" alt=\"Without adding connections I am not getting error\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1578898025257,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1578900656767,
        "Question_score":1.0,
        "Question_view_count":5137.0,
        "Answer_body":"<p>Wondering if you're configuration for Livy endpoint is valid? Livy runs on port 8998. You should check if the port is open in the security group. <\/p>\n\n<p>This might be useful: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/<\/a><\/p>\n\n<p>Also, if that does not help, you should try stopping and restarting the notebook once. That has helped in the past. <\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59711677",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1578942905056,
        "Question_original_content":"get error import sparkcontext librari notebook notebook aw glue etl develop import sparkcontext librari get error tri restart kernel hele explain point code fail fatal error error send http request maximum retri encount thing try sure spark avail resourc jupyt creat spark context contact jupyt administr sure spark magic librari configur correctli restart kernel follow point note creat notebook aw consol aw glue dev endpoint notebook vpc subnet secur group dev endpoint creat rd connect suppos creat dev endpoint network page choos exist connect list connect avail drop vpc subnet secur group automat chosen increas dpu get error abl reach step creat connect rd get error call librari skip network info creat dev end point successfulli abl relev librari screenshot attach suggest connect rd work error code fail come provid connect help help resolv issu",
        "Question_preprocessed_content":"get error import sparkcontext librari notebook notebook aw glue etl develop import sparkcontext librari get error tri restart kernel hele explain point code fail fatal error error send http request maximum retri thing try sure spark avail resourc jupyt creat spark context contact jupyt administr sure spark magic librari configur correctli restart kernel follow point note creat notebook aw consol aw glue dev endpoint notebook vpc subnet secur group dev endpoint creat rd connect suppos creat dev endpoint network page choos exist connect list connect avail drop vpc subnet secur group automat chosen increas dpu get error abl reach step creat connect rd get error call librari skip network info creat dev end point successfulli abl relev librari screenshot attach error come provid connect help help resolv issu",
        "Question_gpt_summary_original":"The user is encountering an error while importing the SparkContext library in SageMaker Notebook for ETL development. The error message suggests that there may not be enough resources for Jupyter to create a Spark context. The user has tried restarting the kernel and increasing the DPU, but the error persists. The error only occurs when providing a connection to RDS, and skipping the networking info allows the user to call all relevant libraries. The user is seeking help in resolving the issue.",
        "Question_gpt_summary":"user encount error import sparkcontext librari notebook etl develop error messag suggest resourc jupyt creat spark context user tri restart kernel increas dpu error persist error occur provid connect rd skip network info allow user relev librari user seek help resolv issu",
        "Answer_original_content":"wonder configur livi endpoint valid livi run port check port open secur group us http aw amazon com blog machin learn build amazon notebook back spark amazon emr help try stop restart notebook help past",
        "Answer_preprocessed_content":"wonder configur livi endpoint valid livi run port check port open secur group us help try stop restart notebook help past",
        "Answer_gpt_summary_original":"possible solutions to the error encountered while importing the sparkcontext library in a notebook in aws glue for etl development are: \n1. checking if the configuration for livy endpoint is valid and if the port 8998 is open in the security group.\n2. stopping and restarting the notebook.",
        "Answer_gpt_summary":"possibl solut error encount import sparkcontext librari notebook aw glue etl develop check configur livi endpoint valid port open secur group stop restart notebook"
    },
    {
        "Question_id":null,
        "Question_title":"Semi-transparent smoothing stopped working",
        "Question_body":"<p>Smoothing suddenly stopped making the original graph semi-transparent.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e.png\" data-download-href=\"\/uploads\/short-url\/6HJfxA7ERNAiH7FwWeL8MzvFDIi.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e_2_512x500.png\" alt=\"image\" data-base62-sha1=\"6HJfxA7ERNAiH7FwWeL8MzvFDIi\" width=\"512\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e_2_512x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">517\u00d7504 48.9 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Any way your could rollback to the previous behavior? Some ETA for fixing this would be much appreciated, so our team could plan accordingly.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1639019539043,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":378.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/semi-transparent-smoothing-stopped-working\/1492",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-09T03:25:43.450Z",
                "Answer_body":"<p>PS. For anyone facing this issue - a colleague of mine found a work-around in the \u201cEdit panel\u201d to turn off the original chart:<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/de2a9528f51e0238785436562d02036c199af134.png\" alt=\"image\" data-base62-sha1=\"vHnellx3bs9OhnqiUZ1lqfZ0unW\" width=\"227\" height=\"463\"><\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-09T12:06:18.979Z",
                "Answer_body":"<p>Hey Sebastian,<br>\nthank you for reporting this. I\u2019ll share this with the team so we can fix this as soon as possible.<br>\nWill let you know once the issue is resolved.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-13T08:19:32.556Z",
                "Answer_body":"<p>Hey there, the issue has been fixed. Thank you for your patience!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-11T08:19:42.541Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"semi transpar smooth stop work smooth suddenli stop make origin graph semi transpar imag wai rollback previou behavior eta fix appreci team plan accordingli",
        "Question_preprocessed_content":"smooth stop work smooth suddenli stop make origin graph imag wai rollback previou behavior eta fix appreci team plan accordingli",
        "Question_gpt_summary_original":"The user is facing a challenge where the semi-transparent smoothing feature has suddenly stopped working, resulting in the original graph not being semi-transparent anymore. The user is requesting a rollback to the previous behavior and an ETA for fixing the issue.",
        "Question_gpt_summary":"user face challeng semi transpar smooth featur suddenli stop work result origin graph semi transpar anymor user request rollback previou behavior eta fix issu",
        "Answer_original_content":"face issu colleagu work edit panel turn origin chart hei sebastian thank report ill share team fix soon possibl let know issu resolv hei issu fix thank patienc topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"face issu colleagu edit panel turn origin chart hei sebastian thank report ill share team fix soon possibl let know issu resolv hei issu fix thank patienc topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"a colleague found a work-around in the edit panel to turn off the original chart. the team is aware of the issue and will fix it as soon as possible. the issue has been fixed and the user is thanked for their patience.",
        "Answer_gpt_summary":"colleagu work edit panel turn origin chart team awar issu fix soon possibl issu fix user thank patienc"
    },
    {
        "Question_id":68193944.0,
        "Question_title":"Set custom environment variables in AWS",
        "Question_body":"<p>I am using AWS sagemaker, I have some secret keys and access keys to access some APIs that I don't want to expose directly in code.<\/p>\n<p>What are the ways like environment variables etc., that can be used to hide these keys and I can use them securely, and how to set them.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625052591003,
        "Question_favorite_count":null,
        "Question_last_edit_time":1625058756808,
        "Question_score":0.0,
        "Question_view_count":277.0,
        "Answer_body":"<p>AWS System Manager (SSM) is designed to store keys and tokens securely.<\/p>\n<p>Depending on how your notebook is defined, you could <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html\" rel=\"nofollow noreferrer\">use the 'env' property<\/a> directly or in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-environment-variables\" rel=\"nofollow noreferrer\">training data<\/a>, or you could access SSM directly from sagemaker. For example this Snowflake KB article explains how to fetch auth info from ssm: <a href=\"https:\/\/community.snowflake.com\/s\/article\/Connecting-a-Jupyter-Notebook-Part-3\" rel=\"nofollow noreferrer\">https:\/\/community.snowflake.com\/s\/article\/Connecting-a-Jupyter-Notebook-Part-3<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68193944",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1625064328872,
        "Question_original_content":"set custom environ variabl aw secret kei access kei access api want expos directli code wai like environ variabl hide kei us secur set",
        "Question_preprocessed_content":"set custom environ variabl aw secret kei access kei access api want expos directli code wai like environ variabl hide kei us secur set",
        "Question_gpt_summary_original":"The user is facing challenges in securely accessing APIs in AWS sagemaker without exposing secret and access keys in the code. They are seeking information on how to set custom environment variables to hide these keys and use them securely.",
        "Question_gpt_summary":"user face challeng secur access api expos secret access kei code seek inform set custom environ variabl hide kei us secur",
        "Answer_original_content":"aw manag ssm design store kei token secur depend notebook defin us env properti directli train data access ssm directli exampl snowflak articl explain fetch auth info ssm http commun snowflak com articl connect jupyt notebook",
        "Answer_preprocessed_content":"aw manag design store kei token secur depend notebook defin us env properti directli train data access ssm directli exampl snowflak articl explain fetch auth info ssm",
        "Answer_gpt_summary_original":"possible solutions to securely store secret keys and access keys in aws to access apis without exposing them directly in code are to use aws system manager (ssm) or the 'env' property directly or in training data. another option is to access ssm directly from the notebook. a snowflake kb article explains how to fetch auth info from ssm.",
        "Answer_gpt_summary":"possibl solut secur store secret kei access kei aw access api expos directli code us aw manag ssm env properti directli train data option access ssm directli notebook snowflak articl explain fetch auth info ssm"
    },
    {
        "Question_id":null,
        "Question_title":"Rendering confusion matrix from 2D array",
        "Question_body":"<p>I have a ConfusionMatrix metric implemented as a subclass of tf.keras.metrics.Metric where the only value is a tf.Variable with shape=(13,13) and dtype=tf.uint32. In wandb UI this just shows up as a histogram, is there some way to reinterpret this as a multilabel confusion matrix?<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/ce373dcc3a5f1e93c916b79e77166b17d8006c2c.png\" data-download-href=\"\/uploads\/short-url\/tqgId0mKmRahaQW7TqrvTeIns4Y.png?dl=1\" title=\"Screen Shot 2022-07-19 at 9.09.20 pm\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ce373dcc3a5f1e93c916b79e77166b17d8006c2c_2_690x446.png\" alt=\"Screen Shot 2022-07-19 at 9.09.20 pm\" data-base62-sha1=\"tqgId0mKmRahaQW7TqrvTeIns4Y\" width=\"690\" height=\"446\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ce373dcc3a5f1e93c916b79e77166b17d8006c2c_2_690x446.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/ce373dcc3a5f1e93c916b79e77166b17d8006c2c.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/ce373dcc3a5f1e93c916b79e77166b17d8006c2c.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ce373dcc3a5f1e93c916b79e77166b17d8006c2c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2022-07-19 at 9.09.20 pm<\/span><span class=\"informations\">874\u00d7566 16.8 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1658228978386,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":46.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/rendering-confusion-matrix-from-2d-array\/2775",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-20T11:01:34.485Z",
                "Answer_body":"<p>Hey Tom, it\u2019s not possible at the moment. You\u2019ll need to use wandb.plot.confusion_matrix() for that.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-17T11:10:29.341Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"render confus matrix arrai confusionmatrix metric implement subclass kera metric metric valu variabl shape dtype uint show histogram wai reinterpret multilabel confus matrix screen shot",
        "Question_preprocessed_content":"render confus matrix arrai confusionmatrix metric implement subclass valu shape show histogram wai reinterpret multilabel confus matrix screen shot",
        "Question_gpt_summary_original":"The user is facing a challenge in reinterpreting a ConfusionMatrix metric implemented as a subclass of tf.keras.metrics.Metric with a tf.Variable of shape=(13,13) and dtype=tf.uint32, which is currently displayed as a histogram in wandb UI, into a multilabel confusion matrix.",
        "Question_gpt_summary":"user face challeng reinterpret confusionmatrix metric implement subclass kera metric metric variabl shape dtype uint current displai histogram multilabel confus matrix",
        "Answer_original_content":"hei tom possibl moment youll need us plot confus matrix topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"hei tom possibl moment youll need us topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"possible solution: the user can use \".plot.confusion_matrix()\" instead of trying to interpret the tf.variable as a multilabel confusion matrix.",
        "Answer_gpt_summary":"possibl solut user us plot confus matrix instead try interpret variabl multilabel confus matrix"
    },
    {
        "Question_id":null,
        "Question_title":"I would like to access the values in a 'run_table' type artifact without downloading the file",
        "Question_body":"<p>I have tried accessing the artifact in various ways, and I seem to get the artifact name, but then the artifact is not recognized as existing if I use wandb.use_artifact() or if I go through logged artifacts and try to access the table, the table does not exist.  I\u2019ve tried two ways:<\/p>\n<p>for key, value in run.summary.items():<br>\nprint(f\"{key}: {value}\")<br>\nif desired_string in key:<br>\nname = (see below for possible combinations of key and value)<br>\nwandb.init()<br>\nartifact = wandb.use_artifact(name)  <span class=\"hashtag\">#I<\/span> get an error here<br>\ntable = artifact.get(name)<br>\ncolumns = table.get_column(\u2018columns\u2019)<br>\nprint(columns)<\/p>\n<p>I\u2019ve tried the following to get the name that will let me use the artifact and table :<br>\nname = key<br>\nname = value[\u2018path\u2019]<br>\nname = ((value[\u2018path\u2019].split(\u2018:\u2019)[0]).split(\u2018\/\u2019)[-1]).split(\u2018.\u2019)[0]<br>\nbecause the path had the format \"media\/table\/given_file_name.table.json<\/p>\n<p>I also tried stepping through the logged objects:<br>\nfor artifact in run.logged_artifacts():<br>\nfname = artifact.name<br>\nftype = artifact.type<br>\nif desired_string in fname and ftype == \u2018run_table\u2019:<br>\nname = fname<br>\ntable = artifact.get(name)<br>\ncolumns = table.get_column(\u2018columns\u2019) # get an error here because table is None<br>\nprint(columns)<\/p>\n<p>I have tried similar string operations as above to get the correct name for the table. I have also tried using the artifact ID instead of the name.<\/p>\n<p>I have no problems downloading the file containing the table data and then processing it, but I would like to access the data directly without downloading. Please help.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1675377409008,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":51.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/i-would-like-to-access-the-values-in-a-run-table-type-artifact-without-downloading-the-file\/3805",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-06T21:52:08.447Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/snuss-warren\">@snuss-warren<\/a> , happy to look into this for you. You had mentioned running into an error when executing<br>\n<code>artifact = wandb.use_artifact(name)<\/code> , could you please provide details into what the error message is? As long as a project contains the artifact, you should be able to <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/run#use_artifact\">fetch it<\/a>. Example for one of my projects.<\/p>\n<pre><code class=\"lang-auto\">import wandb\nwandb.init(project=\"support\")\nartifact = wandb.use_artifact('mnist-preprocess:v0')\n<\/code><\/pre>\n<p>In terms of accessing the data directly or for example, downloading only a few files from an artifact. This isn\u2019t currently  an available feature but there is an active feature request out for this functionality.<\/p>",
                "Answer_score":10.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-07T14:11:05.137Z",
                "Answer_body":"<p>Hi.  Thanks for your help. I believe the error in the use_artifact  command was \u201cfile not found\u201d or similar, and I believe it must have been  difference in the name I read compared to the actual file names.  It would not surprise me if it had to do with the path.  I unfortunately do not have the exact error and found another way to get the information.<\/p>\n<blockquote>\n<p>In terms of accessing the data directly or for example, downloading only a few files from an artifact. This isn\u2019t currently an available feature but there is an active feature request out for this functionality.<\/p>\n<\/blockquote>\n<p>So what I really wanted to do is not possible.  In that case, I have managed to access the needed files by downloading and  I will continue to use that solution. I believe my issue is resolved.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-07T16:40:12.621Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/snuss-warren\">@snuss-warren<\/a> , thank you for the update. I will add your details to the feature request ticket and keep you updated once there\u2019s been movement on it\u2019s implementation. If you are able to reproduce the errors you originally saw with your artifacts and would like to continue the conversation, please do response and we\u2019ll take a close look. In the meantime I will mark the request resolved.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"like access valu run tabl type artifact download file tri access artifact wai artifact artifact recogn exist us us artifact log artifact try access tabl tabl exist iv tri wai kei valu run summari item print kei valu desir string kei possibl combin kei valu init artifact us artifact error tabl artifact column tabl column column print column iv tri follow let us artifact tabl kei valu path valu path split split split path format media tabl given file tabl json tri step log object artifact run log artifact fname artifact ftype artifact type desir string fname ftype run tabl fname tabl artifact column tabl column column error tabl print column tri similar string oper correct tabl tri artifact instead problem download file contain tabl data process like access data directli download help",
        "Question_preprocessed_content":"like access valu type artifact download file tri access artifact wai artifact artifact recogn exist us log artifact try access tabl tabl exist iv tri wai kei valu print kei init artifact error tabl column print iv tri follow let us artifact tabl kei valu path format tri step log object artifact fname ftype fname ftype fname tabl column error tabl print tri similar string oper correct tabl tri artifact instead problem download file contain tabl data process like access data directli download help",
        "Question_gpt_summary_original":"The user is facing challenges in accessing the values in a 'run_table' type artifact without downloading the file. The user has tried accessing the artifact in various ways, but the artifact is not recognized as existing if they use wandb.use_artifact() or try to access the table through logged artifacts. The user has tried different methods to get the correct name for the table, but they have not been successful. The user is able to download the file containing the table data and process it, but they would like to access the data directly without downloading.",
        "Question_gpt_summary":"user face challeng access valu run tabl type artifact download file user tri access artifact wai artifact recogn exist us us artifact try access tabl log artifact user tri differ method correct tabl success user abl download file contain tabl data process like access data directli download",
        "Answer_original_content":"snuss warren happi look mention run error execut artifact us artifact provid detail error messag long project contain artifact abl fetch exampl project import init project support artifact us artifact mnist preprocess term access data directli exampl download file artifact isnt current avail featur activ featur request function thank help believ error us artifact command file similar believ differ read compar actual file name surpris path unfortun exact error wai inform term access data directli exampl download file artifact isnt current avail featur activ featur request function want possibl case manag access need file download continu us solut believ issu resolv snuss warren thank updat add detail featur request ticket updat there movement implement abl reproduc error origin saw artifact like continu convers respons close look meantim mark request resolv",
        "Answer_preprocessed_content":"happi look mention run error execut provid detail error messag long project contain artifact abl fetch exampl project term access data directli exampl download file artifact isnt current avail featur activ featur request function thank help believ error command file similar believ differ read compar actual file name surpris path unfortun exact error wai inform term access data directli exampl download file artifact isnt current avail featur activ featur request function want possibl case manag access need file download continu us solut believ issu resolv thank updat add detail featur request ticket updat there movement implement abl reproduc error origin saw artifact like continu convers respons close look meantim mark request resolv",
        "Answer_gpt_summary_original":"there is currently no feature available to access data directly or download only a few files from an artifact. however, there is an active feature request for this functionality. the user was able to access the needed files by downloading them. the support team will add the user's details to the feature request ticket and keep them updated once there has been movement on its implementation.",
        "Answer_gpt_summary":"current featur avail access data directli download file artifact activ featur request function user abl access need file download support team add user detail featur request ticket updat movement implement"
    },
    {
        "Question_id":49604773.0,
        "Question_title":"Azure Machine Learning Studio vs. Workbench",
        "Question_body":"<p>What is the difference between <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-studio\/\" rel=\"noreferrer\">Azure Machine Learning Studio<\/a> and <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-services\/\" rel=\"noreferrer\">Azure Machine Learning Workbench<\/a>?  What is the <em>intended<\/em> difference? And is it expected that Workbench is heading towards deprecation in favor of Studio?<\/p>\n\n<p>I have gathered an assorted collection of differences:<\/p>\n\n<ul>\n<li>Studio has a hard limit of 10 GB total input of training data per module, whereas Workbench has a variable limit by price.<\/li>\n<li>Studio appears to have a more fully-featured GUI and user-friendly deployment tools, whereas Workbench appears to have more powerful \/ customizable deployment tools.<\/li>\n<li>etc.<\/li>\n<\/ul>\n\n<p>However, I have also found several scattered references claiming that Studio is a renamed updated of Workbench, even though both services appear to still be offered.<\/p>\n\n<p>For a fresh Data Scientist looking to adopt the Microsoft stack (potentially on an enterprise scale within the medium-term and for the long-term), which offering should I prefer?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1522638099293,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":8.0,
        "Question_view_count":3387.0,
        "Answer_body":"<p>Azure Machine Learning Workbench is a preview downloadable application. It provides a UI for many of the Azure Machine Learning CLI commands, particularly around experimentation submission for Python based jobs to DSVM or HDI. The Azure Machine Learning CLI is made up of many key functions, such as job submisison, and creation of real time web services. The workbench installer provided a way to install everything required to participate in the preview. <\/p>\n\n<p>Azure Machine Learning Studio is an older product, and provides a drag and drop interface for creating simply machine learning processes. It has limitations about the size of the data that can be handled (about 10gigs of processing). Learning and customer requests have based on this service have contributed to the design of the new Azure Machine Learning CLI mentioned above.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49604773",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1524806701632,
        "Question_original_content":"studio workbench differ studio workbench intend differ expect workbench head deprec favor studio gather assort collect differ studio hard limit total input train data modul workbench variabl limit price studio appear fulli featur gui user friendli deploy tool workbench appear power customiz deploy tool scatter refer claim studio renam updat workbench servic appear offer fresh data scientist look adopt microsoft stack potenti enterpris scale medium term long term offer prefer",
        "Question_preprocessed_content":"studio workbench differ studio workbench intend differ expect workbench head deprec favor studio gather assort collect differ studio hard limit total input train data modul workbench variabl limit price studio appear gui deploy tool workbench appear power customiz deploy tool scatter refer claim studio renam updat workbench servic appear offer fresh data scientist look adopt microsoft stack offer prefer",
        "Question_gpt_summary_original":"The user is trying to understand the differences between Azure Machine Learning Studio and Azure Machine Learning Workbench, and is facing challenges in deciding which one to use. The user has found differences such as Studio having a limit of 10 GB input of training data per module, while Workbench has a variable limit by price. Studio has a more user-friendly GUI and deployment tools, while Workbench has more powerful and customizable deployment tools. However, the user has also found scattered references claiming that Studio is a renamed and updated version of Workbench, which is causing confusion.",
        "Question_gpt_summary":"user try understand differ studio workbench face challeng decid us user differ studio have limit input train data modul workbench variabl limit price studio user friendli gui deploy tool workbench power customiz deploy tool user scatter refer claim studio renam updat version workbench caus confus",
        "Answer_original_content":"workbench preview download applic provid cli command particularli experiment submiss python base job dsvm hdi cli kei function job submisison creation real time web servic workbench instal provid wai instal requir particip preview studio older product provid drag drop interfac creat simpli machin learn process limit size data handl gig process learn custom request base servic contribut design new cli mention",
        "Answer_preprocessed_content":"workbench preview download applic provid cli command particularli experiment submiss python base job dsvm hdi cli kei function job submisison creation real time web servic workbench instal provid wai instal requir particip preview studio older product provid drag drop interfac creat simpli machin learn process limit size data handl learn custom request base servic contribut design new cli mention",
        "Answer_gpt_summary_original":"the answer explains that workbench is a preview downloadable application that provides a ui for many cli commands, particularly around experimentation submission for python-based jobs to dsvm or hdi. it also mentions that studio is an older product that provides a drag and drop interface for creating simple machine learning processes, but has limitations on the size of data that can be handled. based on this information, the user can choose between workbench and studio based on their specific needs and preferences.",
        "Answer_gpt_summary":"answer explain workbench preview download applic provid cli command particularli experiment submiss python base job dsvm hdi mention studio older product provid drag drop interfac creat simpl machin learn process limit size data handl base inform user choos workbench studio base specif need prefer"
    },
    {
        "Question_id":70397010.0,
        "Question_title":"What would stop credentials from validation on a ClearML server?",
        "Question_body":"<p>I've set up a ClearML server in GCP using the sub-domain approach. I can access all three domains (<code>https:\/\/app.clearml.mydomain.com<\/code>, <code>https:\/\/api.clearml.mydomain.com<\/code> and <code>https:\/\/files.clearml.mydomain.com<\/code>) in a browser and see what I think is the correct response, but when connecting with the python SDK via <code>clearml-init<\/code> I get the following error:<\/p>\n<pre><code>clearml.backend_api.session.session.LoginError: Failed getting token (error 400 from https:\/\/api.clearml.mydomain.com): Bad Request\n<\/code><\/pre>\n<p>Are there any likely causes of this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639763350487,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":194.0,
        "Answer_body":"<p>Following the discussion <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/517\" rel=\"nofollow noreferrer\">here<\/a>, it seemed that the load balancer being used was blocking <code>GET<\/code> requests with a payload which are used by ClearML. A <a href=\"https:\/\/github.com\/allegroai\/clearml\/pull\/521\" rel=\"nofollow noreferrer\">fix<\/a> is being worked on to allow the method to be changed to a <code>POST<\/code> request via an environment variable.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70397010",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1640162037740,
        "Question_original_content":"stop credenti valid server set server gcp sub domain approach access domain http app mydomain com http api mydomain com http file mydomain com browser think correct respons connect python sdk init follow error backend api session session loginerror fail get token error http api mydomain com bad request like caus error",
        "Question_preprocessed_content":"stop credenti valid server set server gcp approach access domain browser think correct respons connect python sdk follow error like caus error",
        "Question_gpt_summary_original":"The user is encountering an error while trying to connect to a ClearML server using the python SDK via clearml-init. The error message indicates that the credentials are not being validated due to a Bad Request error. The user is seeking assistance in identifying the likely causes of this error.",
        "Question_gpt_summary":"user encount error try connect server python sdk init error messag indic credenti valid bad request error user seek assist identifi like caus error",
        "Answer_original_content":"follow discuss load balanc block request payload fix work allow method chang post request environ variabl",
        "Answer_preprocessed_content":"follow discuss load balanc block request payload fix work allow method chang request environ variabl",
        "Answer_gpt_summary_original":"the solution to the issue of credentials not validating on a server set up in gcp resulting in a \"bad request\" error is to change the method from a get request to a post request via an environment variable. this is because the load balancer being used is blocking get requests with a payload.",
        "Answer_gpt_summary":"solut issu credenti valid server set gcp result bad request error chang method request post request environ variabl load balanc block request payload"
    },
    {
        "Question_id":null,
        "Question_title":"Can I connect a Sagemaker \"Studio\" instance to a private github repository?",
        "Question_body":"I've successfully connected a Sagemaker \"notebook\" to a private github repository, but wondering if it isn't possible for a studio instance? Failing that is there an easy way to get the remote codecommit git url for an existing \"studio\" instance so that code there can at least be pulled to my local machine?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656504273482,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":74.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUH33ZXpiAQ_aV2TesXBNOBw\/can-i-connect-a-sagemaker-studio-instance-to-a-private-github-repository",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-30T10:17:37.550Z",
                "Answer_score":1,
                "Answer_body":"Hi tansaku!\n\nFor sure, SageMaker Studio is integrated with Git, so you can connect to both your public and private repositories!\n\nWhen you try to connect to a private repository, you will be asked to enter your username and password. Best practices here are to create a personal access token instead of using your password.\n\nIf you want to cache your credentials avoiding typing them each time you interact with the Github server, you can cache them or store them in the home folder with the following command run from the System Terminal:\n\n$ git config --global git credential.helper [cache|store]\n\n\nIf you choose store to store your credentials, it will be written to the ~\/.git-credentials file located in your home folder. The cache parameter stores credentials in memory and never on disk. It also accepts the --timeout <seconds> option, which changes the amount of time the daemon is kept running (default is \"900\" or 15 minutes).\n\nOnce it executes the command, the next time it pushes it will ask for the credentials and store them, after that it shouldn't ask again.\n\nHope this helps!",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"connect studio instanc privat github repositori successfulli connect notebook privat github repositori wonder isn possibl studio instanc fail easi wai remot codecommit git url exist studio instanc code pull local machin",
        "Question_preprocessed_content":"connect studio instanc privat github repositori successfulli connect notebook privat github repositori wonder isn possibl studio instanc fail easi wai remot codecommit git url exist studio instanc code pull local machin",
        "Question_gpt_summary_original":"The user is facing challenges in connecting a Sagemaker \"Studio\" instance to a private Github repository. They have successfully connected a Sagemaker \"notebook\" to a private Github repository but are unsure if it is possible for a studio instance. They are also looking for an easy way to get the remote codecommit git URL for an existing \"studio\" instance so that code there can be pulled to their local machine.",
        "Question_gpt_summary":"user face challeng connect studio instanc privat github repositori successfulli connect notebook privat github repositori unsur possibl studio instanc look easi wai remot codecommit git url exist studio instanc code pull local machin",
        "Answer_original_content":"tansaku sure studio integr git connect public privat repositori try connect privat repositori ask enter usernam password best practic creat person access token instead password want cach credenti avoid type time interact github server cach store home folder follow command run termin git config global git credenti helper cach store choos store store credenti written git credenti file locat home folder cach paramet store credenti memori disk accept timeout option chang time daemon kept run default minut execut command time push ask credenti store shouldn ask hope help",
        "Answer_preprocessed_content":"tansaku sure studio integr git connect public privat repositori try connect privat repositori ask enter usernam password best practic creat person access token instead password want cach credenti avoid type time interact github server cach store home folder follow command run termin git config git choos store store credenti written file locat home folder cach paramet store credenti memori disk accept option chang time daemon kept run execut command time push ask credenti store shouldn ask hope help",
        "Answer_gpt_summary_original":"the answer suggests that the user can connect to both public and private repositories using the \"studio\" instance. when connecting to a private repository, the user will be prompted to enter their username and password. it is recommended to create a personal access token instead of using a password. the answer also provides instructions on how to cache or store credentials to avoid typing them each time the user interacts with the github server. the user can use the \"git config\" command to store credentials in the home folder. the answer suggests using the \"cache\" parameter to store credentials in memory or the \"store\" parameter to store them in the ~\/.git-credentials file.",
        "Answer_gpt_summary":"answer suggest user connect public privat repositori studio instanc connect privat repositori user prompt enter usernam password recommend creat person access token instead password answer provid instruct cach store credenti avoid type time user interact github server user us git config command store credenti home folder answer suggest cach paramet store credenti memori store paramet store git credenti file"
    },
    {
        "Question_id":73471486.0,
        "Question_title":"How to prevent storing data in Jupyter project tree when writing data from Sagemaker to S3",
        "Question_body":"<p>I am new to AWS Sagemaker and I wrote data to my S3 bucket.\nBut these datasets also appear in the working tree of my jupyter instance.<\/p>\n<p>How can I move data directly to S3 without saving it &quot;locally&quot;?<\/p>\n<p>My code:<\/p>\n<pre><code>import os\nimport pandas as pd\n\nimport sagemaker, boto3\nfrom sagemaker import get_execution_role\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.serializers import CSVSerializer\n\n# please provide your own bucket and folder path of your bucket here\nbucket = &quot;test-bucket2342343&quot;\nsm_sess = sagemaker.Session(default_bucket=bucket)\nfile_path = &quot;Use Cases\/Sagemaker Demo\/xgboost&quot;\n\n# data \ndf_train = pd.DataFrame({'X':[0,100,200,400,450,  550,600,800,1600],\n                         'y':[0,0,  0,  0,  0,    1,  1,  1,  1]})\n\ndf_test = pd.DataFrame({'X':[10,90,240,459,120,  650,700,1800,1300],\n                        'y':[0,0,  0,  0,  0,    1,  1,  1,  1]})\n\n# move to S3 \ndf_train[['y','X']].to_csv('train.csv', header=False, index=False)\n\ndf_val = df_test.copy()\ndf_val[['y','X']].to_csv('val.csv', header=False, index=False)\n\nboto3.Session().resource(&quot;s3&quot;).Bucket(bucket) \\\n.Object(os.path.join(file_path, &quot;train.csv&quot;)).upload_file(&quot;train.csv&quot;)\n\nboto3.Session().resource(&quot;s3&quot;).Bucket(bucket) \\\n.Object(os.path.join(file_path, &quot;val.csv&quot;)).upload_file(&quot;val.csv&quot;)\n\n<\/code><\/pre>\n<p>It successfully appears in my S3 bucket.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/d1yCy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/d1yCy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But it also appears here:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/RjGZr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RjGZr.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661336683943,
        "Question_favorite_count":null,
        "Question_last_edit_time":1661337201248,
        "Question_score":0.0,
        "Question_view_count":17.0,
        "Answer_body":"<p>with Pandas you can save to S3 directly (<a href=\"https:\/\/stackoverflow.com\/a\/56275519\/121956\">relevant answer<\/a>). For example:<\/p>\n<pre><code>import pandas as pd\ndf = pd.DataFrame( [ [1, 1, 1], [2, 2, 2] ], columns=['a', 'b', 'c'])\ndf.to_csv('s3:\/\/test-bucket2342343\/\/tmp.csv', index=False)\n<\/code><\/pre>\n<p>Or, use what you currently do and delete the local files:<\/p>\n<pre><code>import os\nos.remove('train.csv')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73471486",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1661447960212,
        "Question_original_content":"prevent store data jupyt project tree write data new wrote data bucket dataset appear work tree jupyt instanc data directli save local code import import panda import boto import execut role input import traininginput serial import csvserial provid bucket folder path bucket bucket test bucket sess session default bucket bucket file path us case demo xgboost data train datafram test datafram train csv train csv header fals index fals val test copi val csv val csv header fals index fals boto session resourc bucket bucket object path join file path train csv upload file train csv boto session resourc bucket bucket object path join file path val csv upload file val csv successfulli appear bucket appear",
        "Question_preprocessed_content":"prevent store data jupyt project tree write data new wrote data bucket dataset appear work tree jupyt instanc data directli save local code successfulli appear bucket appear",
        "Question_gpt_summary_original":"The user is facing a challenge of data being stored in the Jupyter project tree when writing data from Sagemaker to S3. They are seeking a solution to move data directly to S3 without saving it locally. The user has successfully uploaded the data to their S3 bucket, but it also appears in the working tree of their Jupyter instance.",
        "Question_gpt_summary":"user face challeng data store jupyt project tree write data seek solut data directli save local user successfulli upload data bucket appear work tree jupyt instanc",
        "Answer_original_content":"panda save directli relev answer exampl import panda datafram column csv test bucket tmp csv index fals us current delet local file import remov train csv",
        "Answer_preprocessed_content":"panda save directli exampl us current delet local file",
        "Answer_gpt_summary_original":"the answer suggests two possible solutions to prevent data from being stored in the working tree of a jupyter instance when writing data to an s3 bucket. the first solution is to use pandas to save data directly to s3. the second solution is to delete the local files after writing them to s3 using the current method.",
        "Answer_gpt_summary":"answer suggest possibl solut prevent data store work tree jupyt instanc write data bucket solut us panda save data directli second solut delet local file write current method"
    },
    {
        "Question_id":null,
        "Question_title":"NotFoundError()",
        "Question_body":"<p>Hi,<\/p>\n<p>I was wondering if you have any suggestions on what could be causing the following error:<\/p>\n<p>wandb: Synced 6 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)<br>\nwandb: Find logs at: .\/wandb\/run-20220825_163228-73lfow6y\/logs<br>\nRun 73lfow6y errored: NotFoundError()<br>\nwandb: ERROR Run 73lfow6y errored: NotFoundError()<\/p>\n<p>Please note, I am using:<br>\nPython 3.7.12<br>\nwandb, version 0.13.2<\/p>\n<p>Thank you!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1661445735566,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":268.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/notfounderror\/3006",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-25T18:34:14.111Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dfrances\">@dfrances<\/a>,<br>\nCould you possibly share a minimal replication of your code? Also, if you are using our public cloud service would you mind sending a link to your workspace and I can take a look?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-01T22:07:42.973Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dfrances\">@dfrances<\/a> , since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-31T22:08:12.417Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"notfounderror wonder suggest caus follow error sync file media file artifact file file log run lfowi log run lfowi error notfounderror error run lfowi error notfounderror note python version thank",
        "Question_preprocessed_content":"notfounderror wonder suggest caus follow error sync file media file artifact file file log run lfow error notfounderror error run lfow error notfounderror note python version thank",
        "Question_gpt_summary_original":"The user is encountering a NotFoundError() while using wandb version 0.13.2 with Python 3.7.12. The error occurred during the syncing of 6 W&B files, 0 media files, 0 artifact files, and 0 other files. The user is seeking suggestions on what could be causing the error.",
        "Question_gpt_summary":"user encount notfounderror version python error occur sync file media file artifact file file user seek suggest caus error",
        "Answer_original_content":"dfranc possibl share minim replic code public cloud servic mind send link workspac look thank nate dfranc heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"possibl share minim replic code public cloud servic mind send link workspac look thank nate heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the answer is a request for more information and a notification that the conversation will be closed if there is no response.",
        "Answer_gpt_summary":"solut provid answer answer request inform notif convers close respons"
    },
    {
        "Question_id":null,
        "Question_title":"Multiple dvc runs in parallel",
        "Question_body":"<p>Hello,<\/p>\n<p>I am trying to run multiple dvc experiments simultaneously on a gpu cluster. The way the cluster works is that you send a bash script to the cluster and it runs the job when its your turn. In my bash script, I configure CUDA_VISIBLE_DEVICES, activate my conda environment and run the experiment using <code>dvc exp run --temp<\/code>. Before I submit my job, I make the changes I want for my experiment, e.g. set the variables in params.yaml. I am trying to send a job, wait for it to start running, then make changes and send another. The second job fails with this error:<\/p>\n<p><code>ERROR: Unable to acquire lock. Most likely another DVC process is running or was terminated abruptly. Check the page &lt;https:\/\/dvc.org\/doc\/user-guide\/troubleshooting#lock-issue&gt; for other possible reasons and to learn how to resolve this.<\/code><\/p>\n<p>I was hoping that using <code>--temp<\/code> would allow me run these jobs at the same time because it isolates the project in a separate directory. One thing I should note is that my cache dir is in a separate directory outside my project directory because I am not allowed to use that much space in my user directory.<\/p>\n<p>My question is related to this one: <a href=\"https:\/\/discuss.dvc.org\/t\/lock-error-with-parallelized-dvc-repro\/929\/2\">https:\/\/discuss.dvc.org\/t\/lock-error-with-parallelized-dvc-repro\/929\/2<\/a> in which the suggested course was to use <code>--queue<\/code> and <code>-j<\/code> to run the experiments in parallel. How can I use the built in parallel experiment execution but run the experiments in separate job submissions to the cluster? Or what is the intended workflow for this sort of situation?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":10,
        "Question_comment_count":0,
        "Question_creation_time":1646665895959,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":546.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-dvc-runs-in-parallel\/1102",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-07T15:55:43.930Z",
                "Answer_body":"<p>We don\u2019t have a recommended workflow in such situation per see, thought we might provide some suggestion how you could approach such situation. One way could be to provide GPU via params, for example schedule only the number of experiments that you have available GPU\u2019s, and for each one provide different GPU-id. That will require some legwork, if you have more experiments than GPU\u2019s, because you will have to schedule experiments \u2192 train \u2192 schedule experiments \u2192 train and so on, until all the experiments are run.<\/p>\n<p>Alternative approach, if you have control over the training server, is to figure a way to \u201cautomatically\u201d reserve the GPU. One way to do that would be to use some common directory and files inside as an indicator which GPU\u2019s are reserved, and which are free.<br>\nFor example, following code:<\/p>\n<pre><code class=\"lang-auto\">import zc.lockfile\nfrom contextlib import contextmanager\nimport os\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nGPU_IDS = [0,1,2]\n\nlock_dir = \"locks\"\nos.makedirs(lock_dir, exist_ok=True)\n\ndef lock(gpu_id):\n    path = os.path.join(lock_dir, str(gpu_id))\n    return zc.lockfile.LockFile(path)\n\n@contextmanager\ndef lock_free_gpu():\n    for gpu_id in GPU_IDS:\n        try:\n            l = lock(gpu_id)\n            yield gpu_id\n            l.close()\n            break\n        except zc.lockfile.LockError:\n            print(f\"Can't lock '{gpu_id}', trying another one\")\n            pass\n\ndef training_task(task_number):\n    with lock_free_gpu() as gpu_id:\n        print(f\"Working on task: '{task_number}', gpu: '{gpu_id}'\")\n        # simulate different time of execution\n        time.sleep(gpu_id*5+5)\n\nwith ThreadPoolExecutor(max_workers=3) as executor:\n    for i in range(20):\n        executor.submit(training_task, i)\n<\/code><\/pre>\n<p>\u201cReserves\u201d GPU\u2019s by locking appropriate file, which path equals to <code>locks\/{GPU-id}<\/code>, so that other run know not to use this one. This approach makes sense only if all the server users will use \u201clocking\u201d context manager, you also need to make sure to run the <code>exp<\/code> with <code>--jobs<\/code> &lt;= <code>number_of_gpus<\/code>.<\/p>",
                "Answer_score":28.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-07T16:29:51.140Z",
                "Answer_body":"<p>Thanks for the response!<\/p>\n<p>The problem is that I don\u2019t know which GPU I will get until the cluster assigns me one. The bash script I send to the cluster determines which GPU I have been assigned and sets the environment variable as needed.<\/p>\n<p>Also would specifying the GPU get me around the lock issue? It seems like dvc has an issue with me having multiple processes access the repo or the data or something like that. I was hoping that isolating the experiments with <code>--temp<\/code> would get me around this but it doesn\u2019t seem like it\u2019s working the way I assumed it would.<\/p>",
                "Answer_score":22.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-07T18:35:40.517Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/gregstarr\">@gregstarr<\/a>! Unfortunately, it\u2019s not yet possible to start new experiments while others are running. You would have to queue them and run them simultaneously to take advantage of parallel execution. We are working on addressing this limitation right now, and you can keep track of the progress in <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/5615\" class=\"inline-onebox\">exp queue: command for managing queues \u00b7 Issue #5615 \u00b7 iterative\/dvc \u00b7 GitHub<\/a>.<\/p>",
                "Answer_score":12.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-07T18:41:07.201Z",
                "Answer_body":"<p>Thanks for the response. Bummer that it is not possible at the moment, but I\u2019m glad to see it\u2019s being worked on!<\/p>",
                "Answer_score":2.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-07T18:41:13.903Z",
                "Answer_body":"<p>So, what you can do, is to <code>queue<\/code> few experiments and the <code>dvc exp run --run-all -j {num_jobs}<\/code>. But in order to be able to do that, you need some magic to determine which GPU your process should take. You say that you have a bash script determining that? Then maybe your stage <code>cmd<\/code> should look like<br>\n<code>.\/bash_script_to_set_gpu_id.sh &amp;&amp; python train.py<\/code>?<\/p>",
                "Answer_score":7.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-07T18:50:14.821Z",
                "Answer_body":"<p>By the way, there is one hacky workaround available now if you need to run jobs simultaneously. You can make multiple copies of your repo and run one experiment per copy.<\/p>",
                "Answer_score":7.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-07T18:53:25.725Z",
                "Answer_body":"<p>Ok I think I understand. Thanks for the clarification.<\/p>\n<p>The current workflow is to simply submit my bash script to the cluster job manager. The bash script looks something like this:<\/p>\n<ol>\n<li>misc. configuration<\/li>\n<li><code>name={read the file that says which gpu I am assigned}<\/code><\/li>\n<li><code>export CUDA_VISIBLE_DEVICES=\"${name}\"<\/code><\/li>\n<li>activate conda env<\/li>\n<li><code>dvc exp run --temp<\/code><\/li>\n<\/ol>\n<p>Then I submit the job with a command: <code>qsub qsub_script.sh<\/code>. This submits the job to the job manager and returns.<\/p>\n<p>It sounds like what I need to do instead is have my command submit the job? i.e. I make my changes, queue up my experiments, then do <code>dvc exp run --run-all -j {num_jobs}<\/code>, but my whole pipeline is just the one command <code>qsub qsub_script.sh<\/code>?<\/p>\n<p>Is this right?<\/p>",
                "Answer_score":7.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-07T18:54:23.261Z",
                "Answer_body":"<p>If I go this route, is there a nice way to merge stuff together? Like copying the git\/ref\/exp folder and the .dvc\/tmp\/exp folder?<\/p>",
                "Answer_score":2.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-07T19:14:44.315Z",
                "Answer_body":"<p>Yes, if you are able to determine all experiments you want to run before submitting, you can invert your workflow and have dvc call qsub instead. Whether you submit your whole pipeline as one qsub job or split each stage into its own qsub job is up to you.<\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-11T18:38:13.862Z",
                "Answer_body":"<p>Hello again. This is the solution I ended up trying but I ran into a different issue when trying to <code>dvc exp run<\/code> multiple in parallel. It has trouble cleaning up because some git directory isn\u2019t empty. It sounds exactly like this issue and I posted debug output to this issue:<\/p>\n<aside class=\"onebox githubissue\" data-onebox-src=\"https:\/\/github.com\/iterative\/dvc\/issues\/5641#issuecomment-1064656886\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/5641#issuecomment-1064656886\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com\/iterative\/dvc<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"><\/path><\/svg>\n  <\/div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/5641#issuecomment-1064656886\" target=\"_blank\" rel=\"noopener nofollow ugc\">exp run: cannot clean up temp directory runs on Linux + NFS<\/a>\n    <\/h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2021-03-17\" data-time=\"11:34:20\" data-timezone=\"UTC\">11:34AM - 17 Mar 21 UTC<\/span>\n      <\/div>\n\n        <div class=\"date\">\n          closed <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2021-04-30\" data-time=\"01:18:37\" data-timezone=\"UTC\">01:18AM - 30 Apr 21 UTC<\/span>\n        <\/div>\n\n      <div class=\"user\">\n        <a href=\"https:\/\/github.com\/werthen\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <img alt=\"werthen\" src=\"https:\/\/avatars.githubusercontent.com\/u\/3852492?v=4\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          werthen\n        <\/a>\n      <\/div>\n    <\/div>\n\n    <div class=\"labels\">\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/bug.png?v=12\" title=\"bug\" class=\"emoji\" alt=\"bug\" width=\"20\" height=\"20\"> bug\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          p1-important\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          research\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          A: experiments\n        <\/span>\n    <\/div>\n  <\/div>\n<\/div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\"># Bug Report\n\n&lt;!--\n## Issue name\n\nIssue names must follow the pattern `comm<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026<\/a><\/span><span class=\"excerpt hidden\">and: description` where the command is the dvc command that you are trying to run. The description should describe the consequence of the bug. The environment is special in the sense that it is performed on nfs.\n\nExample: `repro: doesn't detect input changes`\n--&gt;\n\n## Description\n\nWhen using the `exp run --run-all` feature, the command can never finish due to a error deleting a git folder, even though that folder seems to be empty.\n\n### Reproduce\n\n1. `dvc exp run --queue -S training.loss.name=mse`\n2. `dvc exp run --run-all -j1 --verbose`\n\n### Expected\n\nI expect the same output as running `dvc exp run -S training.loss.name=mse`, being a successful experiment with proper metrics shown in `dvc exp show`. Instead, when using the queueing functionality, the command errors and the metrics are not properly saved. The JSON representation of `dvc exp show` also states the experiment is still queued.\n\n### Environment information\n\n&lt;!--\nThis is required to ensure that we can reproduce the bug.\n--&gt;\n\n**Output of `dvc doctor`:**\n\n```console\n$ dvc doctor\nDVC version: 2.0.6 (pip)\n---------------------------------\nPlatform: Python 3.8.5 on Linux-5.4.0-60-generic-x86_64-with-glibc2.10\nSupports: hdfs, http, https\nCache types: hardlink, symlink\nCache directory: nfs4 on [REDACTED]\nCaches: local\nRemotes: None\nWorkspace directory: nfs4 on [REDACTED]\nRepo: dvc, git\n```\n\n**Additional Information (if any):**\n\nPlease note that at the end of this, it states the .git\/object\/pack directory is not empty, even though it can be deleted using rmdir without issue.\n\n```\n2021-03-17 12:02:49,159 DEBUG: state save (152491799, 1615977912613432576, 22244) 0b8958b3e558a41b046785ece234cbee\n2021-03-17 12:02:49,164 DEBUG: state save (152491799, 1615977912613432576, 22244) 0b8958b3e558a41b046785ece234cbee\n2021-03-17 12:02:49,179 DEBUG: state save (152487234, 1615977912709433600, 6498) 6f3ff4ce2d56a0a19e27ed2ebce075fa\n2021-03-17 12:02:49,183 DEBUG: state save (152487234, 1615977912709433600, 6498) 6f3ff4ce2d56a0a19e27ed2ebce075fa\n2021-03-17 12:02:49,226 DEBUG: state save (152547862, 1615978823436614400, 293776) 343f60bca0df1ae495b598bbce29b384\n2021-03-17 12:02:49,231 DEBUG: state save (152547862, 1615978823436614400, 293776) 343f60bca0df1ae495b598bbce29b384\n2021-03-17 12:02:49,236 DEBUG: {'MS_data\/model_weights.h5': 'modified'}\n2021-03-17 12:02:49,260 DEBUG: state save (152544875, 1615978968985396480, 240801) 474d305f96b1fab25155390c8e7d2e2c\n2021-03-17 12:02:49,264 DEBUG: state save (152544875, 1615978968985396480, 240801) 474d305f96b1fab25155390c8e7d2e2c\n2021-03-17 12:02:49,270 DEBUG: {'notebook_results\/Train.ipynb': 'modified'}\n2021-03-17 12:02:49,287 DEBUG: state save (152544214, 1615978823200606208, 244) c0fda638e8a2cc6192b86dab818e8954\n2021-03-17 12:02:49,292 DEBUG: state save (152544214, 1615978823200606208, 244) c0fda638e8a2cc6192b86dab818e8954\n2021-03-17 12:02:49,297 DEBUG: {'metrics.csv': 'modified'}\n2021-03-17 12:02:49,315 DEBUG: state save (152549822, 1615978930064190208, 126) 66eb73c07b00069143c48a321af0abd7\n2021-03-17 12:02:49,319 DEBUG: state save (152549822, 1615978930064190208, 126) 66eb73c07b00069143c48a321af0abd7\n2021-03-17 12:02:49,325 DEBUG: {'train_metrics.json': 'modified'}\n2021-03-17 12:02:49,344 DEBUG: state save (152549303, 1615978965597293312, 126) b8d9d9ec45c24e1f65474a88d22e1b37\n2021-03-17 12:02:49,348 DEBUG: state save (152549303, 1615978965597293312, 126) b8d9d9ec45c24e1f65474a88d22e1b37\n2021-03-17 12:02:49,353 DEBUG: {'val_metrics.json': 'modified'}\n2021-03-17 12:02:49,361 DEBUG: Computed stage: 'train_model' md5: '5ca694409ddab75eb4b0ff0deddbef02'\n2021-03-17 12:02:49,453 DEBUG: state save (152547862, 1615978823436614400, 293776) 343f60bca0df1ae495b598bbce29b384\n2021-03-17 12:02:49,459 DEBUG: Checking out 'MS_data\/model_weights.h5' with cache 'object md5: 343f60bca0df1ae495b598bbce29b384'.\n2021-03-17 12:02:49,474 DEBUG: Created 'copy': ..\/..\/..\/cache\/34\/3f60bca0df1ae495b598bbce29b384 -&gt; MS_data\/model_weights.h5                    \n2021-03-17 12:02:49,475 DEBUG: state save (152550395, 1615978969461410816, 293776) 343f60bca0df1ae495b598bbce29b384\n2021-03-17 12:02:49,544 DEBUG: state save (152544875, 1615978968985396480, 240801) 474d305f96b1fab25155390c8e7d2e2c\n2021-03-17 12:02:49,550 DEBUG: Checking out 'notebook_results\/Train.ipynb' with cache 'object md5: 474d305f96b1fab25155390c8e7d2e2c'.\n2021-03-17 12:02:49,563 DEBUG: Created 'copy': ..\/..\/..\/cache\/47\/4d305f96b1fab25155390c8e7d2e2c -&gt; notebook_results\/Train.ipynb                \n2021-03-17 12:02:49,563 DEBUG: state save (152549836, 1615978969549413376, 240801) 474d305f96b1fab25155390c8e7d2e2c\n2021-03-17 12:02:49,658 DEBUG: state save (152544214, 1615978823200606208, 244) c0fda638e8a2cc6192b86dab818e8954\n2021-03-17 12:02:49,664 DEBUG: Checking out 'metrics.csv' with cache 'object md5: c0fda638e8a2cc6192b86dab818e8954'.\n2021-03-17 12:02:49,670 DEBUG: Created 'copy': ..\/..\/..\/cache\/c0\/fda638e8a2cc6192b86dab818e8954 -&gt; metrics.csv                                 \n2021-03-17 12:02:49,671 DEBUG: state save (152550397, 1615978969657416704, 244) c0fda638e8a2cc6192b86dab818e8954\n2021-03-17 12:02:49,739 DEBUG: state save (152549822, 1615978930064190208, 126) 66eb73c07b00069143c48a321af0abd7\n2021-03-17 12:02:49,745 DEBUG: Checking out 'train_metrics.json' with cache 'object md5: 66eb73c07b00069143c48a321af0abd7'.\n2021-03-17 12:02:49,752 DEBUG: Created 'copy': ..\/..\/..\/cache\/66\/eb73c07b00069143c48a321af0abd7 -&gt; train_metrics.json                          \n2021-03-17 12:02:49,753 DEBUG: state save (152548593, 1615978969741419264, 126) 66eb73c07b00069143c48a321af0abd7\n2021-03-17 12:02:49,799 DEBUG: state save (152549303, 1615978965597293312, 126) b8d9d9ec45c24e1f65474a88d22e1b37\n2021-03-17 12:02:49,805 DEBUG: Checking out 'val_metrics.json' with cache 'object md5: b8d9d9ec45c24e1f65474a88d22e1b37'.\n2021-03-17 12:02:49,811 DEBUG: Created 'copy': ..\/..\/..\/cache\/b8\/d9d9ec45c24e1f65474a88d22e1b37 -&gt; val_metrics.json                            \n2021-03-17 12:02:49,812 DEBUG: state save (152549843, 1615978969801421056, 126) b8d9d9ec45c24e1f65474a88d22e1b37\n2021-03-17 12:02:49,855 DEBUG: stage: 'train_model' was reproduced\nUpdating lock file 'dvc.lock'\n2021-03-17 12:02:50,296 DEBUG: Commit to new experiment branch 'refs\/exps\/25\/6b1cdc9d81d8f2feecd7b4538d526a8ddd28fc\/exp-293c9'\n2021-03-17 12:02:51,133 DEBUG: Collected experiment 'df26527'.                                                                                 \n2021-03-17 12:02:51,135 DEBUG: Removing tmpdir '&lt;ExpTemporaryDirectory '\/home\/sumo\/lwerthen\/ms\/.dvc\/tmp\/exps\/tmpqnsuwjdk'&gt;'\n2021-03-17 12:02:51,135 DEBUG: Removing '\/home\/sumo\/lwerthen\/ms\/.dvc\/tmp\/exps\/tmpqnsuwjdk'\n2021-03-17 12:02:51,319 ERROR: unexpected error - [Errno 39] Directory not empty: '\/home\/sumo\/lwerthen\/ms\/.dvc\/tmp\/exps\/tmpqnsuwjdk\/.git\/objects\/pack'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/shutil.py\", line 654, in _rmtree_safe_fd\n    os.rmdir(entry.name, dir_fd=topfd)\nOSError: [Errno 39] Directory not empty: 'pack'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/site-packages\/dvc\/main.py\", line 50, in main\n    ret = cmd.run()\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/site-packages\/dvc\/command\/experiments.py\", line 525, in run\n    results = self.repo.experiments.run(\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/site-packages\/dvc\/repo\/experiments\/__init__.py\", line 815, in run\n    return run(self.repo, *args, **kwargs)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py\", line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/site-packages\/dvc\/repo\/experiments\/run.py\", line 28, in run\n    return repo.experiments.reproduce_queued(jobs=jobs)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/site-packages\/dvc\/repo\/experiments\/__init__.py\", line 376, in reproduce_queued\n    results = self._reproduce_revs(**kwargs)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/site-packages\/dvc\/repo\/experiments\/__init__.py\", line 46, in wrapper\n    return f(exp, *args, **kwargs)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/site-packages\/dvc\/repo\/experiments\/__init__.py\", line 522, in _reproduce_revs\n    exec_results = self._executors_repro(executors, **kwargs)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/site-packages\/dvc\/repo\/experiments\/__init__.py\", line 648, in _executors_repro\n    executor.cleanup()\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/site-packages\/dvc\/repo\/experiments\/executor\/local.py\", line 74, in cleanup\n    self._tmp_dir.cleanup()\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/site-packages\/dvc\/repo\/experiments\/executor\/local.py\", line 26, in cleanup\n    self._rmtree(self.name)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/site-packages\/dvc\/repo\/experiments\/executor\/local.py\", line 22, in _rmtree\n    remove(name)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/site-packages\/dvc\/utils\/fs.py\", line 135, in remove\n    shutil.rmtree(path, onerror=_chmod)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/shutil.py\", line 715, in rmtree\n    _rmtree_safe_fd(fd, path, onerror)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/shutil.py\", line 652, in _rmtree_safe_fd\n    _rmtree_safe_fd(dirfd, fullname, onerror)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/shutil.py\", line 652, in _rmtree_safe_fd\n    _rmtree_safe_fd(dirfd, fullname, onerror)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/shutil.py\", line 656, in _rmtree_safe_fd\n    onerror(os.rmdir, fullname, sys.exc_info())\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/site-packages\/dvc\/utils\/fs.py\", line 120, in _chmod\n    func(p)\nOSError: [Errno 39] Directory not empty: '\/home\/sumo\/lwerthen\/ms\/.dvc\/tmp\/exps\/tmpqnsuwjdk\/.git\/objects\/pack'\n------------------------------------------------------------\n2021-03-17 12:02:52,023 DEBUG: Version info for developers:\nDVC version: 2.0.6 (pip)\n---------------------------------\nPlatform: Python 3.8.5 on Linux-5.4.0-60-generic-x86_64-with-glibc2.10\nSupports: hdfs, http, https\nCache types: hardlink, symlink\nCache directory: nfs4 on [REDACTED]\nCaches: local\nRemotes: None\nWorkspace directory: nfs4 on [REDACTED]\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2021-03-17 12:02:52,033 DEBUG: Removing '\/home\/sumo\/lwerthen\/ms\/.dvc\/tmp\/exps\/tmpssfyvyik'\nException ignored in: &lt;finalize object at 0x7f8865619de0; dead&gt;\nTraceback (most recent call last):\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/weakref.py\", line 566, in __call__\n    return info.func(*info.args, **(info.kwargs or {}))\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/tempfile.py\", line 818, in _cleanup\n    cls._rmtree(name)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/site-packages\/dvc\/repo\/experiments\/executor\/local.py\", line 22, in _rmtree\n    remove(name)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/site-packages\/dvc\/utils\/fs.py\", line 135, in remove\n    shutil.rmtree(path, onerror=_chmod)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/shutil.py\", line 715, in rmtree\n    _rmtree_safe_fd(fd, path, onerror)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/shutil.py\", line 652, in _rmtree_safe_fd\n    _rmtree_safe_fd(dirfd, fullname, onerror)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/shutil.py\", line 652, in _rmtree_safe_fd\n    _rmtree_safe_fd(dirfd, fullname, onerror)\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/shutil.py\", line 656, in _rmtree_safe_fd\n    onerror(os.rmdir, fullname, sys.exc_info())\n  File \"\/home\/sumo\/lwerthen\/miniconda3\/envs\/tf2.4\/lib\/python3.8\/site-packages\/dvc\/utils\/fs.py\", line 120, in _chmod\n    func(p)\nOSError: [Errno 39] Directory not empty: '\/home\/sumo\/lwerthen\/ms\/.dvc\/tmp\/exps\/tmpssfyvyik\/.git\/objects\/pack'\n2021-03-17 12:02:54,322 DEBUG: Analytics is enabled.\n2021-03-17 12:02:54,501 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmpyel92az0']'\n2021-03-17 12:02:54,505 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmpyel92az0']'\n(tf2.4) lwerthen@sumo-ai:~\/ms$ ls -al \/home\/sumo\/lwerthen\/ms\/.dvc\/tmp\/exps\/tmpssfyvyik\/.git\/objects\/pack\ntotal 33\ndrwxr-xr-x  2 lwerthen sumo  2 M\u00e4r 17 12:02 .\ndrwxr-xr-x 31 lwerthen sumo 31 M\u00e4r 17 12:02 ..\n(tf2.4) lwerthen@sumo-ai:~\/ms$ rmdir \/home\/sumo\/lwerthen\/ms\/.dvc\/tmp\/exps\/tmpssfyvyik\/.git\/objects\/pack\n(tf2.4) lwerthen@sumo-ai:~\/ms$ echo $?\n0\n```<\/span><\/p>\n  <\/div>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>Thanks<\/p>",
                "Answer_score":21.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"multipl run parallel hello try run multipl experi simultan gpu cluster wai cluster work send bash script cluster run job turn bash script configur cuda visibl devic activ conda environ run experi exp run temp submit job chang want experi set variabl param yaml try send job wait start run chang send second job fail error error unabl acquir lock like process run termin abruptli check page possibl reason learn resolv hope temp allow run job time isol project separ directori thing note cach dir separ directori outsid project directori allow us space user directori question relat http discuss org lock error parallel repro suggest cours us queue run experi parallel us built parallel experi execut run experi separ job submiss cluster intend workflow sort situat thank",
        "Question_preprocessed_content":"multipl run parallel hello try run multipl experi simultan gpu cluster wai cluster work send bash script cluster run job turn bash script configur activ conda environ run experi submit job chang want experi set variabl try send job wait start run chang send second job fail error hope allow run job time isol project separ directori thing note cach dir separ directori outsid project directori allow us space user directori question relat suggest cours us run experi parallel us built parallel experi execut run experi separ job submiss cluster intend workflow sort situat thank",
        "Question_gpt_summary_original":"The user is trying to run multiple dvc experiments simultaneously on a gpu cluster by sending a bash script to the cluster. However, the second job fails with an error message indicating that another DVC process is running or was terminated abruptly. The user had hoped that using \"--temp\" would allow them to run these jobs at the same time because it isolates the project in a separate directory. The user's cache dir is in a separate directory outside their project directory because they are not allowed to use that much space in their user directory. The user is seeking advice on how to use the built-in parallel experiment execution but run the experiments in separate job submissions to the cluster or what the intended workflow for this situation is.",
        "Question_gpt_summary":"user try run multipl experi simultan gpu cluster send bash script cluster second job fail error messag indic process run termin abruptli user hope temp allow run job time isol project separ directori user cach dir separ directori outsid project directori allow us space user directori user seek advic us built parallel experi execut run experi separ job submiss cluster intend workflow situat",
        "Answer_original_content":"dont recommend workflow situat thought provid suggest approach situat wai provid gpu param exampl schedul number experi avail gpu provid differ gpu requir legwork experi gpu schedul experi train schedul experi train experi run altern approach control train server figur wai automat reserv gpu wai us common directori file insid indic gpu reserv free exampl follow code import lockfil contextlib import contextmanag import import time concurr futur import threadpoolexecutor gpu id lock dir lock makedir lock dir exist true def lock gpu path path join lock dir str gpu return lockfil lockfil path contextmanag def lock free gpu gpu gpu id try lock gpu yield gpu close break lockfil lockerror print lock gpu try pass def train task task number lock free gpu gpu print work task task number gpu gpu simul differ time execut time sleep gpu threadpoolexecutor max worker executor rang executor submit train task reserv gpu lock appropri file path equal lock gpu run know us approach make sens server user us lock context manag need sure run exp job descript exp run run featur command finish error delet git folder folder reproduc exp run queue train loss mse exp run run verbos expect expect output run exp run train loss mse success experi proper metric shown exp instead queue function command error metric properli save json represent exp state experi queu environ inform output doctor consol doctor version pip platform python linux gener glibc support hdf http http cach type hardlink symlink cach directori nf redact cach local remot workspac directori nf redact repo git addit inform note end state git object pack directori delet rmdir issu debug state save bbeabececbe debug state save bbeabececbe debug state save fffcedaaeedebcefa debug state save fffcedaaeedebcefa debug state save fbcadfaebbbceb debug state save fbcadfaebbbceb debug data model weight modifi debug state save dfbfabcedec debug state save dfbfabcedec debug notebook result train ipynb modifi debug state save cfdaeaccbdab debug state save cfdaeaccbdab debug metric csv modifi debug state save ebcbcaafabd debug state save ebcbcaafabd debug train metric json modifi debug state save bddeccefadeb debug state save bddeccefadeb debug val metric json modifi debug comput stage train model caddabebbffdeddbef debug state save fbcadfaebbbceb debug check data model weight cach object fbcadfaebbbceb debug creat copi cach fbcadfaebbbceb data model weight debug state save fbcadfaebbbceb debug state save dfbfabcedec debug check notebook result train ipynb cach object dfbfabcedec debug creat copi cach dfbfabcedec notebook result train ipynb debug state save dfbfabcedec debug state save cfdaeaccbdab debug check metric csv cach object cfdaeaccbdab debug creat copi cach fdaeaccbdab metric csv debug state save cfdaeaccbdab debug state save ebcbcaafabd debug check train metric json cach object ebcbcaafabd debug creat copi cach ebcbcaafabd train metric json debug state save ebcbcaafabd debug state save bddeccefadeb debug check val metric json cach object bddeccefadeb debug creat copi cach ddeccefadeb val metric json debug state save bddeccefadeb debug stage train model reproduc updat lock file lock debug commit new experi branch ref exp bcdcddffeecdbdafc exp debug collect experi debug remov tmpdir debug remov home sumo lwerthen tmp exp tmpqnsuwjdk error unexpect error errno directori home sumo lwerthen tmp exp tmpqnsuwjdk git object pack traceback recent file home sumo lwerthen miniconda env lib python shutil line rmtree safe rmdir entri dir topfd oserror errno directori pack handl except except occur traceback recent file home sumo lwerthen miniconda env lib python site packag main line main ret cmd run file home sumo lwerthen miniconda env lib python site packag command experi line run result self repo experi run file home sumo lwerthen miniconda env lib python site packag repo experi init line run return run self repo arg kwarg file home sumo lwerthen miniconda env lib python site packag repo init line wrapper return repo arg kwarg file home sumo lwerthen miniconda env lib python site packag repo experi run line run return repo experi reproduc queu job job file home sumo lwerthen miniconda env lib python site packag repo experi init line reproduc queu result self reproduc rev kwarg file home sumo lwerthen miniconda env lib python site packag repo experi init line wrapper return exp arg kwarg file home sumo lwerthen miniconda env lib python site packag repo experi init line reproduc rev exec result self executor repro executor kwarg file home sumo lwerthen miniconda env lib python site packag repo experi init line executor repro executor cleanup file home sumo lwerthen miniconda env lib python site packag repo experi executor local line cleanup self tmp dir cleanup file home sumo lwerthen miniconda env lib python site packag repo experi executor local line cleanup self rmtree self file home sumo lwerthen miniconda env lib python site packag repo experi executor local line rmtree remov file home sumo lwerthen miniconda env lib python site packag util line remov shutil rmtree path onerror chmod file home sumo lwerthen miniconda env lib python shutil line rmtree rmtree safe path onerror file home sumo lwerthen miniconda env lib python shutil line rmtree safe rmtree safe dirfd fullnam onerror file home sumo lwerthen miniconda env lib python shutil line rmtree safe rmtree safe dirfd fullnam onerror file home sumo lwerthen miniconda env lib python shutil line rmtree safe onerror rmdir fullnam sy exc info file home sumo lwerthen miniconda env lib python site packag util line chmod func oserror errno directori home sumo lwerthen tmp exp tmpqnsuwjdk git object pack debug version info develop version pip platform python linux gener glibc support hdf http http cach type hardlink symlink cach directori nf redact cach local remot workspac directori nf redact repo git have troubl hit http org support happi help debug remov home sumo lwerthen tmp exp tmpssfyvyik except ignor traceback recent file home sumo lwerthen miniconda env lib python weakref line return info func info arg info kwarg file home sumo lwerthen miniconda env lib python tempfil line cleanup cl rmtree file home sumo lwerthen miniconda env lib python site packag repo experi executor local line rmtree remov file home sumo lwerthen miniconda env lib python site packag util line remov shutil rmtree path onerror chmod file home sumo lwerthen miniconda env lib python shutil line rmtree rmtree safe path onerror file home sumo lwerthen miniconda env lib python shutil line rmtree safe rmtree safe dirfd fullnam onerror file home sumo lwerthen miniconda env lib python shutil line rmtree safe rmtree safe dirfd fullnam onerror file home sumo lwerthen miniconda env lib python shutil line rmtree safe onerror rmdir fullnam sy exc info file home sumo lwerthen miniconda env lib python site packag util line chmod func oserror errno directori home sumo lwerthen tmp exp tmpssfyvyik git object pack debug analyt enabl debug try spawn daemon analyt tmp tmpyelaz debug spawn daemon analyt tmp tmpyelaz lwerthen sumo home sumo lwerthen tmp exp tmpssfyvyik git object pack total drwxr lwerthen sumo drwxr lwerthen sumo lwerthen sumo rmdir home sumo lwerthen tmp exp tmpssfyvyik git object pack lwerthen sumo echo thank",
        "Answer_preprocessed_content":"dont recommend workflow situat thought provid suggest approach situat wai provid gpu param exampl schedul number experi avail gpu provid differ requir legwork experi gpu schedul experi train schedul experi train experi run altern approach control train server figur wai automat reserv gpu wai us common directori file insid indic gpu reserv free exampl follow code reserv gpu lock appropri file path equal run know us approach make sens server user us lock context manag need sure run descript featur command finish error delet git folder folder reproduc expect expect output run success experi proper metric shown instead queue function command error metric properli save json represent state experi queu environ inform requir ensur reproduc bug output addit inform note end state directori delet rmdir issu thank",
        "Answer_gpt_summary_original":"possible solutions to the challenge of encountering an error when attempting to submit a second job due to a lock issue while running multiple experiments simultaneously on a gpu cluster are: \n1. provide gpu via params and schedule only the number of experiments that you have available gpus, and for each one provide a different gpu id. \n2. figure out a way to automatically reserve the gpu by using some common directory and files inside as an indicator which gpus are reserved and which are free. \n3. queue few experiments and run them simultaneously to take advantage of parallel execution. \n4. make multiple copies of your repo and run one experiment per copy.",
        "Answer_gpt_summary":"possibl solut challeng encount error attempt submit second job lock issu run multipl experi simultan gpu cluster provid gpu param schedul number experi avail gpu provid differ gpu figur wai automat reserv gpu common directori file insid indic gpu reserv free queue experi run simultan advantag parallel execut multipl copi repo run experi copi"
    },
    {
        "Question_id":null,
        "Question_title":"Wandb login issue on git bash",
        "Question_body":"<p>In the login process, this error occurs. It  says install \u201cMkl-service\u201d but the service has already installed and I had tried to fix this error but I was unable to do so<\/p>\n<p>$ wandb login<br>\nc:\\users\\great\\anaconda3\\lib\\site-packages\\numpy_<em>init<\/em>_.py:143: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see <a href=\"http:\/\/github.com\/IntelPython\/mkl-service\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - IntelPython\/mkl-service: Python hooks for Intel(R) Math Kernel Library runtime control settings.<\/a><br>\nfrom . import _distributor_init<br>\nwandb: Appending key for <a href=\"http:\/\/api.wandb.ai\">api.wandb.ai<\/a> to your netrc file: C:\\Users\\great\/.netrc<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1646203887453,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":420.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-login-issue-on-git-bash\/2000",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-22T16:17:17.405Z",
                "Answer_body":"<p>Hi Kushagra,<\/p>\n<p>Thank you for writing in! Can you double check for me that mkl-service  is properly installed? You can verify whether packages are installed by using <code>pip list<\/code> in the terminal<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-25T17:54:59.839Z",
                "Answer_body":"<p>Hi Kushagra,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-28T18:08:11.610Z",
                "Answer_body":"<p>Hi Kushagra, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-01T06:52:05.618Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"login issu git bash login process error occur sai instal mkl servic servic instal tri fix error unabl login user great anaconda lib site packag numpi init userwarn mkl servic packag fail import intel mkl initi ensur correct box oper condit gnu openmp load python process assur instal mkl servic packag github intelpython mkl servic python hook intel math kernel librari runtim control set import distributor init append kei api netrc file user great netrc",
        "Question_preprocessed_content":"login issu git bash login process error occur sai instal servic instal tri fix error unabl login userwarn packag fail import intel mkl initi ensur correct box oper condit gnu openmp load python process assur instal packag github python hook intel math kernel librari runtim control set import append kei netrc file",
        "Question_gpt_summary_original":"The user is encountering an error while trying to log in to Wandb on Git Bash. The error message suggests installing \"Mkl-service,\" but the user has already installed it and is unable to fix the error.",
        "Question_gpt_summary":"user encount error try log git bash error messag suggest instal mkl servic user instal unabl fix error",
        "Answer_original_content":"kushagra thank write doubl check mkl servic properli instal verifi packag instal pip list termin kushagra want follow support request heard let know assist issu resolv kushagra heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"kushagra thank write doubl check properli instal verifi packag instal termin kushagra want follow support request heard let know assist issu resolv kushagra heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the answer does not provide any solutions to the login issue on git bash. it only asks the user to double-check if the \"mkl-service\" package is properly installed and informs them that the support request will be closed if there is no response.",
        "Answer_gpt_summary":"answer provid solut login issu git bash ask user doubl check mkl servic packag properli instal inform support request close respons"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning Studio Designer (preview) - Selective module execution",
        "Question_body":"I find that I am often inserting or modifying one module in a flow, and needing to edit several later items in the DAG,\n\nBut since the prior module is now invalidated, It asks me to SUBMIT and run the full experiment, so that I can do things like select columns.\n\nIs there not a way to disable modules? or selectively execute just a subset of modules?\n\nI am using the preview versions of the ML studio.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1593464903017,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/41135\/azure-machine-learning-studio-designer-preview-sel.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-01T11:32:32.75Z",
                "Answer_score":0,
                "Answer_body":"@StevenGutfreund-9039 Thanks for the feedback.Currently We don't support selected node run yet in Designer. BTW pipeline has the capability to use previous run result if the input data hasn't changed. You can confirm whether a step is reused or not from the recycle icon as shown below:\n\n\n\n\nPlease share your feedback from the ml.azure.com portal to prioritize this feature.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":2.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"studio design preview select modul execut insert modifi modul flow need edit later item dag prior modul invalid ask submit run experi thing like select column wai disabl modul select execut subset modul preview version studio",
        "Question_preprocessed_content":"studio design select modul execut insert modifi modul flow need edit later item dag prior modul invalid ask submit run experi thing like select column wai disabl modul select execut subset modul preview version studio",
        "Question_gpt_summary_original":"The user is facing challenges while using the Azure Machine Learning Studio Designer (preview) as they are unable to disable modules or selectively execute a subset of modules. They often need to edit several later items in the DAG, but since the prior module is invalidated, they are asked to submit and run the full experiment, which is time-consuming.",
        "Question_gpt_summary":"user face challeng studio design preview unabl disabl modul select execut subset modul need edit later item dag prior modul invalid ask submit run experi time consum",
        "Answer_original_content":"stevengutfreund thank feedback current support select node run design btw pipelin capabl us previou run result input data hasn chang confirm step reus recycl icon shown share feedback azur com portal priorit featur",
        "Answer_preprocessed_content":"thank support select node run design btw pipelin capabl us previou run result input data hasn chang confirm step reus recycl icon shown share feedback portal priorit featur",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n- currently, selective execution of a subset of modules in a flow is not supported in designer.\n- pipeline has the capability to use previous run result if the input data hasn't changed.\n- the user can confirm whether a step is reused or not from the recycle icon.\n- the user can share their feedback from the ml.azure.com portal to prioritize the feature of selective execution of a subset of modules in a flow.",
        "Answer_gpt_summary":"possibl solut answer current select execut subset modul flow support design pipelin capabl us previou run result input data hasn chang user confirm step reus recycl icon user share feedback azur com portal priorit featur select execut subset modul flow"
    },
    {
        "Question_id":null,
        "Question_title":"Delete remote artifacts",
        "Question_body":"<p>Hi,<\/p>\n<p>is there a DVC command to delete all remote artifacts referenced by a <code>.dvc<\/code> file without deleting the  <code>.dvc<\/code> file? My understanding is that the (complex) <code>dvc gc<\/code> command offers means to clean unused artifacts but I struggle to find a way to simply remove pushed artifacts from a remote.<br>\nThis question relates to <a href=\"https:\/\/discuss.dvc.org\/t\/zero-byte-files-created-in-artifactory-remote-http-based\/1271\">my other post<\/a> where I tried to delete corrupted zero-byte remote artifacts uploaded by a <code>dvc push<\/code> command.<\/p>\n<p>Thank you for your help!<\/p>\n<p>Best,<br>\nJoe<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659001950703,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":96.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/delete-remote-artifacts\/1272",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-29T10:50:40.645Z",
                "Answer_body":"<p>Hi, Jo. There\u2019s no universal way in DVC to support deletion.<br>\nWe suggest using remote-specific tools for these purposes, eg: <code>gsutil<\/code>\/<code>aws<\/code>\/<code>curl<\/code>, etc, as it\u2019s beyond the scope of DVC, which is a content-addressable storage.<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"delet remot artifact command delet remot artifact referenc file delet file understand complex command offer mean clean unus artifact struggl wai simpli remov push artifact remot question relat post tri delet corrupt zero byte remot artifact upload push command thank help best joe",
        "Question_preprocessed_content":"delet remot artifact command delet remot artifact referenc file delet file understand command offer mean clean unus artifact struggl wai simpli remov push artifact remot question relat post tri delet corrupt remot artifact upload command thank help best joe",
        "Question_gpt_summary_original":"The user is facing challenges in finding a DVC command to delete remote artifacts referenced by a .dvc file without deleting the .dvc file. The user is struggling to remove pushed artifacts from a remote and has encountered corrupted zero-byte remote artifacts uploaded by a dvc push command.",
        "Question_gpt_summary":"user face challeng find command delet remot artifact referenc file delet file user struggl remov push artifact remot encount corrupt zero byte remot artifact upload push command",
        "Answer_original_content":"there univers wai support delet suggest remot specif tool purpos gsutil aw curl scope content address storag",
        "Answer_preprocessed_content":"there univers wai support delet suggest tool purpos scope storag",
        "Answer_gpt_summary_original":"possible solutions to deleting pushed artifacts from a remote without deleting the .file. include using remote-specific tools such as gsutil, aws, or curl, as there is no universal way to support deletion. this is because the remote is a content-addressable storage and deleting pushed artifacts is beyond its scope.",
        "Answer_gpt_summary":"possibl solut delet push artifact remot delet file includ remot specif tool gsutil aw curl univers wai support delet remot content address storag delet push artifact scope"
    },
    {
        "Question_id":null,
        "Question_title":"Can I add OpenAPI specification to a webservice deployed with AzureML in AKS?",
        "Question_body":"I'd like to deploy a machine learning service using AzureML on AKS. I also need to add some OpenAPI specification for it.\n\nFeatures in https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python are neat, but that of having API docs\/swagger for the webservice seems missing.\n\nHaving some documentation is useful especially if the model takes in input several features of different type.\n\nTo overcome this, I currently get models trained in AzureML and include them in Docker containers that use the python FastAPI library to build the API and OpenAPI\/Swagger specs, and those are deployed on some host.\n\nCan I do something equivalent to this with AKS in AzureML instead? If so, how?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1600897231890,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@DavideFiocco-7346 The deployments of Azure ML provide a swagger specification URI that can be used directly. The documentation of this is available here. You can print your swagger_uri of the web service and check if it confirms with the specifications you are creating currently.\n\nIf the above response helps, please accept the response as answer. Thanks!!",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/105437\/can-i-add-openapi-specification-to-a-webservice-de.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-09-24T06:54:05.547Z",
                "Answer_score":2,
                "Answer_body":"@DavideFiocco-7346 The deployments of Azure ML provide a swagger specification URI that can be used directly. The documentation of this is available here. You can print your swagger_uri of the web service and check if it confirms with the specifications you are creating currently.\n\nIf the above response helps, please accept the response as answer. Thanks!!",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1600930445547,
        "Question_original_content":"add openapi specif webservic deploi ak like deploi machin learn servic ak need add openapi specif featur http doc microsoft com azur machin learn deploi azur kubernet servic tab python neat have api doc swagger webservic miss have document us especi model take input featur differ type overcom current model train includ docker contain us python fastapi librari build api openapi swagger spec deploi host equival ak instead",
        "Question_preprocessed_content":"add openapi specif webservic deploi ak like deploi machin learn servic ak need add openapi specif featur neat have api webservic miss have document us especi model take input featur differ type overcom current model train includ docker contain us python fastapi librari build api spec deploi host equival ak instead",
        "Question_gpt_summary_original":"The user is facing a challenge in adding OpenAPI specification to a machine learning service deployed using AzureML on AKS. The user finds the API documentation missing in the features provided by AzureML and is currently using Docker containers with FastAPI library to build the API and OpenAPI\/Swagger specs. The user is seeking guidance on how to add OpenAPI specification to the webservice deployed with AzureML in AKS.",
        "Question_gpt_summary":"user face challeng ad openapi specif machin learn servic deploi ak user find api document miss featur provid current docker contain fastapi librari build api openapi swagger spec user seek guidanc add openapi specif webservic deploi ak",
        "Answer_original_content":"davidefiocco deploy provid swagger specif uri directli document avail print swagger uri web servic check confirm specif creat current respons help accept respons answer thank",
        "Answer_preprocessed_content":"deploy provid swagger specif uri directli document avail print web servic check confirm specif creat current respons help accept respons answer thank",
        "Answer_gpt_summary_original":"possible solutions: \n- use the swagger specification uri provided by the aks deployment directly.\n- check if the swagger_uri of the web service confirms with the specifications being created.",
        "Answer_gpt_summary":"possibl solut us swagger specif uri provid ak deploy directli check swagger uri web servic confirm specif creat"
    },
    {
        "Question_id":63405080.0,
        "Question_title":"SageMaker in local Jupyter notebook: cannot use AWS hosted XGBoost container (\"KeyError: 'S3DistributionType'\" and \"Failed to run: ['docker-compose'\")",
        "Question_body":"<p>Running SageMaker within a local Jupyter notebook (using VS Code) works without issue, except that attempting to train an XGBoost model using the AWS hosted container results in errors (container name: <code>246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1-cpu-py3<\/code>).<\/p>\n<h2>Jupyter Notebook<\/h2>\n<pre class=\"lang-py prettyprint-override\"><code>import sagemaker\n\nsession = sagemaker.LocalSession()\n\n# Load and prepare the training and validation data\n...\n\n# Upload the training and validation data to S3\ntest_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\nval_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\ntrain_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n\nregion = session.boto_region_name\ninstance_type = 'ml.m4.xlarge'\ncontainer = sagemaker.image_uris.retrieve('xgboost', region, '1.0-1', 'py3', instance_type=instance_type)\n\nrole = 'arn:aws:iam::&lt;USER ID #&gt;:role\/service-role\/AmazonSageMaker-ExecutionRole-&lt;ROLE ID #&gt;'\n\nxgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n\nxgb_estimator.set_hyperparameters(max_depth=5, eta=0.2, gamma=4, min_child_weight=6,\n                                  subsample=0.8, objective='reg:squarederror', early_stopping_rounds=10,\n                                  num_round=200)\n\ns3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')\ns3_input_validation = sagemaker.inputs.TrainingInput(s3_data=val_location, content_type='csv')\n\nxgb_estimator.fit({'train': s3_input_train, 'validation': s3_input_validation})\n<\/code><\/pre>\n<h2>Docker Container KeyError<\/h2>\n<pre><code>algo-1-tfcvc_1  | ERROR:sagemaker-containers:Reporting training FAILURE\nalgo-1-tfcvc_1  | ERROR:sagemaker-containers:framework error: \nalgo-1-tfcvc_1  | Traceback (most recent call last):\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_containers\/_trainer.py&quot;, line 84, in train\nalgo-1-tfcvc_1  |     entrypoint()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 94, in main\nalgo-1-tfcvc_1  |     train(framework.training_env())\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 90, in train\nalgo-1-tfcvc_1  |     run_algorithm_mode()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 68, in run_algorithm_mode\nalgo-1-tfcvc_1  |     checkpoint_config=checkpoint_config\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/train.py&quot;, line 115, in sagemaker_train\nalgo-1-tfcvc_1  |     validated_data_config = channels.validate(data_config)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 106, in validate\nalgo-1-tfcvc_1  |     channel_obj.validate(value)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 52, in validate\nalgo-1-tfcvc_1  |     if (value[CONTENT_TYPE], value[TRAINING_INPUT_MODE], value[S3_DIST_TYPE]) not in self.supported:\nalgo-1-tfcvc_1  | KeyError: 'S3DistributionType'\n\n<\/code><\/pre>\n<h2>Local PC Runtime Error<\/h2>\n<pre><code>RuntimeError: Failed to run: ['docker-compose', '-f', '\/tmp\/tmp71tx0fop\/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1\n<\/code><\/pre>\n<p>If the Jupyter notebook is run using the Amazon cloud SageMaker environment (rather than on the local PC), there are no errors. Note that when running on the cloud notebook, the session is initialized as:<\/p>\n<pre><code>session = sagemaker.Session()\n<\/code><\/pre>\n<p>It appears that there is an issue with how the <code>LocalSession()<\/code> works with the hosted docker container.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597366468133,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1174.0,
        "Answer_body":"<p>When running SageMaker in a local Jupyter notebook, it expects the Docker container to be running on the local machine as well.<\/p>\n<p>The key to ensuring that SageMaker (running in a local notebook) uses the AWS hosted docker container, is to omit the <code>LocalSession<\/code> object when initializing the <code>Estimator<\/code>.<\/p>\n<h2>Wrong<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n<\/code><\/pre>\n<h2>Correct<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output')\n<\/code><\/pre>\n<p>\u00a0\u00a0<\/p>\n<h2>Additional info<\/h2>\n<p>The SageMaker Python SDK source code provides the following helpful hints:<\/p>\n<h1>File: <em>sagemaker\/local\/local_session.py<\/em><\/h1>\n<pre><code>class LocalSagemakerClient(object):\n    &quot;&quot;&quot;A SageMakerClient that implements the API calls locally.\n\n    Used for doing local training and hosting local endpoints. It still needs access to\n    a boto client to interact with S3 but it won't perform any SageMaker call.\n    ...\n<\/code><\/pre>\n<h1>File: <em>sagemaker\/estimator.py<\/em><\/h1>\n<pre><code>class EstimatorBase(with_metaclass(ABCMeta, object)):\n    &quot;&quot;&quot;Handle end-to-end Amazon SageMaker training and deployment tasks.\n\n    For introduction to model training and deployment, see\n    http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\n\n    Subclasses must define a way to determine what image to use for training,\n    what hyperparameters to use, and how to create an appropriate predictor instance.\n    &quot;&quot;&quot;\n\n    def __init__(self, role, train_instance_count, train_instance_type,\n                 train_volume_size=30, train_max_run=24 * 60 * 60, input_mode='File',\n                 output_path=None, output_kms_key=None, base_job_name=None, sagemaker_session=None, tags=None):\n        &quot;&quot;&quot;Initialize an ``EstimatorBase`` instance.\n\n        Args:\n            role (str): An AWS IAM role (either name or full ARN). ...\n            \n        ...\n\n            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with\n                Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one\n                using the default AWS configuration chain.\n        &quot;&quot;&quot;\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1597367070867,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63405080",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1597366468132,
        "Question_original_content":"local jupyt notebook us aw host xgboost contain keyerror sdistributiontyp fail run docker compos run local jupyt notebook code work issu attempt train xgboost model aw host contain result error contain dkr ecr west amazonaw com xgboost cpu jupyt notebook import session localsess load prepar train valid data upload train valid data test locat session upload data path join data dir test csv kei prefix prefix val locat session upload data path join data dir valid csv kei prefix prefix train locat session upload data path join data dir train csv kei prefix prefix region session boto region instanc type xlarg contain imag uri retriev xgboost region instanc type instanc type role arn aw iam role servic role amazon executionrol xgb estim estim estim contain role train instanc count train instanc type instanc type output path session default bucket prefix output session session xgb estim set hyperparamet max depth eta gamma min child weight subsampl object reg squarederror earli stop round num round input train input traininginput data train locat content type csv input valid input traininginput data val locat content type csv xgb estim fit train input train valid input valid docker contain keyerror algo tfcvc error contain report train failur algo tfcvc error contain framework error algo tfcvc traceback recent algo tfcvc file miniconda lib python site packag contain trainer line train algo tfcvc entrypoint algo tfcvc file miniconda lib python site packag xgboost contain train line main algo tfcvc train framework train env algo tfcvc file miniconda lib python site packag xgboost contain train line train algo tfcvc run algorithm mode algo tfcvc file miniconda lib python site packag xgboost contain train line run algorithm mode algo tfcvc checkpoint config checkpoint config algo tfcvc file miniconda lib python site packag xgboost contain algorithm mode train line train algo tfcvc valid data config channel valid data config algo tfcvc file miniconda lib python site packag algorithm toolkit channel valid line valid algo tfcvc channel obj valid valu algo tfcvc file miniconda lib python site packag algorithm toolkit channel valid line valid algo tfcvc valu content type valu train input mode valu dist type self support algo tfcvc keyerror sdistributiontyp local runtim error runtimeerror fail run docker compos tmp tmptxfop docker compos yaml build abort contain exit process exit code jupyt notebook run amazon cloud environ local error note run cloud notebook session initi session session appear issu localsess work host docker contain",
        "Question_preprocessed_content":"local jupyt notebook us aw host xgboost contain run local jupyt notebook work issu attempt train xgboost model aw host contain result error jupyt notebook docker contain keyerror local runtim error jupyt notebook run amazon cloud environ error note run cloud notebook session initi appear issu work host docker contain",
        "Question_gpt_summary_original":"The user is encountering challenges when attempting to train an XGBoost model using the AWS hosted container in a local Jupyter notebook. The errors include a KeyError related to S3DistributionType and a RuntimeError when attempting to run the docker container. The Jupyter notebook works without issue when run on the Amazon cloud SageMaker environment. The issue appears to be related to how the LocalSession() works with the hosted docker container.",
        "Question_gpt_summary":"user encount challeng attempt train xgboost model aw host contain local jupyt notebook error includ keyerror relat sdistributiontyp runtimeerror attempt run docker contain jupyt notebook work issu run amazon cloud environ issu appear relat localsess work host docker contain",
        "Answer_original_content":"run local jupyt notebook expect docker contain run local machin kei ensur run local notebook us aw host docker contain omit localsess object initi estim wrong xgb estim estim estim contain role train instanc count train instanc type instanc type output path session default bucket prefix output session session correct xgb estim estim estim contain role train instanc count train instanc type instanc type output path session default bucket prefix output addit info python sdk sourc code provid follow help hint file local local session class localcli object client implement api call local local train host local endpoint need access boto client interact won perform file estim class estimatorbas metaclass abcmeta object handl end end train deploy task introduct model train deploy http doc aw amazon com latest work train html subclass defin wai determin imag us train hyperparamet us creat appropri predictor instanc def init self role train instanc count train instanc type train volum size train max run input mode file output path output km kei base job session tag initi estimatorbas instanc arg role str aw iam role arn session session session session object manag interact api aw servic need specifi estim creat default aw configur chain",
        "Answer_preprocessed_content":"run local jupyt notebook expect docker contain run local machin kei ensur us aw host docker contain omit object initi wrong correct addit info python sdk sourc code provid follow help hint file file",
        "Answer_gpt_summary_original":"to resolve errors when attempting to train an xgboost model using the aws hosted container in a local jupyter notebook, the user should omit the localsession object when initializing the estimator. the correct code should be: \n\n```\nxgb_estimator = estimator.estimator(container, role, train_instance_count=1, train_instance_type=instance_type, output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output')\n```\n\nthe python sdk source code provides additional helpful hints for local training and deployment tasks.",
        "Answer_gpt_summary":"resolv error attempt train xgboost model aw host contain local jupyt notebook user omit localsess object initi estim correct code xgb estim estim estim contain role train instanc count train instanc type instanc type output path session default bucket prefix output python sdk sourc code provid addit help hint local train deploy task"
    },
    {
        "Question_id":null,
        "Question_title":"Wandb.watch with pytorch not logging anything",
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying to use <code>wandb.watch<\/code> for a pytorch model, unfortunately without success. I checked the documentation and these two threads:<\/p>\n<ul>\n<li>Wandb.watch not logging parameters<\/li>\n<li>When is one supposed to run wandb.watch so that weights and biases tracks params and gradients?<\/li>\n<\/ul>\n<p>But none of the suggested solutions solves my problem. I run in my environment the code from the colab notebook linked in <a href=\"https:\/\/community.wandb.ai\/t\/when-is-one-supposed-to-run-wandb-watch-so-that-weights-and-biases-tracks-params-and-gradients\/518\/3\">this post<\/a> (with <code>N, log_freq = 50, 2<\/code>) and still nothing is logged.<\/p>\n<p>Interestingly, if I set the <code>log_graph=True<\/code> there is a JSON file logged as a file, under <code>root \/ media \/ graph<\/code> in the files section. But I was expecting to get a result similar to <a href=\"https:\/\/wandb.ai\/ayush-thakur\/debug-neural-nets\/runs\/jh061uaf\/model\">this<\/a>.<\/p>\n<p>I am using wandb version 0.12.10.<\/p>\n<p>Kind regards,<br>\nMaciej<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1647450501888,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":1152.0,
        "Answer_body":"<p>Hi,<\/p>\n<p>Eureka! Everything was working correctly, but I always use <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a> with project view or run groups view. When I opened the run view both the graph and gradient were there <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>However, there is one problem remaining: <code>parameters<\/code>. When running the colab notebook code with <code>wandb.watch(d, log_freq=log_freq, log=\"all\")<\/code> I still can see only gradients in the run view.<\/p>\n<p><a href=\"https:\/\/wandb.ai\/dmml-heg\/uncategorized\/runs\/2qovzwq9\">Link to run page<\/a>  executed with wandb version 0.12.11 in Google Colab.<\/p>\n<p>EDIT: I found it <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> Code in the notebook was using <code>forward()<\/code> instead of <code>__call__()<\/code>. Forward hooks were not executed.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-watch-with-pytorch-not-logging-anything\/2096",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-17T15:45:11.862Z",
                "Answer_body":"<p>Hi Maciej,<\/p>\n<p>Thank you for writing in and for doing as much research on your own as you can for this issue.<\/p>\n<p>In the colab that you had mentioned with the changed parameters that you used, I as able to log parameters properly. Here\u2019s the run that I used with <code>N, log_freq = 50, 2<\/code>. Can you send me a link of the run page where you are experiencing this issue? Also, can you try rerunning this using CLI-0.12.11 and see if that is able to help?<\/p>\n<p>Warmly,<br>\nLeslie<\/p>",
                "Answer_score":148.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-18T08:58:43.663Z",
                "Answer_body":"<p>Hi,<\/p>\n<p>Eureka! Everything was working correctly, but I always use <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a> with project view or run groups view. When I opened the run view both the graph and gradient were there <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>However, there is one problem remaining: <code>parameters<\/code>. When running the colab notebook code with <code>wandb.watch(d, log_freq=log_freq, log=\"all\")<\/code> I still can see only gradients in the run view.<\/p>\n<p><a href=\"https:\/\/wandb.ai\/dmml-heg\/uncategorized\/runs\/2qovzwq9\">Link to run page<\/a>  executed with wandb version 0.12.11 in Google Colab.<\/p>\n<p>EDIT: I found it <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> Code in the notebook was using <code>forward()<\/code> instead of <code>__call__()<\/code>. Forward hooks were not executed.<\/p>",
                "Answer_score":153.6,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-03-21T15:20:24.555Z",
                "Answer_body":"<p>Yay! That\u2019s great! I\u2019m glad you were able to find out what was wrong with your script <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> I hope you have a beautiful week<\/p>",
                "Answer_score":23.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-17T08:58:44.618Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":13.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1647593923663,
        "Question_original_content":"watch pytorch log try us watch pytorch model unfortun success check document thread watch log paramet suppos run watch track param gradient suggest solut solv problem run environ code colab notebook link post log freq log interestingli set log graph true json file log file root media graph file section expect result similar version kind regard maciej",
        "Question_preprocessed_content":"watch pytorch log try us pytorch model unfortun success check document thread watch log paramet suppos run watch track param gradient suggest solut solv problem run environ code colab notebook link post log interestingli set json file log file file section expect result similar version kind regard maciej",
        "Question_gpt_summary_original":"The user is facing challenges with using wandb.watch for a pytorch model as nothing is being logged. The user has checked the documentation and other threads but none of the suggested solutions have worked. The user has tried running the code from a colab notebook but still nothing is being logged. However, if the user sets log_graph=True, a JSON file is logged as a file, but the user was expecting a result similar to a provided link. The user is using wandb version 0.12.10.",
        "Question_gpt_summary":"user face challeng watch pytorch model log user check document thread suggest solut work user tri run code colab notebook log user set log graph true json file log file user expect result similar provid link user version",
        "Answer_original_content":"eureka work correctli us project view run group view open run view graph gradient problem remain paramet run colab notebook code watch log freq log freq log gradient run view link run page execut version googl colab edit code notebook forward instead forward hook execut",
        "Answer_preprocessed_content":"eureka work correctli us project view run group view open run view graph gradient problem remain run colab notebook code gradient run view link run page execut version googl colab edit code notebook instead forward hook execut",
        "Answer_gpt_summary_original":"the solution to the problem of not being able to log anything using .watch for a pytorch model is to use the run view instead of the project view or run groups view. however, there is still an issue with only being able to see gradients and not parameters. the problem was found to be that the code in the notebook was using forward() instead of __call__(), which caused forward hooks to not be executed.",
        "Answer_gpt_summary":"solut problem abl log watch pytorch model us run view instead project view run group view issu abl gradient paramet problem code notebook forward instead caus forward hook execut"
    },
    {
        "Question_id":null,
        "Question_title":"Two y-axis chart plotting",
        "Question_body":"<p>Hello, I was wondering if it is possible to create a custom chart with two axis in y direction, such that the first one has a value scale and the other have another scale.<\/p>\n<p>Thank you very much in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1634080843431,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":376.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/two-y-axis-chart-plotting\/939",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-13T20:53:16.309Z",
                "Answer_body":"<p>Hi Hany,<\/p>\n<p>Our custom charts are visualized using Vega. In order to have to y-axis, use the \u2018orient\u2019 property and indicate both a left and a right to get a y-axis on both sides.<\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-13T21:52:00.114Z",
                "Answer_body":"<p>Hi Leslie,<\/p>\n<p>Great, thank you very much for the fast reply!!<\/p>",
                "Answer_score":2.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"axi chart plot hello wonder possibl creat custom chart axi direct valu scale scale thank advanc",
        "Question_preprocessed_content":"chart plot hello wonder possibl creat custom chart axi direct valu scale scale thank advanc",
        "Question_gpt_summary_original":"The user is facing a challenge in creating a custom chart with two y-axes, each with a different value scale.",
        "Question_gpt_summary":"user face challeng creat custom chart ax differ valu scale",
        "Answer_original_content":"hani custom chart visual vega order axi us orient properti indic left right axi side lesli great thank fast repli",
        "Answer_preprocessed_content":"hani custom chart visual vega order us orient properti indic left right side lesli great thank fast repli",
        "Answer_gpt_summary_original":"possible solutions: \n- use vega to create a custom chart with two y-axis. \n- use the \"orient\" property and indicate both a left and a right to get a y-axis on both sides.",
        "Answer_gpt_summary":"possibl solut us vega creat custom chart axi us orient properti indic left right axi side"
    },
    {
        "Question_id":59679192.0,
        "Question_title":"Hosting multiple models for multiple datasets in aws sagemaker",
        "Question_body":"<p>I read that there is a way to train and host multiple models using a single endpoint for a single dataset in AWS Sagemaker. But I have 2 different datasets in S3 and have to train a model for each dataset. Can these 2 different models be hosted using a single endpoint? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1578649761440,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":411.0,
        "Answer_body":"<p>Yes, this is called a multi-model endpoint. You can use a large number of models on the same endpoint. They get loaded and unloaded dynamically as needed, and you simply have to pass the model name in your prediction request.<\/p>\n\n<p>Here are some resources:<\/p>\n\n<ul>\n<li><p>Blog post + example : <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/<\/a><\/p><\/li>\n<li><p>Video explaining model deployment scenarios on SageMaker: <a href=\"https:\/\/youtu.be\/dT8jmdF-ZWw\" rel=\"nofollow noreferrer\">https:\/\/youtu.be\/dT8jmdF-ZWw<\/a><\/p><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59679192",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1578755333996,
        "Question_original_content":"host multipl model multipl dataset read wai train host multipl model singl endpoint singl dataset differ dataset train model dataset differ model host singl endpoint",
        "Question_preprocessed_content":"host multipl model multipl dataset read wai train host multipl model singl endpoint singl dataset differ dataset train model dataset differ model host singl endpoint",
        "Question_gpt_summary_original":"The user is facing a challenge of hosting multiple models for multiple datasets in AWS Sagemaker. They have two different datasets in S3 and need to train a model for each dataset. The user is unsure if it is possible to host these two different models using a single endpoint.",
        "Question_gpt_summary":"user face challeng host multipl model multipl dataset differ dataset need train model dataset user unsur possibl host differ model singl endpoint",
        "Answer_original_content":"ye call multi model endpoint us larg number model endpoint load unload dynam need simpli pass model predict request resourc blog post exampl http aw amazon com blog machin learn save infer cost amazon multi model endpoint video explain model deploy scenario http youtu dtjmdf zww",
        "Answer_preprocessed_content":"ye call endpoint us larg number model endpoint load unload dynam need simpli pass model predict request resourc blog post exampl video explain model deploy scenario",
        "Answer_gpt_summary_original":"possible solutions: \n\n- use a multi-model endpoint to host multiple models for multiple datasets in a single endpoint. \n- the models are loaded and unloaded dynamically as needed, and you can pass the model name in your prediction request. \n- check out the provided resources, including a blog post and example, as well as a video explaining model deployment scenarios.",
        "Answer_gpt_summary":"possibl solut us multi model endpoint host multipl model multipl dataset singl endpoint model load unload dynam need pass model predict request check provid resourc includ blog post exampl video explain model deploy scenario"
    },
    {
        "Question_id":61377643.0,
        "Question_title":"Tracking SageMaker Estimator with MLFlow",
        "Question_body":"<p>I'm working on a version tracking system for a ML project and want to use MLflow to do so. My project uses AWS Sagemaker's DeepAR for forecast.<\/p>\n\n<p>What I want to do is very simple. I'm trying do log the Sagemaker DeepAR model (Sagemaker Estimator) with MLFlow. As it doesn't have a \"log_model\" funcion in it's \"mlflow.sagemaker\" module, I tried to use the \"mlflow.pyfunc\" module to do the log. Unfortunatelly it didn't worked. How can I log the Sagemaker model and get the cloudpickle and yaml files generated by MLFlow?<\/p>\n\n<p>My code for now:<\/p>\n\n<p><code>mlflow.pyfunc.log_model(model)<\/code><\/p>\n\n<p>Where model is a sagemaker.estimator.Estimator object and the error I get from the code is<\/p>\n\n<p><code>mlflow.exceptions.MlflowException: Either `loader_module` or `python_model` must be specified. A `loader_module` should be a python module. A `python_model` should be a subclass of PythonModel<\/code><\/p>\n\n<p>I know AWS Sagemaker logs my models, but it is really important to my project to do the log with MLFlow too.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1587603987047,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1587605439876,
        "Question_score":0.0,
        "Question_view_count":437.0,
        "Answer_body":"<p>You cannot use pyfunc to store Any type object.<\/p>\n\n<p>You should either specify one of loader_module as shown in the example below or you must write the wrapper that implements PythonModel interface and provides logic to deserialize your model from  previously-stored artifacts as described here \n <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#example-saving-an-xgboost-model-in-mlflow-format\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/models.html#example-saving-an-xgboost-model-in-mlflow-format<\/a><\/p>\n\n<p>example with loader:<\/p>\n\n<pre><code>    model_uri = 'model.pkl'\n\n    with open(model_uri, 'wb') as f:\n        pickle.dump(model, f)\n\n    mlflow.log_artifact(model_uri, 'model')\n\n    mlflow.pyfunc.log_model(\n        'model', loader_module='mlflow.sklearn', data_path='model.pkl', code_path=['src'], conda_env='environment.yml'\n    )\n<\/code><\/pre>\n\n<p>I think PythonModel is the better way for you because of mlflow doesn't have a built-in loader for SageMaker DeepAR model.<\/p>\n\n<p>Nonetheless, You must have the knowledge how to restore SageMaker model from artifacts, because I am not sure that is possible at all, cuz of some built-in SageMaker algorithms are blackboxes.<\/p>\n\n<p>You can also may be interested in container that allow you to run any MLFlow projects inside Sagemaker: <a href=\"https:\/\/github.com\/odahu\/sagemaker-mlflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/odahu\/sagemaker-mlflow-container<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61377643",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1587720289803,
        "Question_original_content":"track estim work version track project want us project us deepar forecast want simpl try log deepar model estim log model funcion modul tri us pyfunc modul log unfortunatelli work log model cloudpickl yaml file gener code pyfunc log model model model estim estim object error code except except loader modul python model specifi loader modul python modul python model subclass pythonmodel know log model import project log",
        "Question_preprocessed_content":"track estim work version track project want us project us deepar forecast want simpl try log deepar model funcion modul tri us modul log unfortunatelli work log model cloudpickl yaml file gener code model object error code know log model import project log",
        "Question_gpt_summary_original":"The user is facing a challenge in logging the Sagemaker DeepAR model with MLFlow for a version tracking system in their ML project. They attempted to use the \"mlflow.pyfunc\" module but encountered an error message. The user is seeking guidance on how to log the Sagemaker model and generate the cloudpickle and yaml files with MLFlow.",
        "Question_gpt_summary":"user face challeng log deepar model version track project attempt us pyfunc modul encount error messag user seek guidanc log model gener cloudpickl yaml file",
        "Answer_original_content":"us pyfunc store type object specifi loader modul shown exampl write wrapper implement pythonmodel interfac provid logic deseri model previous store artifact describ http org doc latest model html exampl save xgboost model format exampl loader model uri model pkl open model uri pickl dump model log artifact model uri model pyfunc log model model loader modul sklearn data path model pkl code path src conda env environ yml think pythonmodel better wai built loader deepar model nonetheless knowledg restor model artifact sure possibl cuz built algorithm blackbox interest contain allow run project insid http github com odahu contain",
        "Answer_preprocessed_content":"us pyfunc store type object specifi shown exampl write wrapper implement pythonmodel interfac provid logic deseri model artifact describ exampl loader think pythonmodel better wai loader deepar model nonetheless knowledg restor model artifact sure possibl cuz algorithm blackbox interest contain allow run project insid",
        "Answer_gpt_summary_original":"the answer suggests two possible solutions for the user's challenge with tracking their estimator model using . the first solution is to use the \"loader_module\" parameter in the \".pyfunc.log_model\" function or write a wrapper that implements the \"pythonmodel\" interface to deserialize the model from previously-stored artifacts. the second solution is to use a container that allows running any  projects inside.",
        "Answer_gpt_summary":"answer suggest possibl solut user challeng track estim model solut us loader modul paramet pyfunc log model function write wrapper implement pythonmodel interfac deseri model previous store artifact second solut us contain allow run project insid"
    },
    {
        "Question_id":null,
        "Question_title":"Machine Learning",
        "Question_body":"i have problem with detecting the objects using the ML.NET and draw a boundary boxes\nand i cannot find an example or module that i can learn.",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1594208627597,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/44135\/machine-learning.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-14T19:13:54.527Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nObject detection is one of the classic problems in computer vision: Recognize what objects are inside a given image and also where they are in the image. For these cases, you can either use pre-trained models or train your own model to classify images specific to your custom domain. This sample uses a pre-trained model by default, but you can also add your own model exported from Custom Vision.\n\nI have a sample here for you:\n\nhttps:\/\/github.com\/dotnet\/machinelearning-samples\/tree\/master\/samples\/csharp\/end-to-end-apps\/ObjectDetection-Onnx\n\nThis sample consists of two separate apps:\n\nA WPF Core desktop app that renders a live-stream of the device's web cam, runs the video frames through an object detection model using ML.NET, and paints bounding boxes with labels indicating the objects detected in real-time.\nAn ASP.NET Core Web app that allows the user to upload or select an image. The Web app then runs the image through an object detection model using ML.NET, and paints bounding boxes with labels indicating the objects detected.\nThe Web app shows the images listed on the right, and each image may be selected to process. Once the image is processed, it is drawn in the middle of the screen with labeled bounding boxes around each detected object as shown below.\n\nLet me know if you have any question.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":32.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"machin learn problem detect object net draw boundari box exampl modul learn",
        "Question_preprocessed_content":"machin learn problem detect object draw boundari box exampl modul learn",
        "Question_gpt_summary_original":"The user is facing challenges in detecting objects using ML.NET and drawing boundary boxes. They are unable to find any relevant examples or modules to learn from.",
        "Question_gpt_summary":"user face challeng detect object net draw boundari box unabl relev exampl modul learn",
        "Answer_original_content":"object detect classic problem vision recogn object insid given imag imag case us pre train model train model classifi imag specif custom domain sampl us pre train model default add model export custom vision sampl http github com dotnet machinelearn sampl tree master sampl csharp end end app objectdetect onnx sampl consist separ app wpf core desktop app render live stream devic web cam run video frame object detect model net paint bound box label indic object detect real time asp net core web app allow user upload select imag web app run imag object detect model net paint bound box label indic object detect web app show imag list right imag select process imag process drawn middl screen label bound box detect object shown let know question",
        "Answer_preprocessed_content":"object detect classic problem vision recogn object insid given imag imag case us model train model classifi imag specif custom domain sampl us model default add model export custom vision sampl sampl consist separ app wpf core desktop app render devic web cam run video frame object detect model paint bound box label indic object detect core web app allow user upload select imag web app run imag object detect model paint bound box label indic object detect web app show imag list right imag select process imag process drawn middl screen label bound box detect object shown let know question",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer for the user's question about ingesting data from customers and deploying models for real-time and batch inference with minimal technical overheads when transitioning from aws and on-prem to azure. the answer provides a sample code for object detection using pre-trained models or custom models in computer vision. the sample code consists of two separate apps: a desktop app that runs a live-stream of the device's web cam and an asp.net web app that allows the user to upload or select an image. the apps use ml.net to run the image through an object detection model and paint bounding boxes with labels indicating the objects detected.",
        "Answer_gpt_summary":"solut provid answer user question ingest data custom deploi model real time batch infer minim technic overhead transit aw prem azur answer provid sampl code object detect pre train model custom model vision sampl code consist separ app desktop app run live stream devic web cam asp net web app allow user upload select imag app us net run imag object detect model paint bound box label indic object detect"
    },
    {
        "Question_id":62441146.0,
        "Question_title":"Revert a dvc remove -p command",
        "Question_body":"<p>I have just removed a DVC tracking file by mistake using the command <code>dvc remove training_data.dvc -p<\/code>, which led to all my training dataset gone completely. I know in Git, we can easily revert a deleted branch based on its hash. Does anyone know how to revert all my lost data in DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592445622650,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":687.0,
        "Answer_body":"<p>You should be safe (at least data is not gone) most likely. From the <code>dvc remove<\/code> <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remove\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>Note that it does not remove files from the DVC cache or remote storage (see dvc gc). However, remember to run <code>dvc push<\/code> to save the files you actually want to use or share in the future.<\/p>\n<\/blockquote>\n\n<p>So, if you created <code>training_data.dvc<\/code> as with <code>dvc add<\/code> and\/or <code>dvc run<\/code> and <code>dvc remove -p<\/code> didn't ask\/warn you about anything, means that data is cached similar to Git in the <code>.dvc\/cache<\/code>. <\/p>\n\n<p>There are ways to retrieve it, but I would need to know a little bit more details - how exactly did you add your dataset? Did you commit <code>training_data.dvc<\/code> or it's completely gone? Was it the only data you have added so far? (happy to help you in comments).<\/p>\n\n<h2>Recovering a directory<\/h2>\n\n<p>First of all, <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files-and-directories#structure-of-cache-directory\" rel=\"nofollow noreferrer\">here<\/a> is the document that describes briefly how DVC stores directories in the cache.<\/p>\n\n<p>What we can do is to find all <code>.dir<\/code> files in the <code>.dvc\/cache<\/code>:<\/p>\n\n<p><code>find .dvc\/cache -type f -name \"*.dir\"<\/code><\/p>\n\n<p>outputs something like:<\/p>\n\n<pre><code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir\n.dvc\/cache\/00\/db872eebe1c914dd13617616bb8586.dir\n.dvc\/cache\/2d\/1764cb0fc973f68f31f5ff90ee0883.dir\n<\/code><\/pre>\n\n<p>(if the local cache is lost and we are restoring data from the remote storage, the same logic applies, commands (e.g. to find files on S3 with .dir extension) look different)<\/p>\n\n<p>Each <code>.dir<\/code> file is a JSON with a content of one version of a directory (file names, hashes, etc). It has all the information needed to restore it. The next thing we need to do is to understand which one do we need. There is no one single rule for that, what I would recommend to check (and pick depending on your use case):<\/p>\n\n<ul>\n<li>Check the date modified (if you remember when this data was added).<\/li>\n<li>Check the content of those files - if you remember a specific file name that was present only in the directory you are looking for - just grep it.<\/li>\n<li>Try to restore them one by one and check the directory content.<\/li>\n<\/ul>\n\n<p>Okay, now let's imagine we decided that we want to restore <code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir<\/code>, (e.g. because content of it looks like:<\/p>\n\n<pre><code>[\n{\"md5\": \"6f597d341ceb7d8fbbe88859a892ef81\", \"relpath\": \"test.tsv\"}, {\"md5\": \"32b715ef0d71ff4c9e61f55b09c15e75\", \"relpath\": \"train.tsv\"}\n]\n<\/code><\/pre>\n\n<p>and we want to get a directory with <code>train.tsv<\/code>).<\/p>\n\n<p>The only thing we need to do is to create a <code>.dvc<\/code> file that references this directory:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n- md5: 20b786b6e6f80e2b3fcf17827ad18597.dir\n  path: my-directory\n<\/code><\/pre>\n\n<p>(note, that path \/20\/b786b6e6f80e2b3fcf17827ad18597.dir became a hash value: 20b786b6e6f80e2b3fcf17827ad18597.dir)<\/p>\n\n<p>And run <code>dvc pull<\/code> on this file.<\/p>\n\n<p>That should be it.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1592496929888,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62441146",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1592457436920,
        "Question_original_content":"revert remov command remov track file mistak command remov train data led train dataset gone complet know git easili revert delet branch base hash know revert lost data",
        "Question_preprocessed_content":"revert remov command remov track file mistak command led train dataset gone complet know git easili revert delet branch base hash know revert lost data",
        "Question_gpt_summary_original":"The user accidentally removed a DVC tracking file using the command \"dvc remove training_data.dvc -p\" which resulted in the loss of all their training dataset. They are seeking help to revert the lost data in DVC.",
        "Question_gpt_summary":"user accident remov track file command remov train data result loss train dataset seek help revert lost data",
        "Answer_original_content":"safe data gone like remov doc note remov file cach remot storag rememb run push save file actual want us share futur creat train data add run remov ask warn mean data cach similar git cach wai retriev need know littl bit detail exactli add dataset commit train data complet gone data ad far happi help comment recov directori document describ briefli store directori cach dir file cach cach type dir output like cach bbefebfcfad dir cach dbeebecddbb dir cach cbfcfffffee dir local cach lost restor data remot storag logic appli command file dir extens look differ dir file json content version directori file name hash inform need restor thing need understand need singl rule recommend check pick depend us case check date modifi rememb data ad check content file rememb specif file present directori look grep try restor check directori content okai let imagin decid want restor cach bbefebfcfad dir content look like fdcebdfbbeaef relpath test tsv befdffcefbc relpath train tsv want directori train tsv thing need creat file refer directori out bbefebfcfad dir path directori note path bbefebfcfad dir hash valu bbefebfcfad dir run pull file",
        "Answer_preprocessed_content":"safe like doc note remov file cach remot storag rememb run save file actual want us share futur creat mean data cach similar git wai retriev need know littl bit detail exactli add dataset commit complet gone data ad far recov directori document describ briefli store directori cach file output like local cach lost restor data remot storag logic appli command look differ file json content version directori inform need restor thing need understand need singl rule recommend check check date modifi check content file rememb specif file present directori look grep try restor check directori content okai let imagin decid want restor thing need creat file refer directori note path hash valu run file",
        "Answer_gpt_summary_original":"if a user has mistakenly removed a tracking file using the command \"remove training_data. -p\", resulting in the loss of their training dataset, there are ways to retrieve it. the data is cached similar to git in the .\/cache, and there are ways to find all .dir files in the .\/cache. each .dir file is a json with a content of one version of a directory (file names, hashes, etc). it has all the information needed to restore it. the next thing to do is to understand which one to restore. there is no one single rule for that, but one can check the date modified, content of those files, and try to restore them one by one and check the directory content. once the user has decided which directory to restore, they need to create a . file that references this directory and run pull on this file.",
        "Answer_gpt_summary":"user mistakenli remov track file command remov train data result loss train dataset wai retriev data cach similar git cach wai dir file cach dir file json content version directori file name hash inform need restor thing understand restor singl rule check date modifi content file try restor check directori content user decid directori restor need creat file refer directori run pull file"
    },
    {
        "Question_id":71979012.0,
        "Question_title":"Vertex AI scheduled notebooks doesn't recognize existence of folders",
        "Question_body":"<p>I have a managed jupyter notebook in Vertex AI that I want to schedule. The notebook works just fine as long as I start it manually, but as soon as it is scheduled, it fails. There are in fact many things that go wrong when scheduled, some of them are fixable. Before explaining what my trouble is, let me first give some details of the context.<\/p>\n<p>The notebook gathers information from an API for several stores and saves the data in different folders before processing it, saving csv-files to store-specific folders and to bigquery. So, in the location of the notebook, I have:<\/p>\n<ul>\n<li>The notebook<\/li>\n<li>Functions needed for the handling of data (as *.py files)<\/li>\n<li>A series of folders, some of which have subfolders which also have subfolders<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xhOWs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xhOWs.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>When I execute this manually, no problem. Everything works well and all files end up exactly where they should, as well as in different bigQuery tables.<\/p>\n<p>However, when scheduling the execution of the notebook, everything goes wrong. First, the files *.py cannot be read (as <code>import<\/code>). No problem, I added the functions in the notebook.<\/p>\n<p>Now, the following error is where I am at a loss, because I have no idea why it does work or how to fix it. The code that leads to the error is the following:<\/p>\n<pre><code>internal = &quot;https:\/\/api.************************&quot;\n\ndf_descriptions = [] \n\nstoress = internal\nresponse_stores = requests.get(storess,auth = HTTPBasicAuth(userInternal, keyInternal))\npathlib.Path(&quot;stores\/request_1.json&quot;).write_bytes(response_stores.content)\n\nfilepath = &quot;stores&quot;\n\nfiles = os.listdir(filepath)\n\nfor file in files:\n    with open(filepath + &quot;\/&quot;+file) as json_string:\n        jsonstr = json.load(json_string)\n        information = pd.json_normalize(jsonstr)\n    df_descriptions.append(information)\n\nStoreINFO = pd.concat(df_descriptions)\nStoreINFO = StoreINFO.dropna()\nStoreINFO = StoreINFO[StoreINFO['storeIdMappings'].map(lambda d: len(d)) &gt; 0]\n\ncloud_store_ids = list(set(StoreINFO.cloudStoreId))\n\nLastWeek = datetime.date.today()- timedelta(days=2)\nLastWeek =np.datetime64(LastWeek)\n<\/code><\/pre>\n<p>and the error reported is:<\/p>\n<pre><code>FileNotFoundError                         Traceback (most recent call last)\n\/tmp\/ipykernel_165\/2970402631.py in &lt;module&gt;\n      5 storess = internal\n      6 response_stores = requests.get(storess,auth = HTTPBasicAuth(userInternal, keyInternal))\n----&gt; 7 pathlib.Path(&quot;stores\/request_1.json&quot;).write_bytes(response_stores.content)\n      8 \n      9 filepath = &quot;stores&quot;\n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in write_bytes(self, data)\n   1228         # type-check for the buffer interface before truncating the file\n   1229         view = memoryview(data)\n-&gt; 1230         with self.open(mode='wb') as f:\n   1231             return f.write(view)\n   1232 \n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in open(self, mode, buffering, encoding, errors, newline)\n   1206             self._raise_closed()\n   1207         return io.open(self, mode, buffering, encoding, errors, newline,\n-&gt; 1208                        opener=self._opener)\n   1209 \n   1210     def read_bytes(self):\n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in _opener(self, name, flags, mode)\n   1061     def _opener(self, name, flags, mode=0o666):\n   1062         # A stub for the opener argument to built-in open()\n-&gt; 1063         return self._accessor.open(self, flags, mode)\n   1064 \n   1065     def _raw_open(self, flags, mode=0o777):\n\nFileNotFoundError: [Errno 2] No such file or directory: 'stores\/request_1.json'\n<\/code><\/pre>\n<p>I would gladly find another way to do this, for instance by using GCS buckets, but my issue is the existence of sub-folders. There are many stores and I do not wish to do this operation manually because some retailers for which I am doing this have over 1000 stores. My python code generates all these folders and as I understand it, this is not feasible in GCS.<\/p>\n<p>How can I solve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1650711286300,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1650713970783,
        "Question_score":0.0,
        "Question_view_count":247.0,
        "Answer_body":"<p>GCS uses a flat namespace, so folders don't actually exist, but can be simulated as given in this <a href=\"https:\/\/cloud.google.com\/storage\/docs\/folders\" rel=\"nofollow noreferrer\">documentation<\/a>.For your requirement, you can either use absolute path (starting with &quot;\/&quot; -- not relative) or create the &quot;stores&quot; <a href=\"https:\/\/docs.python.org\/3\/library\/pathlib.html#pathlib.Path.mkdir\" rel=\"nofollow noreferrer\">directory<\/a> (with &quot;mkdir&quot;). For more information you can check this <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/schedule-and-execute-notebooks-with-vertex-ai-workbench\" rel=\"nofollow noreferrer\">blog<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1651402328352,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71979012",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1651402011487,
        "Question_original_content":"schedul notebook recogn exist folder manag jupyt notebook want schedul notebook work fine long start manual soon schedul fail fact thing wrong schedul fixabl explain troubl let detail context notebook gather inform api store save data differ folder process save csv file store specif folder bigqueri locat notebook notebook function need handl data file seri folder subfold subfold execut manual problem work file end exactli differ bigqueri tabl schedul execut notebook goe wrong file read import problem ad function notebook follow error loss idea work fix code lead error follow intern http api descript storess intern respons store request storess auth httpbasicauth userintern keyintern pathlib path store request json write byte respons store content filepath store file listdir filepath file file open filepath file json string jsonstr json load json string inform json normal jsonstr descript append inform storeinfo concat descript storeinfo storeinfo dropna storeinfo storeinfo storeinfo storeidmap map lambda len cloud store id list set storeinfo cloudstoreid lastweek datetim date todai timedelta dai lastweek datetim lastweek error report filenotfounderror traceback recent tmp ipykernel storess intern respons store request storess auth httpbasicauth userintern keyintern pathlib path store request json write byte respons store content filepath store opt conda lib python pathlib write byte self data type check buffer interfac truncat file view memoryview data self open mode return write view opt conda lib python pathlib open self mode buffer encod error newlin self rais close return open self mode buffer encod error newlin open self open def read byte self opt conda lib python pathlib open self flag mode def open self flag mode stub open argument built open return self accessor open self flag mode def raw open self flag mode filenotfounderror errno file directori store request json gladli wai instanc gc bucket issu exist sub folder store wish oper manual retail store python code gener folder understand feasibl gc solv issu",
        "Question_preprocessed_content":"schedul notebook recogn exist folder manag jupyt notebook want schedul notebook work fine long start manual soon schedul fail fact thing wrong schedul fixabl explain troubl let detail context notebook gather inform api store save data differ folder process save folder bigqueri locat notebook notebook function need handl data seri folder subfold subfold execut manual problem work file end exactli differ bigqueri tabl schedul execut notebook goe wrong file read problem ad function notebook follow error loss idea work fix code lead error follow error report gladli wai instanc gc bucket issu exist store wish oper manual retail store python code gener folder understand feasibl gc solv issu",
        "Question_gpt_summary_original":"The user is facing challenges with scheduling a managed Jupyter notebook in Vertex AI. The notebook gathers information from an API for several stores and saves the data in different folders before processing it, saving csv-files to store-specific folders and to bigquery. When scheduling the execution of the notebook, the files *.py cannot be read, and the code that leads to the error is related to the existence of sub-folders. The user is looking for a solution to this issue.",
        "Question_gpt_summary":"user face challeng schedul manag jupyt notebook notebook gather inform api store save data differ folder process save csv file store specif folder bigqueri schedul execut notebook file read code lead error relat exist sub folder user look solut issu",
        "Answer_original_content":"gc us flat namespac folder actual exist simul given document requir us absolut path start rel creat store directori mkdir inform check blog",
        "Answer_preprocessed_content":"gc us flat namespac folder actual exist simul given requir us absolut path creat store directori inform check blog",
        "Answer_gpt_summary_original":"possible solutions to the challenge of scheduling a managed jupyter notebook that fails to recognize the existence of folders are: using absolute paths instead of relative paths, or simulating folders by creating a \"stores\" directory with the \"mkdir\" command. it is also recommended to check the provided documentation and blog for more information.",
        "Answer_gpt_summary":"possibl solut challeng schedul manag jupyt notebook fail recogn exist folder absolut path instead rel path simul folder creat store directori mkdir command recommend check provid document blog inform"
    },
    {
        "Question_id":null,
        "Question_title":"Hi everyone! First question - How to point multiple projects to single dataset?",
        "Question_body":"<p>I stumbled upon dvc over the weekend after discussing with my colleagues our need for just such a thing. I got it working in some test repos with data on our private s3 buckets. Very cool! I really appreciate the documentation, it\u2019s very well put-together.<\/p>\n<p>One thing I\u2019m not sure of is:<\/p>\n<p>If I have a dataset on a remote server and I have several different repos\/projects that use it, what is the appropriate way to point them all to that dataset and\/or a single dvc remote representation thereof? (Do I need in each repo\/project to download it, then dvc add it, and assign the dataset to the same remote?)<\/p>\n<p>thanks!<br>\nRory<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1613529152023,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":576.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/hi-everyone-first-question-how-to-point-multiple-projects-to-single-dataset\/671",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-17T03:21:30.548Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/stauntonjr\">@stauntonjr<\/a> !<\/p>\n<p>One way to handle it would be to put it into a data registry and then just <code>dvc import<\/code> it in other projects <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\">https:\/\/dvc.org\/doc\/use-cases\/data-registries<\/a> . Would that work for you?<\/p>",
                "Answer_score":8.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T14:24:22.454Z",
                "Answer_body":"<p>Hi, I have the same problem.<br>\nif I use data registry as you suggested,  and then <code>dvc import<\/code> it to the other projects,<br>\nthe data will be duplicated and downloaded to each project? so I will have multiple copies of the same data in different projects, in the same server?<br>\nthanks!<br>\nNaama<\/p>",
                "Answer_score":7.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T15:02:36.298Z",
                "Answer_body":"<p>Hi.<\/p>\n<p><code>dvc import<\/code>ed data is not affected by <code>dvc push<\/code>, so it will only be stored in the data registry remote with no duplication.<\/p>",
                "Answer_score":7.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T16:25:46.706Z",
                "Answer_body":"<p>thanks,<br>\nwhat do you mean by \" is not affected by <code>dvc push<\/code>?<br>\nI have a data registry, and another code project.<br>\nafter using <code>dvc import<\/code> in the code project, the data is downloaded + its <code>.dvc<\/code> file to the project folder. this data doesn\u2019t take space in my server? is it use links?<\/p>",
                "Answer_score":7.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T16:31:56.204Z",
                "Answer_body":"<p>After using <code>dvc import<\/code>, the data is downloaded locally, it\u2019s <code>.dvc<\/code> is created, but <code>dvc push<\/code> won\u2019t push it to remote. So next time when you run <code>dvc pull<\/code>, it will download it from the data registry it was <code>dvc import<\/code>ed from, so it won\u2019t take space in this project\u2019s remote. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>",
                "Answer_score":7.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"question point multipl project singl dataset stumbl weekend discuss colleagu need thing got work test repo data privat bucket cool appreci document thing sure dataset remot server differ repo project us appropri wai point dataset singl remot represent thereof need repo project download add assign dataset remot thank rori",
        "Question_preprocessed_content":"question point multipl project singl dataset stumbl weekend discuss colleagu need thing got work test repo data privat bucket cool appreci document thing sure dataset remot server differ us appropri wai point dataset singl remot represent thereof thank rori",
        "Question_gpt_summary_original":"The user is seeking guidance on how to point multiple projects to a single dataset using dvc. They are unsure if they need to download the dataset in each project and assign it to the same remote using dvc.",
        "Question_gpt_summary":"user seek guidanc point multipl project singl dataset unsur need download dataset project assign remot",
        "Answer_original_content":"stauntonjr wai handl data registri import project http org doc us case data registri work problem us data registri suggest import project data duplic download project multipl copi data differ project server thank naama import data affect push store data registri remot duplic thank mean affect push data registri code project import code project data download file project folder data doesnt space server us link import data download local creat push wont push remot time run pull download data registri import wont space project remot",
        "Answer_preprocessed_content":"wai handl data registri project work problem us data registri suggest project data duplic download project multipl copi data differ project server thank naama data affect store data registri remot duplic thank mean affect data registri code project code project data download file project folder data doesnt space server us link data download local creat wont push remot time run download data registri wont space project remot",
        "Answer_gpt_summary_original":"one possible solution to the challenge of pointing multiple projects to a single dataset on a remote server is to use a data registry and import the data into other projects. the imported data will not be duplicated and downloaded to each project, but will only be stored in the data registry remote with no duplication. the imported data will not take space in the project's remote and will be downloaded from the data registry it was imported from when running pull.",
        "Answer_gpt_summary":"possibl solut challeng point multipl project singl dataset remot server us data registri import data project import data duplic download project store data registri remot duplic import data space project remot download data registri import run pull"
    },
    {
        "Question_id":55531608.0,
        "Question_title":"Accessing Google BigQuery from AWS SageMaker",
        "Question_body":"<p>When running locally, my Jupyter notebook is able to reference Google BigQuery like so:<\/p>\n\n<pre><code>%%bigquery some_bq_table\n\nSELECT *\nFROM\n  `some_bq_dataset.some_bq_table` \n<\/code><\/pre>\n\n<p>So that later in my notebook I can reference some_bq_table as a pandas dataframe, as exemplified here: <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/visualize-jupyter\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/bigquery\/docs\/visualize-jupyter<\/a><\/p>\n\n<p>I want to run my notebook on AWS SageMaker to test a few things. To authenticate with BigQuery it seems that the only two ways are using a service account on GCP (or locally) or pointing the the SDK to a credentials JSON using an env var (as explained here: <a href=\"https:\/\/cloud.google.com\/docs\/authentication\/getting-started\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/docs\/authentication\/getting-started<\/a>).<\/p>\n\n<p>For example<\/p>\n\n<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"\/home\/user\/Downloads\/[FILE_NAME].json\"\n<\/code><\/pre>\n\n<p>Is there an easy way to connect to bigquery from SageMaker? My best idea right now is to download the JSON from somewhere to the SageMaker instnace and then set the env var from the python code.<\/p>\n\n<p>For example, I would do this:<\/p>\n\n<pre><code>os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"\/home\/user\/Downloads\/[FILE_NAME].json\"\n<\/code><\/pre>\n\n<p>However, this isn't very secure - I don't like the idea of downloading my credentials JSON to a SageMaker instance (this means I would have to upload the credentials to some private s3 bucket and then store them on the SageMaker instance). Not the end of the world but I rather avoid this. <\/p>\n\n<p>Any ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1554454321543,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":1078.0,
        "Answer_body":"<p>As you mentioned GCP currently authenticates using service account, credentials JSON and API tokens. Instead of storing credentials in S3 bucket you can consider using AWS Secrets Manager or AWS Systems Manager Parameter Store to store the GCP credentials and then fetch them in Jupyter notebook. This way credentials can be secured and the credentials file will be created from Secrets Manager only when needed. <\/p>\n\n<p>This is sample code I used previously to connect to BigQuery from SageMaker instance.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nimport boto3\nfrom google.cloud.bigquery import magics\nfrom google.oauth2 import service_account\n\ndef get_gcp_credentials_from_ssm(param_name):\n    # read credentials from SSM parameter store\n    ssm = boto3.client('ssm')\n    # Get the requested parameter\n    response = ssm.get_parameters(Names=[param_name], WithDecryption=True)\n    # Store the credentials in a variable\n    gcp_credentials = response['Parameters'][0]['Value']\n    # save credentials temporarily to a file\n    credentials_file = '\/tmp\/.gcp\/service_credentials.json'\n    with open(credentials_file, 'w') as outfile:  \n        json.dump(json.loads(gcp_credentials), outfile)\n    # create google.auth.credentials.Credentials to use for queries \n    credentials = service_account.Credentials.from_service_account_file(credentials_file)\n    # remove temporary file\n    if os.path.exists(credentials_file):\n        os.remove(credentials_file)\n    return credentials\n\n# this will set the context credentials to use for queries performed in jupyter \n# using bigquery cell magic\nmagics.context.credentials = get_gcp_credentials_from_ssm('my_gcp_credentials')\n<\/code><\/pre>\n\n<p>Please note that SageMaker execution role should have access to SSM and of course other necessary route to connect to GCP. I am not sure if this is the best way though. Hope someone has better way.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55531608",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1554514774276,
        "Question_original_content":"access googl bigqueri run local jupyt notebook abl refer googl bigqueri like bigqueri tabl select dataset tabl later notebook refer tabl panda datafram exemplifi http cloud googl com bigqueri doc visual jupyt want run notebook test thing authent bigqueri wai servic account gcp local point sdk credenti json env var explain http cloud googl com doc authent get start exampl export googl applic credenti home user download file json easi wai connect bigqueri best idea right download json instnac set env var python code exampl environ googl applic credenti home user download file json isn secur like idea download credenti json instanc mean upload credenti privat bucket store instanc end world avoid idea",
        "Question_preprocessed_content":"access googl bigqueri run local jupyt notebook abl refer googl bigqueri like later notebook refer panda datafram exemplifi want run notebook test thing authent bigqueri wai servic account gcp point sdk credenti json env var exampl easi wai connect bigqueri best idea right download json instnac set env var python code exampl isn secur like idea download credenti json instanc end world avoid idea",
        "Question_gpt_summary_original":"The user is facing challenges in accessing Google BigQuery from AWS SageMaker. While the Jupyter notebook can reference BigQuery, the user needs to authenticate with BigQuery to run the notebook on AWS SageMaker. The only two ways to authenticate are using a service account on GCP or pointing the SDK to a credentials JSON using an env var. The user's best idea is to download the JSON to the SageMaker instance and set the env var from the python code, but this is not secure. The user is seeking ideas to connect to BigQuery from SageMaker without compromising security.",
        "Question_gpt_summary":"user face challeng access googl bigqueri jupyt notebook refer bigqueri user need authent bigqueri run notebook wai authent servic account gcp point sdk credenti json env var user best idea download json instanc set env var python code secur user seek idea connect bigqueri compromis secur",
        "Answer_original_content":"mention gcp current authent servic account credenti json api token instead store credenti bucket consid aw secret manag aw system manag paramet store store gcp credenti fetch jupyt notebook wai credenti secur credenti file creat secret manag need sampl code previous connect bigqueri instanc import import json import boto googl cloud bigqueri import magic googl oauth import servic account def gcp credenti ssm param read credenti ssm paramet store ssm boto client ssm request paramet respons ssm paramet name param withdecrypt true store credenti variabl gcp credenti respons paramet valu save credenti temporarili file credenti file tmp gcp servic credenti json open credenti file outfil json dump json load gcp credenti outfil creat googl auth credenti credenti us queri credenti servic account credenti servic account file credenti file remov temporari file path exist credenti file remov credenti file return credenti set context credenti us queri perform jupyt bigqueri cell magic magic context credenti gcp credenti ssm gcp credenti note execut role access ssm cours necessari rout connect gcp sure best wai hope better wai",
        "Answer_preprocessed_content":"mention gcp current authent servic account credenti json api token instead store credenti bucket consid aw secret manag aw system manag paramet store store gcp credenti fetch jupyt notebook wai credenti secur credenti file creat secret manag need sampl code previous connect bigqueri instanc note execut role access ssm cours necessari rout connect gcp sure best wai hope better wai",
        "Answer_gpt_summary_original":"the answer suggests using aws secrets manager or aws systems manager parameter store to store gcp credentials securely instead of storing them in an s3 bucket. the code provided shows how to fetch the credentials from the parameter store and use them to authenticate with bigquery. the execution role should have access to ssm and other necessary routes to connect to gcp.",
        "Answer_gpt_summary":"answer suggest aw secret manag aw system manag paramet store store gcp credenti secur instead store bucket code provid show fetch credenti paramet store us authent bigqueri execut role access ssm necessari rout connect gcp"
    },
    {
        "Question_id":56666667.0,
        "Question_title":"Clienterror: An error occured when calling the CreateModel operation",
        "Question_body":"<p>I want to deploy sklearn model in sagemaker. I created a training script.<\/p>\n\n<p>scripPath=' sklearn.py'<\/p>\n\n<p><code>sklearn=SKLearn(entry_point=scripPath,\n                                 train_instance_type='ml.m5.xlarge',\n                                   role=role,                  output_path='s3:\/\/{}\/{}\/output'.format(bucket,prefix), sagemaker_session=session)\nsklearn.fit({\"train-dir' : train_input})<\/code><\/p>\n\n<p>When I deploy it\n<code>predictor=sklearn.deploy(initial_count=1,instance_type='ml.m5.xlarge')<\/code><\/p>\n\n<p>It throws,\n<code>Clienterror: An error occured when calling the CreateModel operation:Could not find model data at s3:\/\/tree\/sklearn\/output\/model.tar.gz<\/code><\/p>\n\n<p>Can anyone say how to solve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1560943786943,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1033.0,
        "Answer_body":"<p>When deploying models, SageMaker looks up S3 to find your trained model artifact. It seems that there is no trained model artifact at <code>s3:\/\/tree\/sklearn\/output\/model.tar.gz<\/code>. Make sure to persist your model artifact in your training script at the appropriate local location in docker which is <code>\/opt\/ml\/model<\/code>.\nfor example, in your training script this could look like:<\/p>\n\n<pre><code>joblib.dump(model, \/opt\/ml\/model\/mymodel.joblib)\n<\/code><\/pre>\n\n<p>After training, SageMaker will copy the content of <code>\/opt\/ml\/model<\/code> to s3 at the <code>output_path<\/code> location.<\/p>\n\n<p>If you deploy in the same session a <code>model.deploy()<\/code> will map automatically to the artifact path. If you want to deploy a model that you trained elsewhere, possibly during a different session or in a different hardware, you need to explicitly instantiate a model before deploying<\/p>\n\n<pre><code>from sagemaker.sklearn.model import SKLearnModel\n\nmodel = SKLearnModel(\n    model_data='s3:\/\/...model.tar.gz',  # your artifact\n    role=get_execution_role(),\n    entry_point='script.py')  # script containing inference functions\n\nmodel.deploy(\n    instance_type='ml.m5.xlarge',\n    initial_instance_count=1,\n    endpoint_name='your_endpoint_name')\n<\/code><\/pre>\n\n<p>See more about Sklearn in SageMaker here <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56666667",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1560972233448,
        "Question_original_content":"clienterror error occur call createmodel oper want deploi sklearn model creat train script scrippath sklearn sklearn sklearn entri point scrippath train instanc type xlarg role role output path output format bucket prefix session session sklearn fit train dir train input deploi predictor sklearn deploi initi count instanc type xlarg throw clienterror error occur call createmodel oper model data tree sklearn output model tar solv issu",
        "Question_preprocessed_content":"clienterror error occur call createmodel oper want deploi sklearn model creat train script scrippath deploi throw solv issu",
        "Question_gpt_summary_original":"The user is encountering an error when trying to deploy a sklearn model in sagemaker. The error message states that the model data cannot be found at the specified S3 location. The user is seeking assistance in resolving this issue.",
        "Question_gpt_summary":"user encount error try deploi sklearn model error messag state model data specifi locat user seek assist resolv issu",
        "Answer_original_content":"deploi model look train model artifact train model artifact tree sklearn output model tar sure persist model artifact train script appropri local locat docker opt model exampl train script look like joblib dump model opt model mymodel joblib train copi content opt model output path locat deploi session model deploi map automat artifact path want deploi model train possibl differ session differ hardwar need explicitli instanti model deploi sklearn model import sklearnmodel model sklearnmodel model data model tar artifact role execut role entri point script script contain infer function model deploi instanc type xlarg initi instanc count endpoint endpoint sklearn http readthedoc stabl sklearn html",
        "Answer_preprocessed_content":"deploi model look train model artifact train model artifact sure persist model artifact train script appropri local locat docker exampl train script look like train copi content locat deploi session map automat artifact path want deploi model train possibl differ session differ hardwar need explicitli instanti model deploi sklearn",
        "Answer_gpt_summary_original":"possible solutions to the encountered client error when deploying a sklearn model include ensuring that the trained model artifact is persisted in the appropriate local location in docker, copying the content of \/opt\/ml\/model to the s3 output path location after training, and explicitly instantiating a model before deploying if the model was trained elsewhere. additionally, the user can refer to the documentation on using sklearn for more information.",
        "Answer_gpt_summary":"possibl solut encount client error deploi sklearn model includ ensur train model artifact persist appropri local locat docker copi content opt model output path locat train explicitli instanti model deploi model train addition user refer document sklearn inform"
    },
    {
        "Question_id":null,
        "Question_title":"Taking a long time to download artifact of only 300mb",
        "Question_body":"<p>Its taking ages to download an artifact of only 300mb. Its there anyway to enable parallel download ? add multiprocessing ?<\/p>\n<p>wandb: Downloading large artifact Data-Split-802020:v0, 303.21MB. 148470 files\u2026<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1660231132264,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":45.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/taking-a-long-time-to-download-artifact-of-only-300mb\/2909",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-15T22:34:21.610Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/usman94\">@usman94<\/a> , we are actively focusing\/working on improving the performance of Artifact uploads\/downloads. There isn\u2019t an ability to split up and artifact when downloading it. This is under consideration for implementation. Once the engineering team has provided an update, I will let you know.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-14T22:34:53.434Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"take long time download artifact take ag download artifact enabl parallel download add multiprocess download larg artifact data split file",
        "Question_preprocessed_content":"take long time download artifact take ag download artifact enabl parallel download add multiprocess download larg artifact file",
        "Question_gpt_summary_original":"The user is facing challenges with downloading an artifact of only 300mb, which is taking a long time. They are seeking ways to enable parallel download or add multiprocessing to speed up the process.",
        "Question_gpt_summary":"user face challeng download artifact take long time seek wai enabl parallel download add multiprocess speed process",
        "Answer_original_content":"usman activ focus work improv perform artifact upload download isnt abil split artifact download consider implement engin team provid updat let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"activ improv perform artifact isnt abil split artifact download consider implement engin team provid updat let know topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there is currently no solution to split up an artifact when downloading it, but the engineering team is working on improving the performance of artifact uploads\/downloads. the user will be updated once there is progress on this issue.",
        "Answer_gpt_summary":"current solut split artifact download engin team work improv perform artifact upload download user updat progress issu"
    },
    {
        "Question_id":null,
        "Question_title":"Refactor existing project from single root .dvc to subprojects (dvc init --subdir)",
        "Question_body":"<p>I have an existing project that was initialized at the git root with a single .dvc directory.<\/p>\n<p>Are there going to be any gotchas or unexpected behavior if I want to refactor this by running \u201cdvc init --subdir\u201d in a couple of subdirectories to create subprojects with their own cache directories and remotes?  If I understand things correctly, I can:<\/p>\n<ol>\n<li>Update my cache and ensure no one will push changes to remote while the steps below are done<\/li>\n<li>Run <code>dvc init --subdir<\/code> in a subdirectory that I want to turn into a subproject<\/li>\n<li>Set cache dir to the same as the root project, set the remote to the new remote for this subproject<\/li>\n<li>Run <code>dvc push<\/code> to copy current versions of all files in the subproject to the new remote<\/li>\n<li>Change subproject cache dir to its new path<\/li>\n<li>git commit and push<\/li>\n<\/ol>\n<p>Anything I\u2019m missing?<\/p>\n<p>One confusing issue will be that dvc commands will have a different scope depending whether a developer is on a commit upstream or downstream from this change.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634692636711,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":202.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/refactor-existing-project-from-single-root-dvc-to-subprojects-dvc-init-subdir\/928",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-24T10:32:34.755Z",
                "Answer_body":"<p>Hi, <a class=\"mention\" href=\"\/u\/jmiller\">@jmiller<\/a> ! Unfortunately, I haven\u2019t really seen the use case you are proposing so can\u2019t commit to saying that there won\u2019t be any issues.<\/p>\n<p>However, the steps you are proposing sound reasonable. Please don\u2019t hesitate on following up with any problem you might encounter.<\/p>\n<p>The only gotcha might be what you have already pointed out, that you should be clear documenting those changes and notifying the users of the repo (also using whatever mechanism you use in these cases like git tag or releasing)<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"refactor exist project singl root subproject init subdir exist project initi git root singl directori go gotcha unexpect behavior want refactor run init subdir coupl subdirectori creat subproject cach directori remot understand thing correctli updat cach ensur push chang remot step run init subdir subdirectori want turn subproject set cach dir root project set remot new remot subproject run push copi current version file subproject new remot chang subproject cach dir new path git commit push miss confus issu command differ scope depend develop commit upstream downstream chang",
        "Question_preprocessed_content":"refactor exist project singl root subproject exist project initi git root singl directori go gotcha unexpect behavior want refactor run init coupl subdirectori creat subproject cach directori remot understand thing correctli updat cach ensur push chang remot step run subdirectori want turn subproject set cach dir root project set remot new remot subproject run copi current version file subproject new remot chang subproject cach dir new path git commit push miss confus issu command differ scope depend develop commit upstream downstream chang",
        "Question_gpt_summary_original":"The user wants to refactor an existing project from a single root .dvc to subprojects using \"dvc init --subdir\" in a few subdirectories to create subprojects with their own cache directories and remotes. The user plans to update the cache, run \"dvc init --subdir\" in a subdirectory, set the cache directory to the same as the root project, set the remote to the new remote for this subproject, run \"dvc push\" to copy current versions of all files in the subproject to the new remote, change the subproject cache directory to its new path, and commit and push. However, the user is concerned about the scope of dvc commands depending on whether a developer is on a commit upstream or downstream from this change.",
        "Question_gpt_summary":"user want refactor exist project singl root subproject init subdir subdirectori creat subproject cach directori remot user plan updat cach run init subdir subdirectori set cach directori root project set remot new remot subproject run push copi current version file subproject new remot chang subproject cach directori new path commit push user concern scope command depend develop commit upstream downstream chang",
        "Answer_original_content":"jmiller unfortun havent seen us case propos commit sai wont issu step propos sound reason dont hesit follow problem encount gotcha point clear document chang notifi user repo mechan us case like git tag releas",
        "Answer_preprocessed_content":"unfortun havent seen us case propos commit sai wont issu step propos sound reason dont hesit follow problem encount gotcha point clear document chang notifi user repo",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are: \n- the steps proposed by the user for refactoring the project into subprojects with their own cache directories and remotes sound reasonable.\n- clear documentation of the changes made and notifying the users of the repo is important.\n- using a mechanism like git tag or releasing can be helpful in notifying users of the changes. \n\nin summary, the answer suggests that the proposed steps for refactoring the project are reasonable, but clear documentation and notifying users of the changes are important.",
        "Answer_gpt_summary":"possibl solut extract answer step propos user refactor project subproject cach directori remot sound reason clear document chang notifi user repo import mechan like git tag releas help notifi user chang summari answer suggest propos step refactor project reason clear document notifi user chang import"
    },
    {
        "Question_id":64338898.0,
        "Question_title":"How to load an experiment in azureml?",
        "Question_body":"<p>I have many experiment, like:<\/p>\n<p><img src=\"https:\/\/user-images.githubusercontent.com\/40580910\/95883598-82a07d00-0d51-11eb-847d-872452f6caa4.png\" alt=\"image\" \/><\/p>\n<p>and now, i want load an experiment<\/p>\n<pre><code>#%% sumonando os pacotes e verificando azureml.core\nimport azureml.core\nimport pandas as pd\nimport numpy as np\nimport logging\n\nprint(&quot;AzureML SDK Version: &quot;, azureml.core.VERSION)\n\n#%% Conectando ao azure e crinado o exparimento\n\nfrom azureml.core import Workspace, Experiment\n\nws = Workspace.from_config() \nprint(Experiment.list(ws))\n#%%\nExperiment = Experiment.from_directory('teste2-Monitor-Runs') `\n<\/code><\/pre>\n<p>but<\/p>\n<pre><code>&quot;error&quot;: {\n    &quot;message&quot;: &quot;No cache found for current project, try providing resource group and workspace \narguments&quot;\n}`\n<\/code><\/pre>\n<hr \/>\n<p>Content: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.experiment(class)?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.core.Experiment class - Azure Machine Learning Python<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1602604847240,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":254.0,
        "Answer_body":"<p>I believe it is that way.<\/p>\n<pre><code>from azureml.core import Experiment, Workspace\nExperiment = ws.experiments[&quot;teste2-Monitor-Runs&quot;]\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64338898",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1602700300720,
        "Question_original_content":"load experi experi like want load experi sumonando pacot verificando core import core import panda import numpi import log print sdk version core version conectando azur crinado exparimento core import workspac experi workspac config print experi list experi experi directori test monitor run error messag cach current project try provid resourc group workspac argument content core experi class python",
        "Question_preprocessed_content":"load experi experi like want load experi content class python",
        "Question_gpt_summary_original":"The user is facing a challenge in loading an experiment in AzureML. They have tried to connect to Azure and create an experiment, but they encountered an error message stating that no cache was found for the current project and suggesting that they provide resource group and workspace arguments.",
        "Question_gpt_summary":"user face challeng load experi tri connect azur creat experi encount error messag state cach current project suggest provid resourc group workspac argument",
        "Answer_original_content":"believ wai core import experi workspac experi experi test monitor run",
        "Answer_preprocessed_content":"believ wai",
        "Answer_gpt_summary_original":"the answer does not provide any possible solutions to the error encountered when attempting to load an experiment in .core. it only shows a code snippet that imports experiment and workspace from .core and assigns a specific experiment to the variable \"experiment\".",
        "Answer_gpt_summary":"answer provid possibl solut error encount attempt load experi core show code snippet import experi workspac core assign specif experi variabl experi"
    },
    {
        "Question_id":null,
        "Question_title":"Join us on August 4! Machine Learning Day on Google Open Source Live",
        "Question_body":"",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658991180000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":93.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Join-us-on-August-4-Machine-Learning-Day-on-Google-Open-Source\/td-p\/447714\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-16T20:51:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hello @Lauren_vdv\u00a0\n\nThank you for the post, can I register on demand??"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"join august machin learn dai googl open sourc live ",
        "Question_preprocessed_content":"join august machin learn dai googl open sourc live ",
        "Question_gpt_summary_original":"The given text does not provide any information about the challenges faced by the user. Therefore, a summary of challenges cannot be provided.",
        "Question_gpt_summary":"given text provid inform challeng face user summari challeng provid",
        "Answer_original_content":"hello lauren vdv thank post regist demand",
        "Answer_preprocessed_content":"hello thank post regist demand",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer as it is a question about registration for an event.",
        "Answer_gpt_summary":"solut provid answer question registr event"
    },
    {
        "Question_id":67051900.0,
        "Question_title":"How can I generate prediction intervals for Azure AutoML timeseries forecasts?",
        "Question_body":"<p>Is it possible to generate prediction intervals for time series forecasts when using a Azure AutoML trained models? Could we get the training errors out of the process and use them for bootstrapping?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618193869380,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":205.0,
        "Answer_body":"<p>You can generate forecast quantiles. See the following notebook for more details: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-forecast-function\/auto-ml-forecasting-function.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-forecast-function\/auto-ml-forecasting-function.ipynb<\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67051900",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1618942132740,
        "Question_original_content":"gener predict interv azur automl timeseri forecast possibl gener predict interv time seri forecast azur automl train model train error process us bootstrap",
        "Question_preprocessed_content":"gener predict interv azur automl timeseri forecast possibl gener predict interv time seri forecast azur automl train model train error process us bootstrap",
        "Question_gpt_summary_original":"The user is facing challenges in generating prediction intervals for time series forecasts using Azure AutoML trained models. They are unsure if it is possible to obtain training errors for bootstrapping purposes.",
        "Question_gpt_summary":"user face challeng gener predict interv time seri forecast azur automl train model unsur possibl obtain train error bootstrap purpos",
        "Answer_original_content":"gener forecast quantil follow notebook detail http github com azur machinelearningnotebook blob master us autom machin learn forecast forecast function auto forecast function ipynb",
        "Answer_preprocessed_content":"gener forecast quantil follow notebook detail",
        "Answer_gpt_summary_original":"possible solution: the user can generate forecast quantiles using an azure automl trained model. they can refer to a specific notebook for more details on how to do this.",
        "Answer_gpt_summary":"possibl solut user gener forecast quantil azur automl train model refer specif notebook detail"
    },
    {
        "Question_id":46900593.0,
        "Question_title":"Azure ML - Import Hive Query Failing - Hive over ADLS",
        "Question_body":"<p>We are working on Azure ML and ADLS combination. Since HDInsight Cluster is working over ADLS, we are trying to use Hive Query and HDFS route and running into problems. \nRequest your help in solving the problem of reading data from hive query and writing to HDFS. Below is the error URL for reference:<\/p>\n\n<p><a href=\"https:\/\/studioapi.azureml.net\/api\/sharedaccess?workspaceId=025ba20578874d7086e6c495cc49a3f2&amp;signature=ZMUCNMwRjlrksrrmsrx5SaGedSgwMmO%2FfSHvq190%2F1I%3D&amp;sharedAccessUri=https%3A%2F%2Fesprodussouth001.blob.core.windows.net%2Fexperimentoutput%2Fccf9a206-730d-4773-b44e-a2dd8c6e87b9%2Fccf9a206-730d-4773-b44e-a2dd8c6e87b9.txt%3Fsv%3D2015-02-21%26sr%3Db%26sig%3DHkuFm8B2Ba1kEWWIwanqlv%2FcQPWVz0XYveSsZnEa0Wg%3D%26st%3D2017-10-16T18%3A31%3A06Z%26se%3D2017-10-17T18%3A36%3A06Z%26sp%3Dr\" rel=\"nofollow noreferrer\">https:\/\/studioapi.azureml.net\/api\/sharedaccess?workspaceId=025ba20578874d7086e6c495cc49a3f2&amp;signature=ZMUCNMwRjlrksrrmsrx5SaGedSgwMmO%2FfSHvq190%2F1I%3D&amp;sharedAccessUri=https%3A%2F%2Fesprodussouth001.blob.core.windows.net%2Fexperimentoutput%2Fccf9a206-730d-4773-b44e-a2dd8c6e87b9%2Fccf9a206-730d-4773-b44e-a2dd8c6e87b9.txt%3Fsv%3D2015-02-21%26sr%3Db%26sig%3DHkuFm8B2Ba1kEWWIwanqlv%2FcQPWVz0XYveSsZnEa0Wg%3D%26st%3D2017-10-16T18%3A31%3A06Z%26se%3D2017-10-17T18%3A36%3A06Z%26sp%3Dr<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1508806505617,
        "Question_favorite_count":null,
        "Question_last_edit_time":1510470314876,
        "Question_score":0.0,
        "Question_view_count":64.0,
        "Answer_body":"<p>Azure Machine Learning supports Hive but not over ADLS. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1509134225963,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46900593",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1508944940720,
        "Question_original_content":"import hive queri fail hive adl work adl combin hdinsight cluster work adl try us hive queri hdf rout run problem request help solv problem read data hive queri write hdf error url refer http studioapi net api sharedaccess workspaceid badecccaf signatur zmucnmwrjlrksrrmsrxsagedsgwmmo ffshvq sharedaccessuri http fesprodussouth blob core window net fexperimentoutput fccfa addceb fccfa addceb txt fsv sig dhkufmbbakewwiwanqlv fcqpwvzxyvesszneawg",
        "Question_preprocessed_content":"import hive queri fail hive adl work adl combin hdinsight cluster work adl try us hive queri hdf rout run problem request help solv problem read data hive queri write hdf error url refer",
        "Question_gpt_summary_original":"The user is encountering challenges while trying to use Hive Query and HDFS route on Azure ML and ADLS combination. The HDInsight Cluster is working over ADLS, but the user is unable to read data from Hive Query and write to HDFS. The user is seeking help to solve the problem and has provided an error URL for reference.",
        "Question_gpt_summary":"user encount challeng try us hive queri hdf rout adl combin hdinsight cluster work adl user unabl read data hive queri write hdf user seek help solv problem provid error url refer",
        "Answer_original_content":"support hive adl",
        "Answer_preprocessed_content":"support hive adl",
        "Answer_gpt_summary_original":"possible solution: the user can try importing the hive query using a different storage solution that is supported by hdinsight, or find an alternative method to work with azure data lake storage that is compatible with hive.",
        "Answer_gpt_summary":"possibl solut user try import hive queri differ storag solut support hdinsight altern method work azur data lake storag compat hive"
    },
    {
        "Question_id":null,
        "Question_title":"Status flag is \"terminated\" when experiment is still \"running\"",
        "Question_body":"<p>Hi,<\/p>\n<p>I currently run experiments. But their status flag says \u201cterminated\u201d instead of \u201crunning\u201d. This is quite annoying since now I can\u2019t delete all terminated runs without also deleting active runs.<\/p>\n<p>Excerpt of output when running <code>guild runs info<\/code>:<\/p>\n<pre><code>status: terminated\nstarted: 2020-11-25 05:59:38\nstopped:\n<\/code><\/pre>\n<p>Is it bug or did I cause this somehow?<\/p>",
        "Question_answer_count":10,
        "Question_comment_count":0,
        "Question_creation_time":1606300246699,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":358.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/status-flag-is-terminated-when-experiment-is-still-running\/461",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-25T18:04:38.886Z",
                "Answer_body":"<p>When an operation is stopped using an interrupt \u2014 e.g. by pressing <code>Ctrl-c<\/code> or via <code>SIGINT<\/code> or <code>SIGKILL<\/code> (e.g. used by <code>guild stop<\/code>) the status is <code>terminated<\/code>. You can see the exit code in <code>guild runs info<\/code>.<\/p>\n<p>A terminated run is typically not a problem \u2014 there are many reasons you\u2019d want to stop a run.<\/p>\n<p>You can delete terminated runs by filtering with <code>-T<\/code> (FYI this option is changing to <code>-Ft<\/code> in 0.7.1). Run this:<\/p>\n<pre><code class=\"lang-command\">guild runs rm -T\n<\/code><\/pre>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-25T18:07:03.041Z",
                "Answer_body":"<p>I should add that if the operation is in fact still running and shows as terminated, this is a bug.<\/p>\n<p>You can test if the run process is alive by running <code>kill -0 &lt;pid&gt;<\/code> where <code>&lt;pid&gt;<\/code> is the pid that shows up in <code>guild runs info<\/code>.<\/p>",
                "Answer_score":22.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-25T21:55:25.873Z",
                "Answer_body":"<p>Hi Garrett,  my run seems to be terminated my ssh session times out. The run will take a day, so I cant stay on the server for this  amount of time<\/p>",
                "Answer_score":12.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-26T05:36:51.204Z",
                "Answer_body":"<p>Any chances you are running your jobs on a remote servers or a cluster? In that case, its normal and its not supported now because guild grab the PID for the job status and if won\u2019t be able to get it if you run on a cluster. You can see <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/239\" rel=\"noopener nofollow ugc\">this<\/a> issue for more.<\/p>\n<p>You might also consider maintaining a session by using <code>tmux<\/code>, you can install it through <code>apt-get install tmux<\/code>. Use <code>tmux<\/code> to open a persistent session, use <code>tmux ls<\/code> to list existing sessions and to reconnect a session, use <code>tmux attach -t [session]<\/code><\/p>",
                "Answer_score":27.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-26T11:08:04.505Z",
                "Answer_body":"<p>I run my jobs on a remote cluster. Do you already have a plan on how to fix the status flag for remote jobs? I use guildai quite frequently and happy to submit a PR if you give me some hints on how to best attack this issue.<\/p>",
                "Answer_score":6.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-26T11:23:21.095Z",
                "Answer_body":"<p>IMO one elegant way to solve this is to write the status to a file (since cluster most likely have shared file system) and then read it when calling <code>guild compare<\/code> or <code>guild view<\/code>, the file should also check if guild is terminated properly. But I am not sure if this aligns with the Devs design pattern.<\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-27T20:57:05.823Z",
                "Answer_body":"<p>I just ran a test using a long running operation that looks like this:<\/p>\n<pre><code class=\"lang-python\">import time\n\ntime.sleep(1000000)\n<\/code><\/pre>\n<p>I start the run this way:<\/p>\n<pre><code class=\"lang-command\">guild run test -r my-remote\n<\/code><\/pre>\n<p>I can safely <code>Ctrl-c<\/code> the session, which disconnects from the remote operation. I can also explicitly kill the underlying <code>ssh<\/code> command. Either way, the run continues on the remote server. Guild only relies on the <code>ssh<\/code> connection to start the run \u2014 not to actually maintain running. Guild is technically \u201cwatching\u201d the run after it starts to avoid the problem you\u2019re mentioning. The watching is just a log tail. You can kill it and not affect the run itself.<\/p>\n<p>Note that when I run this on a remote, the run does not appear in any local runs list until I explicitly pull the run.<\/p>\n<p>When I view the runs on the remote, I see it running \u2014 even after I kill the ssh connection.<\/p>\n<pre><code class=\"lang-command\">guild runs -r my-remote\n<\/code><\/pre>\n<pre><code class=\"lang-output\">[1:62af7e9e]  gpkg.anonymous-cbedc848\/test  2020-11-27 12:27:41  running  \n<\/code><\/pre>\n<p>When I pull the run, I get the current run at the point of the pull. When I list runs locally, I see that it\u2019s running along with the remote name.<\/p>\n<pre><code class=\"lang-command\">guild pull my-remote 62af7e9e\n<\/code><\/pre>\n<pre><code class=\"lang-command\">guild runs\n<\/code><\/pre>\n<pre><code class=\"lang-output\">[1:62af7e9e]  gpkg.anonymous-cbedc848\/test  2020-11-27 12:27:41  running (my-remote)  \n<\/code><\/pre>\n<p>In this case, Guild reflects the status at the time of the pull. Guild does not automatically sync the status in the background (Guild doesn\u2019t use long-running agents unless you explicitly start them). To get the latest from the remote, run <code>pull<\/code> again.<\/p>\n<p>Guild has <a href=\"\/commands\/sync\"><code>sync<\/code><\/a> command that\u2019s convenient for sync\u2019ing local runs with their remote counterparts. Unfortunately that command is fubar\u2019d in the 0.7.0 release. That\u2019s fixed for 0.7.1 though. You just run <code>guild sync<\/code> and any local runs that are still running are sync\u2019d with the current remote status.<\/p>\n<p>From my end, aside from the broken <code>sync<\/code> command (which you don\u2019t need anyway), this is working as expected. To help track down the issue, could you identify the stage where it breaks down for you?<\/p>",
                "Answer_score":6.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-30T14:06:51.604Z",
                "Answer_body":"<p>thank you this works well!<\/p>",
                "Answer_score":46.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-03T13:16:59.198Z",
                "Answer_body":"<p>You are right that works. For me and I guess a few others as well, the problem is that I submit jobs to a remote cluster to which I can\u2019t ssh into. So I can\u2019t pull new updates about the run.<br>\nBut it\u2019s not that big of a problem just a bit inconvenient.<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-03T15:23:30.254Z",
                "Answer_body":"<p>How do you access files from that server? Do you use a networked file system, locally mounted?<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"statu flag termin experi run current run experi statu flag sai termin instead run annoi delet termin run delet activ run excerpt output run run info statu termin start stop bug caus",
        "Question_preprocessed_content":"statu flag termin experi run current run experi statu flag sai termin instead run annoi delet termin run delet activ run excerpt output run bug caus",
        "Question_gpt_summary_original":"The user is facing a challenge where the status flag of their running experiments is showing as \"terminated\" instead of \"running\". This is causing difficulty in deleting only the terminated runs without deleting the active ones. The user is unsure if this is a bug or if they caused it themselves.",
        "Question_gpt_summary":"user face challeng statu flag run experi show termin instead run caus difficulti delet termin run delet activ on user unsur bug caus",
        "Answer_original_content":"oper stop interrupt press ctrl sigint sigkil stop statu termin exit code run info termin run typic problem reason youd want stop run delet termin run filter fyi option chang run run add oper fact run show termin bug test run process aliv run kill pid show run info garrett run termin ssh session time run dai stai server time chanc run job remot server cluster case normal support grab pid job statu wont abl run cluster issu consid maintain session tmux instal apt instal tmux us tmux open persist session us tmux list exist session reconnect session us tmux attach session run job remot cluster plan fix statu flag remot job us frequent happi submit hint best attack issu imo eleg wai solv write statu file cluster like share file read call compar view file check termin properli sure align dev design pattern ran test long run oper look like import time time sleep start run wai run test remot safe ctrl session disconnect remot oper explicitli kill underli ssh command wai run continu remot server reli ssh connect start run actual maintain run technic watch run start avoid problem your mention watch log tail kill affect run note run remot run appear local run list explicitli pull run view run remot run kill ssh connect run remot afe gpkg anonym cbedc test run pull run current run point pull list run local run remot pull remot afe run afe gpkg anonym cbedc test run remot case reflect statu time pull automat sync statu background doesnt us long run agent explicitli start latest remot run pull sync command that conveni sync local run remot counterpart unfortun command fubard releas that fix run sync local run run syncd current remot statu end asid broken sync command dont need work expect help track issu identifi stage break thank work right work guess problem submit job remot cluster ssh pull new updat run big problem bit inconveni access file server us network file local mount",
        "Answer_preprocessed_content":"oper stop interrupt press statu exit code termin run typic problem reason youd want stop run delet termin run filter run add oper fact run show termin bug test run process aliv run pid show garrett run termin ssh session time run dai stai server time chanc run job remot server cluster case normal support grab pid job statu wont abl run cluster issu consid maintain session instal us open persist session us list exist session reconnect session us run job remot cluster plan fix statu flag remot job us frequent happi submit hint best attack issu imo eleg wai solv write statu file read call file check termin properli sure align dev design pattern ran test long run oper look like start run wai safe session disconnect remot oper explicitli kill underli command wai run continu remot server reli connect start run actual maintain run technic watch run start avoid problem your mention watch log tail kill affect run note run remot run appear local run list explicitli pull run view run remot run kill ssh connect pull run current run point pull list run local run remot case reflect statu time pull automat sync statu background latest remot run command that conveni sync local run remot counterpart unfortun command fubard releas that fix run local run run syncd current remot statu end asid broken command work expect help track issu identifi stage break thank work right work guess problem submit job remot cluster ssh pull new updat run big problem bit inconveni access file server us network file local mount",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are:\n\n- if the experiment status flag is showing as \"terminated\" even though the experiment is still \"running\", you can see the exit code in guild runs info. a terminated run is typically not a problem. you can delete terminated runs by filtering with -t (or -ft in 0.7.1) using guild runs rm -t.\n- if the operation is in fact still running and shows as terminated, this is a bug. you can test if the run process is alive by running kill -0 <pid> where <pid> is the pid that shows up in guild runs info.\n- if you are running your jobs on a remote server or a cluster, it's normal that the run seems to be terminated and it's not supported now because guild grab the pid for the job status and if won't be able to get it if you run on a cluster. you might also consider maintaining a session by using tmux.\n- to",
        "Answer_gpt_summary":"possibl solut extract answer experi statu flag show termin experi run exit code run info termin run typic problem delet termin run filter run oper fact run show termin bug test run process aliv run kill pid show run info run job remot server cluster normal run termin support grab pid job statu won abl run cluster consid maintain session tmux"
    },
    {
        "Question_id":null,
        "Question_title":"How to deploy a scikit learn regression model as a web service?",
        "Question_body":"Hello,\n\nI find the documentation related with ML model deployment overwhelming and I'm struggling with the most basic \"Hello world\" tutorial even after several days of research.\n\nAll I want is to deploy the most basic model as a web service that can be consumed via an API through Power BI or any other web app to serve as a POC. Then we can think about \"scale\", \"dockers\", \"containers\", etc...\n\nThis is my code in Python 3.6:\n\n\n\n import joblib\n    \n from sklearn.datasets import load_diabetes\n from sklearn.linear_model import Ridge    \n    \n dataset_x, dataset_y = load_diabetes(return_X_y=True)\n    \n model = Ridge().fit(dataset_x, dataset_y)\n    \n joblib.dump(model, 'sklearn_regression_model.pkl')\n\n\n\n\nThis model as features as an array like:\n\n array([[ 0.03807591,  0.05068012],[ ... , ...]])\n\n\n\n\nAs you can see, I've got the model serialized into a sklearn_regression_model.pkl file, I've also got the ML environment setup in Azure ML, a compute instance, I'm familiar with Python and the designer and prefer notebooks.\n\nHow can I deploy this model through an API like:\n\nhttps:\/\/api.xxx.com&parameters....\n\nThanks for any help!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629821122530,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/526203\/how-to-deploy-a-scikit-learn-regression-model-as-a.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-25T17:00:18.437Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nThanks for reaching out to us. Please refer to this guidance to deploy your model and service. https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=azcli#deploy-again-and-call-your-service\n\nPlease refer to deploy from local file for your scenario. The workflow is similar no matter where you deploy your model:\n\nRegister the model - Please register from local file\nPrepare an entry script\nPrepare an inference configuration\nDeploy the model locally to ensure everything works\nChoose a compute target\nRe-deploy the model to the cloud\nTest the resulting web service\n\nHope this helps.\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"deploi scikit learn regress model web servic hello document relat model deploy overwhelm struggl basic hello world tutori dai research want deploi basic model web servic consum api power web app serv poc think scale docker contain code python import joblib sklearn dataset import load diabet sklearn linear model import ridg dataset dataset load diabet return true model ridg fit dataset dataset joblib dump model sklearn regress model pkl model featur arrai like arrai got model serial sklearn regress model pkl file got environ setup comput instanc familiar python design prefer notebook deploi model api like http api com paramet thank help",
        "Question_preprocessed_content":"deploi scikit learn regress model web servic hello document relat model deploy overwhelm struggl basic hello world tutori dai research want deploi basic model web servic consum api power web app serv poc think scale docker contain code python import joblib import import ridg model model featur arrai like arrai got model serial file got environ setup comput instanc familiar python design prefer notebook deploi model api like thank help",
        "Question_gpt_summary_original":"The user is struggling to deploy a basic scikit learn regression model as a web service that can be consumed via an API through Power BI or any other web app to serve as a POC. Despite having the model serialized into a file and the ML environment set up in Azure ML, the user is finding the documentation related to ML model deployment overwhelming and is unable to deploy the model through an API.",
        "Question_gpt_summary":"user struggl deploi basic scikit learn regress model web servic consum api power web app serv poc despit have model serial file environ set user find document relat model deploy overwhelm unabl deploi model api",
        "Answer_original_content":"hello thank reach refer guidanc deploi model servic http doc microsoft com azur machin learn deploi tab azcli deploi servic refer deploi local file scenario workflow similar matter deploi model regist model regist local file prepar entri script prepar infer configur deploi model local ensur work choos comput target deploi model cloud test result web servic hope help regard yutong",
        "Answer_preprocessed_content":"hello thank reach refer guidanc deploi model servic refer deploi local file scenario workflow similar matter deploi model regist model regist local file prepar entri script prepar infer configur deploi model local ensur work choos comput target model cloud test result web servic hope help regard yutong",
        "Answer_gpt_summary_original":"the answer provides a link to a guidance document on how to deploy a machine learning model as a web service using azure. the guidance suggests registering the model from a local file, preparing an entry script, preparing an inference configuration, deploying the model locally, choosing a compute target, and re-deploying the model to the cloud. the user is advised to test the resulting web service.",
        "Answer_gpt_summary":"answer provid link guidanc document deploi machin learn model web servic azur guidanc suggest regist model local file prepar entri script prepar infer configur deploi model local choos comput target deploi model cloud user advis test result web servic"
    },
    {
        "Question_id":69219230.0,
        "Question_title":"In GCP Vertex AI, why is Delete Training Pipeline REST endpoint unimplemented?",
        "Question_body":"<p>I used <a href=\"https:\/\/github.com\/googleapis\/java-aiplatform\/blob\/master\/google-cloud-aiplatform\/src\/main\/java\/com\/google\/cloud\/aiplatform\/v1\/PipelineServiceClient.java\" rel=\"nofollow noreferrer\">this code<\/a>, straight from the Javadocs, to delete a VertexAI Training Pipeline<\/p>\n<pre><code>try (PipelineServiceClient pipelineServiceClient = PipelineServiceClient.create()) {\n  TrainingPipelineName name =\n      TrainingPipelineName.of(&quot;[PROJECT]&quot;, &quot;[LOCATION]&quot;, &quot;[TRAINING_PIPELINE]&quot;);\n  pipelineServiceClient.deleteTrainingPipelineAsync(name).get();\n}\n<\/code><\/pre>\n<p>I get this error. From what I can see, this means that this API, though officially documented, is simply unimplemented. How do we delete Training Pipelines using Java?<\/p>\n<pre><code>Error in deleting \/\/aiplatform.googleapis.com\/projects\/746859988231\/locations\/us-central1\/trainingPipelines\/186468439399187392: \njava.util.concurrent.ExecutionException: \ncom.google.api.gax.rpc.UnimplementedException: io.grpc.StatusRuntimeException:\nUNIMPLEMENTED: HTTP status code 404\n...\n&lt;!DOCTYPE html&gt;\n&lt;html lang=en&gt;\n....\n  &lt;p&gt;The requested URL &lt;code&gt;\/google.cloud.aiplatform.v1.PipelineService\n\/DeleteTrainingPipeline&lt;\/code&gt; was not found on this server.  \n&lt;ins&gt;That\u2019s all we know.&lt;\/ins&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1631862868123,
        "Question_favorite_count":null,
        "Question_last_edit_time":1631987478696,
        "Question_score":2.0,
        "Question_view_count":259.0,
        "Answer_body":"<p>According to the official documentation, <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest<\/a> , the supported service URLs for this service are:<\/p>\n<pre><code>https:\/\/us-central1-aiplatform.googleapis.com\nhttps:\/\/us-east1-aiplatform.googleapis.com\nhttps:\/\/us-east4-aiplatform.googleapis.com\nhttps:\/\/us-west1-aiplatform.googleapis.com\nhttps:\/\/northamerica-northeast1-aiplatform.googleapis.com\nhttps:\/\/europe-west1-aiplatform.googleapis.com\nhttps:\/\/europe-west2-aiplatform.googleapis.com\nhttps:\/\/europe-west4-aiplatform.googleapis.com\nhttps:\/\/asia-east1-aiplatform.googleapis.com\nhttps:\/\/asia-northeast1-aiplatform.googleapis.com\nhttps:\/\/asia-northeast3-aiplatform.googleapis.com\nhttps:\/\/asia-southeast1-aiplatform.googleapis.com\nhttps:\/\/australia-southeast1-aiplatform.googleapis.com\n<\/code><\/pre>\n<hr \/>\n<pre><code>  PipelineServiceSettings pipelineServiceSettings =\n        PipelineServiceSettings.newBuilder()\n            .setEndpoint(&quot;us-central1-aiplatform.googleapis.com:443&quot;)\n            .build();\n<\/code><\/pre>\n<hr \/>\n<pre><code> try (PipelineServiceClient pipelineServiceClient =\n      PipelineServiceClient.create(pipelineServiceSettings)) {\n\n  String location = &quot;us-central1&quot;;\n  TrainingPipelineName trainingPipelineName =\n      TrainingPipelineName.of(project, location, trainingPipelineId);\n\n  OperationFuture&lt;Empty, DeleteOperationMetadata&gt; operationFuture =\n      pipelineServiceClient.deleteTrainingPipelineAsync(trainingPipelineName);\n\n  System.out.format(&quot;Operation name: %s\\n&quot;, operationFuture.getInitialFuture().get().getName());\n  System.out.println(&quot;Waiting for operation to finish...&quot;);\n  operationFuture.get(300, TimeUnit.SECONDS);\n\n  System.out.format(&quot;Deleted Training Pipeline.&quot;);\n}\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1631888060710,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69219230",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1631875853752,
        "Question_original_content":"gcp delet train pipelin rest endpoint unimpl code straight javadoc delet vertexai train pipelin try pipelineservicecli pipelineservicecli pipelineservicecli creat trainingpipelinenam trainingpipelinenam project locat train pipelin pipelineservicecli deletetrainingpipelineasync error mean api offici document simpli unimpl delet train pipelin java error delet aiplatform googleapi com project locat central trainingpipelin java util concurr executionexcept com googl api gax rpc unimplementedexcept grpc statusruntimeexcept unimpl http statu code request url googl cloud aiplatform pipelineservic deletetrainingpipelin server that know",
        "Question_preprocessed_content":"gcp delet train pipelin rest endpoint unimpl code straight javadoc delet vertexai train pipelin error mean api offici document simpli unimpl delet train pipelin java",
        "Question_gpt_summary_original":"The user encountered a challenge while trying to delete a VertexAI Training Pipeline using Java code from the official documentation. The Delete Training Pipeline REST endpoint is unimplemented, resulting in an error message and making it impossible to delete Training Pipelines using Java.",
        "Question_gpt_summary":"user encount challeng try delet vertexai train pipelin java code offici document delet train pipelin rest endpoint unimpl result error messag make imposs delet train pipelin java",
        "Answer_original_content":"accord offici document http cloud googl com vertex doc refer rest support servic url servic http central aiplatform googleapi com http east aiplatform googleapi com http east aiplatform googleapi com http west aiplatform googleapi com http northamerica northeast aiplatform googleapi com http europ west aiplatform googleapi com http europ west aiplatform googleapi com http europ west aiplatform googleapi com http asia east aiplatform googleapi com http asia northeast aiplatform googleapi com http asia northeast aiplatform googleapi com http asia southeast aiplatform googleapi com http australia southeast aiplatform googleapi com pipelineserviceset pipelineserviceset pipelineserviceset newbuild setendpoint central aiplatform googleapi com build try pipelineservicecli pipelineservicecli pipelineservicecli creat pipelineserviceset string locat central trainingpipelinenam trainingpipelinenam trainingpipelinenam project locat trainingpipelineid operationfutur operationfutur pipelineservicecli deletetrainingpipelineasync trainingpipelinenam format oper operationfutur getinitialfutur getnam println wait oper finish operationfutur timeunit second format delet train pipelin",
        "Answer_preprocessed_content":"accord offici document support servic url servic",
        "Answer_gpt_summary_original":"the answer provides a code snippet that shows how to delete a vertexai training pipeline using java. it also suggests checking the official documentation for the supported service urls for this service.",
        "Answer_gpt_summary":"answer provid code snippet show delet vertexai train pipelin java suggest check offici document support servic url servic"
    },
    {
        "Question_id":73661090.0,
        "Question_title":"How do I give Vertex AI pipeline component permissions?",
        "Question_body":"<p>In a Vertex AI pipeline component,I try:<\/p>\n<pre><code>def my_comp(project_id: str, location: str, endpoint_id: str, endpoint: Output[Artifact]):\n    import google.cloud.aiplatform as aip\n    endpoints = aip.Endpoint.list()\n...\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code>'aiplatform.endpoints.list' denied on resource '\/\/aiplatform.googleapis.com\/projects\/...\n<\/code><\/pre>\n<p>My service account has owner permissions, and it works outside of the component. What do I need to do?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1662721061203,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":62.0,
        "Answer_body":"<p>This permission denied on resource issue can be resolved by using import statement:<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73661090",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1663223342092,
        "Question_original_content":"pipelin compon permiss pipelin compon try def comp project str locat str endpoint str endpoint output artifact import googl cloud aiplatform aip endpoint aip endpoint list give aiplatform endpoint list deni resourc aiplatform googleapi com project servic account owner permiss work outsid compon need",
        "Question_preprocessed_content":"pipelin compon permiss pipelin compon try give servic account owner permiss work outsid compon need",
        "Question_gpt_summary_original":"The user is encountering a challenge in giving Vertex AI pipeline component permissions. They are receiving an error message stating that 'aiplatform.endpoints.list' is denied on a resource, despite having owner permissions on their service account. The user is seeking advice on how to resolve this issue.",
        "Question_gpt_summary":"user encount challeng give pipelin compon permiss receiv error messag state aiplatform endpoint list deni resourc despit have owner permiss servic account user seek advic resolv issu",
        "Answer_original_content":"permiss deni resourc issu resolv import statement googl cloud import aiplatform aiplatform",
        "Answer_preprocessed_content":"permiss deni resourc issu resolv import statement",
        "Answer_gpt_summary_original":"the solution to the permission denied issue when giving pipeline component permissions is to use the import statement \"from google.cloud import aiplatform_v1 as aiplatform\".",
        "Answer_gpt_summary":"solut permiss deni issu give pipelin compon permiss us import statement googl cloud import aiplatform aiplatform"
    },
    {
        "Question_id":null,
        "Question_title":"ERROR: unexpected error - unable to open database file",
        "Question_body":"<p>Hi,<br>\nI am using dvc exp to run different experiment from my pipeline. However when I try to apply the experiment I have the following error: <code>ERROR: unexpected error - unable to open database file <\/code>.<\/p>\n<p>I will describe all the steps that I made:<\/p>\n<ol>\n<li><code>dvc init --subdir<\/code><\/li>\n<li><code>dvc repro<\/code><\/li>\n<li>\n<code>dvc exp run<\/code> \u2026<\/li>\n<li><code>dvc exp push origin experiment_name<\/code><\/li>\n<li>Then I made some commits (with other work)<\/li>\n<li>Then I tried to do <code>dvc exp apply experiment_name<\/code>\n<\/li>\n<li>I get the error: <code>ERROR: unexpected error - unable to open database file<\/code>\n<\/li>\n<li>I did not know that I needed to be in the same project version as when the experiment was run. So I made reset for the commit where I run the experiences<\/li>\n<li><code>dvc exp apply experiment_name<\/code><\/li>\n<li>I got the same error: <code>ERROR: unexpected error - unable to open database file<\/code>\n<\/li>\n<li><code>dvc exp pull origin experiment_name<\/code><\/li>\n<li>I got the same error: <code>ERROR: unexpected error - unable to open database file<\/code>\n<\/li>\n<\/ol>\n<p>Can anyone help me to understant how can I resolve this issue?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1640012114896,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":683.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-unable-to-open-database-file\/1015",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-21T02:13:43.289Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"dsmendes\" data-post=\"1\" data-topic=\"1015\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/d\/7cd45c\/40.png\" class=\"avatar\"> dsmendes:<\/div>\n<blockquote>\n<p>Can anyone help me to understant how can I resolve this issue?<\/p>\n<\/blockquote>\n<\/aside>\n<p>Hi, <a class=\"mention\" href=\"\/u\/dsmendes\">@dsmendes<\/a><br>\nFrom the current information, it is still not clear what problem with it. Could you please:<\/p>\n<ol>\n<li>provide the the dvc version and environment with <code>dvc doctor<\/code>\n<\/li>\n<li>provide a detailed log with <code>-vv<\/code> flag after your command.<\/li>\n<\/ol>\n<p>database file error usually caused from <code>Sqlite<\/code> handing files in <code>network<\/code> or with wrong permission.<\/p>",
                "Answer_score":21.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-23T00:06:51.383Z",
                "Answer_body":"<p>I could not reproduce the behaviour explained in the post. I repeated all the work with some differences and it worked:<\/p>\n<ol>\n<li><code>dvc repro<\/code><\/li>\n<li><code>dvc push<\/code><\/li>\n<li>\n<code>dvc exp run<\/code> \u2026<\/li>\n<li><code>dvc exp push origin experiment_name<\/code><\/li>\n<li><code>dvc exp apply commit_of_experience<\/code><\/li>\n<\/ol>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error unexpect error unabl open databas file exp run differ experi pipelin try appli experi follow error error unexpect error unabl open databas file step init subdir repro exp run exp push origin experi commit work tri exp appli experi error error unexpect error unabl open databas file know need project version experi run reset commit run experi exp appli experi got error error unexpect error unabl open databas file exp pull origin experi got error error unexpect error unabl open databas file help underst resolv issu thank",
        "Question_preprocessed_content":"error unexpect error unabl open databas file exp run differ experi pipelin try appli experi follow error step commit tri error know need project version experi run reset commit run experi got error got error help underst resolv issu thank",
        "Question_gpt_summary_original":"The user is encountering an error message \"unable to open database file\" while trying to apply an experiment using dvc exp. The user has tried resetting the commit and pulling the experiment from origin, but the error persists. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount error messag unabl open databas file try appli experi exp user tri reset commit pull experi origin error persist user seek help resolv issu",
        "Answer_original_content":"dsmend help underst resolv issu dsmend current inform clear problem provid version environ doctor provid detail log flag command databas file error usual caus sqlite hand file network wrong permiss reproduc behaviour explain post repeat work differ work repro push exp run exp push origin experi exp appli commit experi",
        "Answer_preprocessed_content":"dsmend help underst resolv issu current inform clear problem provid version environ provid detail log flag command databas file error usual caus hand file wrong permiss reproduc behaviour explain post repeat work differ work",
        "Answer_gpt_summary_original":"possible solutions from the answer include providing the version and environment with doctor, providing a detailed log with -vv flag after the command, checking if the database file error is caused by sqlite handling files in the network or with wrong permission, and trying to repeat the work with some differences.",
        "Answer_gpt_summary":"possibl solut answer includ provid version environ doctor provid detail log flag command check databas file error caus sqlite handl file network wrong permiss try repeat work differ"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning - endpoint failure 502 when testing the model",
        "Question_body":"Very basic example from ms-learn does not work!!! So frustrating\n\n\n\n\n\n\n\n\n\nfrom: https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/use-automated-machine-learning\/\n\nCreated a model\n\n\nmodel deployment to ACI\n\n\nI copied the generated py code from the Consume tab into a notebook in the same service\n\n\nget 502",
        "Question_answer_count":4,
        "Question_comment_count":2,
        "Question_creation_time":1642932141937,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/706183\/azure-machine-learning-endpoint-failure-502-tesing.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-24T11:38:32.437Z",
                "Answer_score":0,
                "Answer_body":"@maciejg-2112 Is the deployed service or endpoint in a healthy state? On the Details tab of your endpoint the Deployment state should be healthy and if you also use the test tab the response should be successful.\nI have followed the tutorial and have an endpoint setup which is working as expected with the code from consume tab.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-01T18:14:01.027Z",
                "Answer_score":1,
                "Answer_body":"Additional details:\n\nRemoving:\n\n ,\n     \"GlobalParameters\": {\n     }\n\n\n\nfrom the data variable (in the Endpoints\/Consume code) will result in a proper result.",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-03T06:37:08.647Z",
                "Answer_score":4,
                "Answer_body":"I've experienced the same problem and I've made the case to the Microsoft support. And I've got the correct Python code as follows.\n\nThe problem come from lack of GlobalParameters in the next sentence.\ninput_json = json.dumps({\"Inputs\":{\"data\": x}, \"GlobalParameters\": 1.0})\n\nendpoint = 'YOUR_ENDPOINT' #Replace with your endpoint\nkey = 'YOUR_KEY' #Replace with your key\n\nimport json\nimport requests\n\nAn array of features based on five-day weather forecast\n\nx = [[1,1,2022,1,0,6,0,2,0.344167,0.363625,0.805833,0.160446],\n[2,1,2022,1,0,0,0,2,0.363478,0.353739,0.696087,0.248539],\n[3,1,2022,1,0,1,1,1,0.196364,0.189405,0.437273,0.248309],\n[4,1,2022,1,0,2,1,1,0.2,0.212122,0.590435,0.160296],\n[5,1,2022,1,0,3,1,1,0.226957,0.22927,0.436957,0.1869]]\n\nConvert the array to JSON format\n\ninput_json = json.dumps({\"Inputs\":{\"data\": x}, \"GlobalParameters\": 1.0})\n\nSet the content type and authentication for the request\n\nheaders = {\"Content-Type\":\"application\/json\",\n\"Authorization\":\"Bearer \" + key}\n\nSend the request\n\nresponse = requests.post(endpoint, input_json, headers=headers)\n\nIf we got a valid response, display the predictions\n\nif response.status_code == 200:\ny = response.json()\nprint(\"Predictions:\")\nfor i in range(len(x)):\nprint (\" Day: {}. Predicted rentals: {}\".format(i+1, max(0, round(y[\"Results\"][i]))))\nelse:\nprint(response)\n\n\n\n\n\nbest regards,",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-08T08:58:30.45Z",
                "Answer_score":0,
                "Answer_body":"Thanks @IIJGlobalYamamotoMitsuyoshi-6912 for the fix.\n\nIt's quite frustrating that it's not true someone taking a beginners training can follow the advice: \"Don't worry too much about the details of the code.\" (https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/use-automated-machine-learning\/7-deploy-model).\nThe persona has to either troubleshoot while the $ counter is running or give up on the exercise.\n\nBTW it would be great to have sandboxes for AI-900 like there are for DP-900",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"endpoint failur test model basic exampl learn work frustrat http doc microsoft com learn modul us autom machin learn creat model model deploy aci copi gener code consum tab notebook servic",
        "Question_preprocessed_content":"endpoint failur test model basic exampl work frustrat creat model model deploy aci copi gener code consum tab notebook servic",
        "Question_gpt_summary_original":"The user encountered a challenge with Azure Machine Learning endpoint failure 502 when testing a model. The issue occurred after creating and deploying the model to ACI, and copying the generated py code from the Consume tab into a notebook in the same service. The user expressed frustration with the basic example from ms-learn not working.",
        "Question_gpt_summary":"user encount challeng endpoint failur test model issu occur creat deploi model aci copi gener code consum tab notebook servic user express frustrat basic exampl learn work",
        "Answer_original_content":"maciejg deploi servic endpoint healthi state detail tab endpoint deploy state healthi us test tab respons success follow tutori endpoint setup work expect code consum tab addit detail remov globalparamet data variabl endpoint consum code result proper result experienc problem case microsoft support got correct python code follow problem come lack globalparamet sentenc input json json dump input data globalparamet endpoint endpoint replac endpoint kei kei replac kei import json import request arrai featur base dai weather forecast convert arrai json format input json json dump input data globalparamet set content type authent request header content type applic json author bearer kei send request respons request post endpoint input json header header got valid respons displai predict respons statu code respons json print predict rang len print dai predict rental format max round result print respons best regard thank iijglobalyamamotomitsuyoshi fix frustrat true take beginn train follow advic worri detail code http doc microsoft com learn modul us autom machin learn deploi model persona troubleshoot counter run exercis btw great sandbox like",
        "Answer_preprocessed_content":"deploi servic endpoint healthi state detail tab endpoint deploy state healthi us test tab respons success follow tutori endpoint setup work expect code consum tab addit detail remov globalparamet data variabl result proper result experienc problem case microsoft support got correct python code follow problem come lack globalparamet sentenc globalparamet endpoint replac endpoint kei replac kei import json import request arrai featur base weather forecast convert arrai json format globalparamet set content type authent request header send request respons header header got valid respons displai predict print rang print print best regard thank fix frustrat true take beginn train follow advic worri detail persona troubleshoot counter run exercis btw great sandbox like",
        "Answer_gpt_summary_original":"possible solutions to the 502 endpoint failure issue are checking if the deployed service or endpoint is in a healthy state, ensuring that the deployment state is healthy on the details tab of the endpoint, and using the test tab to verify that the response is successful. additionally, removing \", \"globalparameters\": { }\" from the data variable in the endpoints\/consume code can result in a proper result. the problem may come from the lack of globalparameters in the input_json variable. the solution involves converting the array to json format, setting the content type and authentication for the request headers, and sending the request. if a valid response is received, the predictions can be displayed.",
        "Answer_gpt_summary":"possibl solut endpoint failur issu check deploi servic endpoint healthi state ensur deploy state healthi detail tab endpoint test tab verifi respons success addition remov globalparamet data variabl endpoint consum code result proper result problem come lack globalparamet input json variabl solut involv convert arrai json format set content type authent request header send request valid respons receiv predict displai"
    },
    {
        "Question_id":66020144.0,
        "Question_title":"How can one download the outputs of historical Azure ML experiment Runs via the python API",
        "Question_body":"<p>I'm trying to write a script which can download the outputs from an Azure ML experiment Run after the fact.<\/p>\n<p>Essentially, I want to know how I can get a Run by its <code>runId<\/code> property (or some other identifier).<\/p>\n<p>I am aware that I have access to the Run object when I create it for the purposes of training. What I want is a way to recreate this Run object later in a separate script, possibly from a completely different environment.<\/p>\n<p>What I've found so far is a way to get a list of ScriptRun objects from an experiment via the <code>get_runs()<\/code> function. But I don't see a way to use one of these ScriptRun objects to create a Run object representing the original Run and allowing me to download the outputs.<\/p>\n<p>Any help appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1612316926667,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":1402.0,
        "Answer_body":"<p>I agree that this could probably be better documented, but fortunately, it's a simple implementation.<\/p>\n<p>this is how you get a run object for an already submitted run for <code>azureml-sdk&gt;=1.16.0<\/code> (for the older approach <a href=\"https:\/\/stackoverflow.com\/questions\/62949488\/amls-experiment-run-stuck-in-status-running\/62958369#62958369\">see my answer here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace\n\nws = Workspace.from_config()\nrun = ws.get_run('YOUR_RUN_ID')\n<\/code><\/pre>\n<p>once you have the <code>run<\/code> object, you can call methods like<\/p>\n<ul>\n<li><code>.get_file_names()<\/code> to see what files are available (the logs in <code>azureml-logs\/<\/code> and <code>logs\/azureml\/<\/code> will also be listed)<\/li>\n<li><code>.download_file()<\/code> to download an individual file<\/li>\n<li><code>.download_files()<\/code> to download all files that match a given prefix (or all the files)<\/li>\n<\/ul>\n<p>See the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py&amp;WT.mc_id=AI-MVP-5003930\" rel=\"noreferrer\">Run object docs<\/a> for more details.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66020144",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1612384991070,
        "Question_original_content":"download output histor experi run python api try write script download output experi run fact essenti want know run runid properti identifi awar access run object creat purpos train want wai recreat run object later separ script possibl complet differ environ far wai list scriptrun object experi run function wai us scriptrun object creat run object repres origin run allow download output help appreci",
        "Question_preprocessed_content":"download output histor experi run python api try write script download output experi run fact essenti want know run properti awar access run object creat purpos train want wai recreat run object later separ script possibl complet differ environ far wai list scriptrun object experi function wai us scriptrun object creat run object repres origin run allow download output help appreci",
        "Question_gpt_summary_original":"The user is facing challenges in downloading the outputs of an Azure ML experiment Run after the fact. They are trying to find a way to recreate the Run object later in a separate script, possibly from a completely different environment, but are unable to do so. They have found a way to get a list of ScriptRun objects from an experiment via the get_runs() function, but are unable to use these objects to create a Run object representing the original Run and allowing them to download the outputs.",
        "Question_gpt_summary":"user face challeng download output experi run fact try wai recreat run object later separ script possibl complet differ environ unabl wai list scriptrun object experi run function unabl us object creat run object repres origin run allow download output",
        "Answer_original_content":"agre probabl better document fortun simpl implement run object submit run sdk older approach answer core import workspac workspac config run run run run object method like file name file avail log log log list download file download individu file download file download file match given prefix file run object doc detail",
        "Answer_preprocessed_content":"agre probabl better document fortun simpl implement run object submit run object method like file avail download individu file download file match given prefix run object doc detail",
        "Answer_gpt_summary_original":"possible solutions to the user's problem of recreating a run object from a different environment are provided in the answer. the solution involves getting a run object for an already submitted run using the code provided. once the run object is obtained, methods like .get_file_names(), .download_file(), and .download_files() can be called to see available files, download individual files, or download all files that match a given prefix. the user is advised to refer to the run object documentation for more details.",
        "Answer_gpt_summary":"possibl solut user problem recreat run object differ environ provid answer solut involv get run object submit run code provid run object obtain method like file name download file download file call avail file download individu file download file match given prefix user advis refer run object document detail"
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker - S3 bucket access when logged into us-east-1",
        "Question_body":"My IAM works, but when I login, it logs me into us-east-1. My project is on the S3 instance (sagemaker tool). How can I access buckets in S3 if I am logged into us-east-1 by default.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1652377507550,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":76.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVgOCxivDSgiiHh-HZ9aYYA\/sagemaker-s-3-bucket-access-when-logged-into-us-east-1",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-13T13:01:21.591Z",
                "Answer_score":0,
                "Answer_body":"You can change the region when you are inside a SageMaker notebook or python script. Here you can find many examples of how to access S3 buckets from SageMaker. Please elaborate more on the issue you are facing so we can help you.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"bucket access log east iam work login log east project instanc tool access bucket log east default",
        "Question_preprocessed_content":"bucket access log iam work login log project instanc access bucket log default",
        "Question_gpt_summary_original":"The user is facing a challenge in accessing S3 buckets in their project on the Sagemaker tool as they are automatically logged into us-east-1 when they log in, despite their IAM working. They are seeking a solution to access the S3 instance while logged into us-east-1.",
        "Question_gpt_summary":"user face challeng access bucket project tool automat log east log despit iam work seek solut access instanc log east",
        "Answer_original_content":"chang region insid notebook python script exampl access bucket elabor issu face help",
        "Answer_preprocessed_content":"chang region insid notebook python script exampl access bucket elabor issu face help",
        "Answer_gpt_summary_original":"possible solutions: \n- change the region when inside a notebook or python script.\n- look for examples of how to access s3 buckets from the current region.\n- provide more details about the issue to receive further assistance. \n\nsummary: the user can change the region or look for examples to access the s3 bucket from the current region. they should provide more details about the issue to receive further assistance.",
        "Answer_gpt_summary":"possibl solut chang region insid notebook python script look exampl access bucket current region provid detail issu receiv assist summari user chang region look exampl access bucket current region provid detail issu receiv assist"
    },
    {
        "Question_id":null,
        "Question_title":"How to save\/restore model using artifact on servers that do not have internet access?",
        "Question_body":"<p>Hi,<\/p>\n<p>I just started using wandb tools. According to the instruction <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/save\">here<\/a>, it suggests using Artifact for new code to save models. And I am able to save the model in the offline mode. However, I wonder how to restore the model from an artifact with a particular version (e.g., v3, not necessarily the latest version of the artifact) if I want to resume the training after it\u2019s interrupted?<\/p>\n<p>I am running code on compute nodes that do not have access to internet, so I have to use the offline mode. And in offline mode, I cannot run <code>use_artifact<\/code> command as it only works in online mode. However, I think all the artifact data is already saved locally using <code>log_artifact<\/code> command. So in theory, I should be able to restore a particular version of the artifact? How can I get that? Even though I know the model file location, it is the latest model file, not a particular history version of the model file (that\u2019s the point of using Artifact to track the model?).<\/p>\n<pre><code class=\"lang-auto\">    run = wandb.init(mode='offline', project='test')\n    artifact = run.use_artifact('hello-world:v3')\n    artifact_dir = artifact.download(root=run.dir)\n<\/code><\/pre>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1637040280243,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":300.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-save-restore-model-using-artifact-on-servers-that-do-not-have-internet-access\/1313",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-23T16:12:26.922Z",
                "Answer_body":"<p>Hey Tao,<\/p>\n<p>checking on this for you.<\/p>\n<p>Best,<br>\nArman<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-30T13:56:39.151Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/armanharutyunyan\">@armanharutyunyan<\/a> Hi Arman, did you find a solution by any chance? Thanks.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-02T10:59:40.420Z",
                "Answer_body":"<p>Hey Tao, sorry about the delay on this. The offline run will be logged within your <code>wandb<\/code> folder that gets generated. You can access the run given the <code>run_id<\/code> by constructing the path and accessing the <code>files<\/code> within the directory. Other than that there is no way to access offline runs<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-31T11:00:13.207Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"save restor model artifact server internet access start tool accord instruct suggest artifact new code save model abl save model offlin mode wonder restor model artifact particular version necessarili latest version artifact want resum train interrupt run code comput node access internet us offlin mode offlin mode run us artifact command work onlin mode think artifact data save local log artifact command theori abl restor particular version artifact know model file locat latest model file particular histori version model file that point artifact track model run init mode offlin project test artifact run us artifact hello world artifact dir artifact download root run dir",
        "Question_preprocessed_content":"model artifact server internet access start tool accord instruct suggest artifact new code save model abl save model offlin mode wonder restor model artifact particular version want resum train interrupt run code comput node access internet us offlin mode offlin mode run command work onlin mode think artifact data save local command theori abl restor particular version artifact know model file locat latest model file particular histori version model file",
        "Question_gpt_summary_original":"The user is facing challenges in restoring a specific version of a model from an artifact using Wandb tools in offline mode. They are unable to use the \"use_artifact\" command in offline mode and are looking for a way to restore a particular version of the artifact. They have tried using the \"log_artifact\" command to save the model locally but are unsure how to retrieve a specific version of the artifact.",
        "Question_gpt_summary":"user face challeng restor specif version model artifact tool offlin mode unabl us us artifact command offlin mode look wai restor particular version artifact tri log artifact command save model local unsur retriev specif version artifact",
        "Answer_original_content":"hei tao check best arman armanharutyunyan arman solut chanc thank hei tao sorri delai offlin run log folder get gener access run given run construct path access file directori wai access offlin run topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"hei tao check best arman arman solut chanc thank hei tao sorri delai offlin run log folder get gener access run given construct path access directori wai access offlin run topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"possible solutions: \n- access the offline run within the folder that gets generated and retrieve the run_id to construct the path and access the files within the directory.",
        "Answer_gpt_summary":"possibl solut access offlin run folder get gener retriev run construct path access file directori"
    },
    {
        "Question_id":null,
        "Question_title":"How to use a working pipeline on live dataset?",
        "Question_body":"Hello,\nI would like to ask a bit help in my custom pipeline because I am stuck at that point where my custom pipeline is working and published but I cannot test it with \"live\" dataset.\nI tried to follow this article but I have not found my algorithm's name also the endpoint and model menus are empty:\nhttps:\/\/docs.microsoft.com\/en-us\/learn\/modules\/use-automated-machine-learning\/deploy-model\nCould anyone help how to use my custom pipeline at live dataset?\nThank you",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1635766961413,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/611229\/how-to-use-a-working-pipeline-on-live-dataset.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-02T01:30:39.207Z",
                "Answer_score":0,
                "Answer_body":"Hi, from your post, I'd assume you've deployed your model. So, click on Endpoints > Pipeline Endpoints > Select a specific pipeline to run, consume, or review results of previous runs of the pipeline endpoint. You can use a Client Application or PowerBI to consume the webservice. Feel free to review train and deploy regression model example.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-02T19:32:29Z",
                "Answer_score":0,
                "Answer_body":"Hello GiftA,\nFirst of all, thank you for your reply.\nUnfortunately, this where i am stuck the \"model\" menu is empty at my side (screenshot), i suppose i have not deployed my pipeline yet.\n\n\n\n\nFurther on, I do not have this \"Pipeline Endpoints\" menu what you mentioned before: \"\"Endpoints > Pipeline Endpoints > Select a specific pipeline to run\"\"",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"us work pipelin live dataset hello like ask bit help custom pipelin stuck point custom pipelin work publish test live dataset tri follow articl algorithm endpoint model menu http doc microsoft com learn modul us autom machin learn deploi model help us custom pipelin live dataset thank",
        "Question_preprocessed_content":"us work pipelin live dataset hello like ask bit help custom pipelin stuck point custom pipelin work publish test live dataset tri follow articl algorithm endpoint model menu help us custom pipelin live dataset thank",
        "Question_gpt_summary_original":"The user is facing challenges in testing their custom pipeline with a live dataset. They have followed a tutorial but cannot find their algorithm's name and the endpoint and model menus are empty. They are seeking help on how to use their custom pipeline with a live dataset.",
        "Question_gpt_summary":"user face challeng test custom pipelin live dataset follow tutori algorithm endpoint model menu seek help us custom pipelin live dataset",
        "Answer_original_content":"post assum deploi model click endpoint pipelin endpoint select specif pipelin run consum review result previou run pipelin endpoint us client applic powerbi consum webservic feel free review train deploi regress model exampl kindli accept answer inform help thank hello gifta thank repli unfortun stuck model menu screenshot suppos deploi pipelin pipelin endpoint menu mention endpoint pipelin endpoint select specif pipelin run",
        "Answer_preprocessed_content":"post assum deploi model click endpoint pipelin endpoint select specif pipelin run consum review result previou run pipelin endpoint us client applic powerbi consum webservic feel free review train deploi regress model exampl kindli accept answer inform help thank hello gifta thank repli unfortun stuck model menu suppos deploi pipelin pipelin endpoint menu mention endpoint pipelin endpoint select specif pipelin run",
        "Answer_gpt_summary_original":"possible solutions from the answer include: \n1. click on endpoints > pipeline endpoints > select a specific pipeline to run, consume, or review results of previous runs of the pipeline endpoint. \n2. use a client application or powerbi to consume the webservice. \n3. review the train and deploy regression model example. \n\nhowever, it is unclear if these solutions will directly address the user's issue as they have stated that they have not yet deployed their pipeline and do not have the \"pipeline endpoints\" menu.",
        "Answer_gpt_summary":"possibl solut answer includ click endpoint pipelin endpoint select specif pipelin run consum review result previou run pipelin endpoint us client applic powerbi consum webservic review train deploi regress model exampl unclear solut directli address user issu state deploi pipelin pipelin endpoint menu"
    },
    {
        "Question_id":null,
        "Question_title":"Deleted Azure ML Key Vault",
        "Question_body":"Does anyone have an idea if there's a way to associate an existing ML resource with a different key vault, storage account, etc? The names generated for these resources when you create a ML resource do not match our naming convention, plus, our key vault associated with the ML resource was accidentally deleted.\n\nEDIT: hoping someone has a good solution, but for now, the only thing i can think of is Terraform. I'll pursue that and update the thread",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597243360113,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/65422\/deleted-azure-ml-key-vault.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-08-13T09:57:04.29Z",
                "Answer_score":0,
                "Answer_body":"@EnsembleMark2-1646 Once a Azure ML workspace is created there is no option to change the key vault id on the workspace properties. This is documented here in Azure documentation.\n\nOnce a workspace has been created, you cannot change the settings for confidential data, encryption, key vault ID, or key identifiers. To change these values, you must create a new workspace using the new values.\n\nHowever, if you have enabled soft delete for your key vault it can be restored and your workspace should not be effected after a restore. If you have not enabled this feature then you can enable this after creating a new workspace.\n\nAlso, you can use an existing storage account and keyvault while creating a new workspace by using this ARM template by following the steps mentioned here.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"delet kei vault idea wai associ exist resourc differ kei vault storag account name gener resourc creat resourc match name convent plu kei vault associ resourc accident delet edit hope good solut thing think terraform pursu updat thread",
        "Question_preprocessed_content":"delet kei vault idea wai associ exist resourc differ kei vault storag account name gener resourc creat resourc match name convent plu kei vault associ resourc accident delet edit hope good solut thing think terraform pursu updat thread",
        "Question_gpt_summary_original":"The user is facing a challenge of associating an existing Azure ML resource with a different key vault and storage account. The naming convention of the resources generated by Azure ML does not match their organization's convention, and their key vault associated with the ML resource was accidentally deleted. The user is considering using Terraform as a solution.",
        "Question_gpt_summary":"user face challeng associ exist resourc differ kei vault storag account name convent resourc gener match organ convent kei vault associ resourc accident delet user consid terraform solut",
        "Answer_original_content":"ensemblemark workspac creat option chang kei vault workspac properti document azur document workspac creat chang set confidenti data encrypt kei vault kei identifi chang valu creat new workspac new valu enabl soft delet kei vault restor workspac effect restor enabl featur enabl creat new workspac us exist storag account keyvault creat new workspac arm templat follow step mention",
        "Answer_preprocessed_content":"workspac creat option chang kei vault workspac properti document azur document workspac creat chang set confidenti data encrypt kei vault kei identifi chang valu creat new workspac new valu enabl soft delet kei vault restor workspac effect restor enabl featur enabl creat new workspac us exist storag account keyvault creat new workspac arm templat follow step mention",
        "Answer_gpt_summary_original":"possible solutions from the answer are: \n\n1. if soft delete is enabled for the key vault, it can be restored and the workspace should not be affected after a restore. \n2. if soft delete is not enabled, it can be enabled after creating a new workspace. \n3. create a new workspace using the new values if you want to change the settings for confidential data, encryption, key vault id, or key identifiers. \n4. use an existing storage account and key vault while creating a new workspace by following the steps mentioned in the provided arm template.",
        "Answer_gpt_summary":"possibl solut answer soft delet enabl kei vault restor workspac affect restor soft delet enabl enabl creat new workspac creat new workspac new valu want chang set confidenti data encrypt kei vault kei identifi us exist storag account kei vault creat new workspac follow step mention provid arm templat"
    },
    {
        "Question_id":66231467.0,
        "Question_title":"How to set a minimum number of epoch in Optuna SuccessiveHalvingPruner()?",
        "Question_body":"<p>I'm using Optuna 2.5 to optimize a couple of hyperparameters on a tf.keras CNN model. I want to use pruning so that the optimization skips the less promising corners of the hyperparameters space. I'm using something like this:<\/p>\n<pre><code>study0 = optuna.create_study(study_name=study_name,\n                             storage=storage_name,\n                             direction='minimize', \n                             sampler=TPESampler(n_startup_trials=25, multivariate=True, seed=123),\n                             pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource='auto',\n                             reduction_factor=4, min_early_stopping_rate=0),\n                             load_if_exists=True)\n<\/code><\/pre>\n<p>Sometimes the model stops after 2 epochs, some other times it stops after 12 epochs, 48 and so forth. What I want is to ensure that the model always trains at least 30 epochs before being pruned. I guess that the parameter <code>min_early_stopping_rate<\/code> might have some control on this but I've tried to change it from 0 to 30 and then  the models never get pruned. Can someone explain me a bit better than the Optuna documentation, what these parameters in the  <code>SuccessiveHalvingPruner()<\/code> really do (specially <code>min_early_stopping_rate<\/code>)?\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1613506117800,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":568.0,
        "Answer_body":"<p><code>min_resource<\/code>'s explanation on <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.pruners.SuccessiveHalvingPruner.html\" rel=\"nofollow noreferrer\">the documentation<\/a> says<\/p>\n<blockquote>\n<p>A trial is never pruned until it executes <code>min_resource * reduction_factor ** min_early_stopping_rate<\/code> steps.<\/p>\n<\/blockquote>\n<p>So, I suppose that we need to replace the value of <code>min_resource<\/code> with a specific number depending on <code>reduction_factor<\/code> and <code>min_early_stopping_rate<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66231467",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1616058414416,
        "Question_original_content":"set minimum number epoch successivehalvingprun optim coupl hyperparamet kera cnn model want us prune optim skip promis corner hyperparamet space like studi creat studi studi studi storag storag direct minim sampler tpesampl startup trial multivari true seed pruner pruner successivehalvingprun min resourc auto reduct factor min earli stop rate load exist true model stop epoch time stop epoch forth want ensur model train epoch prune guess paramet min earli stop rate control tri chang model prune explain bit better document paramet successivehalvingprun special min earli stop rate thank",
        "Question_preprocessed_content":"set minimum number epoch successivehalvingprun optim coupl hyperparamet cnn model want us prune optim skip promis corner hyperparamet space like model stop epoch time stop epoch forth want ensur model train epoch prune guess paramet control tri chang model prune explain bit better document paramet thank",
        "Question_gpt_summary_original":"The user is facing challenges in setting a minimum number of epochs in Optuna SuccessiveHalvingPruner() while optimizing hyperparameters on a tf.keras CNN model. The user wants to ensure that the model always trains for at least 30 epochs before being pruned, but changing the min_early_stopping_rate parameter from 0 to 30 did not work. The user is seeking a better understanding of the parameters in SuccessiveHalvingPruner(), especially min_early_stopping_rate.",
        "Question_gpt_summary":"user face challeng set minimum number epoch successivehalvingprun optim hyperparamet kera cnn model user want ensur model train epoch prune chang min earli stop rate paramet work user seek better understand paramet successivehalvingprun especi min earli stop rate",
        "Answer_original_content":"min resourc explan document sai trial prune execut min resourc reduct factor min earli stop rate step suppos need replac valu min resourc specif number depend reduct factor min earli stop rate",
        "Answer_preprocessed_content":"explan document sai trial prune execut step suppos need replac valu specif number depend",
        "Answer_gpt_summary_original":"Possible solutions from the answer are:\n- Replace the value of `min_resource` with a specific number depending on `reduction_factor` and `min_early_stopping_rate`.",
        "Answer_gpt_summary":"possibl solut answer replac valu min resourc specif number depend reduct factor min earli stop rate"
    },
    {
        "Question_id":58806807.0,
        "Question_title":"Create a predictor from an endpoint in a different region",
        "Question_body":"<p>I have created an endpoint on us-east-1. try to create a predictor:<\/p>\n\n<pre><code>In [106]: sagemaker.predictor.RealTimePredictor(&lt;endpoint name&gt;)\n<\/code><\/pre>\n\n<p>and get<\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the DescribeEndpoint operation: \nCould not find endpoint \"arn:aws:sagemaker:us-east-2:&lt;account number&gt;:endpoint\/&lt;endpoint name&gt;\".\n<\/code><\/pre>\n\n<p>which is perfectly correct, since the endpoint is on us-east-1.  Probably I could change some defaults, but I'd rather not - I work on us-east-2 99% of the time.<\/p>\n\n<p>So, how can I set a different region when initializing the predictor?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1573497062720,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":478.0,
        "Answer_body":"<p>The (python) <code>Predictors<\/code> <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"nofollow noreferrer\">documentation<\/a> shows that you can pass a <code>Session<\/code> object. In turn, the <code>Session<\/code> can be <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.Session\" rel=\"nofollow noreferrer\">initialized<\/a> with a <em>client<\/em> and a <em>runtime client<\/em> - the former does everything except endpoint invocations, the latter does... endpoint invocations.<\/p>\n\n<p>Those clients are tied to specific regions. It seems like you should be able to set the runtime client region to match your endpoint, by manually instantiating it, while leaving the regular client alone (disclaimer here: I haven't tried this - if you do, let me\/us know how it goes :)).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58806807",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1573497881147,
        "Question_original_content":"creat predictor endpoint differ region creat endpoint east try creat predictor predictor realtimepredictor clienterror error occur validationexcept call describeendpoint oper endpoint arn aw east endpoint perfectli correct endpoint east probabl chang default work east time set differ region initi predictor",
        "Question_preprocessed_content":"creat predictor endpoint differ region creat endpoint try creat predictor perfectli correct endpoint probabl chang default work time set differ region initi predictor",
        "Question_gpt_summary_original":"The user is facing a challenge in creating a predictor from an endpoint in a different region. They have created an endpoint on us-east-1 but are unable to create a predictor on us-east-2 due to a validation error. The user is unsure how to set a different region when initializing the predictor.",
        "Question_gpt_summary":"user face challeng creat predictor endpoint differ region creat endpoint east unabl creat predictor east valid error user unsur set differ region initi predictor",
        "Answer_original_content":"python predictor document show pass session object turn session initi client runtim client endpoint invoc endpoint invoc client ti specif region like abl set runtim client region match endpoint manual instanti leav regular client disclaim haven tri let know goe",
        "Answer_preprocessed_content":"document show pass object turn initi client runtim client endpoint invoc endpoint invoc client ti specif region like abl set runtim client region match endpoint manual instanti leav regular client",
        "Answer_gpt_summary_original":"the solution to creating a predictor from an endpoint in a different region is to pass a session object and initialize it with a client and a runtime client. the runtime client can be manually instantiated to match the endpoint region while leaving the regular client alone. however, this solution has not been tested yet.",
        "Answer_gpt_summary":"solut creat predictor endpoint differ region pass session object initi client runtim client runtim client manual instanti match endpoint region leav regular client solut test"
    },
    {
        "Question_id":60429339.0,
        "Question_title":"Update live AWS Sagemaker auto scaled endpoint instance type without putting it down",
        "Question_body":"<p>I have one live AWS Sagemaker endpoint where we have auto scaled enabled. \nNow I want to updated it from 'ml.t2.xlarge' to 'ml.t2.2xlarge' but it is showing this error <\/p>\n\n<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the \nUpdateEndpoint operation: The variant(s) \"[config1]\" must be deregistered as scalable targets with \nApplication Auto Scaling before they can be removed or have their instance type updated.\n<\/code><\/pre>\n\n<p>I believe we need to first de-register auto-scaling using this link \n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html<\/a><\/p>\n\n<p>but I doubt if will take our application down and the new model with training will take multiple hours. We can't afford this so please let me know if there are any better way to do it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1582793187027,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":869.0,
        "Answer_body":"<p>You should have no problem updating your Endpoint instance type without taking the availability hit. The basic method looks like this when you have an active autoscaling policy:<\/p>\n\n<ol>\n<li>Create a new EndpointConfig that uses the new instance type, <code>ml.t2.2xlarge<\/code>\n\n<ol>\n<li>Do this by calling <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\"><code>CreateEndpointConfig<\/code><\/a>.<\/li>\n<li>Pass in the same values you used for your previous Endpoint config. You can point to the same <code>ModelName<\/code> that you did as well. By reusing the same model, you don't have to retrain it or anything<\/li>\n<\/ol><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html\" rel=\"nofollow noreferrer\">Delete the existing autoscaling policy<\/a>\n\n<ol>\n<li>Depending on your autoscaling, you might want to increase the desired count of your Endpoint in the event it needs to scale while you are doing this.<\/li>\n<li>If you are experience a spike in traffic while you are making these API calls, you risk an outage of your model if it can't keep up with traffic. Just keep this in mind and possibly scale in advance for this possibility.<\/li>\n<\/ol><\/li>\n<li>Call <code>UpdateEndpoint<\/code> like you did previously and specify this new <code>EndpointConfigName<\/code><\/li>\n<li>Wait for your Endpoint status to be <code>InService<\/code>. This should take 10-20 mins.<\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-add-policy.html\" rel=\"nofollow noreferrer\">Create a new autoscaling policy<\/a> for this new Endpoint and production variant<\/li>\n<\/ol>\n\n<p>You should be good to go without sacrificing availability.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60429339",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1582835892307,
        "Question_original_content":"updat live auto scale endpoint instanc type put live endpoint auto scale enabl want updat xlarg xlarg show error botocor except clienterror error occur validationexcept call updateendpoint oper variant config deregist scalabl target applic auto scale remov instanc type updat believ need regist auto scale link http doc aw amazon com latest endpoint auto scale delet html doubt applic new model train multipl hour afford let know better wai",
        "Question_preprocessed_content":"updat live auto scale endpoint instanc type put live endpoint auto scale enabl want updat show error believ need link doubt applic new model train multipl hour afford let know better wai",
        "Question_gpt_summary_original":"The user is facing a challenge in updating the instance type of a live AWS Sagemaker endpoint with auto scaling enabled. They received an error message stating that the variant(s) must be deregistered as scalable targets with Application Auto Scaling before updating the instance type. The user is concerned that de-registering auto scaling may take their application down and the new model with training will take multiple hours, which they cannot afford. They are seeking advice on a better way to update the instance type without affecting their application.",
        "Question_gpt_summary":"user face challeng updat instanc type live endpoint auto scale enabl receiv error messag state variant deregist scalabl target applic auto scale updat instanc type user concern regist auto scale applic new model train multipl hour afford seek advic better wai updat instanc type affect applic",
        "Answer_original_content":"problem updat endpoint instanc type take avail hit basic method look like activ autosc polici creat new endpointconfig us new instanc type xlarg call createendpointconfig pass valu previou endpoint config point modelnam reus model retrain delet exist autosc polici depend autosc want increas desir count endpoint event need scale experi spike traffic make api call risk outag model traffic mind possibl scale advanc possibl updateendpoint like previous specifi new endpointconfignam wait endpoint statu inservic min creat new autosc polici new endpoint product variant good sacrif avail",
        "Answer_preprocessed_content":"problem updat endpoint instanc type take avail hit basic method look like activ autosc polici creat new endpointconfig us new instanc type call pass valu previou endpoint config point reus model retrain delet exist autosc polici depend autosc want increas desir count endpoint event need scale experi spike traffic make api call risk outag model traffic mind possibl scale advanc possibl like previous specifi new wait endpoint statu min creat new autosc polici new endpoint product variant good sacrif avail",
        "Answer_gpt_summary_original":"possible solutions to update a live auto-scaled endpoint instance type without putting it down are: \n1. create a new endpoint configuration that uses the new instance type and pass in the same values used for the previous endpoint configuration. \n2. delete the existing autoscaling policy and create a new one for the new endpoint and production variant. \n3. increase the desired count of the endpoint in the event it needs to scale while updating. \n4. scale in advance for the possibility of a spike in traffic while making api calls. \n5. wait for the endpoint status to be inservice, which should take 10-20 minutes.",
        "Answer_gpt_summary":"possibl solut updat live auto scale endpoint instanc type put creat new endpoint configur us new instanc type pass valu previou endpoint configur delet exist autosc polici creat new new endpoint product variant increas desir count endpoint event need scale updat scale advanc possibl spike traffic make api call wait endpoint statu inservic minut"
    },
    {
        "Question_id":58766724.0,
        "Question_title":"How do I select which columns to load in a Kedro CSVLocalDataSet?",
        "Question_body":"<p>I have a <code>csv<\/code> file that looks like<\/p>\n\n<pre><code>a,b,c,d\n1,2,3,4\n5,6,7,8\n<\/code><\/pre>\n\n<p>and I want to load it in as a Kedro <code>CSVLocalDataSet<\/code>, but I don't want to read the entire file. I only want a few columns (say <code>a<\/code> and <code>b<\/code> for example).<\/p>\n\n<p>Is there any way for me to specify the list of columns to read\/load?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1573216572997,
        "Question_favorite_count":null,
        "Question_last_edit_time":1573220304527,
        "Question_score":0.0,
        "Question_view_count":306.0,
        "Answer_body":"<p>CSVLocalDataSet uses <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_csv.html\" rel=\"nofollow noreferrer\">pandas.read_csv<\/a>, which takes \"usecols\" parameter. It can easily be proxied by using <code>load_args<\/code> dataset parameter (all datasets support additional parameters passing via <code>load_args<\/code> and <code>save_args<\/code>):<\/p>\n\n<pre><code>my_cool_data:\n  type: CSVLocalDataSet\n  filepath: data\/path.csv\n  load_args: \n    usecols: ['a', 'b']\n<\/code><\/pre>\n\n<p>Also note the same parameters would work for any pandas-based dataset.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58766724",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1573216728420,
        "Question_original_content":"select column load csvlocaldataset csv file look like want load csvlocaldataset want read entir file want column exampl wai specifi list column read load",
        "Question_preprocessed_content":"select column load csvlocaldataset file look like want load want read entir file want column wai specifi list column",
        "Question_gpt_summary_original":"The user is facing a challenge in selecting specific columns to load in a Kedro CSVLocalDataSet instead of reading the entire file. They are seeking a way to specify the list of columns to read\/load.",
        "Question_gpt_summary":"user face challeng select specif column load csvlocaldataset instead read entir file seek wai specifi list column read load",
        "Answer_original_content":"csvlocaldataset us panda read csv take usecol paramet easili proxi load arg dataset paramet dataset support addit paramet pass load arg save arg cool data type csvlocaldataset filepath data path csv load arg usecol note paramet work panda base dataset",
        "Answer_preprocessed_content":"csvlocaldataset us take usecol paramet easili proxi dataset paramet note paramet work dataset",
        "Answer_gpt_summary_original":"the solution to selecting specific columns to load in a csvlocaldataset from a csv file is to use the \"usecols\" parameter in pandas.read_csv. this can be done by using the load_args dataset parameter and specifying the desired columns. the same solution can be applied to any pandas-based dataset.",
        "Answer_gpt_summary":"solut select specif column load csvlocaldataset csv file us usecol paramet panda read csv load arg dataset paramet specifi desir column solut appli panda base dataset"
    },
    {
        "Question_id":63162310.0,
        "Question_title":"Models registered in workspace do not show up in Designer (preview)",
        "Question_body":"<p>I registered a model in my AML workspace, and I can see it in the Model List:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rtL5Q.png\" rel=\"nofollow noreferrer\">Model List view<\/a><\/p>\n<p>But I cannot see it in Designer (preview), which prevents me from using the new model there.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/WpvIb.png\" rel=\"nofollow noreferrer\">Designer view<\/a><\/p>\n<p>Looks like a bug to me. Datasets work fine.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1596056234913,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":81.0,
        "Answer_body":"<p>This is known issue as the models registered in workspace cannot be consumed in Designer without the new custom module capability (in private preview) available.<\/p>\n<p>The models showing up in Designer today are these generated from Designer training -&gt; inference pipeline conversion and can only be used in Designer (not registered in the workspace).\nWe have an effort ongoing to reduce the confusion.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1596073668927,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63162310",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1596073257467,
        "Question_original_content":"model regist workspac design preview regist model aml workspac model list model list view design preview prevent new model design view look like bug dataset work fine",
        "Question_preprocessed_content":"model regist workspac design regist model aml workspac model list model list view design prevent new model design view look like bug dataset work fine",
        "Question_gpt_summary_original":"The user has encountered a challenge where a model registered in their AML workspace is not showing up in Designer (preview), preventing them from using the new model. The user suspects it to be a bug as datasets work fine.",
        "Question_gpt_summary":"user encount challeng model regist aml workspac show design preview prevent new model user suspect bug dataset work fine",
        "Answer_original_content":"known issu model regist workspac consum design new custom modul capabl privat preview avail model show design todai gener design train infer pipelin convers design regist workspac effort ongo reduc confus",
        "Answer_preprocessed_content":"known issu model regist workspac consum design new custom modul capabl avail model show design todai gener design train infer pipelin convers design effort ongo reduc confus",
        "Answer_gpt_summary_original":"possible solutions: currently, models registered in the  workspace cannot be consumed in designer without the new custom module capability (in private preview) available. the models showing up in designer today are those generated from designer training -> inference pipeline conversion and can only be used in designer (not registered in the workspace). the team is working to reduce the confusion.",
        "Answer_gpt_summary":"possibl solut current model regist workspac consum design new custom modul capabl privat preview avail model show design todai gener design train infer pipelin convers design regist workspac team work reduc confus"
    },
    {
        "Question_id":67689868.0,
        "Question_title":"How to generate Inference Schema for Dictionary with nested structure using Azure InferenceSchema package?",
        "Question_body":"<p>In Advanced Scoring Scripting for AzureML webservice, to automatically generate a schema for our web service, we provide a sample of the input and\/or output in the constructor for one of the defined type objects. The type and sample are used to automatically create the schema.\nTo use schema generation, we include the open-source inference-schema package version 1.1.0 or above. The types that I can find include Numpy Type, Pandas Type, Abstract Parameter type.\nHow do we define the schema for a Nested Dictionary of (generalized) format:<\/p>\n<pre><code>{    &quot;top_level_key&quot;: [\n                         {&quot;nested_key_1&quot;: &quot;string_1&quot;,\n                          &quot;nested_key_2&quot;: &lt;float_number&gt;, \n                          &quot;nested_key_3&quot;: &lt;True\/False&gt;}\n                      ]\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1621952227770,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":172.0,
        "Answer_body":"<p>we don\u2019t have a good way to extend the handling for generic Python class objects. However, we are planning to add support for that, basically by providing more information on the necessary hooks, and allowing users to extend a base class to implement the hook to match the desired class structure.\nThese types are currently supported:<\/p>\n<p>pandas\nnumpy\npyspark\nStandard Python object<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#automatically-generate-a-swagger-schema\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#automatically-generate-a-swagger-schema<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67689868",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1622006064007,
        "Question_original_content":"gener infer schema dictionari nest structur azur inferenceschema packag advanc score script webservic automat gener schema web servic provid sampl input output constructor defin type object type sampl automat creat schema us schema gener includ open sourc infer schema packag version type includ numpi type panda type abstract paramet type defin schema nest dictionari gener format level kei nest kei string nest kei nest kei",
        "Question_preprocessed_content":"gener infer schema dictionari nest structur azur inferenceschema packag advanc score script webservic automat gener schema web servic provid sampl input output constructor defin type object type sampl automat creat schema us schema gener includ packag version type includ numpi type panda type abstract paramet type defin schema nest dictionari format",
        "Question_gpt_summary_original":"The user is facing a challenge in generating an inference schema for a nested dictionary with a generalized format using the Azure InferenceSchema package. They are unsure of how to define the schema for the nested dictionary and are seeking guidance on the types available for schema generation.",
        "Question_gpt_summary":"user face challeng gener infer schema nest dictionari gener format azur inferenceschema packag unsur defin schema nest dictionari seek guidanc type avail schema gener",
        "Answer_original_content":"dont good wai extend handl gener python class object plan add support basic provid inform necessari hook allow user extend base class implement hook match desir class structur type current support panda numpi pyspark standard python object http doc microsoft com azur machin learn deploi advanc entri script automat gener swagger schema",
        "Answer_preprocessed_content":"dont good wai extend handl gener python class object plan add support basic provid inform necessari hook allow user extend base class implement hook match desir class structur type current support panda numpi pyspark standard python object",
        "Answer_gpt_summary_original":"possible solutions from the answer are that the azure inferenceschema package currently does not have a good way to extend the handling for generic python class objects. however, the package is planning to add support for that by providing more information on the necessary hooks and allowing users to extend a base class to implement the hook to match the desired class structure. the types currently supported by the package are pandas, numpy, pyspark, and standard python object.",
        "Answer_gpt_summary":"possibl solut answer azur inferenceschema packag current good wai extend handl gener python class object packag plan add support provid inform necessari hook allow user extend base class implement hook match desir class structur type current support packag panda numpi pyspark standard python object"
    },
    {
        "Question_id":null,
        "Question_title":"How to upload(write) data(dataframe) to azure SQL datastore from azure machine learning(azureML) using SDK",
        "Question_body":"From the documentation I could find ways to read data from Azure SQL database registered as datastore in azureML,but not ways to upload or write output data to azure SQL database from azureML.Can anyone please guide me on the same? Also can SQL datastore be used as output for the batch inference step",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_time":1596612863487,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/59457\/how-to-uploadwrite-datadataframe-to-azure-sql-data.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-08-08T00:11:04.25Z",
                "Answer_score":0,
                "Answer_body":"Thanks for reaching out. You need to register your storage as a datastore. Then write dataframe to a local file and upload to datastore as shown below (refer to this post as well):\n\n from azureml.core import Workspace, Dataset\n    \n subscription_id = 'id'\n resource_group = 'resource group'\n workspace_name = 'workspace name'\n    \n ws = Workspace(subscription_id, resource_group, workspace_name)\n    \n #write dataframe to a local file (e.g. csv, parquet)\n local_path = 'data\/prepared.csv'\n df.to_csv(local_path)\n    \n # get the datastore to upload prepared data\n datastore = ws.get_default_datastore()\n    \n # upload the local file from src_dir to the target_path in datastore\n datastore.upload(src_dir='data', target_path='data')\n\n\n\nIf you continue to experience errors, please share your code so we can investigate further, thanks.",
                "Answer_comment_count":5,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-08-26T03:52:48.503Z",
                "Answer_score":0,
                "Answer_body":"@JothyBabu-4144 Thanks, Please follow the below repo and docker-instructions to use tools such as pyodbc and SQLAlchemy for writeback within an experiment, ParallelRunStep, etc without the need for multi-step pipelines.\n\nHere is the repo for write to sql.\n\n\n\n\n\n20383-docker-instructions.pdf",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-08-31T06:31:43.787Z",
                "Answer_score":0,
                "Answer_body":"Thanks, Please follow the below pointers for Reading and updating Azure SQL Database from Azure ML pipeline.\nDoc: https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.data_transfer_step.datatransferstep?view=azure-ml-py\nExample: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-data-transfer.ipynb\n\nNote: When copying data to an Azure SQL Database, data will be appended to an existing table. We also expect the source file to have a header row and the names should exactly match with column names in destination table.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"upload write data datafram azur sql datastor sdk document wai read data azur sql databas regist datastor wai upload write output data azur sql databas guid sql datastor output batch infer step",
        "Question_preprocessed_content":"upload data azur sql datastor sdk document wai read data azur sql databas regist datastor wai upload write output data azur sql databas guid sql datastor output batch infer step",
        "Question_gpt_summary_original":"The user is facing challenges in uploading or writing output data to Azure SQL database from Azure Machine Learning (AzureML) using SDK. They are seeking guidance on how to do this and are also unsure if SQL datastore can be used as output for the batch inference step.",
        "Question_gpt_summary":"user face challeng upload write output data azur sql databas sdk seek guidanc unsur sql datastor output batch infer step",
        "Answer_original_content":"thank reach need regist storag datastor write datafram local file upload datastor shown refer post core import workspac dataset subscript resourc group resourc group workspac workspac workspac subscript resourc group workspac write datafram local file csv parquet local path data prepar csv csv local path datastor upload prepar data datastor default datastor upload local file src dir target path datastor datastor upload src dir data target path data continu experi error share code investig thank jothybabu thank follow repo docker instruct us tool pyodbc sqlalchemi writeback experi parallelrunstep need multi step pipelin repo write sql docker instruct pdf thank follow pointer read updat azur sql databas pipelin doc http doc microsoft com python api pipelin step pipelin step data transfer step datatransferstep view azur exampl http github com azur machinelearningnotebook blob master us machin learn pipelin intro pipelin aml pipelin data transfer ipynb note copi data azur sql databas data append exist tabl expect sourc file header row name exactli match column name destin tabl",
        "Answer_preprocessed_content":"thank reach need regist storag datastor write datafram local file upload datastor shown core import workspac dataset resourc group workspac write datafram local file datastor upload prepar data datastor upload local file datastor continu experi error share code investig thank thank follow repo us tool pyodbc sqlalchemi writeback experi parallelrunstep need pipelin repo write sql thank follow pointer read updat azur sql databas pipelin doc exampl note copi data azur sql databas data append exist tabl expect sourc file header row name exactli match column name destin tabl",
        "Answer_gpt_summary_original":"possible solutions to upload or write output data to an azure sql database from include registering storage as a datastore, writing a dataframe to a local file, and uploading the file to the datastore. additionally, tools such as pyodbc and sqlalchemy can be used for writeback within an experiment or parallelrunstep without the need for multi-step pipelines. for reading and updating an azure sql database from a pipeline, the user can refer to the provided documentation and example. it is important to note that when copying data to an azure sql database, the data will be appended to an existing table and the source file should have a header row with column names that exactly match those in the destination table.",
        "Answer_gpt_summary":"possibl solut upload write output data azur sql databas includ regist storag datastor write datafram local file upload file datastor addition tool pyodbc sqlalchemi writeback experi parallelrunstep need multi step pipelin read updat azur sql databas pipelin user refer provid document exampl import note copi data azur sql databas data append exist tabl sourc file header row column name exactli match destin tabl"
    },
    {
        "Question_id":null,
        "Question_title":"Next Step from Google Colab +Pro",
        "Question_body":"Hi, I'm using Google Colab +pro and unfortunately I`m getting several Ram calls and have not been able to move forward or train some modelsWhich is the next tool that I should get in order to be able to run the Google Colab models without the Ram calls?Should I get a Google Compute Engine and try to connect the google colab files to it?Should I up load the model to vertex AI?What characteristics should I need to take into consideration before I select any of the different tools?",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1652171400000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":231.0,
        "Answer_body":"Hello,\n\nI have provided a few links to help you through configuring your Google Colab Model.\n\nThis link below contains all Google Colab related questions on Stack Overflow:\n\nhttps:\/\/stackoverflow.com\/search?q=colab&s=7e8e7982-76a3-4765-8bad-63af4a9415fb\n\nThe following link explains how to double the Ram in Google Colab:\n\nhttps:\/\/towardsdatascience.com\/double-your-google-colab-ram-in-10-seconds-using-these-10-characters-...\n\nThe last link is a HOW-TO guide:\n\nhttps:\/\/neptune.ai\/blog\/how-to-use-google-colab-for-deep-learning-complete-tutorial#:~:text=Open%20a....\n\nRegards\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Next-Step-from-Google-Colab-Pro\/td-p\/421797\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-13T11:42:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hello,\n\nI have provided a few links to help you through configuring your Google Colab Model.\n\nThis link below contains all Google Colab related questions on Stack Overflow:\n\nhttps:\/\/stackoverflow.com\/search?q=colab&s=7e8e7982-76a3-4765-8bad-63af4a9415fb\n\nThe following link explains how to double the Ram in Google Colab:\n\nhttps:\/\/towardsdatascience.com\/double-your-google-colab-ram-in-10-seconds-using-these-10-characters-...\n\nThe last link is a HOW-TO guide:\n\nhttps:\/\/neptune.ai\/blog\/how-to-use-google-colab-for-deep-learning-complete-tutorial#:~:text=Open%20a....\n\nRegards\n\nView solution in original post"
            },
            {
                "Answer_creation_time":"2022-05-13T11:42:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hello,\n\nI have provided a few links to help you through configuring your Google Colab Model.\n\nThis link below contains all Google Colab related questions on Stack Overflow:\n\nhttps:\/\/stackoverflow.com\/search?q=colab&s=7e8e7982-76a3-4765-8bad-63af4a9415fb\n\nThe following link explains how to double the Ram in Google Colab:\n\nhttps:\/\/towardsdatascience.com\/double-your-google-colab-ram-in-10-seconds-using-these-10-characters-...\n\nThe last link is a HOW-TO guide:\n\nhttps:\/\/neptune.ai\/blog\/how-to-use-google-colab-for-deep-learning-complete-tutorial#:~:text=Open%20a....\n\nRegards"
            },
            {
                "Answer_creation_time":"2022-05-16T15:14:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Thanks a lot!!!"
            },
            {
                "Answer_creation_time":"2022-05-17T13:25:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Ulisses hi, had the oportunity to review all the links that you send me but still not very sure what to do next.\n\nI have a ML model that runs in a Google Colab Pro+ that always went down do to Ram consumption.\n\nThe next step in order to be able to run the entire Google Colab Notebook is to connect the notebook to a Google Cloud virtual machine? Should I upload the notebook to a container and then to\u00a0 Vertex AI and see if all the functions from the notebook runs?\n\nWhat will be your recommendation if Im looking for a step by step escalation?"
            },
            {
                "Answer_creation_time":"2022-08-08T12:08:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hello\u00a0holguinmora\n\nThe\u00a0Google Workspace Community\u00a0is the most appropriate\u00a0place to\u00a0ask troubleshooting questions and get answers from Google Workspace\u00a0product experts and other administrators considering Google Colab Pro+ is a Workspace product."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1652442120000,
        "Question_original_content":"step googl colab pro googl colab pro unfortun get ram call abl forward train modelswhich tool order abl run googl colab model ram call googl comput engin try connect googl colab file load model characterist need consider select differ tool",
        "Question_preprocessed_content":"step googl colab pro googl colab pro unfortun get ram call abl forward train modelswhich tool order abl run googl colab model ram call googl comput engin try connect googl colab file load model characterist need consider select differ tool",
        "Question_gpt_summary_original":"The user is facing challenges with Google Colab +pro as they are receiving multiple RAM calls and are unable to proceed with training models. They are seeking advice on the next tool to use in order to run Google Colab models without encountering RAM calls. The user is considering options such as Google Compute Engine and Vertex AI and is seeking guidance on the characteristics to consider before selecting a tool.",
        "Question_gpt_summary":"user face challeng googl colab pro receiv multipl ram call unabl proce train model seek advic tool us order run googl colab model encount ram call user consid option googl comput engin seek guidanc characterist consid select tool",
        "Answer_original_content":"hello provid link help configur googl colab model link contain googl colab relat question stack overflow http stackoverflow com search colab bad afafb follow link explain doubl ram googl colab http towardsdatasci com doubl googl colab ram second charact link guid http blog us googl colab deep learn complet tutori text open regard view solut origin post",
        "Answer_preprocessed_content":"hello provid link help configur googl colab model link contain googl colab relat question stack overflow follow link explain doubl ram googl colab link guid regard view solut origin post",
        "Answer_gpt_summary_original":"possible solutions from the answer include: \n1. checking out the google colab related questions on stack overflow for help with configuring the model.\n2. doubling the ram in google colab using the method explained in the provided link.\n3. following the how-to guide provided in the last link for using google colab for deep learning.",
        "Answer_gpt_summary":"possibl solut answer includ check googl colab relat question stack overflow help configur model doubl ram googl colab method explain provid link follow guid provid link googl colab deep learn"
    },
    {
        "Question_id":null,
        "Question_title":"using scikit-learn in mlflow pyfunc",
        "Question_body":"Hi All, Is it possible to use scikit-learn inside pyfunc? I have tried it myself in a project i am working on, however, I received ModuleNotFoundError: No module named 'sklearn'. Many thanks, Cheers!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601627709000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":51.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/cnoGg6blwow",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-02T09:13:54",
                "Answer_body":"Yes, certainly. You still need to make sklearn available in the environment where you run your code. This can mean declaring it as a dependency in your .yaml file.\n\n\nOn Fri, Oct 2, 2020 at 7:35 AM Karim Sadek <krms...@gmail.com> wrote:\n\nHi All, Is it possible to use scikit-learn inside pyfunc? I have tried it myself in a project i am working on, however, I received ModuleNotFoundError: No module named 'sklearn'. Many thanks, Cheers!\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/57c75611-30cc-4fa3-a764-73db2b2c8688n%40googlegroups.com."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"scikit learn pyfunc possibl us scikit learn insid pyfunc tri project work receiv modulenotfounderror modul name sklearn thank cheer",
        "Question_preprocessed_content":"pyfunc possibl us insid pyfunc tri project work receiv modulenotfounderror modul name sklearn thank cheer",
        "Question_gpt_summary_original":"The user is facing a challenge in using scikit-learn inside pyfunc and has encountered a ModuleNotFoundError: No module named 'sklearn'.",
        "Question_gpt_summary":"user face challeng scikit learn insid pyfunc encount modulenotfounderror modul name sklearn",
        "Answer_original_content":"ye certainli need sklearn avail environ run code mean declar depend yaml file fri oct karim sadek wrote possibl us scikit learn insid pyfunc tri project work receiv modulenotfounderror modul name sklearn thank cheer receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user dbbcn googlegroup com",
        "Answer_preprocessed_content":"ye certainli need sklearn avail environ run code mean declar depend yaml file fri oct karim sadek wrote possibl us insid pyfunc tri project work receiv modulenotfounderror modul name sklearn thank cheer receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit",
        "Answer_gpt_summary_original":"Solution: The solution mentioned in the discussion is to make scikit-learn available in the environment where the code is run. This can be done by declaring it as a dependency in the .yaml file.",
        "Answer_gpt_summary":"solut solut mention discuss scikit learn avail environ code run declar depend yaml file"
    },
    {
        "Question_id":67123040.0,
        "Question_title":"How to tell programmatically that an AWS Step Function execution has been completed?",
        "Question_body":"<p>I am triggering a Step Function execution via a Python cell in a SageMaker Notebook, like this:<\/p>\n<pre><code>state_machine_arn = 'arn:aws:states:us-west-1:1234567891:stateMachine:alexanderMyPackageStateMachineE3411O13-A1vQWERTP9q9'\nsfn = boto3.client('stepfunctions')\n..\nsfn.start_execution(**kwargs)  # Non Blocking Call\nrun_arn = response['executionArn']\nprint(f&quot;Started run {run_name}. ARN is {run_arn}.&quot;)\n<\/code><\/pre>\n<p>and then in order to check that the execution (which might take hours to complete depending on the input) has been completed, before I start doing some custom post-analysis on the results, I manually execute a cell with:<\/p>\n<pre><code>response = sfn.list_executions(\n    stateMachineArn=state_machine_arn,\n    maxResults=1\n)\nprint(response)\n<\/code><\/pre>\n<p>where I can see from the output the status of the execution, e.g. <code>'status': 'RUNNING'<\/code>.<\/p>\n<p>How can I automate this, i.e. trigger the Step Function and continue the execution on my post-analysis custom logic only after the execution has finished? Is there for example a blocking call to start the execution, or a callback method I could use?<\/p>\n<p>I can think of putting a sleep method, so that the Python Notebook cell would periodically call <code>list_executions()<\/code> and check the status, and only when the execution is completed, continue to rest of the code. I can statistically determine the sleep period, but I was wondering if there is a simpler\/more accurate way.<\/p>\n<hr \/>\n<p>PS: Related: <a href=\"https:\/\/stackoverflow.com\/questions\/46878423\/how-to-avoid-simultaneous-execution-in-aws-step-function\">How to avoid simultaneous execution in aws step function<\/a>, however I would like to avoid creating any new AWS resource, just for this, I would like to do everything from within the Notebook.<\/p>\n<p>PPS: I cannot make any change to <code>MyPackage<\/code> and the Step Function definition.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1618566783027,
        "Question_favorite_count":null,
        "Question_last_edit_time":1618585069787,
        "Question_score":4.0,
        "Question_view_count":1216.0,
        "Answer_body":"<p>Based on the comments.<\/p>\n<p>If no new resources are to be created (no CloudWatch Event rules, lambda functions) nor any changes to existing Step Function are allowed, then <strong>pooling iteratively<\/strong> <code>list_executions<\/code> would be the best solution.<\/p>\n<p>AWS CLI and boto3 have implemented similar solutions (not for Step Functions), but for some other services. They are called <code>waiters<\/code> (e.g. <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/ec2.html#waiters\" rel=\"nofollow noreferrer\">ec2 waiters<\/a>). So basically you would have to create your own <strong>waiter for Step Function<\/strong>, as AWS does not provide one for that. AWS uses <strong>15 seconds<\/strong> sleep time from what I recall for its waiters.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67123040",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1618568305232,
        "Question_original_content":"tell programmat aw step function execut complet trigger step function execut python cell notebook like state machin arn arn aw state west statemachin alexandermypackagestatemachineeo avqwertpq sfn boto client stepfunct sfn start execut kwarg non block run arn respons executionarn print start run run arn run arn order check execut hour complet depend input complet start custom post analysi result manual execut cell respons sfn list execut statemachinearn state machin arn maxresult print respons output statu execut statu run autom trigger step function continu execut post analysi custom logic execut finish exampl block start execut callback method us think put sleep method python notebook cell period list execut check statu execut complet continu rest code statist determin sleep period wonder simpler accur wai relat avoid simultan execut aw step function like avoid creat new aw resourc like notebook pp chang mypackag step function definit",
        "Question_preprocessed_content":"tell programmat aw step function execut complet trigger step function execut python cell notebook like order check execut complet start custom result manual execut cell output statu execut autom trigger step function continu execut custom logic execut finish exampl block start execut callback method us think put sleep method python notebook cell period check statu execut complet continu rest code statist determin sleep period wonder accur wai relat avoid simultan execut aw step function like avoid creat new aw resourc like notebook pp chang step function definit",
        "Question_gpt_summary_original":"The user is triggering a Step Function execution via a Python cell in a SageMaker Notebook and wants to automate the process of checking if the execution has been completed before starting post-analysis custom logic. The user is currently manually executing a cell to check the status of the execution and is considering using a sleep method to periodically call list_executions() to check the status. The user wants to know if there is a simpler\/more accurate way to automate this process without creating any new AWS resource.",
        "Question_gpt_summary":"user trigger step function execut python cell notebook want autom process check execut complet start post analysi custom logic user current manual execut cell check statu execut consid sleep method period list execut check statu user want know simpler accur wai autom process creat new aw resourc",
        "Answer_original_content":"base comment new resourc creat cloudwatch event rule lambda function chang exist step function allow pool iter list execut best solut aw cli boto implement similar solut step function servic call waiter waiter basic creat waiter step function aw provid aw us second sleep time recal waiter",
        "Answer_preprocessed_content":"base comment new resourc creat chang exist step function allow pool iter best solut aw cli boto implement similar solut servic call basic creat waiter step function aw provid aw us second sleep time recal waiter",
        "Answer_gpt_summary_original":"possible solutions for programmatically determining when an aws step function execution has been completed include iteratively pooling list_executions, creating a custom waiter for step function, and using aws cli or boto3 waiters (although they are not currently available for step functions). the suggested sleep time for aws waiters is 15 seconds.",
        "Answer_gpt_summary":"possibl solut programmat determin aw step function execut complet includ iter pool list execut creat custom waiter step function aw cli boto waiter current avail step function suggest sleep time aw waiter second"
    },
    {
        "Question_id":null,
        "Question_title":"Processing Job automatically created when I start a training job",
        "Question_body":"Hi,\nI haven't used sagemaker for a while and today I started a training job (with the same old settings I always used before), but this time I noticed that a processing job has been automatically created and it's running while my training job runs (I don't even know what a processing job is). I also checked in the dashboard to be sure, this was not happening before, it's the second time (first time was in December) but I've been using sagemaker for the last two years..\nIs this a wanted behaviour? I didn't find anything related in the documentation, but it's important to know because I don't want extra costs..\nThis is the image used by the processing job, with a instance type of ml.m5.2xlarge which I didn't set anywhere..\n\n929884845733.dkr.ecr.eu-west-1.amazonaws.com\/sagemaker-debugger-rules:latest  \n\n\nAnd this is how I launch my training job (the entrypoint script is basically Keras code for a MobileNetV3)\n\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\nfrom sagemaker import get_execution_role\n\nbucket = 'mybucket'\n\ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')\n\nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')\n\ns3_output_location = 's3:\/\/{}'.format(bucket)\n\nhyperparameters = {'epochs': 130, 'batch-size' : 512, 'learning-rate' : 0.0002}\n\nmetrics = .. some regex here\n\ntf_estimator = TensorFlow(entry_point='train.py',\nrole=get_execution_role(),\ntrain_instance_count=1,\ntrain_instance_type='ml.p2.xlarge',\ntrain_max_run=172800,\noutput_path=s3_output_location,\nframework_version='2.3.0',\npy_version='py37',\nmetric_definitions = metrics,\nhyperparameters = hyperparameters,\nsource_dir=\"data\")\n\ninputs = {'train': train_data, 'test': validation_data}\nmyJobName = 'myname'\ntf_estimator.fit(inputs=inputs, job_name=myJobName)\n\nEdited by: rokk07 on Jan 25, 2021 2:55 AM",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611570309000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":38.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmy7drPKBRH6hs3Pw_yj3tw\/processing-job-automatically-created-when-i-start-a-training-job",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-25T11:14:01.000Z",
                "Answer_score":0,
                "Answer_body":"I can answer myself. It's described https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/use-debugger-built-in-rules.html , must be a recent feature. The documentation explain also how to disable the debugger.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"process job automat creat start train job haven todai start train job old set time notic process job automat creat run train job run know process job check dashboard sure happen second time time decemb year want behaviour relat document import know want extra cost imag process job instanc type xlarg set dkr ecr west amazonaw com debugg rule latest launch train job entrypoint script basic kera code mobilenetv import tensorflow import tensorflow import execut role bucket mybucket train data format bucket train valid data format bucket test output locat format bucket hyperparamet epoch batch size learn rate metric regex estim tensorflow entri point train role execut role train instanc count train instanc type xlarg train max run output path output locat framework version version metric definit metric hyperparamet hyperparamet sourc dir data input train train data test valid data myjobnam mynam estim fit input input job myjobnam edit rokk jan",
        "Question_preprocessed_content":"process job automat creat start train job haven todai start train job time notic process job automat creat run train job run check dashboard sure happen second time want behaviour relat document import know want extra imag process job instanc type set launch train job import tensorflow import tensorflow import bucket mybucket hyperparamet metric regex metric hyperparamet hyperparamet input myjobnam mynam edit rokk jan",
        "Question_gpt_summary_original":"The user has encountered a challenge where a processing job is automatically created and running while their training job runs, which they did not set up and do not understand. They are concerned about the extra costs and are unsure if this is intended behavior, as they did not find any information about it in the documentation. The processing job is using an image with an instance type that the user did not set anywhere.",
        "Question_gpt_summary":"user encount challeng process job automat creat run train job run set understand concern extra cost unsur intend behavior inform document process job imag instanc type user set",
        "Answer_original_content":"answer describ http doc aw amazon com latest us debugg built rule html recent featur document explain disabl debugg",
        "Answer_preprocessed_content":"answer describ recent featur document explain disabl debugg",
        "Answer_gpt_summary_original":"possible solutions to the unexpected challenge of a processing job running while a training job is running are:\n\n1. check the aws documentation for the built-in debugger rules, as this may be a recent feature that was not previously documented.\n2. follow the instructions in the documentation to disable the debugger if it is causing issues with the processing job.",
        "Answer_gpt_summary":"possibl solut unexpect challeng process job run train job run check aw document built debugg rule recent featur previous document follow instruct document disabl debugg caus issu process job"
    },
    {
        "Question_id":null,
        "Question_title":"Model path for a model folder that has json file and weights.",
        "Question_body":"I am trying to create a score.py file for model deployment. In the folder where the model is present, there are two files present. One is the JSON model file another is a .h5 file which has weights of the model. how do I configure the file in this case in the init function of the score.py file?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608489600887,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/204868\/model-path-for-a-model-folder-that-has-json-file-a.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-21T10:34:00.973Z",
                "Answer_score":0,
                "Answer_body":"@SHUBHAMJAIN01670271019016-8084 For the above scenario you need to save the model as a single file HDF5 file and then register the model using model.register() to register the model with the workspace and load the model in the scoring script using load_model.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":3.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"model path model folder json file weight try creat score file model deploy folder model present file present json model file file weight model configur file case init function score file",
        "Question_preprocessed_content":"model path model folder json file weight try creat file model deploy folder model present file present json model file file weight model configur file case init function file",
        "Question_gpt_summary_original":"The user is facing a challenge in configuring the file path for a model deployment score.py file. The model folder contains two files, a JSON model file and a .h5 file with the model weights. The user is seeking guidance on how to configure the file path in the init function of the score.py file.",
        "Question_gpt_summary":"user face challeng configur file path model deploy score file model folder contain file json model file file model weight user seek guidanc configur file path init function score file",
        "Answer_original_content":"shubhamjain scenario need save model singl file hdf file regist model model regist regist model workspac load model score script load model",
        "Answer_preprocessed_content":"scenario need save model singl file hdf file regist model regist model workspac load model score script",
        "Answer_gpt_summary_original":"the solution to the problem of configuring the init function of the score.py file with two files in the model folder is to save the model as a single hdf5 file, register the model using model.register(), and load the model in the scoring script using load_model.",
        "Answer_gpt_summary":"solut problem configur init function score file file model folder save model singl hdf file regist model model regist load model score script load model"
    },
    {
        "Question_id":null,
        "Question_title":"Shared Deployments of MLFlow",
        "Question_body":"Good afternoon,\n\n\n\nAre there any resources available to centralize MLFlow to a dedicated server which can:\n\n\n1) Archive models\n\n2) Host MLFlow web UI\n3) Host deployed model REST API\n\n\n\nIdeally, each model could be secured by groups or individual users, and shared with other groups \/ users.\n\n\n\nDo any existing example deployments exist? I see Databricks appears to have a hosted version of MLFlow per this announcement: https:\/\/mlflow.org\/news\/2019\/09\/10\/MLflow-Community-Edition\/index.html\n\n\nThanks!\n\n\n-Rob",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1569414037000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":13.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/mLqqkTBBel0",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-09-25T14:29:54",
                "Answer_body":"Hi Robert.\n\n\n1. In the oss, you can use ``mlflow server`` to start a rest api tracking server. The server will host MLflow web UI and tracking rest api.\u00a0\n2. The tracking server does not store models. The models are stored as artifacts and will be stored depending on where you set the artifact root uri for each experiment.\u00a0\nIf you want the models stored on the tracking server, you would need to be able to access it via one of the supported\u00a0artifact\u00a0store\u00a0apis (e.g.\u00a0 sftp server)\n3. The tracking server does not host deployed models. You can deploy models locally via ``mlflow serve``.\u00a0\n4. Yes, Databricks has a hosted MLFlow and it is also available in the (free) Databricks Community edition, you can\u00a0 try it here\u00a0https:\/\/databricks.com\/try-databricks\n\n\nLet me know if you have any questions.\n\n\nTomas\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/61bf1576-410f-4706-a316-47b0a0669052%40googlegroups.com."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"share deploy good afternoon resourc avail central dedic server archiv model host web host deploi model rest api ideal model secur group individu user share group user exist exampl deploy exist databrick appear host version announc http org new commun edit index html thank rob",
        "Question_preprocessed_content":"share deploy good afternoon resourc avail central dedic server archiv model host web host deploi model rest api ideal model secur group individu user share group user exist exampl deploy exist databrick appear host version announc thank rob",
        "Question_gpt_summary_original":"The user is seeking resources to centralize MLFlow to a dedicated server that can archive models, host MLFlow web UI, and host deployed model REST API. They are looking for a solution that can secure each model by groups or individual users and share them with other groups\/users. The user is also inquiring about existing example deployments and mentions Databricks' hosted version of MLFlow.",
        "Question_gpt_summary":"user seek resourc central dedic server archiv model host web host deploi model rest api look solut secur model group individu user share group user user inquir exist exampl deploy mention databrick host version",
        "Answer_original_content":"robert oss us server start rest api track server server host web track rest api track server store model model store artifact store depend set artifact root uri experi want model store track server need abl access supportedartifactstoreapi sftp server track server host deploi model deploi model local serv ye databrick host avail free databrick commun edit try herehttp databrick com try databrick let know question toma receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user googlegroup com",
        "Answer_preprocessed_content":"robert oss us start rest api track server server host web track rest api track server store model model store artifact store depend set artifact root uri experi want model store track server need abl access supportedartifactstoreapi track server host deploi model deploi model local ye databrick host avail databrick commun edit try let know question toma receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Use \"mlflow server\" to start a rest api tracking server that will host MLflow web UI and tracking rest api.\n2. Store models as artifacts and set the artifact root uri for each experiment. If you want the models stored on the tracking server, you would need to be able to access it via one of the supported artifact store apis (e.g. sftp server).\n3. Deploy models locally via \"mlflow serve\".\n4. Use Databricks hosted MLFlow or try it in the free Databricks Community edition.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss us server start rest api track server host web track rest api store model artifact set artifact root uri experi want model store track server need abl access support artifact store api sftp server deploi model local serv us databrick host try free databrick commun edit person opinion bias includ summari"
    },
    {
        "Question_id":70730897.0,
        "Question_title":"sage maker using visual studio code locally instead of notebooks - SDK",
        "Question_body":"<p>I prefer to use my local laptop for cost reasons + having an IDE like Visual Studio Code. As it stands my IT department is also not able\/reluctant to setup database connectivity for could 9 etc. (so I would have to move data into S3 from local laptop). I also investigated <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/randomcutforest.html\" rel=\"nofollow noreferrer\">this<\/a> to SSH into an EC2. However, again IT is unwilling to open up network connectivity ...<\/p>\n<p>The issue I have is, that my laptop only has 8GB (windows). So from time to time I would like\/have to use the cloud (e.g. to do hyperparameter optimisation etc.) and ultimately deploy models (e.g. as docker images R + plumber - Python + FASt API etc.). Coming across code like this (run in sagemaker notebooks):<\/p>\n<pre><code>estimator = sagemaker.estimator.Estimator(\n    role=role_arn,\n    instance_count=1,\n    instance_type='ml.m5.xlarge',\n    image_uri=container,\n    debugger_hook_config=debugger_hook_config,\n    rules=rules,\n    sagemaker_session=session)\n<\/code><\/pre>\n<p>which is <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">SageMaker Python SDK<\/a> code. Can I also execute everything locally via Visual Studio Code? I understand that SDKs like this are a constant moving target but can I use this and are there any books\/tutorials (Google did not return much). Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1642342574523,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":879.0,
        "Answer_body":"<p>You can. Use <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/\" rel=\"nofollow noreferrer\">SageMaker Local<\/a> to execute training and inference locally in docker containers on your laptop.<br \/>\nIn the Estimator, you'll specify: <code>instance_type='local'<\/code> or <code>instance_type='local_gpu'<\/code><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70730897",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1642413829196,
        "Question_original_content":"sage maker visual studio code local instead notebook sdk prefer us local laptop cost reason have id like visual studio code stand depart abl reluct setup databas connect data local laptop investig ssh unwil open network connect issu laptop window time time like us cloud hyperparamet optimis ultim deploi model docker imag plumber python fast api come code like run notebook estim estim estim role role arn instanc count instanc type xlarg imag uri contain debugg hook config debugg hook config rule rule session session python sdk code execut local visual studio code understand sdk like constant move target us book tutori googl return thank",
        "Question_preprocessed_content":"sage maker visual studio code local instead notebook sdk prefer us local laptop cost reason have id like visual studio code stand depart setup databas connect investig ssh unwil open network connect issu laptop time time us cloud ultim deploi model come code like python sdk code execut local visual studio code understand sdk like constant move target us thank",
        "Question_gpt_summary_original":"The user prefers to use their local laptop for cost and IDE reasons, but their IT department is unwilling to set up database connectivity for cloud services like SageMaker. The user's laptop only has 8GB of memory, so they need to use the cloud for tasks like hyperparameter optimization and model deployment. The user is wondering if they can execute SageMaker Python SDK code locally using Visual Studio Code and is looking for resources to help them do so.",
        "Question_gpt_summary":"user prefer us local laptop cost id reason depart unwil set databas connect cloud servic like user laptop memori need us cloud task like hyperparamet optim model deploy user wonder execut python sdk code local visual studio code look resourc help",
        "Answer_original_content":"us local execut train infer local docker contain laptop estim specifi instanc type local instanc type local gpu",
        "Answer_preprocessed_content":"us local execut train infer local docker contain laptop estim specifi",
        "Answer_gpt_summary_original":"possible solutions to the user's challenge of using  with visual studio code locally instead of notebooks are to execute training and inference locally in docker containers on their laptop using the 'local' or 'local_gpu' instance type specified in the estimator.",
        "Answer_gpt_summary":"possibl solut user challeng visual studio code local instead notebook execut train infer local docker contain laptop local local gpu instanc type specifi estim"
    },
    {
        "Question_id":56255154.0,
        "Question_title":"How to use a pretrained model from s3 to predict some data?",
        "Question_body":"<p>I have trained a semantic segmentation model using the sagemaker and the out has been saved to a s3 bucket. I want to load this model from the s3 to predict some images in sagemaker. <\/p>\n\n<p>I know how to predict if I leave the notebook instance running after the training as its just an easy deploy but doesn't really help if I want to use an older model.<\/p>\n\n<p>I have looked at these sources and been able to come up with something myself but it doesn't work hence me being here:<\/p>\n\n<p><a href=\"https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker\" rel=\"noreferrer\">https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker<\/a>\n<a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a><\/p>\n\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb<\/a><\/p>\n\n<p>My code is this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.pipeline import PipelineModel\nfrom sagemaker.model import Model\n\ns3_model_bucket = 'bucket'\ns3_model_key_prefix = 'prefix'\ndata = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\nmodels = ss_model.create_model() # ss_model is my sagemaker.estimator\n\nmodel = PipelineModel(name=data, role=role, models= [models])\nss_predictor = model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1558522248223,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1558524094856,
        "Question_score":6.0,
        "Question_view_count":7404.0,
        "Answer_body":"<p>You can actually instantiate a Python SDK <code>model<\/code> object from existing artifacts, and deploy it to an endpoint. This allows you to deploy a model from trained artifacts, without having to retrain in the notebook. For example, for the semantic segmentation model:<\/p>\n\n<pre><code>trainedmodel = sagemaker.model.Model(\n    model_data='s3:\/\/...model path here..\/model.tar.gz',\n    image='685385470294.dkr.ecr.eu-west-1.amazonaws.com\/semantic-segmentation:latest',  # example path for the semantic segmentation in eu-west-1\n    role=role)  # your role here; could be different name\n\ntrainedmodel.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>\n\n<p>And similarly, you can instantiate a predictor object on a deployed endpoint from any authenticated client supporting the SDK, with the following command:<\/p>\n\n<pre><code>predictor = sagemaker.predictor.RealTimePredictor(\n    endpoint='endpoint name here',\n    content_type='image\/jpeg',\n    accept='image\/png')\n<\/code><\/pre>\n\n<p>More on those abstractions:<\/p>\n\n<ul>\n<li><code>Model<\/code>: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html<\/a><\/li>\n<li><code>Predictor<\/code>:\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":13.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56255154",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1558621559712,
        "Question_original_content":"us pretrain model predict data train semant segment model save bucket want load model predict imag know predict leav notebook instanc run train easi deploi help want us older model look sourc abl come work http cours fast deploy amzn html deploi http aw amazon com get start tutori build train deploi machin learn model http readthedoc stabl pipelin html http github com awslab amazon exampl blob master advanc function infer pipelin sparkml xgboost abalon infer pipelin sparkml xgboost abalon ipynb code pipelin import pipelinemodel model import model model bucket bucket model kei prefix prefix data format model bucket model kei prefix model tar model model creat model model estim model pipelinemodel data role role model model predictor model deploi initi instanc count instanc type xlarg",
        "Question_preprocessed_content":"us pretrain model predict data train semant segment model save bucket want load model predict imag know predict leav notebook instanc run train easi deploi help want us older model look sourc abl come work code",
        "Question_gpt_summary_original":"The user wants to load a semantic segmentation model from an S3 bucket to predict some images in Sagemaker. They have tried to use sources such as Fast.ai, AWS tutorials, and Sagemaker documentation to come up with a solution, but their code is not working. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user want load semant segment model bucket predict imag tri us sourc fast aw tutori document come solut code work user seek help resolv issu",
        "Answer_original_content":"actual instanti python sdk model object exist artifact deploi endpoint allow deploi model train artifact have retrain notebook exampl semant segment model trainedmodel model model model data model path model tar imag dkr ecr west amazonaw com semant segment latest exampl path semant segment west role role role differ trainedmodel deploi initi instanc count instanc type xlarg similarli instanti predictor object deploi endpoint authent client support sdk follow command predictor predictor realtimepredictor endpoint endpoint content type imag jpeg accept imag png abstract model http readthedoc stabl model html predictor http readthedoc stabl predictor html",
        "Answer_preprocessed_content":"actual instanti python sdk object exist artifact deploi endpoint allow deploi model train artifact have retrain notebook exampl semant segment model similarli instanti predictor object deploi endpoint authent client support sdk follow command abstract",
        "Answer_gpt_summary_original":"the solution to the challenge of using a pretrained model from an s3 bucket to predict some data is to instantiate a python sdk model object from existing artifacts and deploy it to an endpoint. this allows you to deploy a model from trained artifacts without having to retrain in the notebook. you can also instantiate a predictor object on a deployed endpoint from any authenticated client supporting the sdk.",
        "Answer_gpt_summary":"solut challeng pretrain model bucket predict data instanti python sdk model object exist artifact deploi endpoint allow deploi model train artifact have retrain notebook instanti predictor object deploi endpoint authent client support sdk"
    },
    {
        "Question_id":64164367.0,
        "Question_title":"Nginx authentication issues when building mlflow through docker-compose",
        "Question_body":"<p>I'm trying to dockerize <a href=\"https:\/\/mlflow.org\/\" rel=\"nofollow noreferrer\">mlflow<\/a> with PostgreSQL and nginx configurations for Google Cloud Run (GCR) on the Google Cloud Platform (GCP).<\/p>\n<p>Before deploying anything to GCP however, I wanted to get a local deployment working. I found <a href=\"https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039\" rel=\"nofollow noreferrer\">this<\/a> guide that details the process of setting up the environment. Having followed the guide (excluding the SQL part), I can see the  mlflow UI on <code>localhost:80<\/code> as nginx redirects traffic on port 80 to 5000. To add authentication, I found <a href=\"https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-set-up-password-authentication-with-nginx-on-ubuntu-14-04\" rel=\"nofollow noreferrer\">here<\/a> that I can do it using <code>sudo htpasswd -c .htpasswd &lt;username&gt;<\/code> in the <code>etc\/nginx\/<\/code> directory and then adding<\/p>\n<pre><code>location \\ {\n   auth_basic &quot;Private Property&quot;;\n   auth_basic_user_file .htpasswd;\n}\n<\/code><\/pre>\n<p>to the <code>nginx.conf<\/code> (or <code>mlflow.conf<\/code> in this case) to make it appear online. Trouble is, when I go to <code>localhost:80<\/code> <em>now<\/em> and enter in my username\/password, I continue to see<\/p>\n<pre><code>[error] 6#6: *1 open() &quot;\/etc\/nginx\/.htpasswd&quot; failed (2: No such file or directory)\n<\/code><\/pre>\n<p>in the <code>docker-compose up<\/code> logs as they are printed to the terminal, and as such <em>I'm not able to see the mlflow UI<\/em> on <code>localhost:80<\/code> (either a blank screen or nginx 403 error).<\/p>\n<p>Now, I've looked at several other posts (such as <a href=\"https:\/\/stackoverflow.com\/questions\/2010677\/nginx-and-auth-basic\">this one<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/16510374\/403-forbidden-nginx-using-correct-credentials\">this one<\/a>) and it seems to me that nginx doesn't have the right permissions to read the <code>.htpasswd<\/code> in the <code>etc\/nginx\/<\/code> directory file or that the path of the file isn't correct, i.e. the path has to be in reference to the <code>nginx.conf<\/code> file.<\/p>\n<p>Even though I made these corrections to the above towards-data-science files, the problem still persists.  I've been stuck for a while on this. Any particular reasons why this may be happening?<\/p>\n<p>Edit:\nHere is my directory structure in case it may help:<\/p>\n<pre><code>mlflow-docker\/:\n  mlflow\/:\n    Dockerfile\n  nginx\/:\n    Dockerfile\n    mlflow.conf\n    nginx.conf\n  docker-compose.yml\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1601594496277,
        "Question_favorite_count":null,
        "Question_last_edit_time":1601616621328,
        "Question_score":0.0,
        "Question_view_count":841.0,
        "Answer_body":"<p>You need to add the .htpasswd file inside your container's file system.<\/p>\n<p>Generate the password file in your project's nginx folder.<\/p>\n<pre><code>sudo htpasswd -c .htpasswd sammy\n<\/code><\/pre>\n<p>Copy the password file to the nginx container's directory. Add following line in nginx dockerfile.<\/p>\n<pre><code>COPY .htpasswd \/etc\/nginx\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1601619366396,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64164367",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1601618748112,
        "Question_original_content":"nginx authent issu build docker compos try docker postgresql nginx configur googl cloud run gcr googl cloud platform gcp deploi gcp want local deploy work guid detail process set environ have follow guid exclud sql localhost nginx redirect traffic port add authent sudo htpasswd htpasswd nginx directori ad locat auth basic privat properti auth basic user file htpasswd nginx conf conf case appear onlin troubl localhost enter usernam password continu error open nginx htpasswd fail file directori docker compos log print termin abl localhost blank screen nginx error look post nginx right permiss read htpasswd nginx directori file path file isn correct path refer nginx conf file correct data scienc file problem persist stuck particular reason happen edit directori structur case help docker dockerfil nginx dockerfil conf nginx conf docker compos yml",
        "Question_preprocessed_content":"nginx authent issu build try docker postgresql nginx configur googl cloud run googl cloud platform deploi gcp want local deploy work guid detail process set environ have follow guid nginx redirect traffic port add authent directori ad appear onlin troubl enter continu log print termin abl look post nginx right permiss read directori file path file isn correct path refer file correct file problem persist stuck particular reason happen edit directori structur case help",
        "Question_gpt_summary_original":"The user is trying to dockerize mlflow with PostgreSQL and nginx configurations for Google Cloud Run (GCR) on the Google Cloud Platform (GCP). They followed a guide to set up the environment and added authentication using htpasswd, but they are encountering an error that says \"open() \"\/etc\/nginx\/.htpasswd\" failed (2: No such file or directory)\" when they try to access the mlflow UI on localhost:80. The user suspects that nginx doesn't have the right permissions to read the .htpasswd file or that the path of the file isn't correct. They have tried to correct the problem but it still persists.",
        "Question_gpt_summary":"user try docker postgresql nginx configur googl cloud run gcr googl cloud platform gcp follow guid set environ ad authent htpasswd encount error sai open nginx htpasswd fail file directori try access localhost user suspect nginx right permiss read htpasswd file path file isn correct tri correct problem persist",
        "Answer_original_content":"need add htpasswd file insid contain file gener password file project nginx folder sudo htpasswd htpasswd sammi copi password file nginx contain directori add follow line nginx dockerfil copi htpasswd nginx",
        "Answer_preprocessed_content":"need add htpasswd file insid contain file gener password file project nginx folder copi password file nginx contain directori add follow line nginx dockerfil",
        "Answer_gpt_summary_original":"solution: to resolve authentication issues when building a docker-compose environment with nginx configurations for google cloud run on the google cloud platform, the user needs to generate a .htpasswd file in the project's nginx folder using the command \"sudo htpasswd -c .htpasswd sammy\", add the password file to the nginx container's directory, and add the line \"copy .htpasswd \/etc\/nginx\" in the nginx dockerfile.",
        "Answer_gpt_summary":"solut resolv authent issu build docker compos environ nginx configur googl cloud run googl cloud platform user need gener htpasswd file project nginx folder command sudo htpasswd htpasswd sammi add password file nginx contain directori add line copi htpasswd nginx nginx dockerfil"
    },
    {
        "Question_id":44367367.0,
        "Question_title":"Converting rating column into boolean column with custom filter in Azure ML",
        "Question_body":"<p>I have a dataset in which the Rating column is an integer column with values ranging from 1 to 10.<\/p>\n\n<p>I would like to convert that column into a simple boolean positive\/negative categorical column, so that if the value is less than 6 it is a negative rating, and if it is greater or equal 6 it would become a positive rating.<\/p>\n\n<p>I'm not sure how to do that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1496660018143,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":240.0,
        "Answer_body":"<p>Azure Machine Learning allows at least 3 options to do that:<\/p>\n\n<ul>\n<li>Apply SQL Transformation <code>select *,case when rating&lt;6 then 0 else 1 end RatingB from t1<\/code><\/li>\n<li>Execute Python Script <code>return dataframe1.rating[dataframe1.rating &lt; 6] = 0<\/code><\/li>\n<li>Execute R Script <code>dataset1$rating[dataset1$rating &lt; 6] &lt;- 0<\/code><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44367367",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1498563773572,
        "Question_original_content":"convert rate column boolean column custom filter dataset rate column integ column valu rang like convert column simpl boolean posit neg categor column valu neg rate greater equal posit rate sure",
        "Question_preprocessed_content":"convert rate column boolean column custom filter dataset rate column integ column valu rang like convert column simpl boolean categor column valu neg rate greater equal posit rate sure",
        "Question_gpt_summary_original":"The user is facing a challenge of converting an integer column with values ranging from 1 to 10 into a boolean positive\/negative categorical column based on a custom filter in Azure ML. The user wants to set a filter where values less than 6 are considered negative and values greater or equal to 6 are considered positive.",
        "Question_gpt_summary":"user face challeng convert integ column valu rang boolean posit neg categor column base custom filter user want set filter valu consid neg valu greater equal consid posit",
        "Answer_original_content":"allow option appli sql transform select case rate end ratingb execut python script return datafram rate datafram rate execut script dataset rate dataset rate",
        "Answer_preprocessed_content":"allow option appli sql transform execut python script execut script",
        "Answer_gpt_summary_original":"there are three possible solutions to convert a rating column with integer values ranging from 1 to 10 into a boolean positive\/negative categorical column. the first solution involves applying an sql transformation using a case statement. the second solution involves executing a python script to return a dataframe with the rating column values less than 6 set to 0. the third solution involves executing an r script to set the rating column values less than 6 to 0 in the dataset.",
        "Answer_gpt_summary":"possibl solut convert rate column integ valu rang boolean posit neg categor column solut involv appli sql transform case statement second solut involv execut python script return datafram rate column valu set solut involv execut script set rate column valu dataset"
    },
    {
        "Question_id":null,
        "Question_title":"Specifying different target types for the same operation",
        "Question_body":"<p>I am trying to link and copy different files from the same operation resource in my guild stage.  It seems that specifying an operation multiple times only results in the last operation being used.  Here is an example that matches what I\u2019m trying to do where a operation train is used as a resource.  The file model.pth is linked while config.json is copied.<\/p>\n<pre><code>test:\n  exec: \"python test.py\"\n  requires:\n    - operation: train\n      select:\n        - file: model.pth\n          target-type: link\n        - file: config.json\n          target-type: copy\n<\/code><\/pre>\n<p>Is there any way to achieve this kind of functionality? Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1631302538330,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":303.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/specifying-different-target-types-for-the-same-operation\/761",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-17T02:15:07.061Z",
                "Answer_body":"<p>You need to do it through a pipeline, or manually assign the run ID yourself.<\/p>\n<pre><code class=\"lang-yaml\">op1:\n  exec: func_1\n  flag: \n    a: 1E5\n    b: 1E-5\n  requires: ...\n\nop2:\n  exec: func_2\n  flag: \n    b: 1E-5\n    c: ''\n  require: \n    - operation: op1\n      select: '[folder or file generated by op1]'\n\npipeline:\n  flag:\n    a: 1E5\n    b: 1E-5\n    c: ''\n  steps:\n    - op1 a=${a} b=${b}\n    - op2 b=${b} c=${c}\n<\/code><\/pre>\n<p>This is covered in <a href=\"https:\/\/my.guild.ai\/t\/pipelines\/163\">https:\/\/my.guild.ai\/t\/pipelines\/163<\/a>.<\/p>\n<p>Also, note that your <code>exec<\/code> attribute should assign the moduel\/function name rather than the command to execute for python scripts. This lead to funny behavior if you have multiple <code>argparsers<\/code> in the script.<\/p>",
                "Answer_score":47.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"specifi differ target type oper try link copi differ file oper resourc stage specifi oper multipl time result oper exampl match try oper train resourc file model pth link config json copi test exec python test requir oper train select file model pth target type link file config json target type copi wai achiev kind function thank",
        "Question_preprocessed_content":"specifi differ target type oper try link copi differ file oper resourc stage specifi oper multipl time result oper exampl match try oper train resourc file link copi wai achiev kind function thank",
        "Question_gpt_summary_original":"The user is facing a challenge in linking and copying different files from the same operation resource in their guild stage. They have tried specifying the operation multiple times, but only the last operation is being used. They are seeking a solution to achieve this functionality.",
        "Question_gpt_summary":"user face challeng link copi differ file oper resourc stage tri specifi oper multipl time oper seek solut achiev function",
        "Answer_original_content":"need pipelin manual assign run exec func flag requir exec func flag requir oper select folder file gener pipelin flag step cover http pipelin note exec attribut assign moduel function command execut python script lead funni behavior multipl argpars script",
        "Answer_preprocessed_content":"need pipelin manual assign run cover note attribut assign command execut python script lead funni behavior multipl script",
        "Answer_gpt_summary_original":"possible solutions to link and copy different files from the same operation resource in guild stage are to use a pipeline or manually assign the run id. the pipeline can be created by specifying the target types for each operation and selecting the folder or file generated by the previous operation. it is also important to assign the module\/function name rather than the command to execute for python scripts to avoid funny behavior with multiple argparsers. more information can be found at https:\/\/my.guild.ai\/t\/pipelines\/163.",
        "Answer_gpt_summary":"possibl solut link copi differ file oper resourc stage us pipelin manual assign run pipelin creat specifi target type oper select folder file gener previou oper import assign modul function command execut python script avoid funni behavior multipl argpars inform http pipelin"
    },
    {
        "Question_id":null,
        "Question_title":"Does Vertex AI support multi model endpoints",
        "Question_body":"We have 100's of models and deploying each one to its independent endpoint is very expensive.We are looking for a way to deploy multiple models to a single endpoint.Our docker image will have all the models and we will be having custom logic to invoke the models based on the request from the endpoint.Similar functionality is available in AWS SageMaker.",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_creation_time":1625630160000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":858.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Does-Vertex-AI-support-multi-model-endpoints\/td-p\/163169\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-08T06:12:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"I read the following Vertex AI documentation page:\n\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment#models-endpoint\n\nThis page seems to say that we can deploy multiple models to the same endpoint.\u00a0 If I understand that correctly, you can then serve multiple models from the same endpoint nodes."
            },
            {
                "Answer_creation_time":"2021-07-08T07:17:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"I think this means deploying multiple versions of the same model and not completely independent model."
            },
            {
                "Answer_creation_time":"2021-07-14T07:33:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi there,\n\nYou may deploy totally different models to the same endpoint on Vertex AI and split the traffic as you wish. There is no technical restriction. From a business point of view, you may prefer to have the same (or similar) targeting goals for the models in order to support your decisions."
            },
            {
                "Answer_creation_time":"2021-11-07T16:30:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi, how would that work though, as in, if the endpoint is the same, how do we make sure that we request a specific model prediction. For example, if we deploy 2 different models, say model1 and model2, to the same endpoint, with a traffic split of 50%, then what this means is that all requests to this endpoint are split to the two models with a probability of 0.5, i.e., if a we make a request, sometimes we will be served by model1 and sometimes by model2. How do we make sure we are served by a specific model in this scenario?"
            },
            {
                "Answer_creation_time":"2021-11-16T08:50:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"you can actually deploy a multi model endpoint and to call a specific model, just add the argument \"TargetModel\": 'yourmodelname.tar.gz'.\n\nfor more information refer to this link :\u00a0https:\/\/towardsdatascience.com\/deploy-multiple-tensorflow-models-to-one-endpoint-65bea81c3f2f"
            },
            {
                "Answer_creation_time":"2021-11-16T09:39:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Yes that option is available in amazon sagemaker as the article suggests.Is it also possible with vertex ai"
            },
            {
                "Answer_creation_time":"2022-10-10T06:29:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi,\n\nCould you please suggest how to do this using the Python API? (https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html)\n\nI have been trying but when specifying a `traffic_split` dict, the keys of this dict have to be\u00a0Deployed Model IDs, which makes no sense because the models are not deployed yet when calling `model.deploy()`\n\nThank you!"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"support multi model endpoint model deploi independ endpoint expens look wai deploi multipl model singl endpoint docker imag model have custom logic invok model base request endpoint similar function avail",
        "Question_preprocessed_content":"support multi model endpoint model deploi independ endpoint look wai deploi multipl model singl docker imag model have custom logic invok model base request function avail",
        "Question_gpt_summary_original":"The user is facing the challenge of deploying hundreds of models to independent endpoints, which is expensive. They are looking for a solution to deploy multiple models to a single endpoint using a docker image with custom logic to invoke the models based on the request. They mention that AWS SageMaker has similar functionality.",
        "Question_gpt_summary":"user face challeng deploi hundr model independ endpoint expens look solut deploi multipl model singl endpoint docker imag custom logic invok model base request mention similar function",
        "Answer_original_content":"read follow document page http cloud googl com vertex doc gener deploy model endpoint page deploi multipl model endpoint understand correctli serv multipl model endpoint node think mean deploi multipl version model complet independ model deploi total differ model endpoint split traffic wish technic restrict busi point view prefer similar target goal model order support decis work endpoint sure request specif model predict exampl deploi differ model model model endpoint traffic split mean request endpoint split model probabl request serv model model sure serv specif model scenario actual deploi multi model endpoint specif model add argument targetmodel yourmodelnam tar inform refer link http towardsdatasci com deploi multipl tensorflow model endpoint beacff ye option avail articl suggest possibl suggest python api http googleapi dev python aiplatform latest aiplatform html try specifi traffic split dict kei dict bedeploi model id make sens model deploi call model deploi thank",
        "Answer_preprocessed_content":"read follow document page page deploi multipl model endpoint understand correctli serv multipl model endpoint node think mean deploi multipl version model complet independ model deploi total differ model endpoint split traffic wish technic restrict busi point view prefer target goal model order support decis work endpoint sure request specif model predict exampl deploi differ model model model endpoint traffic split mean request endpoint split model probabl request serv model model sure serv specif model scenario actual deploi multi model endpoint specif model add argument targetmodel inform refer link ye option avail articl possibl suggest python api try specifi dict kei dict bedeploi model id make sens model deploi call thank",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are:\n\n- deploy multiple models to the same endpoint to reduce the cost of deploying each model to its own endpoint.\n- serve multiple models from the same endpoint nodes.\n- deploy totally different models to the same endpoint and split the traffic as desired.\n- deploy a multi-model endpoint and call a specific model by adding the argument \"targetmodel\": 'yourmodelname.tar.gz'.\n- use  to deploy multiple models to one endpoint.\n- use the python api to deploy multiple models to one endpoint.",
        "Answer_gpt_summary":"possibl solut extract answer deploi multipl model endpoint reduc cost deploi model endpoint serv multipl model endpoint node deploi total differ model endpoint split traffic desir deploi multi model endpoint specif model ad argument targetmodel yourmodelnam tar us deploi multipl model endpoint us python api deploi multipl model endpoint"
    },
    {
        "Question_id":null,
        "Question_title":"Dealing with large numbers of small files",
        "Question_body":"<p>A project I\u2019m working on has many small files, which don\u2019t work well with DVC. Instead, I zip up all the files and push them remotely. However, it would be nice to know if I\u2019ve modified any of the files in the zipped folder and diff them with the remote. Am I supposed to be using a pipeline to accomplish this? Is this just a silly goal to have?<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1587658350621,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":1744.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dealing-with-large-numbers-of-small-files\/362",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-04-24T01:52:31.417Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/seanny123\">@Seanny123<\/a>!<\/p>\n<p>DVC does not currently provide any way to compare the contents of a zip file between your local copy and the remote. However, the <a href=\"https:\/\/github.com\/iterative\/dvc\/releases\/tag\/untagged-817d11944b3efb9afcf9\" rel=\"nofollow noopener\">next release<\/a> of DVC will include several optimizations that substantially improve performance for handling many small files. Ideally, you should be able to push your data directly (without using an intermediate zip file), and run <code>dvc status -c<\/code> to see which files differ between your local copy and the remote.<\/p>\n<p>It sounds like your data set has a large number of files, but only expect that a small percentage of them will have been modified each time you want to push to the remote? Performance in this case will be significantly improved in the upcoming release, and I would recommend upgrading DVC whenever the new version is released to see if the performance without using an intermediate zip meets your needs.<\/p>",
                "Answer_score":163.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-04-24T02:07:13.000Z",
                "Answer_body":"<p>Are these changes already in the master branch? Alternatively, could you please link me to the Pull Request where these changes are being prototyped?<\/p>",
                "Answer_score":53.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-04-24T02:22:04.893Z",
                "Answer_body":"<p>These changes are available in the master branch.<\/p>\n<p>If you are interested, <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/3634\" rel=\"nofollow noopener\">https:\/\/github.com\/iterative\/dvc\/pull\/3634<\/a> also contains some benchmarks (using an S3 remote) for the new\/optimized behavior.<\/p>",
                "Answer_score":88.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-15T15:42:26.305Z",
                "Answer_body":"<p>Those changes work great for adding many small files. However, when pushing them, they are still pushed individually when it would be helpful to zip them together before pushing. Is this \u201czip before pushing\u201d functionality something I should be able to do with a pipeline?<\/p>",
                "Answer_score":43.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-18T18:51:22.317Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/seanny123\">@Seanny123<\/a> that\u2019s an existing feature request, please upvote it and feel to comment your case\/support for it on Github: <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1239\">https:\/\/github.com\/iterative\/dvc\/issues\/1239<\/a> Thanks!<\/p>",
                "Answer_score":12.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-19T17:05:38.467Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/seanny123\">@Seanny123<\/a> could elaborate why do you think zipping them together work better in your case?<\/p>\n<p>To my mind - treating them individually should be better for two reasons:<\/p>\n<ol>\n<li>We can upload in parallel in a very aggressive manner (<code>--jobs<\/code> options)<\/li>\n<li>DVC can upload only delta (only new files) instead of the whole bundle every time.<\/li>\n<\/ol>",
                "Answer_score":27.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-19T18:00:24.760Z",
                "Answer_body":"<p>I mostly just wanted to reduce the amount of time it took to upload a large file set of files. Specifically, the whole folder is 20GBs. However, this might be a case wherein I\u2019m abusing DVC. Additionally, I haven\u2019t checked if I\u2019m using the <code>--jobs<\/code> option, so this might just be premature optimization on my end.<\/p>",
                "Answer_score":42.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-19T23:48:21.741Z",
                "Answer_body":"<p>20GB does not sound like an abuse. But I still hope that pushing\/pulling them as a directory should be faster. DVC still takes times to check change in the directory before pushing\/pulling and it might take a bit of time, but overall it should still be better experience, especially if expect removing\/adding small amount of files to the dir.<\/p>",
                "Answer_score":6.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"deal larg number small file project work small file dont work instead zip file push remot nice know iv modifi file zip folder diff remot suppos pipelin accomplish silli goal",
        "Question_preprocessed_content":"deal larg number small file project work small file dont work instead zip file push remot nice know iv modifi file zip folder diff remot suppos pipelin accomplish silli goal",
        "Question_gpt_summary_original":"The user is facing challenges with managing a project that has many small files, which do not work well with DVC. They have resorted to zipping up the files and pushing them remotely, but they are unsure how to check if any of the files in the zipped folder have been modified and diff them with the remote. They are questioning whether they should use a pipeline to achieve this and if their goal is feasible.",
        "Question_gpt_summary":"user face challeng manag project small file work resort zip file push remot unsur check file zip folder modifi diff remot question us pipelin achiev goal feasibl",
        "Answer_original_content":"seanni current provid wai compar content zip file local copi remot releas includ optim substanti improv perform handl small file ideal abl push data directli intermedi zip file run statu file differ local copi remot sound like data set larg number file expect small percentag modifi time want push remot perform case significantli improv upcom releas recommend upgrad new version releas perform intermedi zip meet need chang master branch altern link pull request chang prototyp chang avail master branch interest http github com iter pull contain benchmark remot new optim behavior chang work great ad small file push push individu help zip push zip push function abl pipelin seanni that exist featur request upvot feel comment case support github http github com iter issu thank seanni elabor think zip work better case mind treat individu better reason upload parallel aggress manner job option upload delta new file instead bundl time want reduc time took upload larg file set file specif folder gb case abus addition havent check job option prematur optim end sound like abus hope push pull directori faster take time check chang directori push pull bit time overal better experi especi expect remov ad small file dir",
        "Answer_preprocessed_content":"current provid wai compar content zip file local copi remot releas includ optim substanti improv perform handl small file ideal abl push data directli run file differ local copi remot sound like data set larg number file expect small percentag modifi time want push remot perform case significantli improv upcom releas recommend upgrad new version releas perform intermedi zip meet need chang master branch altern link pull request chang prototyp chang avail master branch interest contain benchmark behavior chang work great ad small file push push individu help zip push zip push function abl pipelin that exist featur request upvot feel comment github thank elabor think zip work better case mind treat individu better reason upload parallel aggress manner upload delta instead bundl time want reduc time took upload larg file set file specif folder gb case abus addition havent check option prematur optim end sound like abus hope directori faster take time check chang directori bit time overal better experi especi expect small file dir",
        "Answer_gpt_summary_original":"possible solutions to the user's problem of managing a project with many small files include pushing data directly without using an intermediate zip file, running status -c to see which files differ between the local copy and the remote, and upgrading to the upcoming release that includes optimizations for handling many small files. the changes are available in the master branch and can be viewed on github. the user can also upvote an existing feature request to zip files together before pushing and comment on their case\/support for it. treating files individually may be better for uploading in parallel and only uploading new files, but pushing\/pulling files as a directory should still be a faster experience, especially if only a small amount of files are added or removed from the directory.",
        "Answer_gpt_summary":"possibl solut user problem manag project small file includ push data directli intermedi zip file run statu file differ local copi remot upgrad upcom releas includ optim handl small file chang avail master branch view github user upvot exist featur request zip file push comment case support treat file individu better upload parallel upload new file push pull file directori faster experi especi small file ad remov directori"
    },
    {
        "Question_id":null,
        "Question_title":"MLflow 1.21.0 released!",
        "Question_body":"We are happy to announce the availability of\u00a0MLflow\u00a01.21.0!\n\n\n\nMLflow 1.21.0 includes several major features and improvements:\n\nFeatures:\n\n[UI] Add a diff-only toggle to the runs table for filtering out columns with constant values (#4862,\u00a0@marijncv)\n[UI] Add a duration column to the runs table (#4840,\u00a0@marijncv)\n[UI] Display the default column sorting order in the runs table (#4847,\u00a0@marijncv)\n[UI] Add\u00a0start_time\u00a0and\u00a0duration\u00a0information to exported runs CSV (#4851,\u00a0@marijncv)\n[UI] Add lifecycle stage information to the run page (#4848,\u00a0@marijncv)\n[UI] Collapse run page sections by default for space efficiency, limit artifact previews to 50MB (#4917,\u00a0@dbczumar)\n[Tracking] Introduce autologging capabilities for PaddlePaddle model training (#4751,\u00a0@jinminhao)\n[Tracking] Add an optional tags field to the CreateExperiment API (#4788,\u00a0@dbczumar;\u00a0#4795,\u00a0@apurva-koti)\n[Tracking] Add support for deleting artifacts from SFTP stores via the\u00a0mlflow gc\u00a0CLI (#4670,\u00a0@afaul)\n[Tracking] Support AzureDefaultCredential for authenticating with Azure artifact storage backends (#4002,\u00a0@marijncv)\n[Models] Upgrade the fastai model flavor to support fastai V2 (>=2.4.1) (#4715,\u00a0@jinzhang21)\n[Models] Introduce an\u00a0mlflow.prophet\u00a0model flavor for Prophet time series models (#4773,\u00a0@BenWilson2)\n[Models] Introduce a CLI for publishing MLflow Models to the SageMaker Model Registry (#4669,\u00a0@jinnig)\n[Models] Print a warning when inferred model dependencies are not available on PyPI (#4891,\u00a0@dbczumar)\n[Models, Projects] Add\u00a0MLFLOW_CONDA_CREATE_ENV_CMD\u00a0for customizing Conda environment creation (#4746,\u00a0@giacomov)\n\nBug fixes and documentation updates:\n\n[UI] Fix an issue where column selections made in the runs table were persisted across experiments (#4926,\u00a0@sunishsheth2009)\n[UI] Fix an issue where the text\u00a0null\u00a0was displayed in the runs table column ordering dropdown (#4924,\u00a0@harupy)\n[UI] Fix a bug causing the metric plot view to display NaN values upon click (#4858,\u00a0@arpitjasa-db)\n[Tracking] Fix a model load failure for paths containing spaces or special characters on UNIX systems (#4890,\u00a0@BenWilson2)\n[Tracking] Correct a migration issue that impacted usage of MLflow Tracking with SQL Server (#4880,\u00a0@marijncv)\n[Tracking] Spark datasource autologging tags now respect the maximum allowable size for MLflow Tracking (#4809,\u00a0@dbczumar)\n[Model Registry] Add previously-missing certificate sources for Model Registry REST API requests (#4731,\u00a0@ericgosno91)\n[Model Registry] Throw an exception when users supply invalid Model Registry URIs for Databricks (#4877,\u00a0@yunpark93)\n[Scoring] Fix a schema enforcement error that incorrectly cast date-like strings to datetime objects (#4902,\u00a0@wentinghu)\n[Docs] Expand the documentation for the MLflow Skinny Client (#4113,\u00a0@eedeleon)\nFor a comprehensive list of changes, see the\u00a0release change log, and check out the latest documentation on\u00a0mlflow.org.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635184341000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":19.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/CSKBY9hL2bo",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-25T17:57:35",
                "Answer_body":"Version 1.21.0 of the MLflow R package has not yet been released. It will be available on CRAN within the next week.\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"releas happi announc avail includ major featur improv featur add diff toggl run tabl filter column constant valu marijncv add durat column run tabl marijncv displai default column sort order run tabl marijncv addstart timeanddurationinform export run csv marijncv add lifecycl stage inform run page marijncv collaps run page section default space effici limit artifact preview dbczumar track introduc autolog capabl paddlepaddl model train jinminhao track add option tag field createexperi api dbczumar apurva koti track add support delet artifact sftp store gccli afaul track support azuredefaultcredenti authent azur artifact storag backend marijncv model upgrad fastai model flavor support fastai jinzhang model introduc prophetmodel flavor prophet time seri model benwilson model introduc cli publish model model registri jinnig model print warn infer model depend avail pypi dbczumar model project add conda creat env cmdfor custom conda environ creation giacomov bug fix document updat fix issu column select run tabl persist experi sunishsheth fix issu textnullwa displai run tabl column order dropdown harupi fix bug caus metric plot view displai nan valu click arpitjasa track fix model load failur path contain space special charact unix system benwilson track correct migrat issu impact usag track sql server marijncv track spark datasourc autolog tag respect maximum allow size track dbczumar model registri add previous miss certif sourc model registri rest api request ericgosno model registri throw except user suppli invalid model registri uri databrick yunpark score fix schema enforc error incorrectli cast date like string datetim object wentinghu doc expand document skinni client eedeleon comprehens list chang thereleas chang log check latest document org",
        "Question_preprocessed_content":"releas happi announc avail includ major featur improv featur add toggl run tabl filter column constant valu add durat column run tabl displai default column sort order run tabl export run csv add lifecycl stage inform run page collaps run page section default space effici limit artifact preview track introduc autolog capabl paddlepaddl model train track add option tag field createexperi api track add support delet artifact sftp store gccli track support azuredefaultcredenti authent azur artifact storag backend model upgrad fastai model flavor support fastai model introduc flavor prophet time seri model model introduc cli publish model model registri model print warn infer model depend avail pypi model project custom conda environ creation bug fix document updat fix issu column select run tabl persist experi fix issu textnullwa displai run tabl column order dropdown fix bug caus metric plot view displai nan valu click track fix model load failur path contain space special charact unix system track correct migrat issu impact usag track sql server track spark datasourc autolog tag respect maximum allow size track model registri add certif sourc model registri rest api request model registri throw except user suppli invalid model registri uri databrick score fix schema enforc error incorrectli cast string datetim object doc expand document skinni client comprehens list chang thereleas chang log check latest document",
        "Question_gpt_summary_original":"The given text does not provide any information about challenges faced by the user. It is an announcement of the availability of MLflow 1.21.0, which includes several major features, improvements, bug fixes, and documentation updates.",
        "Question_gpt_summary":"given text provid inform challeng face user announc avail includ major featur improv bug fix document updat",
        "Answer_original_content":"version packag releas avail cran week",
        "Answer_preprocessed_content":"version packag releas avail cran week",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut mention discuss"
    },
    {
        "Question_id":null,
        "Question_title":"Azure machine learning",
        "Question_body":"Is there any way to integrate MS Dynamics Customer Insights with Azure Machine Learning (designer)?I know there is an integration between CI and Azure Machine Learning studio (classic). Please help to integrate these two services.",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1656618921300,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @Yasuo-9899\n\nThanks for reaching out to us for this question. Are you looking for this document? https:\/\/docs.microsoft.com\/en-us\/dynamics365\/customer-insights\/azure-machine-learning-experiments\n\nI have found one pic which is described the structure well:\n\n\n\n\n\nAnd also a repo you may want to refer to: https:\/\/github.com\/ArtisConsulting\/customer-insights-azure-data-workshop\/blob\/main\/README.md\n\nPlease let us know more details you are interested in so that we can help. Thanks.\n\nRegards,\nYutong",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/909965\/azure-machine-learning-2.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-30T23:43:14.977Z",
                "Answer_score":0,
                "Answer_body":"Hello @Yasuo-9899\n\nThanks for reaching out to us for this question. Are you looking for this document? https:\/\/docs.microsoft.com\/en-us\/dynamics365\/customer-insights\/azure-machine-learning-experiments\n\nI have found one pic which is described the structure well:\n\n\n\n\n\nAnd also a repo you may want to refer to: https:\/\/github.com\/ArtisConsulting\/customer-insights-azure-data-workshop\/blob\/main\/README.md\n\nPlease let us know more details you are interested in so that we can help. Thanks.\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-07-01T22:59:10.557Z",
                "Answer_score":0,
                "Answer_body":"Hi @Yasuo-9899 ,\n\nI believe below document will be helpful.\nGeneral Documentation: https:\/\/docs.microsoft.com\/en-us\/dynamics365\/customer-insights\/custom-models\n\nPre-requisites for correct configuration of a pipeline: https:\/\/docs.microsoft.com\/en-us\/dynamics365\/customer-insights\/azure-machine-learning-experiments\n\nTutorial Documentation: https:\/\/github.com\/naravill\/CustomerInsightsML\n\nRegards,\nPritee",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1656632594976,
        "Question_original_content":" wai integr dynam custom insight design know integr studio classic help integr servic",
        "Question_preprocessed_content":" wai integr dynam custom insight know integr studio help integr servic",
        "Question_gpt_summary_original":"The user is facing a challenge in integrating MS Dynamics Customer Insights with Azure Machine Learning (designer) and is seeking help to integrate these two services. They are aware of the integration between CI and Azure Machine Learning studio (classic).",
        "Question_gpt_summary":"user face challeng integr dynam custom insight design seek help integr servic awar integr studio classic",
        "Answer_original_content":"hello yasuo thank reach question look document http doc microsoft com dynam custom insight azur machin learn experi pic describ structur repo want refer http github com artisconsult custom insight azur data workshop blob main readm let know detail interest help thank regard yutong",
        "Answer_preprocessed_content":"hello thank reach question look document pic describ structur repo want refer let know detail interest help thank regard yutong",
        "Answer_gpt_summary_original":"the answer does not provide any specific solutions for integrating microsoft dynamics customer insights with designer. instead, it provides a link to a document on  experiments and a repository on github that the user may find helpful. the answer also asks for more details from the user to provide further assistance.",
        "Answer_gpt_summary":"answer provid specif solut integr microsoft dynam custom insight design instead provid link document experi repositori github user help answer ask detail user provid assist"
    },
    {
        "Question_id":null,
        "Question_title":"Endopint not consumable after successful model deployment to azure instance container (machine learning studio - designer)",
        "Question_body":"hi, after i register a model and then deploy it on Azure container istance via the graphical interface of the machine learning studio - designer, although the state is \"heathy\" i cannot test the endpoint with data or consume the endpoint. These are the logs of the deploiment\n\n2021-07-27 09:28:09,742 | root | INFO | 500 127.0.0.1 - - [27\/Jul\/2021:09:28:09 +0000] \"POST \/score?verbose=true HTTP\/1.0\" 500 37 \"-\" \"Go-http-client\/1.1\" Exception in worker process Traceback (most recent call last): File \"\/azureml-envs\/azureml_66791b0cfb22a4c054c681ce0ae95fcf\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py\", line 589, in spawn_worker worker.init_process() File \"\/azureml-envs\/azureml_66791b0cfb22a4c054c681ce0ae95fcf\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py\", line 142, in init_process self.run() File \"\/azureml-envs\/azureml_66791b0cfb22a4c054c681ce0ae95fcf\/lib\/python3.6\/site-packages\/gunicorn\/workers\/sync.py\", line 125, in run self.run_for_one(timeout) File \"\/azureml-envs\/azureml_66791b0cfb22a4c054c681ce0ae95fcf\/lib\/python3.6\/site-packages\/gunicorn\/workers\/sync.py\", line 84, in run_for_one self.wait(timeout) File \"\/azureml-envs\/azureml_66791b0cfb22a4c054c681ce0ae95fcf\/lib\/python3.6\/site-packages\/gunicorn\/workers\/sync.py\", line 36, in wait ret = select.select(self.wait_fds, [], [], timeout) File \"\/var\/azureml-server\/routes_common.py\", line 162, in alarm_handler raise TimeoutException(error_message) timeout_exception.TimeoutException Worker exiting (pid: 90) worker timeout is set to 300 Booting worker with pid: 330 SPARK_HOME not set. Skipping PySpark Initialization. Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (azureml-core 1.32.0 (\/azureml-envs\/azureml_66791b0cfb22a4c054c681ce0ae95fcf\/lib\/python3.6\/site-packages), Requirement.parse('azureml-core~=1.31.0')). Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (azureml-core 1.32.0 (\/azureml-envs\/azureml_66791b0cfb22a4c054c681ce0ae95fcf\/lib\/python3.6\/site-packages), Requirement.parse('azureml-core~=1.31.0')). Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (azureml-core 1.32.0 (\/azureml-envs\/azureml_66791b0cfb22a4c054c681ce0ae95fcf\/lib\/python3.6\/site-packages), Requirement.parse('azureml-core~=1.31.0')). Initializing logger 2021-07-27 09:29:12,687 | root | INFO | Starting up app insights client 2021-07-27 09:29:12,687 | root | INFO | Starting up request id generator 2021-07-27 09:29:12,687 | root | INFO | Starting up app insight hooks 2021-07-27 09:29:12,687 | root | INFO | Invoking user's init function 2021-07-27 09:29:12,882 | root | INFO | Users's init has completed successfully 2021-07-27 09:29:12,884 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled. 2021-07-27 09:29:12,884 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled. 2021-07-27 09:29:12,888 | root | INFO | Scoring timeout is found from os.environ: 60000 ms 2021-07-27 09:55:11,506 | root | INFO | Swagger file not present 2021-07-27 09:55:11,506 | root | INFO | 404 127.0.0.1 - - [27\/Jul\/2021:09:55:11 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"Go-http-client\/1.1\"\n\nalso if i try to consume the endpoint it ends with error 502, the specific error is this JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\ni'm trying to deploy the trained model, but the same thing happen if i try to deploy the inference pipeline\n\ni'm referring to this documentation\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-model-designer\n\nmy dount really is \" it seems that i can deploy a model to an Azure Instance Container directly from the deployment tab without creating a container instance separately before, since it seems that it is created at the moment. The process should be authomatic. then the deployment state is healthy, so its ok, but somewhere during the actual deployment something fails, because i can't consume the endpoint\"\n\nthanks for the support",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1627381840390,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/491097\/endopint-not-consumable-after-successful-model-dep.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-27T12:16:12.313Z",
                "Answer_score":0,
                "Answer_body":"@TommasoBassignana-0564 Based on the error I think the input passed to your endpoint is not handled correctly by the entry script that is used. During the deployment process the entry script should contain the init() method to load your model and the run() method should parse the input or JSON and return the result. You can lookup a sample entry script here.\nIf there are any dependencies to be installed you can also add them as part of the conda dependencies file during the deployment. A healthy endpoint indicates successful creation of endpoint but it could still error out if the entry script does not handle the inputs. Thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"endopint consum success model deploy azur instanc contain machin learn studio design regist model deploi azur contain istanc graphic interfac machin learn studio design state heathi test endpoint data consum endpoint log deploiment root info jul post score verbos true http http client except worker process traceback recent file env bcfbaccceaefcf lib python site packag gunicorn arbit line spawn worker worker init process file env bcfbaccceaefcf lib python site packag gunicorn worker base line init process self run file env bcfbaccceaefcf lib python site packag gunicorn worker sync line run self run timeout file env bcfbaccceaefcf lib python site packag gunicorn worker sync line run self wait timeout file env bcfbaccceaefcf lib python site packag gunicorn worker sync line wait ret select select self wait fd timeout file var server rout common line alarm handler rais timeoutexcept error messag timeout except timeoutexcept worker exit pid worker timeout set boot worker pid spark home set skip pyspark initi failur load run type provid fail load entrypoint pipelinerun pipelin core run pipelinerun dto except core env bcfbaccceaefcf lib python site packag requir pars core failur load run type provid fail load entrypoint reusedsteprun pipelin core run steprun reus dto except core env bcfbaccceaefcf lib python site packag requir pars core failur load run type provid fail load entrypoint steprun pipelin core run steprun dto except core env bcfbaccceaefcf lib python site packag requir pars core initi logger root info start app insight client root info start request gener root info start app insight hook root info invok user init function root info user init complet successfulli root info skip middlewar dbg model info enabl root info skip middlewar dbg resourc usag enabl root info score timeout environ root info swagger file present root info jul swagger json http http client try consum endpoint end error specif error jsondecodeerror expect valu line column char try deploi train model thing happen try deploi infer pipelin refer document http doc microsoft com azur machin learn deploi model design dount deploi model azur instanc contain directli deploy tab creat contain instanc separ creat moment process authomat deploy state healthi actual deploy fail consum endpoint thank support",
        "Question_preprocessed_content":"endopint consum success model deploy azur instanc contain regist model deploi azur contain istanc graphic interfac machin learn studio design state heathi test endpoint data consum endpoint log deploiment root info post except worker process traceback file line file line file line run file line file line wait ret timeout file line rais worker exit worker timeout set boot worker pid set skip pyspark initi failur load fail load entrypoint pipelinerun except failur load fail load entrypoint reusedsteprun except failur load fail load entrypoint steprun except initi logger root info start app insight client root info start request gener root info start app insight hook root info invok user init function root info user init complet successfulli root info skip middlewar enabl root info skip middlewar enabl root info score timeout root info swagger file present root info try consum endpoint end error specif error jsondecodeerror expect valu line column try deploi train model thing happen try deploi infer pipelin refer document dount deploi model azur instanc contain directli deploy tab creat contain instanc separ creat moment process authomat deploy state healthi actual deploy fail consum endpoint thank support",
        "Question_gpt_summary_original":"The user is facing challenges with testing and consuming the endpoint after successfully deploying a model on Azure container instance via the graphical interface of the machine learning studio - designer. Although the deployment state is \"healthy\", the user is unable to test the endpoint with data or consume the endpoint. The logs of the deployment show errors related to worker timeout, loading of entry points, and JSON decoding. The user is referring to the documentation for deploying the model and is unsure if the container instance needs to be created separately before deployment.",
        "Question_gpt_summary":"user face challeng test consum endpoint successfulli deploi model azur contain instanc graphic interfac machin learn studio design deploy state healthi user unabl test endpoint data consum endpoint log deploy error relat worker timeout load entri point json decod user refer document deploi model unsur contain instanc need creat separ deploy",
        "Answer_original_content":"tommasobassignana base error think input pass endpoint handl correctli entri script deploy process entri script contain init method load model run method pars input json return result lookup sampl entri script depend instal add conda depend file deploy healthi endpoint indic success creation endpoint error entri script handl input thank",
        "Answer_preprocessed_content":"base error think input pass endpoint handl correctli entri script deploy process entri script contain init method load model run method pars input json return result lookup sampl entri script depend instal add conda depend file deploy healthi endpoint indic success creation endpoint error entri script handl input thank",
        "Answer_gpt_summary_original":"the possible solutions to the challenge of a non-consumable endpoint despite a \"healthy\" deployment state in  studio designer are: checking the entry script for the init() and run() methods, ensuring that the input or json is parsed correctly, adding dependencies to the conda dependencies file during deployment, and verifying that the entry script handles the inputs correctly.",
        "Answer_gpt_summary":"possibl solut challeng non consum endpoint despit healthi deploy state studio design check entri script init run method ensur input json pars correctli ad depend conda depend file deploy verifi entri script handl input correctli"
    },
    {
        "Question_id":70955450.0,
        "Question_title":"How to return all labels and scores in SageMaker Inference?",
        "Question_body":"<p>I am using the <code>HuggingFacePredictor<\/code> from <code>sagemaker.huggingface<\/code> to inference some text and I would like to get all label scores.<\/p>\n<p>Is there any way of getting, as response from the endpoint:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;labels&quot;: [&quot;help&quot;, &quot;Greeting&quot;, &quot;Farewell&quot;] ,\n    &quot;score&quot;: [0.81, 0.1, 0.09],\n}\n<\/code><\/pre>\n<p>(or similar)<\/p>\n<p>Instead of:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;label&quot;: &quot;help&quot;,\n    &quot;score&quot;: 0.81,\n}\n<\/code><\/pre>\n<p>Here is some example code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nfrom sagemaker.huggingface import HuggingFacePredictor\nfrom sagemaker.session import Session\n\nsagemaker_session = Session(boto_session=boto3.session.Session())\n\npredictor = HuggingFacePredictor(\n    endpoint_name=project, sagemaker_session=sagemaker_session\n)\nprediciton = predictor.predict({&quot;inputs&quot;: text})[0]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1643804083130,
        "Question_favorite_count":null,
        "Question_last_edit_time":1643805132912,
        "Question_score":1.0,
        "Question_view_count":193.0,
        "Answer_body":"<p>With your current code sample, it is not quite clear what specific task you are performing, but for the sake of this answer, I'll assume you're doing text classification.<\/p>\n<p>Most importantly, though, we can read the following in <a href=\"https:\/\/huggingface.co\/docs\/sagemaker\/reference#inference-toolkit-api\" rel=\"nofollow noreferrer\">Huggingface's Sagemaker reference document<\/a> (bold highlight by me):<\/p>\n<blockquote>\n<p>The Inference Toolkit accepts inputs in the inputs key, and <strong>supports additional <code>pipelines<\/code> parameters in the <code>parameters<\/code> key<\/strong>. You can provide any of the supported <code>kwargs<\/code> from <code>pipelines<\/code> as parameters.<\/p>\n<\/blockquote>\n<p>If we check out the <a href=\"https:\/\/huggingface.co\/docs\/transformers\/v4.16.2\/en\/main_classes\/pipelines#transformers.TextClassificationPipeline.__call__\" rel=\"nofollow noreferrer\">accepted arguments by the <code>TextClassificationPipeline<\/code><\/a>, we can see that there is indeed one that returns all samples:<\/p>\n<blockquote>\n<p><code>return_all_scores<\/code> (bool, optional, defaults to False) \u2014 Whether to return scores for all labels.<\/p>\n<\/blockquote>\n<p>While I unfortunately don't have access to Sagemaker inference, I can run a sample to illustrate the output with a local pipeline:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import pipeline\n# uses 2-way sentiment classification model per default\npipe = pipeline(&quot;text-classification&quot;) \n\npipe(&quot;I am really angry right now &gt;:(&quot;, return_all_scores=True)\n# Output: [[{'label': 'NEGATIVE', 'score': 0.9989138841629028},\n#           {'label': 'POSITIVE', 'score': 0.0010860705515369773}]]\n<\/code><\/pre>\n<p>Based on the slightly different input format expected by Sagemaker, coupled with the example given in <a href=\"https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/sagemaker\/10_deploy_model_from_s3\/deploy_transformer_model_from_s3.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a>, I would assume that a corrected input in your own example code should look like this:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;inputs&quot;: text,\n    &quot;parameters&quot;: {&quot;return_all_scores&quot;: True}\n}\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1643813773008,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70955450",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1643809235227,
        "Question_original_content":"return label score infer huggingfacepredictor huggingfac infer text like label score wai get respons endpoint label help greet farewel score similar instead label help score exampl code import boto huggingfac import huggingfacepredictor session import session session session boto session boto session session predictor huggingfacepredictor endpoint project session session prediciton predictor predict input text",
        "Question_preprocessed_content":"return label score infer infer text like label score wai get respons endpoint similar instead exampl code",
        "Question_gpt_summary_original":"The user is using HuggingFacePredictor from sagemaker.huggingface to inference some text and wants to get all label scores instead of just one label and score. The user is seeking a way to get a response from the endpoint that includes all labels and scores, rather than just one label and score.",
        "Question_gpt_summary":"user huggingfacepredictor huggingfac infer text want label score instead label score user seek wai respons endpoint includ label score label score",
        "Answer_original_content":"current code sampl clear specif task perform sake answer assum text classif importantli read follow huggingfac refer document bold highlight infer toolkit accept input input kei support addit pipelin paramet paramet kei provid support kwarg pipelin paramet check accept argument textclassificationpipelin return sampl return score bool option default fals return score label unfortun access infer run sampl illustr output local pipelin transform import pipelin us wai sentiment classif model default pipe pipelin text classif pipe angri right return score true output label neg score label posit score base slightli differ input format expect coupl exampl given notebook assum correct input exampl code look like input text paramet return score true",
        "Answer_preprocessed_content":"current code sampl clear specif task perform sake answer assum text classif importantli read follow huggingfac refer document infer toolkit accept input input kei support addit paramet kei provid support paramet check accept argument return sampl return score label unfortun access infer run sampl illustr output local pipelin base slightli differ input format expect coupl exampl given notebook assum correct input exampl code look like",
        "Answer_gpt_summary_original":"the answer suggests that the user can use the \"return_all_scores\" parameter in the \"textclassificationpipeline\" to return all labels and scores in inference using the huggingfacepredictor from .huggingface. the answer also provides a sample code to illustrate the output. the user needs to provide the input in the correct format with the \"inputs\" and \"parameters\" keys.",
        "Answer_gpt_summary":"answer suggest user us return score paramet textclassificationpipelin return label score infer huggingfacepredictor huggingfac answer provid sampl code illustr output user need provid input correct format input paramet kei"
    },
    {
        "Question_id":null,
        "Question_title":"Stage trials error",
        "Question_body":"<p>I\u2019ve tested the <code>--stage-trials<\/code> feature with <code>queues<\/code> today, all worked great. But now I call <code>guild run op--staged-trials<\/code>, and check with <code>guild runs<\/code>, some runs are staged, others show errors<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/8fee18f78b819bce49fcc791997addfc1beed81c.png\" data-download-href=\"\/uploads\/short-url\/kxgoN75AKZNvWS4wd90nbgU9u0I.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/8fee18f78b819bce49fcc791997addfc1beed81c.png\" alt=\"image\" data-base62-sha1=\"kxgoN75AKZNvWS4wd90nbgU9u0I\" width=\"690\" height=\"175\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/8fee18f78b819bce49fcc791997addfc1beed81c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">753\u00d7192 5.41 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>A minute later I try <code>guild runs<\/code> again, and all runs show as <code>error<\/code> without me having done anything. What could cause the staged trials to flip to errors all of a sudden? This always happens now.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/3e9499c087f43e99cf0b7df8049e6a1fdd03a241.png\" data-download-href=\"\/uploads\/short-url\/8VBYFfkCYF3cwNgAMfGh4A2J561.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/3e9499c087f43e99cf0b7df8049e6a1fdd03a241.png\" alt=\"image\" data-base62-sha1=\"8VBYFfkCYF3cwNgAMfGh4A2J561\" width=\"690\" height=\"176\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/3e9499c087f43e99cf0b7df8049e6a1fdd03a241_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">757\u00d7194 5.21 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1627609081354,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":275.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/stage-trials-error\/741",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-30T17:29:23.645Z",
                "Answer_body":"<p>I believe I was able to fix this. After running <code>ps -u username<\/code>, I found there were some zombie <code>guild<\/code> and <code>python<\/code> processes still running. After killing them, the staging immediately worked again. This may or may not be related to me using <code>queue<\/code> the day before. I did stop them with <code>guild stop -Fo queue<\/code>, but it may not have completely killed the processes.<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-14T16:58:07.631Z",
                "Answer_body":"<p>Yeah that sounds like a queue that is still running \u2014 cleaning up the errant processes is the right solution there.<\/p>\n<p>Guild uses runs to track these queue processes \u2014 this is why they\u2019re run as runs and not some other Guild process, which would otherwise disappear from view when run. Runs are all represented on disk (the run dir) and tied to OS processes via pid and lock files. This is standard operating procedure for process orchestration.<\/p>\n<p>Guild could use some housekeeping functions, for example, scanning for orphaned processes and providing a way to forceably stop them.<\/p>",
                "Answer_score":21.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-15T18:08:22.833Z",
                "Answer_body":"<p>Yeah I think some sort of automatic housekeeping would be great as this seems to be a common issue. Thanks!<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"stage trial error iv test stage trial featur queue todai work great run stage trial check run run stage error imag minut later try run run error have caus stage trial flip error sudden happen imag",
        "Question_preprocessed_content":"stage trial error iv test featur todai work great check run stage error imag minut later try run have caus stage trial flip error sudden happen imag",
        "Question_gpt_summary_original":"The user encountered challenges with the \"--stage-trials\" feature while running \"guild run op--staged-trials\". Some runs were staged while others showed errors. Additionally, all runs showed errors a minute later without any action from the user. The cause of this sudden flip to errors is unknown.",
        "Question_gpt_summary":"user encount challeng stage trial featur run run stage trial run stage show error addition run show error minut later action user caus sudden flip error unknown",
        "Answer_original_content":"believ abl fix run usernam zombi python process run kill stage immedi work relat queue dai stop stop queue complet kill process yeah sound like queue run clean errant process right solut us run track queue process theyr run run process disappear view run run repres disk run dir ti process pid lock file standard oper procedur process orchestr us housekeep function exampl scan orphan process provid wai forceabl stop yeah think sort automat housekeep great common issu thank",
        "Answer_preprocessed_content":"believ abl fix run zombi process run kill stage immedi work relat dai stop complet kill process yeah sound like queue run clean errant process right solut us run track queue process theyr run run process disappear view run run repres disk ti process pid lock file standard oper procedur process orchestr us housekeep function exampl scan orphan process provid wai forceabl stop yeah think sort automat housekeep great common issu thank",
        "Answer_gpt_summary_original":"possible solutions to the issue of staged trials suddenly flipping to errors without any changes being made are to check for zombie guild and python processes still running and kill them, as well as to clean up any errant processes related to the queue. the user may have stopped the queue processes with \"guild stop -fo queue,\" but it may not have completely killed the processes. guild could use some housekeeping functions, such as scanning for orphaned processes and providing a way to forcibly stop them.",
        "Answer_gpt_summary":"possibl solut issu stage trial suddenli flip error chang check zombi python process run kill clean errant process relat queue user stop queue process stop queue complet kill process us housekeep function scan orphan process provid wai forcibl stop"
    },
    {
        "Question_id":null,
        "Question_title":"Find the \"run history at log step\" for a particular artifact using the API?",
        "Question_body":"<p>In the web UI, for a particular dashboard, under metadata I am interested in the \u201cRun history at log step\u201d information. I\u2019d like to retrieve this using the API.<\/p>\n<p>If I use run.scan_history, I get rows with model scores and the epoch and step, but not the artifact ID.<\/p>\n<p>If I have a public API Artifact, I have the artifact ID and version but not the epoch or step. Artifact<\/p>\n<p>How do I find the run history at log step for a particular artifact? I\u2019ve checked the API code and docs but still can\u2019t figure it out.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1675496600113,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":33.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/find-the-run-history-at-log-step-for-a-particular-artifact-using-the-api\/3819",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-07T18:29:14.634Z",
                "Answer_body":"<p>Hello Joseph!<\/p>\n<p>Just to get a better understanding of your question:<\/p>\n<ul>\n<li>Could you share your workspace that you are working on?<\/li>\n<li>What do you mean by \u201cRun history at log step\u201d?<\/li>\n<li>Are you trying to pull an artifact generated by a run or an artifact that you have logged?<\/li>\n<\/ul>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-10T00:03:04.176Z",
                "Answer_body":"<p>Hi Joseph, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"run histori log step particular artifact api web particular dashboard metadata interest run histori log step inform like retriev api us run scan histori row model score epoch step artifact public api artifact artifact version epoch step artifact run histori log step particular artifact iv check api code doc figur",
        "Question_preprocessed_content":"run histori log step particular artifact api web particular dashboard metadata interest run histori log step inform like retriev api us row model score epoch step artifact public api artifact artifact version epoch step artifact run histori log step particular artifact iv check api code doc figur",
        "Question_gpt_summary_original":"The user is facing challenges in retrieving the \"Run history at log step\" information for a particular artifact using the API. They have tried using the run.scan_history method but it does not provide the artifact ID. They also have the artifact ID and version but not the epoch or step. The user is seeking guidance on how to find the run history at log step for a specific artifact.",
        "Question_gpt_summary":"user face challeng retriev run histori log step inform particular artifact api tri run scan histori method provid artifact artifact version epoch step user seek guidanc run histori log step specif artifact",
        "Answer_original_content":"hello joseph better understand question share workspac work mean run histori log step try pull artifact gener run artifact log joseph heard go close request like open convers let know",
        "Answer_preprocessed_content":"hello joseph better understand question share workspac work mean run histori log step try pull artifact gener run artifact log joseph heard go close request like convers let know",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer as the responder is seeking clarification on the user's question.",
        "Answer_gpt_summary":"solut provid answer respond seek clarif user question"
    },
    {
        "Question_id":52705769.0,
        "Question_title":"Azure ML Tune Model Hyper Parameters",
        "Question_body":"<p>Here's question proposed at the end of the chapter in 70-774 exam reference book. <\/p>\n\n<blockquote>\n  <p>If you connect a neural network with a Tune Model Hyperparameters module configured\n  with Random Sweep and Maximum number of runs on random sweep = 1, how\n  many neural networks are trained during the execution of the experiment? Why? If you\n  connect a validation dataset to the third input of the Tune Model Hyperparameters\n  module, how many neural networks are trained now?<\/p>\n<\/blockquote>\n\n<p>And the answer is :<\/p>\n\n<blockquote>\n  <p>Without validation dataset 11 (10 of k-fold cross validation + 1 trained with all the data\n  with the best combination of hyperparameters). With the validation set only 1 neural\n  network is trained, so the best model is not trained using the validation set if you provide\n  it.<\/p>\n<\/blockquote>\n\n<p>Where does 10 come from? As far as I understand the number should be 2 and 1 respectively. Shouldn't it create n-folds where n is equal to the number of runs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1539013176610,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":329.0,
        "Answer_body":"<p>When you use the Tune Model Hyperparameters module without a validation dataset, this means, when you use only the 2nd input data port, the module works in cross-validation mode. So the best-parameters model is found by doing cross-validation over the provided dataset, and to do this, the dataset is splitted in k-folds. By default, the module splits the data in 10 folds. In case you want to split the data in a different number of folds, you can connect a Partition and Sample module at the 2nd input, selecting Assign to Folds and indicating the number of folds desired. In many cases k=5 is a reasonable option.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52705769",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1539202336323,
        "Question_original_content":"tune model hyper paramet question propos end chapter exam refer book connect neural network tune model hyperparamet modul configur random sweep maximum number run random sweep neural network train execut experi connect valid dataset input tune model hyperparamet modul neural network train answer valid dataset fold cross valid train data best combin hyperparamet valid set neural network train best model train valid set provid come far understand number respect shouldn creat fold equal number run",
        "Question_preprocessed_content":"tune model hyper paramet question propos end chapter exam refer book connect neural network tune model hyperparamet modul configur random sweep maximum number run random sweep neural network train execut experi connect valid dataset input tune model hyperparamet modul neural network train answer valid dataset valid set neural network train best model train valid set provid come far understand number respect shouldn creat equal number run",
        "Question_gpt_summary_original":"The user is questioning the answer to a question in a reference book regarding the number of neural networks trained during the execution of an experiment using a Tune Model Hyperparameters module configured with Random Sweep and Maximum number of runs on random sweep = 1. The given answer states that without a validation dataset, 11 neural networks are trained (10 of k-fold cross-validation + 1 trained with all the data with the best combination of hyperparameters), while with a validation dataset, only 1 neural network is trained, and the best model is not trained using the validation set if provided. The user is confused about where the number 10 comes from and believes it should be 2 and 1, respectively.",
        "Question_gpt_summary":"user question answer question refer book number neural network train execut experi tune model hyperparamet modul configur random sweep maximum number run random sweep given answer state valid dataset neural network train fold cross valid train data best combin hyperparamet valid dataset neural network train best model train valid set provid user confus number come believ respect",
        "Answer_original_content":"us tune model hyperparamet modul valid dataset mean us input data port modul work cross valid mode best paramet model cross valid provid dataset dataset split fold default modul split data fold case want split data differ number fold connect partit sampl modul input select assign fold indic number fold desir case reason option",
        "Answer_preprocessed_content":"us tune model hyperparamet modul valid dataset mean us input data port modul work mode model provid dataset dataset split default modul split data fold case want split data differ number fold connect partit sampl modul input select assign fold indic number fold desir case reason option",
        "Answer_gpt_summary_original":"possible solutions to the challenge of understanding the number of neural networks trained during an experiment with a tune model hyperparameters module configured with random sweep and maximum number of runs on random sweep = 1 are: \n- when using the tune model hyperparameters module without a validation dataset, the module works in cross-validation mode, which means the dataset is split into k-folds (default is 10 folds) to find the best-parameters model.\n- to split the data into a different number of folds, a partition and sample module can be connected at the 2nd input, selecting assign to folds and indicating the desired number of folds (k=5 is a reasonable option in many cases).",
        "Answer_gpt_summary":"possibl solut challeng understand number neural network train experi tune model hyperparamet modul configur random sweep maximum number run random sweep tune model hyperparamet modul valid dataset modul work cross valid mode mean dataset split fold default fold best paramet model split data differ number fold partit sampl modul connect input select assign fold indic desir number fold reason option case"
    },
    {
        "Question_id":null,
        "Question_title":"Failure while loading azureml_run_type_providers",
        "Question_body":"I am running RStudio on azureml compute instance.\n\nRunning the test examples in vignettes used to work.\n\nHowever, today I encountered this error:\n\n\n\n library(azuremlsdk)\n # current directory set to source code \n # config file in current folder \"config.js\"\n ws <- load_workspace_from_config() \n    \n Error:\n Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (portalocker 2.0.0 (\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages), Requirement.parse('portalocker~=1.0'), {'msal-extensions'}).\n\n\n\nHerman",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1599302705087,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/87272\/failure-while-loading-azureml-run-type-providers.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-09-07T14:13:15.257Z",
                "Answer_score":0,
                "Answer_body":"@HermanTan-5956 Thanks for the question. Can you please share the sample that you are trying. If possible please share the full service logs to check.\n\nWe have forwarded to the product team to check on this error.\nPlease follow the below at the azuremlsdk package, which operationalizes R to Azure ML service and is built on reticulate.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-r-experiment\n\nhttps:\/\/azure.github.io\/azureml-sdk-for-r\/\n\nhttps:\/\/github.com\/Azure\/azureml-sdk-for-r",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-09-18T14:28:53.323Z",
                "Answer_score":2,
                "Answer_body":"I was able to get rid of it using this pip install\n\n$ pip install azureml-sdk[notebooks]",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"failur load run type provid run rstudio comput instanc run test exampl vignett work todai encount error librari sdk current directori set sourc code config file current folder config load workspac config error failur load run type provid fail load entrypoint automl train automl run automlrun run dto except portalock anaconda env lib python site packag requir pars portalock msal extens herman",
        "Question_preprocessed_content":"failur load run rstudio comput instanc run test exampl vignett work todai encount error librari current directori set sourc code config file current folder error failur load fail load entrypoint automl except herman",
        "Question_gpt_summary_original":"The user encountered an error while running RStudio on an azureml compute instance. The error message indicated a failure while loading azureml_run_type_providers, specifically the entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto. The error was caused by a missing requirement, portalocker~=1.0, which was conflicting with msal-extensions.",
        "Question_gpt_summary":"user encount error run rstudio comput instanc error messag indic failur load run type provid specif entrypoint automl train automl run automlrun run dto error caus miss requir portalock conflict msal extens",
        "Answer_original_content":"hermantan thank question share sampl try possibl share servic log check forward product team check error follow sdk packag operation servic built reticul http doc microsoft com azur machin learn tutori experi http azur github sdk http github com azur sdk abl rid pip instal pip instal sdk notebook",
        "Answer_preprocessed_content":"thank question share sampl try possibl share servic log check forward product team check error follow sdk packag operation servic built reticul abl rid pip instal pip instal sdk",
        "Answer_gpt_summary_original":"possible solutions from the answer include sharing the sample that caused the error and providing full service logs for checking. the product team has been notified of the error. the answer also suggests using the sdk package built on reticulate and provides links to relevant documentation. additionally, the answer mentions getting rid of the error by using the command \"pip install -sdk[notebooks].\"",
        "Answer_gpt_summary":"possibl solut answer includ share sampl caus error provid servic log check product team notifi error answer suggest sdk packag built reticul provid link relev document addition answer mention get rid error command pip instal sdk notebook"
    },
    {
        "Question_id":null,
        "Question_title":"how can i retrain the model after a period of time",
        "Question_body":"Hello everyone, i'm using lambda architecture to build a fraud detection project , i build my model using machine learning in databricks , after saving the model , i load the model in the speed layer to predict the incoming data, i want to know how can i retrain this model using new incoming data from eventhub ??\ndoes the retrain should be in the batch layer ?\nthanks for helping",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1619232734657,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/370080\/how-can-i-retrain-the-model-after-a-period-of-time.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-27T06:45:34.227Z",
                "Answer_score":0,
                "Answer_body":"@NadineBenharrath-4116 Thanks, Please follow the document to retrain the model using the new data.\n\nCurrently Data Drift Monitor (Data Drift->EventGrid->LogicApp->Trigger Pipeline) allows to trigger a pipeline when dataset drift has been detected.\n\nPublic Repo of the architecture and code: https:\/\/github.com\/Microsoft\/MLOps",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-27T07:10:26.91Z",
                "Answer_score":0,
                "Answer_body":"@ramr-msft the problem that i trained my model using databricks and i tracked it with mlflow , i'm not using azure ML because my dataset is imbalanced ?",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"retrain model period time hello lambda architectur build fraud detect project build model machin learn databrick save model load model speed layer predict incom data want know retrain model new incom data eventhub retrain batch layer thank help",
        "Question_preprocessed_content":"retrain model period time hello lambda architectur build fraud detect project build model machin learn databrick save model load model speed layer predict incom data want know retrain model new incom data eventhub retrain batch layer thank help",
        "Question_gpt_summary_original":"The user is facing a challenge of retraining a machine learning model used for fraud detection in a lambda architecture. They want to know how to retrain the model using new incoming data from eventhub and whether the retraining should be done in the batch layer.",
        "Question_gpt_summary":"user face challeng retrain machin learn model fraud detect lambda architectur want know retrain model new incom data eventhub retrain batch layer",
        "Answer_original_content":"nadinebenharrath thank follow document retrain model new data current data drift monitor data drift eventgrid logicapp trigger pipelin allow trigger pipelin dataset drift detect public repo architectur code http github com microsoft mlop ramr msft problem train model databrick track dataset imbalanc",
        "Answer_preprocessed_content":"thank follow document retrain model new data current data drift monitor allow trigger pipelin dataset drift detect public repo architectur code problem train model databrick track dataset imbalanc",
        "Answer_gpt_summary_original":"possible solutions from the answer include following a document to retrain the machine learning model using new data, using a data drift monitor to trigger a pipeline when dataset drift is detected, and accessing a public repository of the architecture and code. the answer also mentions a problem with using  due to an imbalanced dataset.",
        "Answer_gpt_summary":"possibl solut answer includ follow document retrain machin learn model new data data drift monitor trigger pipelin dataset drift detect access public repositori architectur code answer mention problem imbalanc dataset"
    },
    {
        "Question_id":36297520.0,
        "Question_title":"Using Sacred Module with iPython",
        "Question_body":"<p>I am trying to set up <a href=\"https:\/\/pypi.python.org\/pypi\/sacred\" rel=\"nofollow noreferrer\"><code>sacred<\/code><\/a> for Python and I am going through the <a href=\"http:\/\/sacred.readthedocs.org\/en\/latest\/quickstart.html\" rel=\"nofollow noreferrer\">tutorial<\/a>. I was able to set up sacred using <code>pip install sacred<\/code> with no issues. I am having trouble running the basic code:<\/p>\n\n<pre><code>from sacred import Experiment\n\nex = Experiment(\"hello_world\")\n<\/code><\/pre>\n\n<p>Running this code returns the a <code>ValueError<\/code>:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-25-66f549cfb192&gt; in &lt;module&gt;()\n      1 from sacred import Experiment\n      2 \n----&gt; 3 ex = Experiment(\"hello_world\")\n\n\/Users\/ryandevera\/anaconda\/lib\/python2.7\/site-packages\/sacred\/experiment.pyc in __init__(self, name, ingredients)\n     42         super(Experiment, self).__init__(path=name,\n     43                                          ingredients=ingredients,\n---&gt; 44                                          _caller_globals=caller_globals)\n     45         self.default_command = \"\"\n     46         self.command(print_config, unobserved=True)\n\n\/Users\/ryandevera\/anaconda\/lib\/python2.7\/site-packages\/sacred\/ingredient.pyc in __init__(self, path, ingredients, _caller_globals)\n     48         self.doc = _caller_globals.get('__doc__', \"\")\n     49         self.sources, self.dependencies = \\\n---&gt; 50             gather_sources_and_dependencies(_caller_globals)\n     51 \n     52     # =========================== Decorators ==================================\n\n\/Users\/ryandevera\/anaconda\/lib\/python2.7\/site-packages\/sacred\/dependencies.pyc in gather_sources_and_dependencies(globs)\n    204 def gather_sources_and_dependencies(globs):\n    205     dependencies = set()\n--&gt; 206     main = Source.create(globs.get('__file__'))\n    207     sources = {main}\n    208     experiment_path = os.path.dirname(main.filename)\n\n\/Users\/ryandevera\/anaconda\/lib\/python2.7\/site-packages\/sacred\/dependencies.pyc in create(filename)\n     61         if not filename or not os.path.exists(filename):\n     62             raise ValueError('invalid filename or file not found \"{}\"'\n---&gt; 63                              .format(filename))\n     64 \n     65         mainfile = get_py_file_if_possible(os.path.abspath(filename))\n\nValueError: invalid filename or file not found \"None\"\n<\/code><\/pre>\n\n<p>I am not sure why this error is returning. The documentation does not say anything about setting up an Experiment file prior to running the code. Any help would be greatly appreciated!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":5,
        "Question_creation_time":1459297642757,
        "Question_favorite_count":null,
        "Question_last_edit_time":1505634379180,
        "Question_score":4.0,
        "Question_view_count":1818.0,
        "Answer_body":"<p>The traceback given indicates that the constructor for <code>Experiment<\/code> searches its namespace to find the file in which its defined.<\/p>\n\n<p>Thus, to make the example work, place the example code into a file and run that file directly.<\/p>\n\n<p>If you are using <code>ipython<\/code>, then you could always try using the <code>%%python<\/code> command, which will effectively capture the code you give it into a file before running it (in a separate python process).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36297520",
        "Tool":"Sacred",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1459298810363,
        "Question_original_content":"modul ipython try set python go tutori abl set pip instal issu have troubl run basic code import experi experi hello world run code return valueerror valueerror traceback recent import experi experi hello world user ryandevera anaconda lib python site packag experi pyc init self ingredi super experi self init path ingredi ingredi caller global caller global self default command self command print config unobserv true user ryandevera anaconda lib python site packag ingredi pyc init self path ingredi caller global self doc caller global doc self sourc self depend gather sourc depend caller global decor user ryandevera anaconda lib python site packag depend pyc gather sourc depend glob def gather sourc depend glob depend set main sourc creat glob file sourc main experi path path dirnam main filenam user ryandevera anaconda lib python site packag depend pyc creat filenam filenam path exist filenam rais valueerror invalid filenam file format filenam mainfil file possibl path abspath filenam valueerror invalid filenam file sure error return document set experi file prior run code help greatli appreci",
        "Question_preprocessed_content":"modul ipython try set python go tutori abl set issu have troubl run basic code run code return sure error return document set experi file prior run code help greatli appreci",
        "Question_gpt_summary_original":"The user is encountering a ValueError when trying to run basic code using the Sacred module in Python. The error message states that the filename is invalid or not found. The user is unsure why this error is occurring and is seeking assistance.",
        "Question_gpt_summary":"user encount valueerror try run basic code modul python error messag state filenam invalid user unsur error occur seek assist",
        "Answer_original_content":"traceback given indic constructor experi search namespac file defin exampl work place exampl code file run file directli ipython try python command effect captur code file run separ python process",
        "Answer_preprocessed_content":"traceback given indic constructor search namespac file defin exampl work place exampl code file run file directli try command effect captur code file run",
        "Answer_gpt_summary_original":"possible solutions to the valueerror encountered by the user when setting up a module for python and running basic code are: placing the example code into a file and running that file directly, or using the %%python command if using ipython, which captures the code into a file before running it in a separate python process. the traceback indicates that the constructor for experiment searches its namespace to find the file in which it's defined.",
        "Answer_gpt_summary":"possibl solut valueerror encount user set modul python run basic code place exampl code file run file directli python command ipython captur code file run separ python process traceback indic constructor experi search namespac file defin"
    },
    {
        "Question_id":null,
        "Question_title":"Has SAS code ever been successfully ran on SageMaker?",
        "Question_body":"Has SAS code ever been successfully ran on SageMaker?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596105364000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":94.0,
        "Answer_body":"I\u2019ve helped customers run SAS on a notebook through a kernel and that was their main use case, but we also showed them how they can containerize SAS. Worked well",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqMU2EBqGTDCMTPsB5rjNoQ\/has-sas-code-ever-been-successfully-ran-on-sage-maker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-30T14:02:45.000Z",
                "Answer_score":0,
                "Answer_body":"I\u2019ve helped customers run SAS on a notebook through a kernel and that was their main use case, but we also showed them how they can containerize SAS. Worked well",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1596117765000,
        "Question_original_content":"sa code successfulli ran sa code successfulli ran",
        "Question_preprocessed_content":"sa code successfulli ran sa code successfulli ran",
        "Question_gpt_summary_original":"The user is facing a challenge of determining whether SAS code can be successfully run on SageMaker.",
        "Question_gpt_summary":"user face challeng determin sa code successfulli run",
        "Answer_original_content":"iv help custom run sa notebook kernel main us case show container sa work",
        "Answer_preprocessed_content":"iv help custom run sa notebook kernel main us case show container sa work",
        "Answer_gpt_summary_original":"possible solutions: \n- running sas on a notebook through a kernel\n- containerizing sas",
        "Answer_gpt_summary":"possibl solut run sa notebook kernel container sa"
    },
    {
        "Question_id":null,
        "Question_title":"Best practices for specifying dependencies",
        "Question_body":"<p>In a pipeline, if <code>folder1<\/code> is the output of <code>stage1<\/code> and used in <code>stage2<\/code>, then dvc will know about it if we run <code>dvc run -d folder1 -o folder2 python stage2.py<\/code><\/p>\n<p>Equivalently, since stage1 creates a dvc file, we could run: <code>dvc run -d stage1.dvc -o folder2 python stage2.py<\/code><\/p>\n<p>What is the recommended way ?<\/p>\n<p>Thanks !<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1536759423939,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":760.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practices-for-specifying-dependencies\/96",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-09-12T14:08:22.122Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tmain\">@tmain<\/a> !<\/p>\n<p>If you specify the <code>stage1.dvc<\/code> as a dependency for <code>stage2.dvc<\/code>, dvc will not be able to track dependencies of <code>stage1.dvc<\/code> when you call <code>dvc repro stage2.dvc<\/code> because the DAG is built using explicitly specified dependencies and outputs by tying them together, but <code>stage1.dvc<\/code> doesn\u2019t consider <code>stage1.dvc<\/code> file as an output of the stage, but rather as a metafile describing the stage. So the recommended way would be to explicitly specify outputs of stage1 as dependency of the stage2.<\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":2.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-09-12T14:08:59.218Z",
                "Answer_body":"<p>Thanks for the prompt clarification !<\/p>",
                "Answer_score":2.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"best practic specifi depend pipelin folder output stage stage know run run folder folder python stage equival stage creat file run run stage folder python stage recommend wai thank",
        "Question_preprocessed_content":"best practic specifi depend pipelin output know run equival stage creat file run recommend wai thank",
        "Question_gpt_summary_original":"The user is seeking advice on the recommended way to specify dependencies in a pipeline using dvc, particularly in cases where an output from one stage is used in another stage.",
        "Question_gpt_summary":"user seek advic recommend wai specifi depend pipelin particularli case output stage stage",
        "Answer_original_content":"tmain specifi stage depend stage abl track depend stage repro stage dag built explicitli specifi depend output ty stage doesnt consid stage file output stage metafil describ stage recommend wai explicitli specifi output stage depend stage thank ruslan thank prompt clarif",
        "Answer_preprocessed_content":"specifi depend abl track depend dag built explicitli specifi depend output ty doesnt consid file output stage metafil describ stage recommend wai explicitli specifi output stage depend stage thank ruslan thank prompt clarif",
        "Answer_gpt_summary_original":"the answer suggests that the best practice for specifying dependencies in a pipeline is to explicitly specify the outputs of the first stage as dependencies for the second stage. this ensures that the output of one stage is correctly used in the next.",
        "Answer_gpt_summary":"answer suggest best practic specifi depend pipelin explicitli specifi output stage depend second stage ensur output stage correctli"
    },
    {
        "Question_id":null,
        "Question_title":"Score Recommender system - Item Recommendation failed to run",
        "Question_body":"I am getting an error when trying to run the Score Recommender system for Item Recommendation.\n\nThe status details is 'Failed to run task; exceeded retry count for operation'. Can someone help to advise on this error?",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1626681386490,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/480658\/score-recommender-system-item-recommendation-faile.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-21T02:53:53.933Z",
                "Answer_score":0,
                "Answer_body":"The experiment i am trying to conduct here is a E-commerce Product Recommender to generate the top 5 item recommendations for each user.\nWhen i set the Recommended Item selection to be 'From All Items', the Score Recommender System will fail to run.\n\nExperiment link as below:\nhttps:\/\/studio.azureml.net\/Home\/ViewWorkspaceCached\/50eecd898bf54c7baca7704f89bc737b#Workspaces\/Experiments\/Experiment\/50eecd898bf54c7baca7704f89bc737b.f-id.671a7b68cb8a456282ed5ab7a8951f8e\/ViewExperiment",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"score recommend item recommend fail run get error try run score recommend item recommend statu detail fail run task exceed retri count oper help advis error",
        "Question_preprocessed_content":"score recommend item recommend fail run get error try run score recommend item recommend statu detail fail run task exceed retri count oper help advis error",
        "Question_gpt_summary_original":"The user is encountering an error while trying to run the Score Recommender system for Item Recommendation. The status details indicate that the task failed to run and exceeded the retry count for operation. The user is seeking advice on how to resolve this error.",
        "Question_gpt_summary":"user encount error try run score recommend item recommend statu detail indic task fail run exceed retri count oper user seek advic resolv error",
        "Answer_original_content":"experi try conduct commerc product recommend gener item recommend user set recommend item select item score recommend fail run experi link http studio net home viewworkspacecach eecdbfcbacafbcb workspac experi experi eecdbfcbacafbcb abcbaedabaf viewexperi",
        "Answer_preprocessed_content":"experi try conduct product recommend gener item recommend user set recommend item select item score recommend fail run experi link",
        "Answer_gpt_summary_original":"there is no solution provided in the answer to the error encountered when running a score recommender system for item recommendation. the answer only provides context about the experiment being conducted, which is an e-commerce product recommender to generate the top 5 item recommendations for each user. the error occurs when the recommended item selection is set to 'from all items'.",
        "Answer_gpt_summary":"solut provid answer error encount run score recommend item recommend answer provid context experi conduct commerc product recommend gener item recommend user error occur recommend item select set item"
    },
    {
        "Question_id":null,
        "Question_title":"UserScriptFilledDisk - what disk?",
        "Question_body":"I am using AZ Machine Learning and have been running python scripts on VMs to train mnist and output some summary statistics on the trained networks. It worked fine for the first few jobs, but when I submitted a few more, all of them failed with a USerScriptFilledDisk error:\n\n\"UserError: AzureMLCompute job failed. UserScriptFilledDisk: User script filled the disk. Consider using VM SKU with larger disk size. If the issue persists contact Azure Support.\"\n\nI am using nodes with only 7GB disk space, but it still does not make sense to me that I should have exceeded that just with mounting mnist and writing less than 1MB of numpy arrays to '.\/outputs\/'. The problem does not seem to be specific to any one or few nodes on my cluster. I made a new cluster and tried running my scripts on it. It still throws the same error. So how can I find out what disk I have filled up and how do fix it and keep it from happening again?\n\nThanks in advance!\n\nMore details:\n\nI created an Azure machine learning compute cluster\n\n compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-main1\")\n compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 100)\n vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_DS1_V2\")\n    \n if compute_name in ws.compute_targets:\n     compute_target = ws.compute_targets[compute_name]\n else:\n     provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n                                                                 min_nodes = compute_min_nodes, \n                                                                 max_nodes = compute_max_nodes)\n    \n     compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)    \n     compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n\n\n\nI added a data set\n\n workspace = Workspace(subscription_id, resource_group, workspace_name)\n dataset = Dataset.get_by_name(workspace, name='mnist_zip')\n dataset.download(target_path='.', overwrite=True)\n dataset = dataset.register(workspace=ws,\n                            name='mnist_zip',\n                            description='zip file with preprocesses mnist data set',\n                            create_new_version=False)\n\n\n\nI submitted jobs to the cluster\n\n runs = [ 0 for _ in range(30)]\n for i in range(30):\n     args = ['--dataset', dataset.as_mount(), '--id', i] \n     #also tried '.as_download()' - did not seem to make a difference\n     src = ScriptRunConfig(source_directory=script_folder,\n                           script='script.py', \n                           arguments=args,\n                           compute_target=compute_target,\n                           environment=env)\n     runs[i] = exp.submit(config=src)",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1617995934433,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/351791\/userscriptfilleddisk-what-disk.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-27T04:22:07.893Z",
                "Answer_score":0,
                "Answer_body":"@schwarze-7702 Thanks for the details. We would recommend to raise a Azure support desk ticket from Help+Support blade from Azure portal for your resource. This will help you to share the details securely and work with an engineer who can provide more insights about the issue that if it can be replicated.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-25T09:24:54.727Z",
                "Answer_score":0,
                "Answer_body":"Is there any update regarding this question? I experience the same issue, I am trying to register the dataset and UserScriptFilledDisk error occurs without any specific reason during the operation",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"userscriptfilleddisk disk machin learn run python script vm train mnist output summari statist train network work fine job submit fail userscriptfilleddisk error usererror comput job fail userscriptfilleddisk user script fill disk consid sku larger disk size issu persist contact azur support node disk space sens exceed mount mnist write numpi arrai output problem specif node cluster new cluster tri run script throw error disk fill fix happen thank advanc detail creat comput cluster comput environ aml comput cluster cpu main comput min node environ aml comput cluster min node comput max node environ aml comput cluster max node size environ aml comput cluster sku standard comput comput target comput target comput target comput provis config amlcomput provis configur size size min node comput min node max node comput max node comput target computetarget creat comput provis config comput target wait complet output true min node count timeout minut ad data set workspac workspac subscript resourc group workspac dataset dataset workspac mnist zip dataset download target path overwrit true dataset dataset regist workspac mnist zip descript zip file preprocess mnist data set creat new version fals submit job cluster run rang rang arg dataset dataset mount tri download differ src scriptrunconfig sourc directori script folder script script argument arg comput target comput target environ env run exp submit config src",
        "Question_preprocessed_content":"userscriptfilleddisk disk machin learn run python script vm train mnist output summari statist train network work fine job submit fail userscriptfilleddisk error usererror comput job fail userscriptfilleddisk user script fill disk consid sku larger disk size issu persist contact azur node disk space sens exceed mount mnist write numpi arrai problem specif node cluster new cluster tri run script throw error disk fill fix happen thank advanc detail creat comput cluster ad data set workspac dataset overwrit true dataset descript zip file preprocess mnist data set submit job cluster run rang arg tri differ src argument arg environ env run",
        "Question_gpt_summary_original":"The user is encountering a \"UserScriptFilledDisk\" error while running python scripts on VMs to train mnist and output summary statistics on the trained networks using Azure Machine Learning. The error suggests that the user has filled up the disk and recommends using a VM SKU with a larger disk size. However, the user is unsure of which disk has been filled up and how to fix it. The user has tried running the scripts on a new cluster, but the error persists. The user has provided details of the code used to create the Azure machine learning compute cluster, add a dataset, and submit jobs to the cluster.",
        "Question_gpt_summary":"user encount userscriptfilleddisk error run python script vm train mnist output summari statist train network error suggest user fill disk recommend sku larger disk size user unsur disk fill fix user tri run script new cluster error persist user provid detail code creat comput cluster add dataset submit job cluster",
        "Answer_original_content":"schwarz thank detail recommend rais azur support desk ticket help support blade azur portal resourc help share detail secur work engin provid insight issu replic updat question experi issu try regist dataset userscriptfilleddisk error occur specif reason oper",
        "Answer_preprocessed_content":"thank detail recommend rais azur support desk ticket help support blade azur portal resourc help share detail secur work engin provid insight issu replic updat question experi issu try regist dataset userscriptfilleddisk error occur specif reason oper",
        "Answer_gpt_summary_original":"possible solutions mentioned in the answer include raising a support ticket from the azure portal to work with an engineer who can provide more insights about the issue and help replicate it. no specific solution is mentioned for the \"userscriptfilleddisk\" error.",
        "Answer_gpt_summary":"possibl solut mention answer includ rais support ticket azur portal work engin provid insight issu help replic specif solut mention userscriptfilleddisk error"
    },
    {
        "Question_id":null,
        "Question_title":"Save_format of shared layers",
        "Question_body":"<p>Hi, I am running a model which uses the same embedding layer (and variables)  several places in the model. During training, I use the standard WandbCallback() with no additional parameters, however, I get this warning from TensorFlow:<\/p>\n<blockquote>\n<p>WARNING:tensorflow:Found duplicated <code>Variable<\/code>s in Model\u2019s <code>weights<\/code>. This is usually caused by <code>Variable<\/code>s being shared by Layers in the Model. These <code>Variable<\/code>s will be treated as separate <code>Variable<\/code>s when the Model is restored. To avoid this, please save with <code>save_format=\"tf\"<\/code>.<\/p>\n<\/blockquote>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1653042141698,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":120.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/save-format-of-shared-layers\/2463",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-21T00:32:49.383Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/adrianlangseth\">@adrianlangseth<\/a> ,<\/p>\n<p>I\u2019ll be happy to look into this for you. Can you please provide me with the following:<\/p>\n<ul>\n<li>wandb version you are using<\/li>\n<li>keras\/tensorflow versions<\/li>\n<li>Code you are running<\/li>\n<\/ul>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-21T09:09:23.021Z",
                "Answer_body":"<p>Versions:<\/p>\n<ul>\n<li>tf.<strong>version<\/strong>=\u20182.4.1\u2019<\/li>\n<li>tf.keras.<strong>version<\/strong>=\u20182.4.0\u2019<\/li>\n<li>wandb.<strong>version<\/strong>=\u20180.12.16\u2019<\/li>\n<\/ul>\n<p>The code I am running is way to long to post here, but the source of the issue is a \u201cKeras embedding\u201d layer being used multiple times in a layer. The standard save format used by wandb saves the weights of these independently and therefore on loading the model, these will no longer correspond to the same weights, as during further training, the backprop will update these layers independently. So the core issue is that the save_format which comes standard in W&amp;B, is not very functional for models with shared layers, and I cannot find a way to change this or pass some argument to W&amp;B which specifies save_format.<\/p>\n<p>Thank You,<br>\nAdrian<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-01T00:58:37.755Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/adrianlangseth\">@adrianlangseth<\/a> ,<\/p>\n<p>Apologies for the delay on my follow up response.  Currently is configured to work with the latest version of <code>Tensorflow TF 2.X<\/code> where the default model <code>save_format<\/code>  is <code>\"tf\"<\/code>. I agree we should support more customization here and allow the user to set their own format. I\u2019m making a feature request for this internally and will let you know when we make progress on this. In the meantime please let me know if you have additional questions.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-31T00:58:38.750Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"save format share layer run model us embed layer variabl place model train us standard callback addit paramet warn tensorflow warn tensorflow duplic variabl model weight usual caus variabl share layer model variabl treat separ variabl model restor avoid save save format",
        "Question_preprocessed_content":"share layer run model us embed layer place model train us standard callback addit paramet warn tensorflow warn tensorflow duplic model usual caus share layer model treat separ model restor avoid save",
        "Question_gpt_summary_original":"The user is encountering a warning from TensorFlow while running a model that uses the same embedding layer and variables in multiple places. The warning indicates that there are duplicated variables in the model's weights, which is usually caused by variables being shared by layers in the model. The warning suggests that the user should save with \"save_format='tf'\" to avoid this issue.",
        "Question_gpt_summary":"user encount warn tensorflow run model us embed layer variabl multipl place warn indic duplic variabl model weight usual caus variabl share layer model warn suggest user save save format avoid issu",
        "Answer_original_content":"adrianlangseth ill happi look provid follow version kera tensorflow version code run regard mohammad version version kera version version code run wai long post sourc issu kera embed layer multipl time layer standard save format save weight independ load model longer correspond weight train backprop updat layer independ core issu save format come standard function model share layer wai chang pass argument specifi save format thank adrian adrianlangseth apolog delai follow respons current configur work latest version tensorflow default model save format agre support custom allow user set format make featur request intern let know progress meantim let know addit question regard mohammad topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"ill happi look provid follow version version code run regard mohammad version code run wai long post sourc issu kera embed layer multipl time layer standard save format save weight independ load model longer correspond weight train backprop updat layer independ core issu come standard function model share layer wai chang pass argument specifi thank adrian apolog delai follow respons current configur work latest version default model agre support custom allow user set format make featur request intern let know progress meantim let know addit question regard mohammad topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no specific solutions provided in the answer, but the issue is identified as being caused by the standard save format used by w&b not being functional for models with shared layers. the user is advised that the current version of w&b is configured to work with the latest version of tensorflow, and a feature request has been made to support more customization in the save format. no immediate solution is provided, but the user is invited to ask additional questions if needed.",
        "Answer_gpt_summary":"specif solut provid answer issu identifi caus standard save format function model share layer user advis current version configur work latest version tensorflow featur request support custom save format immedi solut provid user invit ask addit question need"
    },
    {
        "Question_id":null,
        "Question_title":"Is it possible to continue training with additional epochs? Also where can I find logs in local?",
        "Question_body":"<p>Dear wandb team,<\/p>\n<p>I currently start using pytorch-lightning combining with wandb, so I will use WandbLogger. (Due to link limits, I didn\u2019t put url for the documentation.)<\/p>\n<p>Suppose I had trained a model, let\u2019s say for 10 epochs. The <em>project<\/em> is <strong>wandb_toy<\/strong>,  and the <em>name or ID<\/em> is <strong>toy<\/strong>. After training, it will automatically create two folders under <code>.\/<\/code>, i.e., <code>.\/wandb<\/code> and <code>.\/wandb_toy<\/code>. I know that the checkpoints will be saved in <code>wandb_toy\/toy\/checkpoints<\/code>. Also there will be a new run folder in <code>.\/wandb<\/code>, let\u2019s say <code>.\/wandb\/run-20230101_102000<\/code>.<\/p>\n<p>Now if I want to continue the training from <em>epochs=10<\/em> to <em>epochs=20<\/em>, I know I can load the model state with adding <em>ckpt_path<\/em> during <em>trainer.fit()<\/em>, also update the config with <em>allow_val_change<\/em> setting to <em>True<\/em>. However, there is still a new additional run folder created, e.g., <code>.\/wandb\/run-20230202_104000<\/code>.<\/p>\n<hr>\n<p><strong>My questions are:<\/strong><\/p>\n<ul>\n<li>I want to keep saving or update things in the original run folder <code>.\/wandb\/run-20230101_102000<\/code>.<\/li>\n<li>Also is there any way to name the run folder, for example, change  <code>.\/wandb\/run-20230101_102000<\/code> to  <code>.\/wandb\/my_toy_run<\/code>. I have tried keywords like <em>dir<\/em> or <em>save_dir<\/em> already, but seems not right.<\/li>\n<li>If I delete the project in wandb ai (without delete folders in <code>.\/wandb<\/code>), then continue training from <em>epoch=10<\/em> to <em>epoch=20<\/em>, It will log only from epoch=10~20. Is there any way to still get the previous log from epoch=0~10? I have tried to look up <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/save-restore#examples-of-wandb.restore\"> Save &amp; Restore Files<\/a> or <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming\">Resume Runs<\/a>, but unfortunately I couldn\u2019t figure it out.<\/li>\n<li>If I want to see each epochs log (e.g., accuracy and loss) in  my local, which file should I look for?<\/li>\n<\/ul>\n<p>Thanks for reading and I apologize if I couldn\u2019t make things clear.<\/p>\n<p>Best wishes,<br>\nYian<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1673356093482,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":53.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/is-it-possible-to-continue-training-with-additional-epochs-also-where-can-i-find-logs-in-local\/3667",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-11T14:23:51.279Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ntuyianchen\">@ntuyianchen<\/a>, thanks for writing in and for sharing this detailed explanation of your use-case! Answering your questions:<\/p>\n<ul>\n<li>This is not possible at the moment as every time you resume a run a new folder is created <code>run-date_time<\/code> (this name is not mutable either other than manually) although the run is the same. This is useful to be able to track easily the different processes.<\/li>\n<li>You cannot resume a run at a previous step, it will only be resumed from the last step. The intention of this is to avoid overwritting previous logged data.<\/li>\n<li>Log metrics are not saved locally (other than the last epoch in <code>files\/wandb-metadata.json<\/code>), but they are accesible through our public API by accesing <code>run.history()<\/code>. <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/public-api\/run#history\">Here<\/a> you can have a look at the documentation on how to do this.<\/li>\n<\/ul>\n<p>Please let me know if these answers would be useful. Also, if you would like to have any of these features available, feel free to explain which ones and I will create a new request!<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-11T17:18:22.250Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/luis_bergua1\">@luis_bergua1<\/a> , thanks for the concrete reply! It\u2019s really helpful and now I can understand why and how wandb designed this way. The <code>run.history()<\/code> is really the thing I was looking for, thank you very much!<\/p>\n<p>Thanks to wandb team for making this awesome package and documentations. I have one thing that\u2019s irrelevant to our discussion. I found that <strong>the text format seemed to be wrong<\/strong> at <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/public-api\/run?_gl=1*1culllr*_ga*MTkwNzMzMTI4LjE2NzI0NzI2OTM.*_ga_JH1SJHJQXJ*MTY3MzQ1NTQ2OS40OC4xLjE2NzM0NTY1MTYuNDkuMC4w#history\">here<\/a> (the table for <code>Arguments<\/code> and <code>Returns<\/code> don\u2019t break new lines correctly in my Mac Safari, which makes it a little hard to read.) This  is not really an immediate big deal and I think it can be fixed in the future some day <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> .<\/p>\n<p>Cheers!<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/5\/56624e0424bcaa43a1d179393b9d14a71de4878f.png\" data-download-href=\"\/uploads\/short-url\/ckbG7FMxBS7IwaZs55NAnQ9Y8PB.png?dl=1\" title=\"\u622a\u5716 2023-01-12 \u4e0a\u53481.15.42\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/5\/56624e0424bcaa43a1d179393b9d14a71de4878f_2_581x499.png\" alt=\"\u622a\u5716 2023-01-12 \u4e0a\u53481.15.42\" data-base62-sha1=\"ckbG7FMxBS7IwaZs55NAnQ9Y8PB\" width=\"581\" height=\"499\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/5\/56624e0424bcaa43a1d179393b9d14a71de4878f_2_581x499.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/5\/56624e0424bcaa43a1d179393b9d14a71de4878f_2_871x748.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/5\/56624e0424bcaa43a1d179393b9d14a71de4878f_2_1162x998.png 2x\" data-dominant-color=\"F7F8F9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">\u622a\u5716 2023-01-12 \u4e0a\u53481.15.42<\/span><span class=\"informations\">1550\u00d71332 208 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-13T15:16:55.454Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ntuyianchen\">@ntuyianchen<\/a>, great to see that <code>run.history()<\/code> works for you! Thank you very much for the kind feedback, we really appreciate it! Regarding the table in the docs, I am not fully understanding you, would you like to have a different align foe the text\/have it justified?<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-15T15:16:44.781Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/luis_bergua1\">@luis_bergua1<\/a> , sorry for my ambiguous feedback. I was expecting that the argument should be something like<\/p>\n<blockquote>\n<p>samples (int, optional): The number \u2026<br>\npandas (bool, optional): Return \u2026<br>\nkeys (list optional): Only return \u2026<\/p>\n<\/blockquote>\n<p>Since without line-breaking, it\u2019s quite hard to find the argument at first sight.<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-19T12:02:57.186Z",
                "Answer_body":"<p>Thanks a lot for clarifying <a class=\"mention\" href=\"\/u\/ntuyianchen\">@ntuyianchen<\/a>! I see the issue now, I\u2019ll report it!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"possibl continu train addit epoch log local dear team current start pytorch lightn combin us logger link limit didnt url document suppos train model let epoch project toi toi train automat creat folder toi know checkpoint save toi toi checkpoint new run folder let run want continu train epoch epoch know load model state ad ckpt path trainer fit updat config allow val chang set true new addit run folder creat run question want save updat thing origin run folder run wai run folder exampl chang run toi run tri keyword like dir save dir right delet project delet folder continu train epoch epoch log epoch wai previou log epoch tri look save restor file resum run unfortun figur want epoch log accuraci loss local file look thank read apolog thing clear best wish yian",
        "Question_preprocessed_content":"possibl continu train addit epoch log local dear team current start combin us logger suppos train model let epoch project toi train automat creat folder know checkpoint save new run folder let want continu train epoch epoch know load model state ad updat config set true new addit run folder creat question want save updat thing origin run folder wai run folder exampl chang tri keyword like dir right delet project continu train epoch epoch log epoch wai previou log epoch tri look save restor file resum run unfortun figur want epoch log local file look thank read apolog thing clear best wish yian",
        "Question_gpt_summary_original":"The user is facing several challenges related to using pytorch-lightning with WandbLogger. They want to continue training a model from epoch 10 to epoch 20 while saving or updating things in the original run folder, and also want to name the run folder. Additionally, they want to know if there is a way to retrieve previous logs from epoch 0 to 10 after deleting the project in Wandb AI. Finally, they are looking for the file that contains each epoch's log (e.g., accuracy and loss) in their local system.",
        "Question_gpt_summary":"user face challeng relat pytorch lightn logger want continu train model epoch epoch save updat thing origin run folder want run folder addition want know wai retriev previou log epoch delet project final look file contain epoch log accuraci loss local",
        "Answer_original_content":"ntuyianchen thank write share detail explan us case answer question possibl moment time resum run new folder creat run date time mutabl manual run us abl track easili differ process resum run previou step resum step intent avoid overwrit previou log data log metric save local epoch file metadata json acces public api acces run histori look document let know answer us like featur avail feel free explain on creat new request lui bergua thank concret repli help understand design wai run histori thing look thank thank team make awesom packag document thing that irrelev discuss text format wrong tabl argument return dont break new line correctli mac safari make littl hard read immedi big deal think fix futur dai cheer ntuyianchen great run histori work thank kind feedback appreci tabl doc fulli understand like differ align foe text justifi lui bergua sorri ambigu feedback expect argument like sampl int option number panda bool option return kei list option return line break hard argument sight thank lot clarifi ntuyianchen issu ill report",
        "Answer_preprocessed_content":"thank write share detail explan answer question possibl moment time resum run new folder creat run us abl track easili differ process resum run previou step resum step intent avoid overwrit previou log data log metric save local acces public api acces look document let know answer us like featur avail feel free explain on creat new request thank concret repli help understand design wai thing look thank thank team make awesom packag document thing that irrelev discuss text format wrong immedi big deal think fix futur dai cheer great work thank kind feedback appreci tabl doc fulli understand like differ align foe justifi sorri ambigu feedback expect argument like sampl number panda return kei return hard argument sight thank lot clarifi issu ill report",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are:\n\n- it is not possible to resume a run at a previous step, it will only be resumed from the last step.\n- log metrics are not saved locally, but they are accessible through the public api by accessing run.history().\n- if the user would like to have any of these features available, they can explain which ones and the team will create a new request.\n- the user can use run.history() to access logs.\n- the team will report the issue with the table in the documentation.",
        "Answer_gpt_summary":"possibl solut extract answer possibl resum run previou step resum step log metric save local access public api access run histori user like featur avail explain on team creat new request user us run histori access log team report issu tabl document"
    },
    {
        "Question_id":58307950.0,
        "Question_title":"Azure Machine Learning Compute quota?",
        "Question_body":"<p>The <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-manage-quotas#azure-machine-learning-compute\" rel=\"nofollow noreferrer\">Manage and request quotas for Azure resources<\/a> documentation page states that the default quota depends \"on your subscription offer type\". The quota doesn't show up in Azure web portal. Is there a way to find out current quota values using SDK, CLI, REST API?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570636386640,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":354.0,
        "Answer_body":"<p>You probably want to try something like this command : <\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>az vm list-usage --location eastus --out table\n<\/code><\/pre>\n\n<p>It would get you the core usage for the region, which is what is important for deployment of resources.<\/p>\n\n<p>Other choices (az + Powershell) are available <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/networking\/check-usage-against-limits\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Hope this helps!<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58307950",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1570669528590,
        "Question_original_content":"comput quota manag request quota azur resourc document page state default quota depend subscript offer type quota azur web portal wai current quota valu sdk cli rest api",
        "Question_preprocessed_content":"comput quota manag request quota azur resourc document page state default quota depend subscript offer type quota azur web portal wai current quota valu sdk cli rest api",
        "Question_gpt_summary_original":"The user is facing challenges in finding out the current quota values for Azure Machine Learning Compute using SDK, CLI, or REST API as the default quota depends on the subscription offer type and does not show up in the Azure web portal.",
        "Question_gpt_summary":"user face challeng find current quota valu comput sdk cli rest api default quota depend subscript offer type azur web portal",
        "Answer_original_content":"probabl want try like command list usag locat eastu tabl core usag region import deploy resourc choic powershel avail hope help",
        "Answer_preprocessed_content":"probabl want try like command core usag region import deploy resourc choic avail hope help",
        "Answer_gpt_summary_original":"possible solutions: \n- use the command \"az vm list-usage --location eastus --out table\" to get the core usage for the region, which is important for deployment of resources. \n- explore other choices such as using azure powershell.",
        "Answer_gpt_summary":"possibl solut us command list usag locat eastu tabl core usag region import deploy resourc explor choic azur powershel"
    },
    {
        "Question_id":68402406.0,
        "Question_title":"MLflow webserver returns 400 status, \"Incompatible input types for column X. Can not safely convert float64 to <U0.\"",
        "Question_body":"<p>I am implementing an anomaly detection web service using <code>MLflow<\/code> and <code>sklearn.pipeline.Pipeline()<\/code>. The aim of the model is to detect web crawlers using server log and <code>response_length<\/code> column is one of my features. After serving model, for testing the web service I send below request that contains the 20 first columns of the train data.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>$ curl  --location --request POST '127.0.0.1:8000\/invocations'\n        --header 'Content-Type: text\/csv' \\\n        --data-binary 'datasets\/test.csv'\n<\/code><\/pre>\n<p>But response of the web server has status code 400 (BAD REQUEST) and this JSON body:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;error_code&quot;: &quot;BAD_REQUEST&quot;,\n    &quot;message&quot;: &quot;Incompatible input types for column response_length. Can not safely convert float64 to &lt;U0.&quot;\n}\n<\/code><\/pre>\n<p>Here is the model compilation MLflow Tracking component log:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>[Pipeline] ......... (step 1 of 3) Processing transform, total=11.8min\n[Pipeline] ............... (step 2 of 3) Processing pca, total=   4.8s\n[Pipeline] ........ (step 3 of 3) Processing rule_based, total=   0.0s\n2021\/07\/16 04:55:12 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n2021\/07\/16 04:55:12 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: &quot;\/home\/matin\/workspace\/Rahnema College\/venv\/lib\/python3.8\/site-packages\/mlflow\/models\/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values &lt;https:\/\/www.mlflow.org\/docs\/latest\/models.html#handling-integers-with-missing-values&gt;`_ for more details.&quot;\nLogged data and model in run: 8843336f5c31482c9e246669944b1370\n\n---------- logged params ----------\n{'memory': 'None',\n 'pca': 'PCAEstimator()',\n 'rule_based': 'RuleBasedEstimator()',\n 'steps': &quot;[('transform', &lt;log_transformer.LogTransformer object at &quot;\n          &quot;0x7f05a8b95760&gt;), ('pca', PCAEstimator()), ('rule_based', &quot;\n          'RuleBasedEstimator())]',\n 'transform': '&lt;log_transformer.LogTransformer object at 0x7f05a8b95760&gt;',\n 'verbose': 'True'}\n\n---------- logged metrics ----------\n{}\n\n---------- logged tags ----------\n{'estimator_class': 'sklearn.pipeline.Pipeline', 'estimator_name': 'Pipeline'}\n\n---------- logged artifacts ----------\n['model\/MLmodel',\n 'model\/conda.yaml',\n 'model\/model.pkl',\n 'model\/requirements.txt']\n<\/code><\/pre>\n<p>Could anyone tell me exactly how I can fix this model serve problem?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626398499507,
        "Question_favorite_count":null,
        "Question_last_edit_time":1657456057947,
        "Question_score":1.0,
        "Question_view_count":787.0,
        "Answer_body":"<p>The problem caused by <code>mlflow.utils.autologging_utils<\/code> WARNING.<\/p>\n<p>When the model is created, data input signature is saved on the <code>MLmodel<\/code> file with some.\nYou should change <code>response_length<\/code> signature input type from <code>string<\/code> to <code>double<\/code> by replacing<\/p>\n<pre><code>{&quot;name&quot;: &quot;response_length&quot;, &quot;type&quot;: &quot;double&quot;}\n<\/code><\/pre>\n<p>instead of<\/p>\n<pre><code>{&quot;name&quot;: &quot;response_length&quot;, &quot;type&quot;: &quot;string&quot;}\n<\/code><\/pre>\n<p>so it doesn't need to be converted. After serving the model with edited <code>MLmodel<\/code> file, the web server worked as expected.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68402406",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1626398499507,
        "Question_original_content":"webserv return statu incompat input type column safe convert float implement anomali detect web servic sklearn pipelin pipelin aim model detect web crawler server log respons length column featur serv model test web servic send request contain column train data curl locat request post invoc header content type text csv data binari dataset test csv respons web server statu code bad request json bodi error code bad request messag incompat input type column respons length safe convert float detail log data model run fcceb log param memori pca pcaestim rule base rulebasedestim step transform pca pcaestim rule base rulebasedestim transform verbos true log metric log tag estim class sklearn pipelin pipelin estim pipelin log artifact model mlmodel model conda yaml model model pkl model requir txt tell exactli fix model serv problem",
        "Question_preprocessed_content":"webserv return statu incompat input type column safe convert float implement anomali detect web servic aim model detect web crawler server log column featur serv model test web servic send request contain column train data respons web server statu code json bodi model compil track compon log tell exactli fix model serv problem",
        "Question_gpt_summary_original":"The user is encountering a challenge with their anomaly detection web service using MLflow and sklearn.pipeline.Pipeline(). The web server returns a 400 status with an error message stating \"Incompatible input types for column X. Can not safely convert float64 to <U0.\" The user has provided a request containing the 20 first columns of the train data, but the response from the server is still a bad request. The user has also shared the model compilation MLflow Tracking component log, which includes a warning about training metrics not being recorded and a suggestion to handle integer columns with missing values. The user is seeking help to fix the model serve problem.",
        "Question_gpt_summary":"user encount challeng anomali detect web servic sklearn pipelin pipelin web server return statu error messag state incompat input type column safe convert float user provid request contain column train data respons server bad request user share model compil track compon log includ warn train metric record suggest handl integ column miss valu user seek help fix model serv problem",
        "Answer_original_content":"problem caus util autolog util warn model creat data input signatur save mlmodel file chang respons length signatur input type string doubl replac respons length type doubl instead respons length type string need convert serv model edit mlmodel file web server work expect",
        "Answer_preprocessed_content":"problem caus warn model creat data input signatur save file chang signatur input type replac instead need convert serv model edit file web server work expect",
        "Answer_gpt_summary_original":"the solution to the problem of encountering a 400 status code and an \"incompatible input types for column x. can not safely convert float64 to <u0.\" error when serving a model using sklearn.pipeline.pipeline() is to change the response_length signature input type from string to double by replacing {\"name\": \"response_length\", \"type\": \"string\"} with {\"name\": \"response_length\", \"type\": \"double\"} in the mlmodel file. this will prevent the need for conversion and allow the web server to work as expected.",
        "Answer_gpt_summary":"solut problem encount statu code incompat input type column safe convert float error serv model sklearn pipelin pipelin chang respons length signatur input type string doubl replac respons length type string respons length type doubl mlmodel file prevent need convers allow web server work expect"
    },
    {
        "Question_id":34948242.0,
        "Question_title":"Azure: plot without labels",
        "Question_body":"<p>Suppose I have a dataframe (myDataframe) with two column of values (third and fourth). I want to plot them in a bi-dimensional graph. If I do it in R it works, but it returns me a graph without labels when I run the script from Azure Machine Learning. Someone with ideas?<\/p>\n\n<pre><code>...\nplot(myDataframe[,3],myDataframe[,4], \n       main=\"my title\",\n       xlab= \"x\"\n       ylab= \"y\",\n       col= \"blue\", pch = 19, cex = 0.1, lty = \"solid\", lwd = 2)\n\n# lines(x,y=x, col=\"yellow\")\n\n# add LABELS\ntext(DF_relativo[,A], DF_relativo[,B], \n       labels=DF_relativo$names, cex= 0.7, pos=2)\n...\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1453470518060,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":91.0,
        "Answer_body":"<p>using ggplot2() in AzureML is bit different. It can use to have plots with labels. Here's the <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/b1c26728eb6c4e4d80dddceae992d653\" rel=\"nofollow\">Cortana Intelligence gallery example<\/a> for the particular task.  <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":-1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34948242",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1466402236047,
        "Question_original_content":"azur plot label suppos datafram mydatafram column valu fourth want plot dimension graph work return graph label run script idea plot mydatafram mydatafram main titl xlab ylab col blue pch cex lty solid lwd line col yellow add label text relativo relativo label relativo name cex po",
        "Question_preprocessed_content":"azur plot label suppos datafram column valu want plot graph work return graph label run script idea",
        "Question_gpt_summary_original":"The user is facing a challenge in plotting a bi-dimensional graph in Azure Machine Learning without labels, even though it works in R. They are seeking ideas on how to add labels to the graph.",
        "Question_gpt_summary":"user face challeng plot dimension graph label work seek idea add label graph",
        "Answer_original_content":"ggplot bit differ us plot label cortana intellig galleri exampl particular task",
        "Answer_preprocessed_content":"ggplot bit differ us plot label cortana intellig galleri exampl particular task",
        "Answer_gpt_summary_original":"possible solution: the user can use ggplot2() to plot two columns of values from a dataframe in a bi-dimensional graph with labels. they can refer to the cortana intelligence gallery example for guidance.",
        "Answer_gpt_summary":"possibl solut user us ggplot plot column valu datafram dimension graph label refer cortana intellig galleri exampl guidanc"
    },
    {
        "Question_id":null,
        "Question_title":"Visual Bug in Documentation",
        "Question_body":"<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/fd0f258d7049524defde72d94701022480abd440.png\" data-download-href=\"\/uploads\/short-url\/A6FfBmGoIRNds6FR4YCdptOz7Fe.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_690x264.png\" alt=\"image\" data-base62-sha1=\"A6FfBmGoIRNds6FR4YCdptOz7Fe\" width=\"690\" height=\"264\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_690x264.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_1035x396.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_1380x528.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">2560\u00d7981 120 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>I think there is a missing ``` to finish the code block<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1654672339920,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":156.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/visual-bug-in-documentation\/2572",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-08T09:08:22.120Z",
                "Answer_body":"<p>Hey Aryan, thank you for flagging this. I\u2019ll update the docs to fix this.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-07T07:12:39.513Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"visual bug document imag think miss finish code block",
        "Question_preprocessed_content":"visual bug document imag think miss finish code block",
        "Question_gpt_summary_original":"The user has encountered a challenge with a visual bug in the documentation, specifically a missing ``` to finish a code block.",
        "Question_gpt_summary":"user encount challeng visual bug document specif miss finish code block",
        "Answer_original_content":"hei aryan thank flag ill updat doc fix topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"hei aryan thank flag ill updat doc fix topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the solution to the visual bug in the documentation is that the person who answered the question will update the docs to fix it. however, it is also mentioned that new replies are no longer allowed as the topic was automatically closed 60 days after the last reply.",
        "Answer_gpt_summary":"solut visual bug document person answer question updat doc fix mention new repli longer allow topic automat close dai repli"
    },
    {
        "Question_id":null,
        "Question_title":"The columns appears with another name",
        "Question_body":"Hello I'm practicing in Microsoft Machine Learning Studio.\n\nIn a experiment I use de control \"Import data\", then in the properties I use the data source: https:\/\/github.com\/rdiazconcha\/lil-azure-machine-learning-y-ai\/blob\/master\/modulo-2\/power-export_min.csv\n\nThat is the file to practice in my course.\n\nThe resto of the fields are filled like this:\n\n\n\n\nBut, when I use the choice visualize, appears like this:\n\n\n\n\n\nBut, those are not the names of the columns.\nWhat I'm doing wrong?\n\nThanks a lot for your help.",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1624312518567,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@CASTANEDARODRIGUEZDAMIAN-2301 Hello, I got you! Please click \"raw\" and use the url then to use the resource file. https:\/\/raw.githubusercontent.com\/rdiazconcha\/lil-azure-machine-learning-y-ai\/master\/modulo-2\/power-export_min.csv\n\n\nThen you should be good! Thanks.\n\nRegards,\nYutong",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/445579\/the-columns-appears-with-another-name.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-22T17:40:47.527Z",
                "Answer_score":0,
                "Answer_body":"@CASTANEDARODRIGUEZDAMIAN-2301 Hello, I got you! Please click \"raw\" and use the url then to use the resource file. https:\/\/raw.githubusercontent.com\/rdiazconcha\/lil-azure-machine-learning-y-ai\/master\/modulo-2\/power-export_min.csv\n\n\nThen you should be good! Thanks.\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-06-22T00:04:45.333Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nThanks for reaching out to us.\n\nSince you are using the data source wizard, you need to tell the studio you have a header row. Studio will use the header row as your column name as below.\n\n\n\n\nLet me know if you have more questions! Thanks! ^^\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-06-22T14:09:59.227Z",
                "Answer_score":0,
                "Answer_body":"Hello YutongTie-MSFT\n\nI did it. I selected that checkbox:\n\n\n\n\nBut is appearing like this yet:\n\n\n\n\n\nIs missing me something to do?\n\nThanks for your help.\n\nRegards.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-06-23T15:37:54.913Z",
                "Answer_score":0,
                "Answer_body":"@YutongTie-MSFT : It works!\nThank you so much!\nRegards.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1624383647527,
        "Question_original_content":"column appear hello practic microsoft machin learn studio experi us control import data properti us data sourc http github com rdiazconcha lil azur machin learn blob master modulo power export min csv file practic cours resto field fill like us choic visual appear like name column wrong thank lot help",
        "Question_preprocessed_content":"column appear hello practic microsoft machin learn studio experi us control import data properti us data sourc file practic cours resto field fill like us choic visual appear like name column wrong thank lot help",
        "Question_gpt_summary_original":"The user is encountering a challenge in Microsoft Machine Learning Studio where the columns appear with different names than the ones in the original file. The user is unsure of what they are doing wrong and is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount challeng microsoft machin learn studio column appear differ name on origin file user unsur wrong seek help resolv issu",
        "Answer_original_content":"castanedarodriguezdamian hello got click raw us url us resourc file http raw githubusercont com rdiazconcha lil azur machin learn master modulo power export min csv good thank regard yutong",
        "Answer_preprocessed_content":"hello got click raw us url us resourc file good thank regard yutong",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer to the question about incorrect column names in a data source. the answer suggests using a specific url to access the resource file, but it is unclear how this would address the issue.",
        "Answer_gpt_summary":"solut provid answer question incorrect column name data sourc answer suggest specif url access resourc file unclear address issu"
    },
    {
        "Question_id":null,
        "Question_title":"Is there a way to install R libraries in SageMaker that receive a non-zero exit status?",
        "Question_body":"Hi all, I'm having an issue with an R kernel\/Jupyter notebook. I've come across two different libraries that result in the following error:\n\nWarning message in install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\"):\n\u201cinstallation of package \u2018XML\u2019 had non-zero exit status\u201dUpdating HTML index of packages in '.Library'\nMaking 'packages.html' ... done\n\n\nXML is the second package that I have run into this issue with. The other is rJava.\n\nI found a workaround that could work if I had root access, which is installing via the command line in a terminal. Which involves commands such as:\n\nyum install r-cran-rjava\n\n\nHowever, I don't have root access and cannot install as I get the message \"You need to be root to perform this command.\" So this workaround hasn't been possible.\n\nAfter checking the documentation for rJava and XML, I am running the requirements for JDK and other system requirements in SageMaker. This issue wasn't reproducible on a local RStudio environment. XML is a dependency for multiple R libraries (as is rJava). Is there a way that I can still install these packages?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1527798496000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":464.0,
        "Answer_body":"For Amazon SageMaker notebook Instances, you have the ability to assume root privileges, so instead of:\n\n$ yum install r-cran-rjava\n\nyou can try:\n\n$ sudo yum install r-cran-rjava\n\nwhich will allow you to impersonate the superuser (ie. root) for that command 1\n\nBut I don't believe that package exists in the available repos (ie. may be valid for another distro of Linux, but does not appear to be available in the yum repos -- running yum search 'r-cran-rjava' returned no results 2)\n\nInstead, from a prompt, install R + the necessary development files for later installation of R packages:\n\n$ sudo yum install -y R-java-devel.x86_64\n\nAnd finally, install the necessary XML libraries to support the XML package in R:\n\n$ sudo yum install -y libxml2-devel 3\n\nAfter which you can then open R (either as root user...)\n\n$ sudo R\n\nor personal\/local user\n\n$ R\n\nand execute the package installation:\n\n> install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\")\n\nEDITED TO FIX RJAVA PACKAGE INSTALLATION\n\nIt looks like the installation is requiring libgomp.spec\/libgomp.a files, so you can first find that file:\n\n$ sudo find \/ -iname libgomp.spec\n\nwhich should be located at \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec -- if so, you can manually create symlinks to fix this:\n\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec \/usr\/lib64\/\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.a \/usr\/lib64\/\n\n\nIf that ran correctly, you should now see both files in \/usr\/lib64 path:\n\n$ ls \/usr\/lib64\/libgomp*\n\nOnce confirmed, you can run the install.package('rJava') command.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE-0c9SwxRViGhd-lAYtsuw\/is-there-a-way-to-install-r-libraries-in-sage-maker-that-receive-a-non-zero-exit-status",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-06-01T02:52:15.000Z",
                "Answer_score":0,
                "Answer_body":"For Amazon SageMaker notebook Instances, you have the ability to assume root privileges, so instead of:\n\n$ yum install r-cran-rjava\n\nyou can try:\n\n$ sudo yum install r-cran-rjava\n\nwhich will allow you to impersonate the superuser (ie. root) for that command 1\n\nBut I don't believe that package exists in the available repos (ie. may be valid for another distro of Linux, but does not appear to be available in the yum repos -- running yum search 'r-cran-rjava' returned no results 2)\n\nInstead, from a prompt, install R + the necessary development files for later installation of R packages:\n\n$ sudo yum install -y R-java-devel.x86_64\n\nAnd finally, install the necessary XML libraries to support the XML package in R:\n\n$ sudo yum install -y libxml2-devel 3\n\nAfter which you can then open R (either as root user...)\n\n$ sudo R\n\nor personal\/local user\n\n$ R\n\nand execute the package installation:\n\n> install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\")\n\nEDITED TO FIX RJAVA PACKAGE INSTALLATION\n\nIt looks like the installation is requiring libgomp.spec\/libgomp.a files, so you can first find that file:\n\n$ sudo find \/ -iname libgomp.spec\n\nwhich should be located at \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec -- if so, you can manually create symlinks to fix this:\n\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec \/usr\/lib64\/\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.a \/usr\/lib64\/\n\n\nIf that ran correctly, you should now see both files in \/usr\/lib64 path:\n\n$ ls \/usr\/lib64\/libgomp*\n\nOnce confirmed, you can run the install.package('rJava') command.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1527821535000,
        "Question_original_content":"wai instal librari receiv non zero exit statu have issu kernel jupyt notebook come differ librari result follow error warn messag instal packag xml repo http cran project org instal packag xml non zero exit statusupd html index packag librari make packag html xml second packag run issu rjava workaround work root access instal command line termin involv command yum instal cran rjava root access instal messag need root perform command workaround hasn possibl check document rjava xml run requir jdk requir issu wasn reproduc local rstudio environ xml depend multipl librari rjava wai instal packag",
        "Question_preprocessed_content":"wai instal librari receiv exit statu have issu notebook come differ librari result follow error warn messag repo instal packag xml exit statusupd html index packag make xml second packag run issu rjava workaround work root access instal command line termin involv command yum instal root access instal messag need root perform workaround hasn possibl check document rjava xml run requir jdk requir issu wasn reproduc local rstudio environ xml depend multipl librari wai instal packag",
        "Question_gpt_summary_original":"The user is facing challenges in installing R libraries, specifically XML and rJava, in SageMaker due to non-zero exit status errors. The user has tried a workaround that involves installing via the command line in a terminal, but cannot do so as they do not have root access. The issue was not reproducible on a local RStudio environment, and the user is running the requirements for JDK and other system requirements in SageMaker. The user is seeking a solution to install these packages without root access.",
        "Question_gpt_summary":"user face challeng instal librari specif xml rjava non zero exit statu error user tri workaround involv instal command line termin root access issu reproduc local rstudio environ user run requir jdk requir user seek solut instal packag root access",
        "Answer_original_content":"notebook instanc abil assum root privileg instead yum instal cran rjava try sudo yum instal cran rjava allow imperson superus root command believ packag exist avail repo valid distro linux appear avail yum repo run yum search cran rjava return result instead prompt instal necessari develop file later instal packag sudo yum instal java devel final instal necessari xml librari support xml packag sudo yum instal libxml devel open root user sudo person local user execut packag instal instal packag xml repo http cran project org edit fix rjava packag instal look like instal requir libgomp spec libgomp file file sudo inam libgomp spec locat usr lib gcc amazon linux libgomp spec manual creat symlink fix sudo usr lib gcc amazon linux libgomp spec usr lib sudo usr lib gcc amazon linux libgomp usr lib ran correctli file usr lib path usr lib libgomp confirm run instal packag rjava command",
        "Answer_preprocessed_content":"notebook instanc abil assum root privileg instead yum instal try sudo yum instal allow imperson superus command believ packag exist avail repo instead prompt instal necessari develop file later instal packag sudo yum instal final instal necessari xml librari support xml packag sudo yum instal open sudo user execut packag instal repo edit fix rjava packag instal look like instal requir file file sudo inam locat manual creat symlink fix sudo sudo ran correctli file path confirm run command",
        "Answer_gpt_summary_original":"the solution to the challenge of installing r libraries in a jupyter notebook without root access is to assume root privileges using the command \"sudo yum install\" and then install the necessary development files and xml libraries. after that, open r and execute the package installation using the command \"install.packages\". if the installation requires libgomp.spec\/libgomp.a files, they can be found and manually created using the \"find\" and \"ln -s\" commands.",
        "Answer_gpt_summary":"solut challeng instal librari jupyt notebook root access assum root privileg command sudo yum instal instal necessari develop file xml librari open execut packag instal command instal packag instal requir libgomp spec libgomp file manual creat command"
    },
    {
        "Question_id":null,
        "Question_title":"TensorBoard Hparm Parallel Coordinate View Problem",
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m having an issue with the TensorBoard Hparm Parallel Coordinate View. It seems to be duplicated the axes on the lower half.<\/p>\n<p>This happens when I make a new project and just follow the get-started.ipynb file.<\/p>\n<p>I\u2019m using a Mac and Safari browser, and here are the versions info:<\/p>\n<pre><code>guild_version:             0.7.3\ntensorboard_version:       2.6.0\n<\/code><\/pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb.jpeg\" data-download-href=\"\/uploads\/short-url\/oDDDcjhGMAurXRar09EHL2ltw3p.jpeg?dl=1\" title=\"Screen Shot 2021-08-20 at 4.24.55 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_517x336.jpeg\" alt=\"Screen Shot 2021-08-20 at 4.24.55 PM\" data-base62-sha1=\"oDDDcjhGMAurXRar09EHL2ltw3p\" width=\"517\" height=\"336\" srcset=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_517x336.jpeg, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_775x504.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_1034x672.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2021-08-20 at 4.24.55 PM<\/span><span class=\"informations\">1920\u00d71249 181 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Additionally, when I remove one of the variables, the entire plot grows at least 30% in size. Is there a way I can keep this constant.<\/p>\n<p>Lastly, runs in a notebook don\u2019t seem to save an output.index file and guild view throws an error from this.<\/p>\n<p>Thanks for the help! Let me know if I need to share anything else.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1629491848267,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":259.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/tensorboard-hparm-parallel-coordinate-view-problem\/753",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-04T00:50:55.914Z",
                "Answer_body":"<p>Can you share a minimum working example?<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-14T16:06:42.952Z",
                "Answer_body":"<p>In this case you\u2019re running TensorBoard as provided by Google (Guild doesn\u2019t monkey patch TensorBoard or otherwise modify it. I find TB to have a number of strange sizing issues. The underlying framework Google uses is <em>very<\/em> complex and hard to contribute to\/fix so our strategy has been to \u201ctake it as is and be forever grateful\u201d <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=11\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>Things I try in these cases:<\/p>\n<ul>\n<li>\n<p>First and foremost, after modifying Guild runs (new runs, deleted runs) restart TensorBoard \u2014 the dynamic behavior works well in most cases but in some cases TB just doesn\u2019t handle new data well. A restart often cleans that up.<\/p>\n<\/li>\n<li>\n<p>Play around with browser zoom - sometimes that balances out strange font and UI sizing.<\/p>\n<\/li>\n<li>\n<p>Play with resizing the browser window.<\/p>\n<\/li>\n<\/ul>\n<p>I wish I could give you a better answer here \u2014 I\u2019m basically blaming TensorBoard for all of this. But in this case, Guild really isn\u2019t involved beyond laying out the TB log dir.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"tensorboard hparm parallel coordin view problem have issu tensorboard hparm parallel coordin view duplic ax lower half happen new project follow start ipynb file mac safari browser version info version tensorboard version screen shot addition remov variabl entir plot grow size wai constant lastli run notebook dont save output index file view throw error thank help let know need share",
        "Question_preprocessed_content":"tensorboard hparm parallel coordin view problem have issu tensorboard hparm parallel coordin view duplic ax lower half happen new project follow file mac safari browser version info screen shot addition remov variabl entir plot grow size wai constant lastli run notebook dont save file view throw error thank help let know need share",
        "Question_gpt_summary_original":"The user is facing challenges with TensorBoard Hparm Parallel Coordinate View, as it is duplicating the axes on the lower half. The issue occurs when a new project is created and the get-started.ipynb file is followed. The user is using a Mac and Safari browser with guild_version 0.7.3 and tensorboard_version 2.6.0. Removing one of the variables causes the entire plot to grow by at least 30%, and runs in a notebook do not save an output.index file, resulting in an error in guild view.",
        "Question_gpt_summary":"user face challeng tensorboard hparm parallel coordin view duplic ax lower half issu occur new project creat start ipynb file follow user mac safari browser version tensorboard version remov variabl caus entir plot grow run notebook save output index file result error view",
        "Answer_original_content":"share minimum work exampl case your run tensorboard provid googl doesnt monkei patch tensorboard modifi number strang size issu underli framework googl us complex hard contribut fix strategi forev grate thing try case foremost modifi run new run delet run restart tensorboard dynam behavior work case case doesnt handl new data restart clean plai browser zoom balanc strang font size plai resiz browser window wish better answer basic blame tensorboard case isnt involv lai log dir",
        "Answer_preprocessed_content":"share minimum work exampl case your run tensorboard provid googl restart tensorboard dynam behavior work case case doesnt handl new data restart clean plai browser zoom balanc strang font size plai resiz browser window wish better answer basic blame tensorboard case isnt involv lai log dir",
        "Answer_gpt_summary_original":"possible solutions to issues with tensorboard hparm parallel coordinate view include restarting tensorboard after modifying guild runs, playing around with browser zoom and resizing the browser window. the answer suggests that tensorboard has complex underlying frameworks that are hard to contribute to or fix, and that the best strategy is to take it as is and be grateful.",
        "Answer_gpt_summary":"possibl solut issu tensorboard hparm parallel coordin view includ restart tensorboard modifi run plai browser zoom resiz browser window answer suggest tensorboard complex underli framework hard contribut fix best strategi grate"
    },
    {
        "Question_id":null,
        "Question_title":"FileNotFoundError: [Errno 2] No such file or directory: '\/tmp\/tmpqie8i33i\/MLmodel' - Tutorial: Score machine learning models with PREDICT in serverless Apache Spark pool",
        "Question_body":"Hello,\n\nI am following the tutorial score machine learning models using PREDICT.\n\nI receive the following error: FileNotFoundError: [Errno 2] No such file or directory: '\/tmp\/tmpqie8i33i\/MLmodel'\n\nIn a previous post it was advised to ensure to upload the mlflow folder to AML, not the parent folder to AML.\n\nCould I get a bit more clarity on these steps as I was unable to solve my issue.\n\nThanks in advance,\n\nAdam\n][3]",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1643645716927,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/716755\/filenotfounderror-errno-2-no-such-file-or-director.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-01T11:42:25.703Z",
                "Answer_score":0,
                "Answer_body":"@AdamNevin-8126 After checking the referenced thread I believe @PRADEEPCHEEKATLA-MSFT is referring to uploading your model folder to Azure ML workspace and register it as a model.\n\nIf you are using AML Model as seen in the model_uri parameter above I think you need to navigate to ml.azure.com and register the model from the models tab. Then use the model name and appropriate version for the AML_MODEL_URI env variable.\n\nEx:\n\n AML_MODEL_URI = \"aml:\/\/test1:1\"\n\n\n\n\nIf all the steps to register and authenticate the AML workspace mentioned in the earlier part of the document are run successfully then I think this should work.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"filenotfounderror errno file directori tmp tmpqieii mlmodel tutori score machin learn model predict serverless apach spark pool hello follow tutori score machin learn model predict receiv follow error filenotfounderror errno file directori tmp tmpqieii mlmodel previou post advis ensur upload folder aml parent folder aml bit clariti step unabl solv issu thank advanc adam",
        "Question_preprocessed_content":"filenotfounderror file directori tutori score machin learn model predict serverless apach spark pool hello follow tutori score machin learn model predict receiv follow error filenotfounderror file directori previou post advis ensur upload folder aml parent folder aml bit clariti step unabl solv issu thank advanc adam",
        "Question_gpt_summary_original":"The user encountered a \"FileNotFoundError\" error with the message \"No such file or directory: '\/tmp\/tmpqie8i33i\/MLmodel'\" while following a tutorial on scoring machine learning models using PREDICT. They were advised to upload the \"mlflow\" folder to AML instead of the parent folder, but they still could not resolve the issue and are seeking further clarification.",
        "Question_gpt_summary":"user encount filenotfounderror error messag file directori tmp tmpqieii mlmodel follow tutori score machin learn model predict advis upload folder aml instead parent folder resolv issu seek clarif",
        "Answer_original_content":"adamnevin check referenc thread believ pradeepcheekatla msft refer upload model folder workspac regist model aml model seen model uri paramet think need navig azur com regist model model tab us model appropri version aml model uri env variabl aml model uri aml test step regist authent aml workspac mention earlier document run successfulli think work",
        "Answer_preprocessed_content":"check referenc thread believ refer upload model folder workspac regist model aml model seen paramet think need navig regist model model tab us model appropri version env variabl step regist authent aml workspac mention earlier document run successfulli think work",
        "Answer_gpt_summary_original":"the solution to the filenotfounderror encountered by the user adam while following the tutorial \"score machine learning models with predict in serverless apache spark pool\" is to upload the model folder to the workspace and register it as a model. if using aml model, the user needs to navigate to ml.azure.com and register the model from the models tab. then, use the model name and appropriate version for the aml_model_uri env variable. if all the steps to register and authenticate the aml workspace mentioned in the earlier part of the document are run successfully, this should work.",
        "Answer_gpt_summary":"solut filenotfounderror encount user adam follow tutori score machin learn model predict serverless apach spark pool upload model folder workspac regist model aml model user need navig azur com regist model model tab us model appropri version aml model uri env variabl step regist authent aml workspac mention earlier document run successfulli work"
    },
    {
        "Question_id":null,
        "Question_title":"WandB not using user PID when updating",
        "Question_body":"<p>Hello,<\/p>\n<p>I used  <code>tempfile.mkdtemp() <\/code> to create a temporary directory for my runs (as I don\u2019t want a persistent folder with tons of runs)<\/p>\n<p>For training everything works fine but when resuming the run to do some validation \/ evaluation updates, and using <code>run.summary.update({\"key\": value})<\/code> I got a<\/p>\n<pre><code class=\"lang-auto\">wandb: WARNING Path \/tmp\/tmpq5uafy4d\/wandb\/ wasn't writable, using system temp directory\n<\/code><\/pre>\n<p>with obviously<\/p>\n<pre><code class=\"lang-auto\">File \"\/mnt\/Projets\/nlp\/.venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 855, in _update_summary\n    with open(summary_path, \"w\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: '\/tmp\/tmpq5uafy4d\/wandb\/run-20211102_153311-37264m5k\/files\/wandb-summary.json'\n<\/code><\/pre>\n<p>As in the doc of <a href=\"https:\/\/docs.python.org\/3.9\/library\/tempfile.html#tempfile.mkdtemp\" rel=\"noopener nofollow ugc\"><code>mkdtemp<\/code><\/a> :<\/p>\n<pre><code class=\"lang-auto\"> The directory is readable, writable, and searchable only by the creating user ID.\n<\/code><\/pre>\n<p>So I guess WandB is not using the user ID and thus is not able to write in the directory for updating.<br>\nNote that this directory is different from the training one (as it\u2019s random at each init)<\/p>\n<p>Thanks in advance for any help.<br>\nHave a great day.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1635882593147,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":289.0,
        "Answer_body":"<p>As of now, there isn\u2019t an option to enable that by default. One issue you should be aware of is that if you are currently logging a run when you call <code>wandb sync --clean<\/code> bad things will happen. We\u2019re working on improving the the robustness of these features and will likely support an automatic clean option in the future.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-not-using-user-pid-when-updating\/1204",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-03T15:31:05.874Z",
                "Answer_body":"<p>For the posterity :<\/p>\n<p>After searching for a long time with it, wandb firstly deletes my class object (which call deletion of the temp folder) <strong>and then<\/strong> try to update the run.<\/p>\n<p>To avoid that you need to first call <code>run.finish() or wandb.finish()<\/code> which will first update and then delete your object (or let the garbage collector do it)  thus it will be synced before the object is destroyed and the temp file removed.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-03T16:45:59.689Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ierezell\">@ierezell<\/a>, there are some reasons why you don\u2019t want to automatically delete your run folders, especially if there is an issue with the run. However, I do understand your desire to manage the clutter in your file system. It sounds like having a feature where you could tell wandb to delete the local files it created after a successful run would be the most preferable option for you. Currently, calling <code>wandb sync --clean<\/code> will sync any unsynced runs and then remove those run folders from your computer. The wandb dir will still be there and there but it will only be taking up bytes of space as most of the information will have been deleted. If you still find it annoying to have mostly empty <code>.\/wandb<\/code> folders in your project dirs, you can set the <code>WANDB_DIR<\/code> environment variable to an absolute path where all of your run data will be stored until you call <code>wandb sync --clean<\/code>.<\/p>",
                "Answer_score":20.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-03T18:08:15.337Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/aidanjd\">@aidanjd<\/a>,<\/p>\n<p>I didn\u2019t know the <code>wandb sync --clean<\/code> option!<br>\nIt\u2019s quite what I wanted to do, saving space and your solution will only delete finished runs which is nice <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>I don\u2019t mind deleting failed run as I can just relaunch them again (it\u2019s small models on only one machine). This is why I put all my wandb folders in <code>\/tmp<\/code> which means I keep the folder until I reboot.<\/p>\n<p>Is there any option to have the <code>clean<\/code> feature enabled by default? I mean for any run if sync is complete: delete the folder.<\/p>\n<p>Thanks for the response, I guess it solves it but we can continue discussing it.<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-03T18:21:20.198Z",
                "Answer_body":"<p>As of now, there isn\u2019t an option to enable that by default. One issue you should be aware of is that if you are currently logging a run when you call <code>wandb sync --clean<\/code> bad things will happen. We\u2019re working on improving the the robustness of these features and will likely support an automatic clean option in the future.<\/p>",
                "Answer_score":15.4,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-01-02T18:22:08.008Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1635963680198,
        "Question_original_content":"user pid updat hello tempfil mkdtemp creat temporari directori run dont want persist folder ton run train work fine resum run valid evalu updat run summari updat kei valu got warn path tmp tmpquafyd wasn writabl temp directori obvious file mnt projet nlp venv lib python site packag sdk intern sender line updat summari open summari path filenotfounderror errno file directori tmp tmpquafyd run file summari json doc mkdtemp directori readabl writabl searchabl creat user guess user abl write directori updat note directori differ train random init thank advanc help great dai",
        "Question_preprocessed_content":"user pid updat hello creat temporari directori run train work fine resum run valid evalu updat got obvious doc guess user abl write directori updat note directori differ train thank advanc help great dai",
        "Question_gpt_summary_original":"The user encountered a challenge when using WandB to update their runs. They created a temporary directory using tempfile.mkdtemp() for their runs, but when resuming the run to do some validation\/evaluation updates and using run.summary.update({\"key\": value}), they received a warning that the path wasn't writable, and a FileNotFoundError occurred. The user suspects that WandB is not using the user ID and thus is not able to write in the directory for updating.",
        "Question_gpt_summary":"user encount challeng updat run creat temporari directori tempfil mkdtemp run resum run valid evalu updat run summari updat kei valu receiv warn path wasn writabl filenotfounderror occur user suspect user abl write directori updat",
        "Answer_original_content":"isnt option enabl default issu awar current log run sync clean bad thing happen work improv robust featur like support automat clean option futur",
        "Answer_preprocessed_content":"isnt option enabl default issu awar current log run bad thing happen work improv robust featur like support automat clean option futur",
        "Answer_gpt_summary_original":"there is currently no option to enable the temporary directory created by tempfile.mkdtemp() to be writable by the user's id. however, the team is working on improving the robustness of these features and may support an automatic clean option in the future. it is important to note that if the user is currently logging a run when they call sync --clean, bad things will happen.",
        "Answer_gpt_summary":"current option enabl temporari directori creat tempfil mkdtemp writabl user team work improv robust featur support automat clean option futur import note user current log run sync clean bad thing happen"
    },
    {
        "Question_id":null,
        "Question_title":"Moving resources across the regions",
        "Question_body":"Hi,\nWe have resource group(RG1) created in Germany region. Resource created are\n1) Vnet\n2) VPN gateway\n3) Disks - HDD\n4) VM (database installed)\n\nWe wanted to use Azure machine learning service to read data from database(present in resource group RG1 in German region). We later found that azure machine learning service is not available in Germany region and in order to create end points for Azure ML service both the networks(vnets of the DB and Azure ML ) should be in the same region.\n\nSo we tried migrating the resource and resource group (RG1) from German region to west Europe by using wizard on the portal. But we got prompted that disk , vnet and vpn gateway cannot be moved to different region ) .\n\nIs there anyway we could move them ? or any alternate solutions ? .Else, we would end up recreating every thing in westeurope which i would like to avoid.\n\nRegards,\nSuman",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1606983526427,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Maybe this is helpful:\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/site-recovery\/azure-to-azure-tutorial-migrate\n\n(If the reply was helpful please don't forget to upvote and\/or accept as answer, thank you)\n\nRegards\nAndreas Baumgarten",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/184565\/moving-resources-across-the-regions.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-03T08:30:58.243Z",
                "Answer_score":0,
                "Answer_body":"Maybe this is helpful:\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/site-recovery\/azure-to-azure-tutorial-migrate\n\n(If the reply was helpful please don't forget to upvote and\/or accept as answer, thank you)\n\nRegards\nAndreas Baumgarten",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2020-12-03T09:20:21.54Z",
                "Answer_score":1,
                "Answer_body":"@SumanKaranam-2803, There is a process in Azure to move VNET across region using ARM template. But, it is almost kind of recreating the resource with a automated template with desired configuration.\n\nFor VNET:\n\nMethod1: You can use an Azure Resource Manager template to complete the move of the virtual network to another region. You do this by exporting the virtual network to a template, modifying the parameters to match the destination region, and then deploying the template to the new region.\n\nRef Doc : Move an Azure virtual network to another region by using the Azure portal\n\nOR\n\nMethod2: You can use the latest Azure service called \"Azure Resource Mover\" to move VNET's across regions,\n\nRef Doc : Azure Resource Mover\n\nFor VPN gateway: AFAIK, today you can move VPN Gateway across Resource Groups and subscriptions only.\n\nSS:1\n\nNote : If you would like to request\/upvote for this feature use this feedback section for its future availability.\n\nPlease do not forget to \"Accept the answer\" wherever the information provided helps you to help others in the community.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-04T09:02:14.08Z",
                "Answer_score":0,
                "Answer_body":"@SumanKaranam-2803,\n\nIf you think your question has been answered, click \"Mark as Answer\" if just helped click \"Vote as helpful\". This can be beneficial to other community members reading this forum thread.\n\n\nBest regards\nSubhash",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1606984258243,
        "Question_original_content":"move resourc region resourc group creat germani region resourc creat vnet vpn gatewai disk hdd databas instal want us servic read data databas present resourc group german region later servic avail germani region order creat end point servic network vnet region tri migrat resourc resourc group german region west europ wizard portal got prompt disk vnet vpn gatewai move differ region altern solut end recreat thing westeurop like avoid regard suman",
        "Question_preprocessed_content":"move resourc region resourc group creat germani region resourc creat vnet vpn gatewai disk hdd want us servic read data databas later servic avail germani region order creat end point servic network region tri migrat resourc resourc group german region west europ wizard portal got prompt disk vnet vpn gatewai move differ region altern solut end recreat thing westeurop like avoid regard suman",
        "Question_gpt_summary_original":"The user is facing challenges in moving their resource group from the German region to West Europe in order to use Azure machine learning service to read data from a database. They discovered that Azure machine learning service is not available in the German region and in order to create endpoints for Azure ML service, both networks should be in the same region. However, they were unable to move the disk, Vnet, and VPN gateway to a different region using the wizard on the portal. The user is seeking alternative solutions to avoid recreating everything in West Europe.",
        "Question_gpt_summary":"user face challeng move resourc group german region west europ order us servic read data databas discov servic avail german region order creat endpoint servic network region unabl disk vnet vpn gatewai differ region wizard portal user seek altern solut avoid recreat west europ",
        "Answer_original_content":"mayb help http doc microsoft com azur site recoveri azur azur tutori migrat repli help forget upvot accept answer thank regard andrea baumgarten",
        "Answer_preprocessed_content":"mayb help repli help forget upvot accept answer thank regard andrea baumgarten",
        "Answer_gpt_summary_original":"the answer suggests checking out a tutorial on azure site recovery for migrating resources across regions.",
        "Answer_gpt_summary":"answer suggest check tutori azur site recoveri migrat resourc region"
    },
    {
        "Question_id":null,
        "Question_title":"Model changes",
        "Question_body":"It appears that the default model for GCP Cloud Vision API has changed. Specifically, the current API results lack localized objects that were available previously, say around June 2021. How does the team communicate, if at all, what entities the model supports? Thanks.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634645580000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":322.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Model-changes\/td-p\/173396\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"As per the release notes [1], it doesn't seem like there was any change to the default model.\n\nFrom all of the available models [2], for your use case you can use the object localizer [3].\n\n[1] https:\/\/cloud.google.com\/vision\/docs\/release-notes\n[2] https:\/\/cloud.google.com\/vision\/docs\/reference\/rest\/v1\/Feature#type\n[3] https:\/\/cloud.google.com\/vision\/docs\/object-localizer"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"model chang appear default model gcp cloud vision api chang specif current api result lack local object avail previous june team commun entiti model support thank",
        "Question_preprocessed_content":"model chang appear default model gcp cloud vision api chang specif current api result lack local object avail previous june team commun entiti model support thank",
        "Question_gpt_summary_original":"The user is facing challenges with the GCP Cloud Vision API as the default model has changed and the current API results lack localized objects that were available previously. The user is seeking information on how the team communicates the entities that the model supports.",
        "Question_gpt_summary":"user face challeng gcp cloud vision api default model chang current api result lack local object avail previous user seek inform team commun entiti model support",
        "Answer_original_content":"releas note like chang default model avail model us case us object local http cloud googl com vision doc releas note http cloud googl com vision doc refer rest featur type http cloud googl com vision doc object local",
        "Answer_preprocessed_content":"releas note like chang default model avail model us case us object local",
        "Answer_gpt_summary_original":"possible solutions to the user's challenge with the changes to the default model for gcp cloud vision api are: \n\n1. according to the release notes, there was no change to the default model. \n2. the user can explore other available models for their use case. \n3. the object localizer can be used for localized objects.",
        "Answer_gpt_summary":"possibl solut user challeng chang default model gcp cloud vision api accord releas note chang default model user explor avail model us case object local local object"
    },
    {
        "Question_id":null,
        "Question_title":"VM Ram vs Google Colab Ram",
        "Question_body":"Hi, @Eduardo_Ortiz  @josegutierrez sorry to bother but I`m completely lostDays a go I bought a VM that has the next configurations, when I connect to the VM with Google Colab get the next results as you can see in the next image.VM Configuration : GPUs1 x NVIDIA Tesla V100  +  n1-highmem-8 (vCPUs: 8, RAM: 52GB)Ram obtained in Google Colab from the VM: 1.31 Gb \/ 51.01 Gb Disc 43.79 \/ 186.52As you realized,  althoug I have buy a better configuration than Google Coalb Pro+ Im getting fewer RAM from the VM instance....What could be the error or situation? How can I get into colab the real VM capacity bought? Or which configuration do I need in order to have better performance than Google Colab pro+?In the next screen shot the ram and disck that I got from Google Colab:Thanks a lot for any help ",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1658926260000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":61.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/VM-Ram-vs-Google-Colab-Ram\/td-p\/447466\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-27T12:51:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi, @Eduardo_Ortiz\u00a0 @josegutierrez\u00a0sorry to bother but I`m completely lost\n\nDays a go I bought a VM that has the next configurations, when I connect to the VM with Google Colab get the next results as you can see in the next image.\n\nVM Configuration : GPUs1 x NVIDIA Tesla V100\u00a0 +\u00a0\u00a0n1-highmem-8 (vCPUs: 8, RAM: 52GB)\n\nRam obtained in Google Colab from the VM: 1.31 Gb \/ 51.01 Gb Disc 43.79 \/ 186.52\n\nAs you realized,\u00a0 althoug I have buy a better configuration than Google Coalb Pro+ Im getting fewer RAM from the VM instance....\n\nWhat could be the error or situation? How can I get into colab the real VM capacity bought? Or which configuration do I need in order to have better performance than Google Colab pro+?\n\nIn the next screen shot the ram and disck that I got from Google Colab:\n\nThanks a lot for any help"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"ram googl colab ram eduardo ortiz josegutierrez sorri bother complet lostdai bought configur connect googl colab result imag configur gpu nvidia tesla highmem vcpu ram ram obtain googl colab disc realiz althoug bui better configur googl coalb pro get fewer ram instanc error situat colab real capac bought configur need order better perform googl colab pro screen shot ram disck got googl colab thank lot help",
        "Question_preprocessed_content":"ram googl colab ram sorri bother complet lostdai bought configur connect googl colab result configur gpu nvidia tesla ram obtain googl colab disc realiz althoug bui better configur googl coalb pro get fewer ram error situat colab real capac bought configur need order better perform googl colab pro screen shot ram disck got googl colab thank lot help",
        "Question_gpt_summary_original":"The user has encountered a challenge with their VM configuration and Google Colab RAM. Despite purchasing a VM with better specifications than Google Colab Pro+, they are only getting 1.31 GB of RAM from the VM instance when connected to Google Colab. The user is seeking help to understand the error or situation and how to access the real VM capacity or improve performance beyond Google Colab Pro+.",
        "Question_gpt_summary":"user encount challeng configur googl colab ram despit purchas better specif googl colab pro get ram instanc connect googl colab user seek help understand error situat access real capac improv perform googl colab pro",
        "Answer_original_content":"eduardo ortiz josegutierrezsorri bother complet lost dai bought configur connect googl colab result imag configur gpu nvidia tesla highmem vcpu ram ram obtain googl colab disc realiz althoug bui better configur googl coalb pro get fewer ram instanc error situat colab real capac bought configur need order better perform googl colab pro screen shot ram disck got googl colab thank lot help",
        "Answer_preprocessed_content":"bother complet lost dai bought configur connect googl colab result imag configur gpu nvidia tesla ram obtain googl colab disc realiz althoug bui better configur googl coalb pro get fewer ram error situat colab real capac bought configur need order better perform googl colab pro screen shot ram disck got googl colab thank lot help",
        "Answer_gpt_summary_original":"there are no explicit solutions provided in the answer. the user is seeking help to understand why they are not able to obtain the full ram and disk capacity of their vm instance when connecting to google colab. they are also asking for suggestions on how to get the real vm capacity bought or which configuration they need to have better performance than google colab pro+.",
        "Answer_gpt_summary":"explicit solut provid answer user seek help understand abl obtain ram disk capac instanc connect googl colab ask suggest real capac bought configur need better perform googl colab pro"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning - Input \"Untrained model\" has invalid type \"DataTable\" error",
        "Question_body":"First time trying Azure ML using Two-Class Logistic Regression.\n\nData source is a table in AzureSQL and it's throwing InvalidLearnerError: Input \"Untrained model\" has invalid type \"DataTable\".\n\nHelp?",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1663734079143,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1016607\/azure-machine-learning-input-34untrained-model34-h.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-03T13:17:10.397Z",
                "Answer_score":0,
                "Answer_body":"@tw22 Thanks for the details. If the component output data is in a tabular format, you must choose to register the output as a file dataset or tabular dataset. Here is the document that can help.\nhttps:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-designer-import-data#register-a-dataset",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"input untrain model invalid type datat error time try class logist regress data sourc tabl azuresql throw invalidlearnererror input untrain model invalid type datat help",
        "Question_preprocessed_content":"input untrain model invalid type datat error time try logist regress data sourc tabl azuresql throw invalidlearnererror input untrain model invalid type datat help",
        "Question_gpt_summary_original":"The user is encountering an InvalidLearnerError while trying to use Azure Machine Learning for Two-Class Logistic Regression. The error message states that the input \"Untrained model\" has an invalid type \"DataTable\". The user is seeking assistance to resolve this issue.",
        "Question_gpt_summary":"user encount invalidlearnererror try us class logist regress error messag state input untrain model invalid type datat user seek assist resolv issu",
        "Answer_original_content":"thank detail compon output data tabular format choos regist output file dataset tabular dataset document help http learn microsoft com azur machin learn design import data regist dataset",
        "Answer_preprocessed_content":"thank detail compon output data tabular format choos regist output file dataset tabular dataset document help",
        "Answer_gpt_summary_original":"the solution to the \"invalidlearnererror\" when using two-class logistic regression with a data source from an azure sql table is to register the output as a file dataset or tabular dataset. the user can refer to the provided document for more information on how to do this.",
        "Answer_gpt_summary":"solut invalidlearnererror class logist regress data sourc azur sql tabl regist output file dataset tabular dataset user refer provid document inform"
    },
    {
        "Question_id":null,
        "Question_title":"Speech to Text using Microphone",
        "Question_body":"Hi there, Was trying to use convert speech to text using a microphone, getting a pop-up security error saying \"failed to construct 'worker': script at ..........\"Any idea why?Also, how can I use the speech to text service with microphone input, I tested uploading a video file and worked perfectly, but no idea how to use the service with microphone input?Please let me know AJ",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637533140000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":99.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-to-Text-using-Microphone\/td-p\/176201\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-22T15:56:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"- The error message [1] can be seen in Chrome as Chrome doesn't let you load web workers when running scripts from a local file. You can try using a different web browser like Firefox and verify if it works for you.\n\n- In order to read from microphone, you can install PyAudio in your machine.\n\nYou can refer to the documentation [2] [3] to use the speech to text service with microphone input.\n\n[1] \"failed to construct 'worker': script at\n\n[2] https:\/\/www.thepythoncode.com\/article\/using-speech-recognition-to-convert-speech-to-text-python\n\n[3] https:\/\/pypi.org\/project\/SpeechRecognition\/"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"speech text microphon try us convert speech text microphon get pop secur error sai fail construct worker script idea us speech text servic microphon input test upload video file work perfectli idea us servic microphon input let know",
        "Question_preprocessed_content":"speech text microphon try us convert speech text microphon get secur error sai fail construct worker script idea us speech text servic microphon input test upload video file work perfectli idea us servic microphon input let know",
        "Question_gpt_summary_original":"The user is encountering a security error while trying to convert speech to text using a microphone. They are seeking assistance in resolving the error and also in understanding how to use the speech to text service with microphone input.",
        "Question_gpt_summary":"user encount secur error try convert speech text microphon seek assist resolv error understand us speech text servic microphon input",
        "Answer_original_content":"error messag seen chrome chrome let load web worker run script local file try differ web browser like firefox verifi work order read microphon instal pyaudio machin refer document us speech text servic microphon input fail construct worker script http thepythoncod com articl speech recognit convert speech text python http pypi org project speechrecognit",
        "Answer_preprocessed_content":"error messag seen chrome chrome let load web worker run script local file try differ web browser like firefox verifi work order read microphon instal pyaudio machin refer document us speech text servic microphon input fail construct worker script",
        "Answer_gpt_summary_original":"possible solutions from the answer are: \n- try using a different web browser like firefox to see if the speech to text service works.\n- install pyaudio in your machine and refer to the documentation to use the speech to text service with microphone input.",
        "Answer_gpt_summary":"possibl solut answer try differ web browser like firefox speech text servic work instal pyaudio machin refer document us speech text servic microphon input"
    },
    {
        "Question_id":null,
        "Question_title":"is there any list of brands\/logos supported by google vision api?",
        "Question_body":"is there anyplace i can see the list of brands\/logos that are currently supported by the google vision api's logo recognition service?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636578060000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":233.0,
        "Answer_body":"The product team does not currently publish such a list. I recommend for you to submit a Feature Request to the Vision API product team.\u00a0\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/is-there-any-list-of-brands-logos-supported-by-google-vision-api\/td-p\/175425\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-23T11:41:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"The product team does not currently publish such a list. I recommend for you to submit a Feature Request to the Vision API product team.\u00a0\n\nView solution in original post"
            },
            {
                "Answer_creation_time":"2021-11-23T11:41:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"The product team does not currently publish such a list. I recommend for you to submit a Feature Request to the Vision API product team."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1637667660000,
        "Question_original_content":"list brand logo support googl vision api anyplac list brand logo current support googl vision api logo recognit servic",
        "Question_preprocessed_content":"list support googl vision api anyplac list current support googl vision api logo recognit servic",
        "Question_gpt_summary_original":"The user is seeking information about a list of brands and logos that are currently supported by the Google Vision API's logo recognition service.",
        "Question_gpt_summary":"user seek inform list brand logo current support googl vision api logo recognit servic",
        "Answer_original_content":"product team current publish list recommend submit featur request vision api product team view solut origin post",
        "Answer_preprocessed_content":"product team current publish list recommend submit featur request vision api product team view solut origin post",
        "Answer_gpt_summary_original":"the user is looking for a list of brands\/logos supported by google vision api's logo recognition service. unfortunately, the product team does not currently publish such a list. however, the user can submit a feature request to the vision api product team.",
        "Answer_gpt_summary":"user look list brand logo support googl vision api logo recognit servic unfortun product team current publish list user submit featur request vision api product team"
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker debugger built in rule CreateXgboostRule not generating report as expected",
        "Question_body":"I'm currently working with a SageMaker hosted XGBoost model; I've added the built in rule \"CreateXgboostRule\" to generate a training report, however, only the ProfilerReport is generated in the S3 rule-output folder - the expected result based on the dev doc is for a CreateXGBoostRule folder as well within this same folder.\n\nThe code I'm using is based directly on the example provided in: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/debugger-training-xgboost-report.html\n\nimport boto3\nimport sagemaker\nfrom sagemaker.estimator import Estimator\nfrom sagemaker import image_uris\nfrom sagemaker.debugger import Rule, rule_configs\n\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\n\nregion = boto3.Session().region_name\nxgboost_container=sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n\nestimator=Estimator(\n    role=sagemaker.get_execution_role()\n    image_uri=xgboost_container,\n    base_job_name=\"debugger-xgboost-report-demo\",\n    instance_count=1,\n    instance_type=\"ml.m5.2xlarge\",\n    \n    # Add the Debugger XGBoost report rule\n    rules=rules\n)\n\nestimator.fit(wait=False)\n\n\nI've tried rewriting the estimator a number of ways, verified \"rules\" is receiving an array of objects, tried different versions of XGBoost within the region, but everything still results in the built in rule only creating the ProfilerReport with no CreateXGBoostRule directory under rule-output.\n\nAny ideas would be greatly appreciated! Thanks.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662562294513,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":48.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuZEDvbaeRfqOT_g2Sxlw7w\/sage-maker-debugger-built-in-rule-create-xgboost-rule-not-generating-report-as-expected",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-08T13:53:26.107Z",
                "Answer_score":0,
                "Answer_body":"Hi Dennis, I tried a code above in AWS SageMaker notebook and didn't get any ProfilerReport as well.\n\nThe possible issue is that there is no data to train hence nothing to report. To prove this hypothesis I decided to take a sample XGBoost notebook and to add report functionality in it. This link tells how to access sample notebooks. I used \"xgboost_customer_churn.ipynb\".\n\nHere some changes that I made:\n\nIn order to be able to run it smoothly, instead of \"1.6-1\" XGBoost image I set the \"1.2-1\":\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", sess.boto_region_name, \"1.2-1\")\n\nI added reporting to the training cell:\nfrom sagemaker.debugger import Rule, rule_configs\n\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\nsess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(\n    container,\n    role,\n    instance_count=1,\n    instance_type=\"ml.m4.xlarge\",\n    output_path=\"s3:\/\/{}\/{}\/output\".format(bucket, prefix),\n    sagemaker_session=sess,\n    base_job_name=\"debugger-xgboost-report-demo\",\n    rules=rules\n)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    gamma=4,\n    min_child_weight=6,\n    subsample=0.8,\n    verbosity=0,\n    objective=\"binary:logistic\",\n    num_round=100,\n)\n\nxgb.fit({\"train\": s3_input_train, \"validation\": s3_input_validation})\n\n\nAll the rest I run without changes.\n\nHowever, it was a little confusing to fing a ProfilerReport. In this particular example it had the path: sagemaker-region-11122233\/sagemaker\/DEMO-xgboost-churn\/output\/debugger-xgboost-report-demo-2022-00-000\/rule-output\/CreateXgboostReport\/\n\nIn order to get this path you can use (run this code in the SageMaker notebook):\n\nxgb.output_path - gives the first part of the path (including the bucket name)\n\nxgb.latest_training_job.job_name - gives the second part of the path (training job name)\n\nIf you combine these two, you will get the full path towards the rule-output directory.\n\nI hope this helps.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"debugg built rule createxgboostrul gener report expect current work host xgboost model ad built rule createxgboostrul gener train report profilerreport gener rule output folder expect result base dev doc createxgboostrul folder folder code base directli exampl provid http doc aw amazon com latest debugg train xgboost report html import boto import estim import estim import imag uri debugg import rule rule config rule rule rule config creat xgboost report region boto session region xgboost contain imag uri retriev xgboost region estim estim role execut role imag uri xgboost contain base job debugg xgboost report demo instanc count instanc type xlarg add debugg xgboost report rule rule rule estim fit wait fals tri rewrit estim number wai verifi rule receiv arrai object tri differ version xgboost region result built rule creat profilerreport createxgboostrul directori rule output idea greatli appreci thank",
        "Question_preprocessed_content":"debugg built rule createxgboostrul gener report expect current work host xgboost model ad built rule createxgboostrul gener train report profilerreport gener folder expect result base dev doc createxgboostrul folder folder code base directli exampl provid import boto import estim import estim import debugg import rule rule region region estim estim add debugg xgboost report rule rule rule tri rewrit estim number wai verifi rule receiv arrai object tri differ version xgboost region result built rule creat profilerreport createxgboostrul directori idea greatli appreci thank",
        "Question_gpt_summary_original":"The user is facing a challenge with the SageMaker debugger built-in rule \"CreateXgboostRule\" not generating the expected training report. The expected result is for a CreateXGBoostRule folder to be generated in the S3 rule-output folder, but only the ProfilerReport is being generated. The user has tried rewriting the estimator and verifying that \"rules\" is receiving an array of objects, but the issue persists. The user is seeking ideas to resolve the problem.",
        "Question_gpt_summary":"user face challeng debugg built rule createxgboostrul gener expect train report expect result createxgboostrul folder gener rule output folder profilerreport gener user tri rewrit estim verifi rule receiv arrai object issu persist user seek idea resolv problem",
        "Answer_original_content":"denni tri code notebook profilerreport possibl issu data train report prove hypothesi decid sampl xgboost notebook add report function link tell access sampl notebook xgboost custom churn ipynb chang order abl run smoothli instead xgboost imag set contain imag uri retriev xgboost sess boto region ad report train cell debugg import rule rule config rule rule rule config creat xgboost report sess session xgb estim estim contain role instanc count instanc type xlarg output path output format bucket prefix session sess base job debugg xgboost report demo rule rule xgb set hyperparamet max depth eta gamma min child weight subsampl verbos object binari logist num round xgb fit train input train valid input valid rest run chang littl confus fing profilerreport particular exampl path region demo xgboost churn output debugg xgboost report demo rule output createxgboostreport order path us run code notebook xgb output path give path includ bucket xgb latest train job job give second path train job combin path rule output directori hope help",
        "Answer_preprocessed_content":"denni tri code notebook profilerreport possibl issu data train report prove hypothesi decid sampl xgboost notebook add report function link tell access sampl notebook chang order abl run smoothli instead xgboost imag set contain ad report train cell debugg import rule rule sess session xgb contain role prefix rule rule gamma verbos object binari logist valid rest run chang littl confus fing profilerreport particular exampl path order path us give path give second path combin path directori hope help",
        "Answer_gpt_summary_original":"the possible solution to the issue of the debugger not generating the expected report is that there might be no data to train. to test this hypothesis, the user can access a sample xgboost notebook and add reporting functionality to it. the user can also make some changes to the code and run it without any further changes. the path to the profiler report can be found by combining the output path and the training job name.",
        "Answer_gpt_summary":"possibl solut issu debugg gener expect report data train test hypothesi user access sampl xgboost notebook add report function user chang code run chang path profil report combin output path train job"
    },
    {
        "Question_id":64974572.0,
        "Question_title":"RecordIO: \"The header of the MXNet RecordIO record...does not start with a valid magic number\"",
        "Question_body":"<p>Using Linear Learner in Sagemaker with MXNet RecordIO, I get <code>&quot;The header of the MXNet RecordIO record at position 5,089,840 in the dataset does not start with a valid magic number&quot;<\/code> after <code>fit()<\/code> has been running 38 minutes.<\/p>\n<p>The file was generated using this code. Note that I tried two ways to upload to S3. I also tried direct upload of the <code>BytesIO<\/code> as well as upload of a file, shown here.<\/p>\n<pre><code>train_file = 'linear_train.data'\n\nf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))\nf.seek(0)\n \n# Write the stuff\nwith open(train_file, &quot;wb&quot;) as fl:\n    fl.write(f.getvalue())\n\n# Alternative for upload\n# boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', train_file)).upload_fileobj(f)\nboto3.client('s3').upload_file(train_file,\n                                  Bucket=bucket,\n                                  Key=os.path.join(prefix, 'train', train_file),\n                                  ExtraArgs={'ACL': 'bucket-owner-full-control'})\n<\/code><\/pre>\n<p>To check that the file was non-corrupt, I downloaded it from S3 and simply read it as follows.<\/p>\n<pre><code>   record = MXRecordIO(fl, 'r')\n   while True:\n      item = record.read()\n      # Here we print the item, break when we reach the end, etc. This confirms that the RecordIO is valid.\n<\/code><\/pre>\n<p>So, the file seems OK.<\/p>\n<p>How can I run Linear Learner?<\/p>\n<p>Here is the error message:<\/p>\n<pre><code>Failure reason if any: ClientError: Unable to read data channel 'train'. Requested content-type is 'application\/x-recordio-protobuf'. Please verify the data matches the requested content-type. (caused by MXNetError)\n\nCaused by: [17:04:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsCppLibs\/AIAlgorithmsCppLibs-2.0.3446.0\/AL2012\/generic-flavor\/src\/src\/aialgs\/io\/iterator_base.cpp:100: (Input Error) The header of the MXNet RecordIO record at position 5,089,840 in the dataset does not start with a valid magic number.\n\nStack trace returned 10 entries:\n[bt] (0) \/opt\/amazon\/lib\/libaialgs.so(+0xbca0) [0x7f337885cca0]\n[bt] (1) \/opt\/amazon\/lib\/libaialgs.so(+0xbffa) [0x7f337885cffa]\n[bt] (2) \/opt\/amazon\/lib\/libaialgs.so(aialgs::iterator_base::Next()+0x4a6) [0x7f33788675e6]\n[bt] (3) \/opt\/amazon\/lib\/libmxnet.so(MXDataIterNext+0x21) [0x7f3367272141]\n[bt] (4) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call_unix64+0x4c) [0x7f3378893958]\n[bt] (5) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call+0x15f) [0x7f33\nFailed 39.0 min; Failure reason if any: ClientError: Unable to read data channel 'train'. Requested content-type is 'application\/x-recordio-protobuf'. Please verify the data matches the requested content-type. (caused by MXNetError)\n\nCaused by: [17:04:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsCppLibs\/AIAlgorithmsCppLibs-2.0.3446.0\/AL2012\/generic-flavor\/src\/src\/aialgs\/io\/iterator_base.cpp:100: (Input Error) The header of the MXNet RecordIO record at position 5,089,840 in the dataset does not start with a valid magic number.\n\nStack trace returned 10 entries:\n[bt] (0) \/opt\/amazon\/lib\/libaialgs.so(+0xbca0) [0x7f337885cca0]\n[bt] (1) \/opt\/amazon\/lib\/libaialgs.so(+0xbffa) [0x7f337885cffa]\n[bt] (2) \/opt\/amazon\/lib\/libaialgs.so(aialgs::iterator_base::Next()+0x4a6) [0x7f33788675e6]\n[bt] (3) \/opt\/amazon\/lib\/libmxnet.so(MXDataIterNext+0x21) [0x7f3367272141]\n[bt] (4) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call_unix64+0x4c) [0x7f3378893958]\n[bt] (5) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call+0x15f) [0x7f33\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606157531417,
        "Question_favorite_count":null,
        "Question_last_edit_time":1606158102500,
        "Question_score":0.0,
        "Question_view_count":178.0,
        "Answer_body":"<p>Is was because of CSV files in the same S3 folder as the RecordIO.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64974572",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1606206060190,
        "Question_original_content":"recordio header mxnet recordio record start valid magic number linear learner mxnet recordio header mxnet recordio record posit dataset start valid magic number fit run minut file gener code note tri wai upload tri direct upload bytesio upload file shown train file linear train data bytesio smac write numpi dens tensor train astyp float train astyp float seek write stuff open train file write getvalu altern upload boto session resourc bucket bucket object path join prefix train train file upload fileobj boto client upload file train file bucket bucket kei path join prefix train train file extraarg acl bucket owner control check file non corrupt download simpli read follow record mxrecordio true item record read print item break reach end confirm recordio valid file run linear learner error messag failur reason clienterror unabl read data channel train request content type applic recordio protobuf verifi data match request content type caus mxneterror caus opt brazil pkg cach packag aialgorithmscpplib aialgorithmscpplib gener flavor src src aialg iter base cpp input error header mxnet recordio record posit dataset start valid magic number stack trace return entri opt amazon lib libaialg xbca xfcca opt amazon lib libaialg xbffa xfcffa opt amazon lib libaialg aialg iter base xfe opt amazon lib libmxnet mxdataiternext opt amazon python lib python lib dynload ctype ffi unix opt amazon python lib python lib dynload ctype ffi fail min failur reason clienterror unabl read data channel train request content type applic recordio protobuf verifi data match request content type caus mxneterror caus opt brazil pkg cach packag aialgorithmscpplib aialgorithmscpplib gener flavor src src aialg iter base cpp input error header mxnet recordio record posit dataset start valid magic number stack trace return entri opt amazon lib libaialg xbca xfcca opt amazon lib libaialg xbffa xfcffa opt amazon lib libaialg aialg iter base xfe opt amazon lib libmxnet mxdataiternext opt amazon python lib python lib dynload ctype ffi unix opt amazon python lib python lib dynload ctype ffi",
        "Question_preprocessed_content":"recordio header mxnet recordio start valid magic number linear learner mxnet recordio run minut file gener code note tri wai upload tri direct upload upload file shown check file download simpli read follow file run linear learner error messag",
        "Question_gpt_summary_original":"The user encountered an error message while using Linear Learner in Sagemaker with MXNet RecordIO. The error message states that the header of the MXNet RecordIO record does not start with a valid magic number. The user tried to upload the file in two ways and checked that the file was non-corrupt, but the error still persists. The user is seeking help to resolve the issue and run Linear Learner.",
        "Question_gpt_summary":"user encount error messag linear learner mxnet recordio error messag state header mxnet recordio record start valid magic number user tri upload file wai check file non corrupt error persist user seek help resolv issu run linear learner",
        "Answer_original_content":"csv file folder recordio",
        "Answer_preprocessed_content":"csv file folder recordio",
        "Answer_gpt_summary_original":"possible solution: remove the csv files from the same s3 folder as the recordio files.",
        "Answer_gpt_summary":"possibl solut remov csv file folder recordio file"
    },
    {
        "Question_id":null,
        "Question_title":"How to configured S3 connection to upload\/download artifacts programmatically",
        "Question_body":"We\u2019re implementing a few things and I\u2019ve got a quick question.\nI see the example here https:\/\/github.com\/polyaxon\/polyaxon\/blob\/faec6649ed6a09ad29365f17795a404cc714c22e\/site\/integrations\/data-on-s3.md but I don\u2019t quite understand how to setup that S3Service(...) object. what would I pass in? a connection? how do I create the s3 connection object to pass in? I\u2019ve got the connection defined in polyaxon\u2019s config\/polyaxonfile but I\u2019m not sure what to create in python there.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649333190000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1480",
        "Tool":"Polyaxon",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-07T12:11:52Z",
                "Answer_score":1,
                "Answer_body":"We have to update that section, those services are deprecated, I would use the new implementation:\n\nfrom polyaxon.fs.fs import get_fs_from_name\n\nfs = get_fs_from_name(\"model-registry-s3\")\n\nThis will return a fully resolved s3fs object. More information about how to use the fs object: https:\/\/s3fs.readthedocs.io\/en\/latest\/#examples.\n\nNote 1: The s3 rquirement is not installed by default, you wiil need pip install \"polyaxon[s3]\"\n\nNote2: You will have to request the connection:\n\nrun:\n  connections: [\"model-registry-s3\"]\n\nAlso by requesting the connection, the secret\/config will be available in the container, so you can also use boto3 automatically if you do not like the to the s3fs implementation.\n\nThe docs for:\n\nGCS\nS3\nAzure"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0,
        "Question_closed_time":null,
        "Question_original_content":"configur connect upload download artifact programmat implement thing iv got quick question exampl http github com blob faecedaadfaccc site integr data dont understand setup sservic object pass connect creat connect object pass iv got connect defin config file sure creat python",
        "Question_preprocessed_content":"configur connect artifact programmat implement thing iv got quick question exampl dont understand setup object pass connect creat connect object pass iv got connect defin sure creat python",
        "Question_gpt_summary_original":"The user is facing challenges in configuring an S3 connection to upload\/download artifacts programmatically. They are struggling to understand how to set up the S3Service object and what to pass in, particularly with regards to creating the S3 connection object in Python. They have the connection defined in Polyaxon's config\/polyaxonfile but are unsure how to proceed.",
        "Question_gpt_summary":"user face challeng configur connect upload download artifact programmat struggl understand set sservic object pass particularli regard creat connect object python connect defin config file unsur proce",
        "Answer_original_content":"updat section servic deprec us new implement import model registri return fulli resolv sf object inform us object http sf readthedoc latest exampl note rquirement instal default wiil need pip instal note request connect run connect model registri request connect secret config avail contain us boto automat like sf implement doc gc azur",
        "Answer_preprocessed_content":"updat section servic deprec us new implement import return fulli resolv object inform us object note rquirement instal default wiil need pip instal note request connect run connect request connect avail contain us boto automat like implement doc gc azur",
        "Answer_gpt_summary_original":"Solution: The solution provided is to update the section and use the new implementation by importing the \"get_fs_from_name\" function from \"polyaxon.fs.fs\" and passing the name of the service as an argument to get the fully resolved s3fs object. The user needs to install the s3 requirement by running \"pip install 'polyaxon[s3]'\" and request the connection by adding \"connections: ['model-registry-s3']\" to the run section. The secret\/config will be available in the container, and the user can also use boto3 automatically. The documentation for GCS, S3, and Azure is also provided.",
        "Answer_gpt_summary":"solut solut provid updat section us new implement import function pass servic argument fulli resolv sf object user need instal requir run pip instal request connect ad connect model registri run section secret config avail contain user us boto automat document gc azur provid"
    },
    {
        "Question_id":null,
        "Question_title":"Meetup on machine translation for low-resource languages this Friday!",
        "Question_body":"The last machine translation meetup featured a PM for the Google Cloud Translation API in person.The next machine translation meetup is all about low-resource machine translation and it'll be online.",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1666181580000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":19.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Meetup-on-machine-translation-for-low-resource-languages-this\/td-p\/479955\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-19T12:13:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"The last machine translation meetup featured a PM for the Google Cloud Translation API in person.\n\nThe next machine translation meetup is all about low-resource machine translation\u00a0and it'll be online.\n\n\u00a0\nmachinetranslate.org\/meetup\n\u00a0\nThe 25-minute panel features guests from Meta AI,\u00a0NeuralSpace, LoResMT, and Masakhane!\n\nRegister to join us\u00a0this Friday at 8am PST"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"meetup machin translat low resourc languag fridai machin translat meetup featur googl cloud translat api person machin translat meetup low resourc machin translat onlin",
        "Question_preprocessed_content":"meetup machin translat languag fridai machin translat meetup featur googl cloud translat api machin translat meetup machin translat onlin",
        "Question_gpt_summary_original":"The user is facing challenges related to low-resource machine translation and is interested in attending an upcoming online meetup on the topic.",
        "Question_gpt_summary":"user face challeng relat low resourc machin translat interest attend upcom onlin meetup topic",
        "Answer_original_content":"machin translat meetup featur googl cloud translat api person machin translat meetup low resourc machin translationand onlin machinetransl org meetup minut panel featur guest meta neuralspac loresmt masakhan regist join usthi fridai pst",
        "Answer_preprocessed_content":"machin translat meetup featur googl cloud translat api person machin translat meetup machin translationand onlin panel featur guest meta neuralspac loresmt masakhan regist join usthi fridai pst",
        "Answer_gpt_summary_original":"summary: the next machine translation meetup is about low-resource machine translation and will be held online. the 25-minute panel will feature guests from meta ai, neuralspace, loresmt, and masakhane. the user can register to join on friday at 8am pst.",
        "Answer_gpt_summary":"summari machin translat meetup low resourc machin translat held onlin minut panel featur guest meta neuralspac loresmt masakhan user regist join fridai pst"
    },
    {
        "Question_id":null,
        "Question_title":"GCP Idle Model Charging",
        "Question_body":"Hello!\nI would like to deploy the ML Model into GCP.Most of the time the model will be sleeping. Sometimes I should use it through Endpoint for some seconds.\nI don't want to pay for full-time GPU instance and I need fast responses at the same time, without deployment from scratch everytime I need it.Is this possible in GCP ?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1667794200000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":113.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/GCP-Idle-Model-Charging\/td-p\/486604\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-08T15:02:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"According to the Pricing for AutoML models documentation:\n\nPricing for AutoML models\n\nFor Vertex AI AutoML models, you pay for three main activities:\n\nTraining the model\nDeploying the model to an endpoint\nUsing the model to make predictions\n\nVertex AI uses predefined machine configurations for Vertex AutoML models, and the hourly rate for these activities reflects the resource usage. ... You pay for each model deployed to an endpoint, even if no prediction is made. You must undeploy your model to stop incurring further charges. Models that are not deployed or have failed to deploy are not charged.\n\nYou pay only for compute hours used; if training fails for any reason other than a user-initiated cancellation, you are not billed for the time. You are charged for training time if you cancel the operation.\n\nCustom-trained models\nTraining\n\nThe tables Machine types and Accelerators provide the approximate price per hour of various training configurations. You can choose a custom configuration of selected machine types. To calculate pricing, sum the costs of the virtual machines you use.\n\nIf you use Compute Engine machine types and attach accelerators, the cost of the accelerators is separate. To calculate this cost, multiply the prices in the table of accelerators below by how many machine hours of each type of accelerator you use.\n\nFor further information about pricing, please refer to the Vertex AI pricing documentation, or you can connect with our sales team to get a custom quote."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"gcp idl model charg hello like deploi model gcp time model sleep us endpoint second want pai time gpu instanc need fast respons time deploy scratch everytim need possibl gcp",
        "Question_preprocessed_content":"gcp idl model charg hello like deploi model time model sleep us endpoint second want pai gpu instanc need fast respons time deploy scratch everytim need possibl gcp",
        "Question_gpt_summary_original":"The user wants to deploy an ML model in GCP but faces the challenge of not wanting to pay for a full-time GPU instance as the model will mostly be idle. They also require fast responses without having to redeploy the model from scratch every time it is needed. The user is seeking a solution to these challenges.",
        "Question_gpt_summary":"user want deploi model gcp face challeng want pai time gpu instanc model idl requir fast respons have redeploi model scratch time need user seek solut challeng",
        "Answer_original_content":"accord price automl model document price automl model automl model pai main activ train model deploi model endpoint model predict us predefin machin configur vertex automl model hourli rate activ reflect resourc usag pai model deploi endpoint predict undeploi model stop incur charg model deploi fail deploi charg pai comput hour train fail reason user initi cancel bill time charg train time cancel oper custom train model train tabl machin type acceler provid approxim price hour train configur choos custom configur select machin type calcul price sum cost virtual machin us us comput engin machin type attach acceler cost acceler separ calcul cost multipli price tabl acceler machin hour type acceler us inform price refer price document connect sale team custom quot",
        "Answer_preprocessed_content":"accord price automl model document price automl model automl model pai main activ train model deploi model endpoint model predict us predefin machin configur vertex automl model hourli rate activ reflect resourc usag pai model deploi endpoint predict undeploi model stop incur charg model deploi fail deploi charg pai comput hour train fail reason cancel bill time charg train time cancel oper model train tabl machin type acceler provid approxim price hour train configur choos custom configur select machin type calcul price sum cost virtual machin us us comput engin machin type attach acceler cost acceler separ calcul cost multipli price tabl acceler machin hour type acceler us inform price refer price document connect sale team custom quot",
        "Answer_gpt_summary_original":"possible solutions from the answer include using automl models in gcp, which charges for training, deploying, and using the model to make predictions based on resource usage. the hourly rate for these activities reflects the resource usage, and you pay for each model deployed to an endpoint, even if no prediction is made. to reduce costs, you can undeploy your model to stop incurring further charges. additionally, you can choose a custom configuration of selected machine types to calculate pricing and connect with the sales team to get a custom quote.",
        "Answer_gpt_summary":"possibl solut answer includ automl model gcp charg train deploi model predict base resourc usag hourli rate activ reflect resourc usag pai model deploi endpoint predict reduc cost undeploi model stop incur charg addition choos custom configur select machin type calcul price connect sale team custom quot"
    },
    {
        "Question_id":null,
        "Question_title":"Wandb.log inconsistent behavior with step parameter",
        "Question_body":"<p>Why does these code snippets produce different results?<\/p>\n<pre><code class=\"lang-auto\">for i in range(100):\n    wandb.log({\"train\/loss\": i}, step=i)\n    \nfor i in range(100):\n    wandb.log({\"val\/loss\": i**2}, step=i)\n<\/code><\/pre>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/ioofli05?workspace=user-dminn\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/ioofli05?workspace=user-dminn\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/626b9c1d1ceb6cb5d3bb90cf6ab8d2894a6b8b14.png\" class=\"thumbnail onebox-avatar\" width=\"128\" height=\"128\">\n\n<h3><a href=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/ioofli05?workspace=user-dminn\" target=\"_blank\" rel=\"noopener\">dminn<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<pre><code class=\"lang-auto\">for i in range(100):\n    wandb.log({\"train\/loss\": i}, step=i)\n    wandb.log({\"val\/loss\": i**2}, step=i)\n<\/code><\/pre>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/146hdnar?workspace=user-dminn\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/146hdnar?workspace=user-dminn\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/626b9c1d1ceb6cb5d3bb90cf6ab8d2894a6b8b14.png\" class=\"thumbnail onebox-avatar\" width=\"128\" height=\"128\">\n\n<h3><a href=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/146hdnar?workspace=user-dminn\" target=\"_blank\" rel=\"noopener\">dminn<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_creation_time":1658196834539,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":132.0,
        "Answer_body":"<p>As far as I understand the step variable, once the step is incremented, the value is stored immutably. So, if chronologically, you execute:<\/p>\n<pre><code class=\"lang-python\">\nwandb.log({'potato': 1}, step=0}\nwandb.log({'tomato': 1}, step=0}\nwandb.log({'potato': 2}, step=1}\nwandb.log({'tomato': 2}, step=1}\n\n<\/code><\/pre>\n<p>It\u2019ll be fine but instead if you execute:<\/p>\n<pre><code class=\"lang-python\">\nwandb.log({'potato': 1}, step=0}\nwandb.log({'potato': 2}, step=1}\nwandb.log({'tomato': 1}, step=0}\nwandb.log({'tomato': 2}, step=1}\n\n<\/code><\/pre>\n<p>the third command (tomato = 1, step 0) will not be executed since the logger has already moved past step 0.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-log-inconsistent-behavior-with-step-parameter\/2771",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-20T19:14:35.236Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/dminn\">@dminn<\/a>, thanks for flagging this. I\u2019ll check internally on what\u2019s causing this issue and get back to you.<\/p>",
                "Answer_score":2.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-21T13:55:33.624Z",
                "Answer_body":"<p>As far as I understand the step variable, once the step is incremented, the value is stored immutably. So, if chronologically, you execute:<\/p>\n<pre><code class=\"lang-python\">\nwandb.log({'potato': 1}, step=0}\nwandb.log({'tomato': 1}, step=0}\nwandb.log({'potato': 2}, step=1}\nwandb.log({'tomato': 2}, step=1}\n\n<\/code><\/pre>\n<p>It\u2019ll be fine but instead if you execute:<\/p>\n<pre><code class=\"lang-python\">\nwandb.log({'potato': 1}, step=0}\nwandb.log({'potato': 2}, step=1}\nwandb.log({'tomato': 1}, step=0}\nwandb.log({'tomato': 2}, step=1}\n\n<\/code><\/pre>\n<p>the third command (tomato = 1, step 0) will not be executed since the logger has already moved past step 0.<\/p>",
                "Answer_score":16.6,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-07-27T01:07:39.894Z",
                "Answer_body":"<p>Thanks that makes sense, I like to keep my training\/validation steps separate. Are there any solutions\/plans to accommodate  metrics with different steps aside from storing the different steps as a dictionary.<\/p>\n<p>e.g.<\/p>\n<pre><code class=\"lang-auto\">for i in range(100):\n    wandb.log({\"potato\": i, \"step\": i})\n    \nfor i in range(100):\n    wandb.log({\"tomato\": i**2, \"step\": i})\n<\/code><\/pre>",
                "Answer_score":16.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-01T20:45:49.418Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/dminn\">@dminn<\/a>, correctly pointed out by <a class=\"mention\" href=\"\/u\/geraltofrivia783\">@geraltofrivia783<\/a> , as soon you call <code>wandb.log()<\/code> with a different value for step than the previous one, W&amp;B will write all the collected keys and values to the history, and start collection over again. Therefore, <code>wandb.log<\/code> doesn\u2019t let you write to any history step that you\u2019d like, only the \u201ccurrent\u201d one and the \u201cnext\u201d one.<br>\nAs a potential workaround\/solution, you can <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log#customize-axes\">make use of <code>define_metric<\/code><\/a> which will meet your needs for this use-case.<\/p>\n<p>For example, you can update your script to<\/p>\n<pre><code class=\"lang-auto\"># define your custom x axis metric\nwandb.define_metric(\"custom_step\")\n\n# define which metrics will be plotted against it\nwandb.define_metric(\"potato\", step_metric=\"custom_step\")\nwandb.define_metric(\"tomato\", step_metric=\"custom_step\")\n\n...\n\nfor i in range(100):\n  log_dict = {\n      \"custom_step\": i,\n      \"potato\": i,\n  }\n  wandb.log(log_dict)\n\n...\n\nfor i in range(100):\n  log_dict = {\n      \"custom_step\": i,\n      \"tomato\": i**2,\n  }\n  wandb.log(log_dict)\n<\/code><\/pre>\n<p>Hope this helps.<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-05T05:25:05.941Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dminn\">@dminn<\/a> , I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-10T17:11:54.127Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dminn\">@dminn<\/a> , since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-09T17:12:21.438Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1658411733624,
        "Question_original_content":"log inconsist behavior step paramet code snippet produc differ result rang log train loss step rang log val loss step dminn develop tool machin learn rang log train loss step log val loss step dminn develop tool machin learn",
        "Question_preprocessed_content":"log inconsist behavior step paramet code snippet produc differ result dminn develop tool machin learn dminn develop tool machin learn",
        "Question_gpt_summary_original":"The user is experiencing inconsistent behavior with the step parameter in the wandb.log function. They have provided two code snippets that produce different results and are seeking an explanation for the discrepancy.",
        "Question_gpt_summary":"user experienc inconsist behavior step paramet log function provid code snippet produc differ result seek explan discrep",
        "Answer_original_content":"far understand step variabl step increment valu store immut chronolog execut log potato step log tomato step log potato step log tomato step itll fine instead execut log potato step log potato step log tomato step log tomato step command tomato step execut logger move past step",
        "Answer_preprocessed_content":"far understand step variabl step increment valu store immut chronolog execut itll fine instead execut command execut logger move past step",
        "Answer_gpt_summary_original":"the solution to the inconsistent behavior when using the step parameter with the .log() function in the w&b developer tools for machine learning is to ensure that the step parameter is incremented chronologically and not repeated or skipped. the value of the step parameter is stored immutably once incremented.",
        "Answer_gpt_summary":"solut inconsist behavior step paramet log function develop tool machin learn ensur step paramet increment chronolog repeat skip valu step paramet store immut increment"
    },
    {
        "Question_id":70080915.0,
        "Question_title":"kedro context and catalog missing from ipython session",
        "Question_body":"<p>I launched ipython session and trying to load a dataset.<br \/>\nI am running<br \/>\ndf = catalog.load(&quot;test_dataset&quot;)<br \/>\nFacing the below error<br \/>\n<code>NameError: name 'catalog' is not defined<\/code><\/p>\n<p>I also tried %reload_kedro but got the below error<\/p>\n<p><code>UsageError: Line magic function `%reload_kedro` not found.<\/code><\/p>\n<p>Even not able to load context either.\nI am running the kedro environment from a Docker container.\nI am not sure where I am going wrong.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637670434953,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":617.0,
        "Answer_body":"<p>new in 0.17.5 there is a fallback option, please run the following commands in your Jupyter\/IPython session:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>%load_ext kedro.extras.extensions.ipython\n%reload_kedro &lt;path_to_project_root&gt;\n<\/code><\/pre>\n<p>This should help you get up and running.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70080915",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1637671420496,
        "Question_original_content":"context catalog miss ipython session launch ipython session try load dataset run catalog load test dataset face error nameerror catalog defin tri reload got error usageerror line magic function reload abl load context run environ docker contain sure go wrong",
        "Question_preprocessed_content":"context catalog miss ipython session launch ipython session try load dataset run face error tri got error abl load context run environ docker contain sure go wrong",
        "Question_gpt_summary_original":"The user is encountering challenges with loading a dataset in an ipython session. They are receiving a NameError indicating that 'catalog' is not defined and have also tried using %reload_kedro, which resulted in a UsageError. Additionally, they are unable to load the context and are running the kedro environment from a Docker container. The user is unsure of what they are doing wrong.",
        "Question_gpt_summary":"user encount challeng load dataset ipython session receiv nameerror indic catalog defin tri reload result usageerror addition unabl load context run environ docker contain user unsur wrong",
        "Answer_original_content":"new fallback option run follow command jupyt ipython session load ext extra extens ipython reload help run",
        "Answer_preprocessed_content":"new fallback option run follow command session help run",
        "Answer_gpt_summary_original":"solution: the user can try running the following commands in their jupyter\/ipython session: %load_ext .extras.extensions.ipython %reload_ <path_to_project_root>. this should help them get up and running.",
        "Answer_gpt_summary":"solut user try run follow command jupyt ipython session load ext extra extens ipython reload help run"
    },
    {
        "Question_id":60157184.0,
        "Question_title":"Tensorflow Parameter Servers on SageMaker",
        "Question_body":"<p>I am trying to understand how parameters servers (PS's) work for distributed training in Tensorflow on Amazon SageMaker. <\/p>\n\n<p>To make things more concrete, I am able to run the example from AWS using PS's: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/tf-distributed-training.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/tf-distributed-training.ipynb<\/a><\/p>\n\n<p>Here is the code block that initializes the estimator for Tensorflow:<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ngit_config = {'repo': 'https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode', 'branch': 'master'}\n\nps_instance_type = 'ml.p3.2xlarge'\nps_instance_count = 2\n\nmodel_dir = \"\/opt\/ml\/model\"\n\ndistributions = {'parameter_server': {\n                    'enabled': True}\n                }\nhyperparameters = {'epochs': 60, 'batch-size' : 256}\n\nestimator_ps = TensorFlow(\n                       git_config=git_config,\n                       source_dir='tf-distribution-options\/code',\n                       entry_point='train_ps.py', \n                       base_job_name='ps-cifar10-tf',\n                       role=role,\n                       framework_version='1.13',\n                       py_version='py3',\n                       hyperparameters=hyperparameters,\n                       train_instance_count=ps_instance_count, \n                       train_instance_type=ps_instance_type,\n                       model_dir=model_dir,\n                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'dist'}],\n                       distributions=distributions)\n<\/code><\/pre>\n\n<p>Going through the documentation for Tensorflow, it seems that a device scope can be used for assigning a variable to a particular worker. However, I never see this done when running training jobs on SageMaker. In the example from AWS, the model is defined by:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/code\/model_def.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/code\/model_def.py<\/a><\/p>\n\n<p>Here is a snippet:<\/p>\n\n<pre><code>def get_model(learning_rate, weight_decay, optimizer, momentum, size, mpi=False, hvd=False):\n\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), padding='same', input_shape=(HEIGHT, WIDTH, DEPTH)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Conv2D(32, (3, 3)))\n\n    ...\n\n    model.add(Flatten())\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(NUM_CLASSES))\n    model.add(Activation('softmax'))\n\n    if mpi:\n        size = hvd.size()\n\n    if optimizer.lower() == 'sgd':\n        ...\n\n    if mpi:\n        opt = hvd.DistributedOptimizer(opt)\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=opt,\n                  metrics=['accuracy'])\n\n    return model\n<\/code><\/pre>\n\n<p>Here, there are no references to distribution strategies (except with MPI, but that flag is set to False for PS's). Somehow, Tensorflow or the SageMaker container is able to decide where the variables for each layer should be stored. However, I'm not seeing anything in the container code that does anything with the distribution strategy.<\/p>\n\n<p>I am able to run this code and train the model using 1 and 2 instances. When i do so, I see a decrease of almost 50% in the runtime, suggesting that a distributed training is occurring.<\/p>\n\n<p>My questions are:<\/p>\n\n<ol>\n<li>How does Tensorflow decide the distribution of variables on the PS's? In the example code, there is no explicit reference to devices. Somehow the distribution is done automatically.<\/li>\n<li>Is it possible to see which parameters have been assigned to each PS? Or to see what the communication between PS's looks like? If so, how?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1581363222683,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":307.0,
        "Answer_body":"<blockquote>\n<p>My questions are:<\/p>\n<p>How does Tensorflow decide the distribution of variables on the PS's?\nIn the example code, there is no explicit reference to devices.\nSomehow the distribution is done automatically.<\/p>\n<\/blockquote>\n<p>The TensorFlow image provided by SageMaker has the code to setup TF_CONFIG and launching parameter server for multi work training. See the code [here][1] The setup is for each node in the cluster there is a PS and a worker thread configured.<\/p>\n<p>It's not using any DistributionStrategy so the default strategy is used. See [here][2].<\/p>\n<p>If you would like to use a different DistributionStrategy or different TF_CONFIG you will need to disable <code>parameter_server<\/code> option when launching the SageMaker training job and set everything up in your training script.<\/p>\n<blockquote>\n<p>Is it possible to see which parameters have been assigned to each PS?\nOr to see what the communication between PS's looks like? If so, how?<\/p>\n<\/blockquote>\n<p>You should be able to get some information from the output log which can be found in CloudWatch. The link is available on the Training Job console page.\n[1]: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-training-toolkit\/blob\/master\/src\/sagemaker_tensorflow_container\/training.py#L37\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-training-toolkit\/blob\/master\/src\/sagemaker_tensorflow_container\/training.py#L37<\/a>\n[2]: <a href=\"https:\/\/www.tensorflow.org\/guide\/distributed_training#default_strategy\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/guide\/distributed_training#default_strategy<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60157184",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1600204151310,
        "Question_original_content":"tensorflow paramet server try understand paramet server work distribut train tensorflow thing concret abl run exampl aw http github com aw sampl amazon script mode blob master distribut option distribut train ipynb code block initi estim tensorflow tensorflow import tensorflow git config repo http github com aw sampl amazon script mode branch master instanc type xlarg instanc count model dir opt model distribut paramet server enabl true hyperparamet epoch batch size estim tensorflow git config git config sourc dir distribut option code entri point train base job cifar role role framework version version hyperparamet hyperparamet train instanc count instanc count train instanc type instanc type model dir model dir tag kei project valu cifar kei tensorboard valu dist distribut distribut go document tensorflow devic scope assign variabl particular worker run train job exampl aw model defin http github com aw sampl amazon script mode blob master distribut option code model def snippet def model learn rate weight decai optim momentum size mpi fals hvd fals model sequenti model add convd pad input shape height width depth model add batchnorm model add activ relu model add convd model add flatten model add dens model add activ relu model add dropout model add dens num class model add activ softmax mpi size hvd size optim lower sgd mpi opt hvd distributedoptim opt model compil loss categor crossentropi optim opt metric accuraci return model refer distribut strategi mpi flag set fals tensorflow contain abl decid variabl layer store see contain code distribut strategi abl run code train model instanc decreas runtim suggest distribut train occur question tensorflow decid distribut variabl exampl code explicit refer devic distribut automat possibl paramet assign commun look like",
        "Question_preprocessed_content":"tensorflow paramet server try understand paramet server work distribut train tensorflow thing concret abl run exampl aw code block initi estim tensorflow go document tensorflow devic scope assign variabl particular worker run train job exampl aw model defin snippet refer distribut strategi tensorflow contain abl decid variabl layer store see contain code distribut strategi abl run code train model instanc decreas runtim suggest distribut train occur question tensorflow decid distribut variabl exampl code explicit refer devic distribut automat possibl paramet assign commun look like",
        "Question_gpt_summary_original":"The user is trying to understand how parameter servers work for distributed training in Tensorflow on Amazon SageMaker. They are able to run an example from AWS using parameter servers, but they are unsure how Tensorflow decides the distribution of variables on the parameter servers. The user is also wondering if it is possible to see which parameters have been assigned to each parameter server and what the communication between parameter servers looks like.",
        "Question_gpt_summary":"user try understand paramet server work distribut train tensorflow abl run exampl aw paramet server unsur tensorflow decid distribut variabl paramet server user wonder possibl paramet assign paramet server commun paramet server look like",
        "Answer_original_content":"question tensorflow decid distribut variabl exampl code explicit refer devic distribut automat tensorflow imag provid code setup config launch paramet server multi work train code setup node cluster worker thread configur distributionstrategi default strategi like us differ distributionstrategi differ config need disabl paramet server option launch train job set train script possibl paramet assign commun look like abl inform output log cloudwatch link avail train job consol page http github com aw tensorflow train toolkit blob master src tensorflow contain train http tensorflow org guid distribut train default strategi",
        "Answer_preprocessed_content":"question tensorflow decid distribut variabl exampl code explicit refer devic distribut automat tensorflow imag provid code setup launch paramet server multi work train code setup node cluster worker thread configur distributionstrategi default strategi like us differ distributionstrategi differ need disabl option launch train job set train script possibl paramet assign commun look like abl inform output log cloudwatch link avail train job consol page",
        "Answer_gpt_summary_original":"the answer suggests that tensorflow automatically distributes variables on parameter servers and provides an example code for setting up tf_config and launching parameter servers for multi-work training. it also mentions that if a different distribution strategy or tf_config is required, the parameter_server option needs to be disabled when launching the training job and everything should be set up in the training script. the answer also suggests that information about the parameters assigned to each ps and communication between ps's can be obtained from the output log available in cloudwatch.",
        "Answer_gpt_summary":"answer suggest tensorflow automat distribut variabl paramet server provid exampl code set config launch paramet server multi work train mention differ distribut strategi config requir paramet server option need disabl launch train job set train script answer suggest inform paramet assign commun obtain output log avail cloudwatch"
    },
    {
        "Question_id":53636589.0,
        "Question_title":"AWS SageMaker: CapacityError: Unable to provision requested ML compute capacity.",
        "Question_body":"<p>We were running two TrainingJob instances of type (1) <em>ml.p3.8xlarge<\/em> and (2) <em>ml.p3.2xlarge<\/em> . <\/p>\n\n<p>Each training job is running a custom algorithm with Tensorflow plus a Keras backend.<\/p>\n\n<p>The instance (1) is running ok, while the instance (2) after a reported time of training of 1 hour, with any logging in CloudWatch (any text tow log), exits with this error:<\/p>\n\n<pre><code>Failure reason\nCapacityError: Unable to provision requested ML compute capacity. Please retry using a different ML instance type.\n<\/code><\/pre>\n\n<p>I'm not sure what this message mean.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1544027022437,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":3875.0,
        "Answer_body":"<p>This message mean SageMaker tried to launch the instance but EC2 was not having enough capacity of this instance hence after waiting for some time(in this case 1 hour) SageMaker gave up and failed the training job.<\/p>\n\n<p>For more information about capacity issue from ec2, please visit: \n<a href=\"https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/troubleshooting-launch.html#troubleshooting-launch-capacity\" rel=\"noreferrer\">troubleshooting-launch-capacity<\/a><\/p>\n\n<p>To solve this, you can either try running jobs with different instance type as suggested in failure reason or wait a few minutes and then submit your request again as suggested by EC2.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1544571385672,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53636589",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1544569877892,
        "Question_original_content":"capacityerror unabl provis request comput capac run trainingjob instanc type xlarg xlarg train job run custom algorithm tensorflow plu kera backend instanc run instanc report time train hour log cloudwatch text tow log exit error failur reason capacityerror unabl provis request comput capac retri differ instanc type sure messag mean",
        "Question_preprocessed_content":"capacityerror unabl provis request comput capac run trainingjob instanc type train job run custom algorithm tensorflow plu kera backend instanc run instanc report time train hour log cloudwatch exit error sure messag mean",
        "Question_gpt_summary_original":"The user encountered a CapacityError while running two TrainingJob instances on AWS SageMaker, with one instance running fine and the other exiting with the error message \"Unable to provision requested ML compute capacity\". The user is unsure about the meaning of this message.",
        "Question_gpt_summary":"user encount capacityerror run trainingjob instanc instanc run fine exit error messag unabl provis request comput capac user unsur mean messag",
        "Answer_original_content":"messag mean tri launch instanc have capac instanc wait time case hour gave fail train job inform capac issu visit troubleshoot launch capac solv try run job differ instanc type suggest failur reason wait minut submit request suggest",
        "Answer_preprocessed_content":"messag mean tri launch instanc have capac instanc wait time gave fail train job inform capac issu visit solv try run job differ instanc type suggest failur reason wait minut submit request suggest",
        "Answer_gpt_summary_original":"possible solutions to the challenge of encountering a capacity error while launching a training job instance of type ml.p3.2xlarge are to either try running the job with a different instance type or wait for a few minutes and then resubmit the request. for more information on capacity issues from ec2, one can visit the troubleshooting-launch-capacity page.",
        "Answer_gpt_summary":"possibl solut challeng encount capac error launch train job instanc type xlarg try run job differ instanc type wait minut resubmit request inform capac issu visit troubleshoot launch capac page"
    },
    {
        "Question_id":65830524.0,
        "Question_title":"SQLAlchemy Oracle - InvalidRequestError: could not retrieve isolation level",
        "Question_body":"<p>I am having problems accessing tables in an Oracle database over a SQLAlchemy connection. Specifically, I am using Kedro <code>catalog.load('table_name')<\/code> and getting the error message <code>Table table_name not found<\/code>. So I decided to test my connection using the method listed in this answer: <a href=\"https:\/\/stackoverflow.com\/questions\/41887344\/how-to-verify-sqlalchemy-engine-object\">How to verify SqlAlchemy engine object<\/a>.<\/p>\n<pre><code>from sqlalchemy import create_engine\nengine = create_engine('oracle+cx_oracle:\/\/USER:PASSWORD@HOST:PORT\/?service_name=SERVICE_NAME')\nengine.connect()\n<\/code><\/pre>\n<p>Error: <code>InvalidRequestError: could not retrieve isolation level<\/code><\/p>\n<p>I have tried explicitly adding an isolation level as explained in the <a href=\"https:\/\/docs.sqlalchemy.org\/en\/14\/core\/connections.html#setting-transaction-isolation-levels-including-dbapi-autocommit\" rel=\"nofollow noreferrer\">documentation<\/a> like this:<\/p>\n<pre><code>engine = create_engine('oracle+cx_oracle:\/\/USER:PASSWORD@HOST:PORT\/?service_name=SERVICE_NAME', execution_options={'isolation_level': 'AUTOCOMMIT'})\n<\/code><\/pre>\n<p>and this:<\/p>\n<pre><code>engine.connect().execution_options(isolation_level='AUTOCOMMIT')\n<\/code><\/pre>\n<p>and this:<\/p>\n<pre><code>connection = engine.connect()\nconnection = connection.execution_options(\n    isolation_level=&quot;AUTOCOMMIT&quot;\n)\n<\/code><\/pre>\n<p>but I get the same error in all cases.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1611242774307,
        "Question_favorite_count":null,
        "Question_last_edit_time":1611265283776,
        "Question_score":0.0,
        "Question_view_count":196.0,
        "Answer_body":"<p>Upgrading from SqlAlchemy 1.3.21 to 1.3.22 solved the problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65830524",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1611332594540,
        "Question_original_content":"sqlalchemi oracl invalidrequesterror retriev isol level have problem access tabl oracl databas sqlalchemi connect specif catalog load tabl get error messag tabl tabl decid test connect method list answer verifi sqlalchemi engin object sqlalchemi import creat engin engin creat engin oracl oracl user password host port servic servic engin connect error invalidrequesterror retriev isol level tri explicitli ad isol level explain document like engin creat engin oracl oracl user password host port servic servic execut option isol level autocommit engin connect execut option isol level autocommit connect engin connect connect connect execut option isol level autocommit error case",
        "Question_preprocessed_content":"sqlalchemi oracl invalidrequesterror retriev isol level have problem access tabl oracl databas sqlalchemi connect specif get error messag decid test connect method list answer verifi sqlalchemi engin object error tri explicitli ad isol level explain document like error case",
        "Question_gpt_summary_original":"The user is encountering an error while trying to access tables in an Oracle database over a SQLAlchemy connection. They have tried various methods to add an isolation level to the connection, as suggested in the documentation, but are still receiving the same error message.",
        "Question_gpt_summary":"user encount error try access tabl oracl databas sqlalchemi connect tri method add isol level connect suggest document receiv error messag",
        "Answer_original_content":"upgrad sqlalchemi solv problem",
        "Answer_preprocessed_content":"upgrad sqlalchemi solv problem",
        "Answer_gpt_summary_original":"upgrading from sqlalchemy version 1.3.21 to version 1.3.22 could potentially solve the issue of challenges accessing tables in an oracle database over a sqlalchemy connection and being unable to set the isolation level.",
        "Answer_gpt_summary":"upgrad sqlalchemi version version potenti solv issu challeng access tabl oracl databas sqlalchemi connect unabl set isol level"
    },
    {
        "Question_id":null,
        "Question_title":"How to stop logging locally but only save to wandb's servers and have wandb work using soft links?",
        "Question_body":"<p>I am having a weird issue where I change the location of all my code &amp; data to a different location with more disk space, then I soft link my projects &amp; data to those locations with more space. I assume there must be some file handle issue because wandb\u2019s logger is throwing me issues. So my questions:<\/p>\n<ol>\n<li>how do I have wandb only log  online and not locally? (e.g. stop trying to log anything to <code>.\/wandb<\/code>[or any secret place it might be logging to]  since it\u2019s creating issues). Note my code was running fine after I  stopped logging to wandb so I assume that was the issue. note that the <code>dir=None<\/code> is the default to wandb\u2019s param.<\/li>\n<li>how do I resolve this issue entirely so that it works seemlessly with all my projects softlinked somewhere else?<\/li>\n<\/ol>\n<hr>\n<h1>\n<a name=\"more-details-on-the-error-1\" class=\"anchor\" href=\"#more-details-on-the-error-1\"><\/a>More details on the error<\/h1>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/logging\/__init__.py\", line 1087, in emit\n    self.flush()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/logging\/__init__.py\", line 1067, in flush\n    self.stream.flush()\nOSError: [Errno 116] Stale file handle\nCall stack:\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/threading.py\", line 930, in _bootstrap\n    self._bootstrap_inner()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/threading.py\", line 973, in _bootstrap_inner\n    self.run()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/vendor\/watchdog\/observers\/api.py\", line 199, in run\n    self.dispatch_events(self.event_queue, self.timeout)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/vendor\/watchdog\/observers\/api.py\", line 368, in dispatch_events\n    handler.dispatch(event)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/vendor\/watchdog\/events.py\", line 454, in dispatch\n    _method_map[event_type](event)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/filesync\/dir_watcher.py\", line 275, in _on_file_created\n    logger.info(\"file\/dir created: %s\", event.src_path)\nMessage: 'file\/dir created: %s'\nArguments: ('\/shared\/rsaas\/miranda9\/diversity-for-predictive-success-of-meta-learning\/wandb\/run-20221023_170722-1tfzh49r\/files\/output.log',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/logging\/__init__.py\", line 1087, in emit\n    self.flush()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/logging\/__init__.py\", line 1067, in flush\n    self.stream.flush()\nOSError: [Errno 116] Stale file handle\nCall stack:\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/threading.py\", line 930, in _bootstrap\n    self._bootstrap_inner()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/threading.py\", line 973, in _bootstrap_inner\n    self.run()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 50, in run\n    self._run()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 101, in _run\n    self._process(record)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/internal.py\", line 263, in _process\n    self._hm.handle(record)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/handler.py\", line 130, in handle\n    handler(record)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/handler.py\", line 138, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: stop_status'\nArguments: ()\nN\/A% (0 of 100000) |      | Elapsed Time: 0:00:00 | ETA:  --:--:-- |   0.0 s\/it\n\nTraceback (most recent call last):\n  File \"\/home\/miranda9\/diversity-for-predictive-success-of-meta-learning\/div_src\/diversity_src\/experiment_mains\/main_dist_maml_l2l.py\", line 1814, in &lt;module&gt;\n    main()\n  File \"\/home\/miranda9\/diversity-for-predictive-success-of-meta-learning\/div_src\/diversity_src\/experiment_mains\/main_dist_maml_l2l.py\", line 1747, in main\n    train(args=args)\n  File \"\/home\/miranda9\/diversity-for-predictive-success-of-meta-learning\/div_src\/diversity_src\/experiment_mains\/main_dist_maml_l2l.py\", line 1794, in train\n    meta_train_iterations_ala_l2l(args, args.agent, args.opt, args.scheduler)\n  File \"\/home\/miranda9\/ultimate-utils\/ultimate-utils-proj-src\/uutils\/torch_uu\/training\/meta_training.py\", line 167, in meta_train_iterations_ala_l2l\n    log_zeroth_step(args, meta_learner)\n  File \"\/home\/miranda9\/ultimate-utils\/ultimate-utils-proj-src\/uutils\/logging_uu\/wandb_logging\/meta_learning.py\", line 92, in log_zeroth_step\n    log_train_val_stats(args, args.it, step_name, train_loss, train_acc, training=True)\n  File \"\/home\/miranda9\/ultimate-utils\/ultimate-utils-proj-src\/uutils\/logging_uu\/wandb_logging\/supervised_learning.py\", line 55, in log_train_val_stats\n    _log_train_val_stats(args=args,\n  File \"\/home\/miranda9\/ultimate-utils\/ultimate-utils-proj-src\/uutils\/logging_uu\/wandb_logging\/supervised_learning.py\", line 116, in _log_train_val_stats\n    args.logger.log('\\n')\n  File \"\/home\/miranda9\/ultimate-utils\/ultimate-utils-proj-src\/uutils\/logger.py\", line 89, in log\n    print(msg, flush=flush)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/lib\/redirect.py\", line 640, in write\n    self._old_write(data)\nOSError: [Errno 116] Stale file handle\nwandb: Waiting for W&amp;B process to finish... (failed 1). Press Control-C to abort syncing.\nwandb: Synced vit_mi Adam_rfs_cifarfs Adam_cosine_scheduler_rfs_cifarfs 0.001: args.jobid=101161: https:\/\/wandb.ai\/brando\/entire-diversity-spectrum\/runs\/1tfzh49r\nwandb: Synced 6 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nwandb: Find logs at: .\/wandb\/run-20221023_170722-1tfzh49r\/logs\n--- Logging error ---\nTraceback (most recent call last):\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/router_sock.py\", line 27, in _read_message\n    resp = self._sock_client.read_server_response(timeout=1)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 283, in read_server_response\n    data = self._read_packet_bytes(timeout=timeout)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 269, in _read_packet_bytes\n    raise SockClientClosedError()\nwandb.sdk.lib.sock_client.SockClientClosedError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/router.py\", line 70, in message_loop\n    msg = self._read_message()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/router_sock.py\", line 29, in _read_message\n    raise MessageRouterClosedError\nwandb.sdk.interface.router.MessageRouterClosedError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/logging\/__init__.py\", line 1087, in emit\n    self.flush()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/logging\/__init__.py\", line 1067, in flush\n    self.stream.flush()\nOSError: [Errno 116] Stale file handle\nCall stack:\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/threading.py\", line 930, in _bootstrap\n    self._bootstrap_inner()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/threading.py\", line 973, in _bootstrap_inner\n    self.run()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/threading.py\", line 910, in run\n    self._target(*self._args, **self._kwargs)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/router.py\", line 77, in message_loop\n    logger.warning(\"message_loop has been closed\")\nMessage: 'message_loop has been closed'\nArguments: ()\n\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/tempfile.py:817: ResourceWarning: Implicitly cleaning up &lt;TemporaryDirectory '\/srv\/condor\/execute\/dir_27749\/tmpmvf78q6owandb'&gt;\n  _warnings.warn(warn_message, ResourceWarning)\n\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/tempfile.py:817: ResourceWarning: Implicitly cleaning up &lt;TemporaryDirectory '\/srv\/condor\/execute\/dir_27749\/tmpt5etqpw_wandb-artifacts'&gt;\n  _warnings.warn(warn_message, ResourceWarning)\n\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/tempfile.py:817: ResourceWarning: Implicitly cleaning up &lt;TemporaryDirectory '\/srv\/condor\/execute\/dir_27749\/tmp55lzwviywandb-media'&gt;\n  _warnings.warn(warn_message, ResourceWarning)\n\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/tempfile.py:817: ResourceWarning: Implicitly cleaning up &lt;TemporaryDirectory '\/srv\/condor\/execute\/dir_27749\/tmprmk7lnx4wandb-media'&gt;\n  _warnings.warn(warn_message, ResourceWarning)\n<\/code><\/pre>\n<p>cross: <a href=\"https:\/\/stackoverflow.com\/questions\/74175401\/how-to-stop-logging-locally-but-only-save-to-wandbs-servers-and-have-wandb-work\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">python - How to stop logging locally but only save to wandb's servers and have wandb work using soft links? - Stack Overflow<\/a><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1666566211012,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":178.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-stop-logging-locally-but-only-save-to-wandbs-servers-and-have-wandb-work-using-soft-links\/3305",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-24T16:50:03.404Z",
                "Answer_body":"<p>to document what I\u2019ve tried so far.<\/p>\n<ul>\n<li>I tried removing the old .wandb logs that are saved locally automatically and symlinked everything else (e.g. the data folder and where the models are checkpointing). But it still failed. I am surprised because I don\u2019t understand what could be so large in wandb that it make me have disk quota error. The main solution is that the wandb team should make wandb work regardless where my data, project, etc folders are or if I use symlinks. Any advice <a class=\"mention\" href=\"\/u\/_scott\">@_scott<\/a> ?<\/li>\n<\/ul>\n<hr>\n<p>gitissue: <a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/4409\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">[CLI]: wandb saving to local does not work when soft links to the current project are used \u00b7 Issue #4409 \u00b7 wandb\/wandb \u00b7 GitHub<\/a><\/p>",
                "Answer_score":21.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-25T03:56:22.967Z",
                "Answer_body":"<p>Hey Brando,<\/p>\n<p>I have responded to your GitHub post, lets keep the conversation there for greater visibility for other folks  in the future.<\/p>",
                "Answer_score":10.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-25T04:42:19.258Z",
                "Answer_body":"<p><a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/4409\" rel=\"noopener nofollow ugc\">[CLI]: wandb saving to local does not work when soft links to the current project are used \u00b7 Issue #4409 \u00b7 wandb\/wandb \u00b7 GitHub<\/a><\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-24T04:42:28.497Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"stop log local save server work soft link have weird issu chang locat code data differ locat disk space soft link project data locat space assum file handl issu logger throw issu question log onlin local stop try log secret place log creat issu note code run fine stop log assum issu note dir default param resolv issu entir work seemlessli project softlink detail error traceback recent file home miranda miniconda env metalearn gpu lib python log init line emit self flush file home miranda miniconda env metalearn gpu lib python log init line flush self stream flush oserror errno stale file handl stack file home miranda miniconda env metalearn gpu lib python thread line bootstrap self bootstrap inner file home miranda miniconda env metalearn gpu lib python thread line bootstrap inner self run file home miranda miniconda env metalearn gpu lib python site packag vendor watchdog observ api line run self dispatch event self event queue self timeout file home miranda miniconda env metalearn gpu lib python site packag vendor watchdog observ api line dispatch event handler dispatch event file home miranda miniconda env metalearn gpu lib python site packag vendor watchdog event line dispatch method map event type event file home miranda miniconda env metalearn gpu lib python site packag filesync dir watcher line file creat logger info file dir creat event src path messag file dir creat argument share rsaa miranda divers predict success meta learn run tfzhr file output log log error traceback recent file home miranda miniconda env metalearn gpu lib python log init line emit self flush file home miranda miniconda env metalearn gpu lib python log init line flush self stream flush oserror errno stale file handl stack file home miranda miniconda env metalearn gpu lib python thread line bootstrap self bootstrap inner file home miranda miniconda env metalearn gpu lib python thread line bootstrap inner self run file home miranda miniconda env metalearn gpu lib python site packag sdk intern intern util line run self run file home miranda miniconda env metalearn gpu lib python site packag sdk intern intern util line run self process record file home miranda miniconda env metalearn gpu lib python site packag sdk intern intern line process self handl record file home miranda miniconda env metalearn gpu lib python site packag sdk intern handler line handl handler record file home miranda miniconda env metalearn gpu lib python site packag sdk intern handler line handl request logger debug handl request request type messag handl request stop statu argument elaps time eta traceback recent file home miranda divers predict success meta learn div src divers src experi main main dist maml line main file home miranda divers predict success meta learn div src divers src experi main main dist maml line main train arg arg file home miranda divers predict success meta learn div src divers src experi main main dist maml line train meta train iter ala arg arg agent arg opt arg schedul file home miranda ultim util ultim util proj src uutil torch train meta train line meta train iter ala log zeroth step arg meta learner file home miranda ultim util ultim util proj src uutil log log meta learn line log zeroth step log train val stat arg arg step train loss train acc train true file home miranda ultim util ultim util proj src uutil log log supervis learn line log train val stat log train val stat arg arg file home miranda ultim util ultim util proj src uutil log log supervis learn line log train val stat arg logger log file home miranda ultim util ultim util proj src uutil logger line log print msg flush flush file home miranda miniconda env metalearn gpu lib python site packag sdk lib redirect line write self old write data oserror errno stale file handl wait process finish fail press control abort sync sync vit adam rf cifarf adam cosin schedul rf cifarf arg jobid http brando entir divers spectrum run tfzhr sync file media file artifact file file log run tfzhr log log error traceback recent file home miranda miniconda env metalearn gpu lib python site packag sdk interfac router sock line read messag resp self sock client read server respons timeout file home miranda miniconda env metalearn gpu lib python site packag sdk lib sock client line read server respons data self read packet byte timeout timeout file home miranda miniconda env metalearn gpu lib python site packag sdk lib sock client line read packet byte rais sockclientclosederror sdk lib sock client sockclientclosederror handl except except occur traceback recent file home miranda miniconda env metalearn gpu lib python site packag sdk interfac router line messag loop msg self read messag file home miranda miniconda env metalearn gpu lib python site packag sdk interfac router sock line read messag rais messagerouterclosederror sdk interfac router messagerouterclosederror handl except except occur traceback recent file home miranda miniconda env metalearn gpu lib python log init line emit self flush file home miranda miniconda env metalearn gpu lib python log init line flush self stream flush oserror errno stale file handl stack file home miranda miniconda env metalearn gpu lib python thread line bootstrap self bootstrap inner file home miranda miniconda env metalearn gpu lib python thread line bootstrap inner self run file home miranda miniconda env metalearn gpu lib python thread line run self target self arg self kwarg file home miranda miniconda env metalearn gpu lib python site packag sdk interfac router line messag loop logger warn messag loop close messag messag loop close argument home miranda miniconda env metalearn gpu lib python tempfil resourcewarn implicitli clean warn warn warn messag resourcewarn home miranda miniconda env metalearn gpu lib python tempfil resourcewarn implicitli clean warn warn warn messag resourcewarn home miranda miniconda env metalearn gpu lib python tempfil resourcewarn implicitli clean warn warn warn messag resourcewarn home miranda miniconda env metalearn gpu lib python tempfil resourcewarn implicitli clean warn warn warn messag resourcewarn cross python stop log local save server work soft link stack overflow",
        "Question_preprocessed_content":"stop log local save server work soft link have weird issu chang locat code data differ locat disk space soft link project data locat space assum file handl issu logger throw issu question log onlin local note code run fine stop log assum issu note default param resolv issu entir work seemlessli project softlink detail error cross python stop log local save server work soft link stack overflow",
        "Question_gpt_summary_original":"The user is encountering issues with wandb's logger when changing the location of their code and data to a different location with more disk space and soft linking their projects and data to those locations. They are seeking solutions to have wandb only log online and not locally and to resolve the issue entirely so that it works seamlessly with all their projects softlinked somewhere else. The error message provided indicates an OSError with a stale file handle and a traceback of the call stack.",
        "Question_gpt_summary":"user encount issu logger chang locat code data differ locat disk space soft link project data locat seek solut log onlin local resolv issu entir work seamlessli project softlink error messag provid indic oserror stale file handl traceback stack",
        "Answer_original_content":"document iv tri far tri remov old log save local automat symlink data folder model checkpoint fail surpris dont understand larg disk quota error main solut team work regardless data project folder us symlink advic scott gitissu cli save local work soft link current project issu github hei brando respond github post let convers greater visibl folk futur cli save local work soft link current project issu github topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"document iv tri far tri remov old log save local automat symlink fail surpris dont understand larg disk quota error main solut team work regardless data project folder us symlink advic gitissu save local work soft link current project issu github hei brando respond github post let convers greater visibl folk futur cli save local work soft link current project issu github topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the user has tried removing old logs and symlinked everything else, but still encountered a disk quota error. the main solution suggested is for the team to make the work regardless of where the user's data, project, etc folders are or if they use symlinks. there is also a suggestion to continue the conversation on github for greater visibility for other users.",
        "Answer_gpt_summary":"user tri remov old log symlink encount disk quota error main solut suggest team work regardless user data project folder us symlink suggest continu convers github greater visibl user"
    },
    {
        "Question_id":35304901.0,
        "Question_title":"Azure Machine Learning Reader + Table Storage",
        "Question_body":"<p>Duplicating: <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/6560c2d6-9836-41a1-8076-caf0d514222a\/azure-machine-learning-reader-table-storage?forum=MachineLearning\" rel=\"nofollow\">https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/6560c2d6-9836-41a1-8076-caf0d514222a\/azure-machine-learning-reader-table-storage?forum=MachineLearning<\/a><\/p>\n\n<p>I currently have a table storage setup which is constantly performing insertions. There is approximately 260 million rows in the table storage. <\/p>\n\n<p>I have set up two machine learning experiments to use a 'Reader' to read the data from the 'Azure Table'. <\/p>\n\n<p>Experiment 1 is set to read all the rows to train the model.<\/p>\n\n<p>Experiment 2 is set to read only the top 1,000 rows to train the model.<\/p>\n\n<p>Experiment 1 has been running for over 5 hours with no results.<\/p>\n\n<p>Experiment 2 has been running for over 1 hour with no results.<\/p>\n\n<p>It is stuck on the 'Reader' process.<\/p>\n\n<p>I do not understand why experiment 2 is taking so long. I know I have set this up right as I tested the 'Reader's with another table storage. Thanks in advance for any help\/suggestions.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1455064585877,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":104.0,
        "Answer_body":"<p>A lot of this will probably depend on the design of your tables. Table Storage is a key \/ value store (think of it as a dictionary). It has some capabilities for scanning within a partition and across partitions - but the latencies will differ greatly. Ideally if you want to query 1000 rows they should be localized within a partition. See <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/storage-table-design-guide\/\" rel=\"nofollow\">Table Design Guide<\/a> and <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/storage-performance-checklist\/\" rel=\"nofollow\">Perf and Scalability Checklist<\/a> for full details.  <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35304901",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1455137464832,
        "Question_original_content":"reader tabl storag duplic http social msdn microsoft com forum azur cafda azur machin learn reader tabl storag forum machinelearn current tabl storag setup constantli perform insert approxim million row tabl storag set machin learn experi us reader read data azur tabl experi set read row train model experi set read row train model experi run hour result experi run hour result stuck reader process understand experi take long know set right test reader tabl storag thank advanc help suggest",
        "Question_preprocessed_content":"reader tabl storag duplic current tabl storag setup constantli perform insert approxim million row tabl storag set machin learn experi us reader read data azur tabl experi set read row train model experi set read row train model experi run hour result experi run hour result stuck reader process understand experi take long know set right test reader tabl storag thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges with Azure Machine Learning Reader and Table Storage. They have set up two experiments to read data from Azure Table, with Experiment 1 set to read all rows and Experiment 2 set to read only the top 1,000 rows. However, both experiments have been running for several hours with no results, and the Reader process seems to be stuck. The user is unsure why Experiment 2 is taking so long, despite having tested the Readers with another table storage.",
        "Question_gpt_summary":"user face challeng reader tabl storag set experi read data azur tabl experi set read row experi set read row experi run hour result reader process stuck user unsur experi take long despit have test reader tabl storag",
        "Answer_original_content":"lot probabl depend design tabl tabl storag kei valu store think dictionari capabl scan partit partit latenc differ greatli ideal want queri row local partit tabl design guid perf scalabl checklist detail",
        "Answer_preprocessed_content":"lot probabl depend design tabl tabl storag kei valu store capabl scan partit partit latenc differ greatli ideal want queri row local partit tabl design guid perf scalabl checklist detail",
        "Answer_gpt_summary_original":"the answer suggests that the design of the azure table storage may be causing the issue with the 'reader' process. the solution may involve optimizing the table design to ensure that the queried rows are localized within a partition. the table design guide and perf and scalability checklist provide more details on how to optimize the table design.",
        "Answer_gpt_summary":"answer suggest design azur tabl storag caus issu reader process solut involv optim tabl design ensur queri row local partit tabl design guid perf scalabl checklist provid detail optim tabl design"
    },
    {
        "Question_id":62949488.0,
        "Question_title":"AMLS Experiment run stuck in status \"Running\"",
        "Question_body":"<p>I made an Azure Machine Learning Service Experiment run and logged neural network losses with Jupyter Notebook. Logging worked fine and NN training completed as it should. However, the experiment is stuck in the running status. Shutting down the compute resources does not shut down the Experiment run and I cannot cancel it from the Experiment panel. In addition, the run does not have any log-files.<\/p>\n<p>Has anyone had the same behavior? Run has now lasted for over 24 hours.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KzAoS.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KzAoS.jpg\" alt=\"AMLS Experiment run\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1594972260903,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1594989303640,
        "Question_score":2.0,
        "Question_view_count":887.0,
        "Answer_body":"<p>this totally happens from time to time. it is certainly frustrating especially because the &quot;Cancel&quot; button it grayed out. You can use either the CLI or Python SDK  to cancel the run.<\/p>\n<h2>SDK<\/h2>\n<h3>&gt;= 1.16.0<\/h3>\n<p>As of version <code>1.16.0<\/code> you no longer an <code>Experiment<\/code> object is no longer needed. Instead you can access using the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py#get-workspace--run-id-&amp;WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\"><code>Run<\/code><\/a> or <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace(class)?view=azure-ml-py#get-run-run-id-&amp;WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\"><code>Workspace<\/code><\/a> objects directly<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace, Experiment, Run, VERSION\nprint(&quot;SDK version:&quot;, VERSION)\n\nws = Workspace.from_config()\n\nrun = ws.get_run('YOUR_RUN_ID')\nrun = Run().get(ws, 'YOUR_RUN_ID') # also works\nrun.cancel()\n<\/code><\/pre>\n<h3>&lt; 1.16.0<\/h3>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace, Experiment, Run, VERSION\nprint(&quot;SDK version:&quot;, VERSION)\n\nws = Workspace.from_config()\nexp = Experiment(workspace = ws, name = 'YOUR_EXP_NAME')\n\nrun = Run(exp, run_id='YOUR STEP RUN ID')\n\nrun.cancel() # or run.fail()\n<\/code><\/pre>\n<h1>CLI<\/h1>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-azure-machine-learning-cli#install-the-extension\" rel=\"nofollow noreferrer\">More CLI details here<\/a><\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>az login\naz ml run cancel --run YOUR_RUN_ID\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1604912331752,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62949488",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1595004339510,
        "Question_original_content":"aml experi run stuck statu run servic experi run log neural network loss jupyt notebook log work fine train complet experi stuck run statu shut comput resourc shut experi run cancel experi panel addit run log file behavior run last hour",
        "Question_preprocessed_content":"aml experi run stuck statu run servic experi run log neural network loss jupyt notebook log work fine train complet experi stuck run statu shut comput resourc shut experi run cancel experi panel addit run behavior run last hour",
        "Question_gpt_summary_original":"The user encountered challenges with an Azure Machine Learning Service Experiment run that got stuck in the \"Running\" status even though the neural network losses were logged successfully and the training completed. The user was unable to cancel the run from the Experiment panel and shutting down the compute resources did not help. Additionally, the run did not have any log-files. The user sought help as the run had lasted for over 24 hours.",
        "Question_gpt_summary":"user encount challeng servic experi run got stuck run statu neural network loss log successfulli train complet user unabl cancel run experi panel shut comput resourc help addition run log file user sought help run last hour",
        "Answer_original_content":"total happen time time certainli frustrat especi cancel button grai us cli python sdk cancel run sdk version longer experi object longer need instead access run workspac object directli core import workspac experi run version print sdk version version workspac config run run run run run run work run cancel core import workspac experi run version print sdk version version workspac config exp experi workspac exp run run exp run step run run cancel run fail cli cli detail login run cancel run run",
        "Answer_preprocessed_content":"total happen time time certainli frustrat especi cancel button grai us cli python sdk cancel run sdk version longer object longer need instead access object directli cli cli detail",
        "Answer_gpt_summary_original":"possible solutions to the issue of an  service experiment run stuck in the \"running\" status and cannot be cancelled or shut down are to use either the cli or python sdk to cancel the run. for sdk version 1.16.0 and above, an experiment object is no longer needed, and instead, you can access the run or workspace objects directly. for versions below 1.16.0, you can use the experiment object to cancel the run. additionally, you can use the cli command \"az ml run cancel --run your_run_id\" to cancel the run.",
        "Answer_gpt_summary":"possibl solut issu servic experi run stuck run statu cancel shut us cli python sdk cancel run sdk version experi object longer need instead access run workspac object directli version us experi object cancel run addition us cli command run cancel run run cancel run"
    },
    {
        "Question_id":null,
        "Question_title":"Document AI Form Processor Parse Table Structure Incorrectly",
        "Question_body":"Hi All,We are currently using Document AI for form parsing some PDF document and half of times the default former processor either missing a col or messed up some col structure.Let's say the expected file headerSales | Dollar Volume | Average Price For example, I saw cases like1. Missing HeaderSales|Average Price2. Wrong structureSalesDollar|Volume|Average PriceThe content of first two cols are messed up as well. The cell could be missing value or incomplete value.Any recommendation to improve this? If no easy way, any guidance with examples to train or deploy one's own form processor? PS: the document has the same structure. ",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659385020000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":93.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Document-AI-Form-Processor-Parse-Table-Structure-Incorrectly\/td-p\/449252\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-04T10:57:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"You can improve the data results by using Document AI Parser with AI Platform Notebooks. Also you can use Vision AI to create your own Parser."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"document form processor pars tabl structur incorrectli current document form pars pdf document half time default processor miss col mess col structur let expect file headersal dollar volum averag price exampl saw case like miss headersal averag price wrong structuresalesdollar volum averag priceth content col mess cell miss valu incomplet valu recommend improv easi wai guidanc exampl train deploi form processor document structur",
        "Question_preprocessed_content":"document form processor pars tabl structur incorrectli current document form pars pdf document half time default processor miss col mess col expect file headersal dollar volum averag price exampl saw case like miss headersal averag price wrong structuresalesdollar volum averag priceth content col mess cell miss valu incomplet recommend improv easi wai guidanc exampl train deploi form processor document structur",
        "Question_gpt_summary_original":"The user is facing challenges with the default form processor of Document AI while parsing PDF documents. The processor is either missing a column or incorrectly structuring the columns, resulting in missing or incomplete values. The user is seeking recommendations to improve the parsing process or guidance on training and deploying their own form processor.",
        "Question_gpt_summary":"user face challeng default form processor document pars pdf document processor miss column incorrectli structur column result miss incomplet valu user seek recommend improv pars process guidanc train deploi form processor",
        "Answer_original_content":"improv data result document parser platform notebook us vision creat parser",
        "Answer_preprocessed_content":"improv data result document parser platform notebook us vision creat parser",
        "Answer_gpt_summary_original":"possible solutions to the challenge of document ai's form processor incorrectly parsing table structures are:\n\n1. using document ai parser with ai platform notebooks to improve the data results.\n2. creating your own parser using vision ai.",
        "Answer_gpt_summary":"possibl solut challeng document form processor incorrectli pars tabl structur document parser platform notebook improv data result creat parser vision"
    },
    {
        "Question_id":66887340.0,
        "Question_title":"How do you clear the persistent storage for a notebook instance on AWS SageMaker?",
        "Question_body":"<p>So I'm running into the following error on AWS SageMaker when trying to save:<\/p>\n<blockquote>\n<p>Unexpected error while saving file: untitled.ipynb [Errno 28] No space left on device<\/p>\n<\/blockquote>\n<p>If I remove my notebook, create a new identical one and run it, everything works fine. However, I'm suspecting the Jupyter checkpoint takes up too much space if I save the notebook while it's running and therefore I'm running out of space. Sadly, getting more storage is not an option for me, so I'm wondering if there's any command I can use to clear the storage before running my notebook?<\/p>\n<p>More specifically, clearing the persistent storage in the beginning and at the end of the training process.<\/p>\n<p>I have googled like a maniac but there is no suggestion aside from &quot;just increase the amount of storage bro&quot; and that's why I'm asking the question here.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617191466313,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":973.0,
        "Answer_body":"<p>If you don't want your data to be persistent across multiple notebook runs, just store them in <code>\/tmp<\/code> which is not persistent. You have at least 10GB. More details <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-create-ws.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66887340",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1617198319367,
        "Question_original_content":"clear persist storag notebook instanc run follow error try save unexpect error save file untitl ipynb errno space left devic remov notebook creat new ident run work fine suspect jupyt checkpoint take space save notebook run run space sadli get storag option wonder command us clear storag run notebook specif clear persist storag begin end train process googl like maniac suggest asid increas storag bro ask question thank advanc",
        "Question_preprocessed_content":"clear persist storag notebook instanc run follow error try save unexpect error save file space left devic remov notebook creat new ident run work fine suspect jupyt checkpoint take space save notebook run run space sadli get storag option wonder command us clear storag run notebook specif clear persist storag begin end train process googl like maniac suggest asid increas storag bro ask question thank advanc",
        "Question_gpt_summary_original":"The user is encountering an error on AWS SageMaker when trying to save their notebook due to insufficient storage space. They suspect that the Jupyter checkpoint is taking up too much space and are looking for a command to clear the persistent storage before and after the training process. The user has searched for solutions but has not found any aside from increasing the storage capacity.",
        "Question_gpt_summary":"user encount error try save notebook insuffici storag space suspect jupyt checkpoint take space look command clear persist storag train process user search solut asid increas storag capac",
        "Answer_original_content":"want data persist multipl notebook run store tmp persist detail",
        "Answer_preprocessed_content":"want data persist multipl notebook run store persist detail",
        "Answer_gpt_summary_original":"the solution to avoid running out of space when saving a notebook instance is to store the data in \/tmp which is not persistent. this will ensure that the data is not saved across multiple notebook runs. \/tmp has at least 10gb of storage available. more details can be found in the provided link.",
        "Answer_gpt_summary":"solut avoid run space save notebook instanc store data tmp persist ensur data save multipl notebook run tmp storag avail detail provid link"
    },
    {
        "Question_id":65102618.0,
        "Question_title":"struggling to install python package via amazon sagemaker",
        "Question_body":"<p>I am using Amazon Sagemaker and trying to install gaapi4py package via anaconda python3 notebook.<\/p>\n<p>So far I've tried the following commands:<\/p>\n<pre><code>%conda install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>conda install gaapi4py\n\nGot same error:\n\nCollecting package metadata (current_repodata.json): failed\n\nCondaHTTPError: HTTP 000 CONNECTION FAILED for url &lt;https:\/\/conda.anaconda.org\/conda-forge\/linux-64\/current_repodata.json&gt;\nElapsed: -\n\nAn HTTP error occurred when trying to retrieve this URL.\nHTTP errors are often intermittent, and a simple retry will get you on your way.\n'https:\/\/conda.anaconda.org\/conda-forge\/linux-64'\n\n\n\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>Alternatively I've tried the below but it failed as well:<\/p>\n<pre><code>pip install gaapi4py\n<\/code><\/pre>\n<p>Error text:<\/p>\n<pre><code>WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803c50&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c8035f8&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803550&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803400&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803358&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nERROR: Could not find a version that satisfies the requirement gaapi4py (from versions: none)\nERROR: No matching distribution found for gaapi4py\nWARNING: You are using pip version 20.0.2; however, version 20.3 is available.\nYou should consider upgrading via the '\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python -m pip install --upgrade pip' command.\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>What am I doing wrong? All previous packages worked well.<\/p>\n<p>UPD:<\/p>\n<p>Tried also as recommended in amazon book:<\/p>\n<pre><code>import sys\n!{sys.executable} -m pip install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>import sys\n!conda install -y --prefix {sys.prefix} gaapi4py\n<\/code><\/pre>\n<p>Both didn't work neither, getting same errors as above.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1606886205577,
        "Question_favorite_count":null,
        "Question_last_edit_time":1606887753310,
        "Question_score":1.0,
        "Question_view_count":1148.0,
        "Answer_body":"<p>After talking back-in-forth with our IT department I figured out that custom libraries installation was blocked for security reasons.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65102618",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1644193786212,
        "Question_original_content":"struggl instal python packag try instal gaapipi packag anaconda python notebook far tri follow command conda instal gaapipi conda instal gaapipi got error collect packag metadata current repodata json fail condahttperror http connect fail url elaps http error occur try retriev url http error intermitt simpl retri wai http conda anaconda org conda forg linux note need restart kernel us updat packag altern tri fail pip instal gaapipi error text warn retri retri total connect read redirect statu connect broken newconnectionerror fail establish new connect errno network unreach simpl gaapipi warn retri retri total connect read redirect statu connect broken newconnectionerror fail establish new connect errno network unreach simpl gaapipi warn retri retri total connect read redirect statu connect broken newconnectionerror fail establish new connect errno network unreach simpl gaapipi warn retri retri total connect read redirect statu connect broken newconnectionerror fail establish new connect errno network unreach simpl gaapipi warn retri retri total connect read redirect statu connect broken newconnectionerror fail establish new connect errno network unreach simpl gaapipi error version satisfi requir gaapipi version error match distribut gaapipi warn pip version version avail consid upgrad home user anaconda env python bin python pip instal upgrad pip command note need restart kernel us updat packag wrong previou packag work upd tri recommend amazon book import sy sy execut pip instal gaapipi import sy conda instal prefix sy prefix gaapipi work get error",
        "Question_preprocessed_content":"struggl instal python packag try instal gaapi packag anaconda python notebook far tri follow command altern tri fail error text wrong previou packag work upd tri recommend amazon book work get error",
        "Question_gpt_summary_original":"The user is facing challenges while trying to install the gaapi4py package via anaconda python3 notebook on Amazon Sagemaker. The user has tried various commands, including 'conda install' and 'pip install', but has encountered errors such as HTTP 000 CONNECTION FAILED and Failed to establish a new connection. The user has also tried alternative methods recommended by Amazon, but none of them have worked.",
        "Question_gpt_summary":"user face challeng try instal gaapipi packag anaconda python notebook user tri command includ conda instal pip instal encount error http connect fail fail establish new connect user tri altern method recommend amazon work",
        "Answer_original_content":"talk forth depart figur custom librari instal block secur reason",
        "Answer_preprocessed_content":"talk depart figur custom librari instal block secur reason",
        "Answer_gpt_summary_original":"possible solutions: \n- contact the it department to request permission to install custom libraries.\n- use a different package or library that is already installed and approved for use.",
        "Answer_gpt_summary":"possibl solut contact depart request permiss instal custom librari us differ packag librari instal approv us"
    },
    {
        "Question_id":null,
        "Question_title":"Azure machine learning data labelling- Is it possible to assign different labelers to label same data in a single project to reach a consensus?",
        "Question_body":"Currently! I'm experimenting with the azure data labelling tool in the machine learning workspace for image classification, what I found was azure shows only the unlabelled data to each user i.e if a user has already labelled an image, other users won't be shown the same image again.\nIs there any setting that exists, which can be enabled or disabled so that we can let more than one labeller label the same data?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625056620740,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Thanks for reaching to us. This capability is currently in development, and expected to release soon.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/458004\/azure-machine-learning-data-labelling-is-it-possib.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-30T17:13:49.547Z",
                "Answer_score":0,
                "Answer_body":"Thanks for reaching to us. This capability is currently in development, and expected to release soon.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1625073229547,
        "Question_original_content":"data label possibl assign differ label label data singl project reach consensu current experi azur data label tool machin learn workspac imag classif azur show unlabel data user user label imag user won shown imag set exist enabl disabl let label label data",
        "Question_preprocessed_content":"data label possibl assign differ label label data singl project reach consensu current experi azur data label tool machin learn workspac imag classif azur show unlabel data user user label imag user won shown imag set exist enabl disabl let label label data",
        "Question_gpt_summary_original":"The user is facing a challenge with the Azure data labelling tool in the machine learning workspace for image classification. The tool only shows unlabelled data to each user, which means that if a user has already labelled an image, other users won't be shown the same image again. The user is looking for a setting that can be enabled or disabled to allow more than one labeller to label the same data.",
        "Question_gpt_summary":"user face challeng azur data label tool machin learn workspac imag classif tool show unlabel data user mean user label imag user won shown imag user look set enabl disabl allow label label data",
        "Answer_original_content":"thank reach capabl current develop expect releas soon",
        "Answer_preprocessed_content":"thank reach capabl current develop expect releas soon",
        "Answer_gpt_summary_original":"possible solutions: none currently available, but the capability to enable multiple labelers to label the same data in a single project is currently in development and expected to release soon.",
        "Answer_gpt_summary":"possibl solut current avail capabl enabl multipl label label data singl project current develop expect releas soon"
    },
    {
        "Question_id":null,
        "Question_title":"CML runner specify ec2 ami id",
        "Question_body":"<p>Hello,<\/p>\n<p>I am reading through the CML documentation and is anyone aware of a was to specify the AMI id of the ec2 instance in the cml runner command?<\/p>\n<p>thanks,<br>\nKunal.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1623680757727,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":252.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cml-runner-specify-ec2-ami-id\/792",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-15T08:06:51.610Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kapoork31\">@kapoork31<\/a> the AMI image for the runner should not needed to be specified and hence you can not specify it. In our terraform provider that setting is computed.<br>\nIdeally you should setup your stack within the workflow specifying a docker image.<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-06-15T09:50:05.081Z",
                "Answer_body":"<p>Thanks,<\/p>\n<p>I completely forgot it runs a container on the ec2 instance, around 5 minutes later someone at work reminded me and then I realised this is not an issue.<\/p>\n<p>thanks.<\/p>",
                "Answer_score":16.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"cml runner specifi ami hello read cml document awar specifi ami instanc cml runner command thank kunal",
        "Question_preprocessed_content":"cml runner specifi ami hello read cml document awar specifi ami instanc cml runner command thank kunal",
        "Question_gpt_summary_original":"The user is facing a challenge in specifying the AMI id of the EC2 instance in the CML runner command and is seeking assistance.",
        "Question_gpt_summary":"user face challeng specifi ami instanc cml runner command seek assist",
        "Answer_original_content":"kapoork ami imag runner need specifi specifi terraform provid set comput ideal setup stack workflow specifi docker imag thank complet forgot run contain instanc minut later work remind realis issu thank",
        "Answer_preprocessed_content":"ami imag runner need specifi specifi terraform provid set comput ideal setup stack workflow specifi docker imag thank complet forgot run contain instanc minut later work remind realis issu thank",
        "Answer_gpt_summary_original":"there are no possible solutions provided in the answer as the user is informed that specifying the ami id for the cml runner command is not needed and cannot be done. the user is advised to set up their stack within the workflow by specifying a docker image.",
        "Answer_gpt_summary":"possibl solut provid answer user inform specifi ami cml runner command need user advis set stack workflow specifi docker imag"
    },
    {
        "Question_id":41903982.0,
        "Question_title":"Web service output - Azure ML Studio",
        "Question_body":"<p>I am new to Azure ML Studio. I tried creating an experiment that takes a numeric value as input and a gives a data table type output. I works fine when I run it in the portal , but not when I run it as a web service. It shows a single value numeric output , when it has to be a data table type.<\/p>\n\n<p>Is there a way to change the output type of web service output? <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/oq5Xb.png\" rel=\"nofollow noreferrer\">Visualizing output in portal<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wUmN7.png\" rel=\"nofollow noreferrer\">Test RRS output(web service)<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1485555724117,
        "Question_favorite_count":null,
        "Question_last_edit_time":1485558200660,
        "Question_score":1.0,
        "Question_view_count":587.0,
        "Answer_body":"<p>Make is a classic web service and see the JSON output getting from it. If it's providing all data you need.. go for it.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41903982",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1486009486972,
        "Question_original_content":"web servic output studio new studio tri creat experi take numer valu input give data tabl type output work fine run portal run web servic show singl valu numer output data tabl type wai chang output type web servic output visual output portal test rr output web servic",
        "Question_preprocessed_content":"web servic output studio new studio tri creat experi take numer valu input give data tabl type output work fine run portal run web servic show singl valu numer output data tabl type wai chang output type web servic output visual output portal test rr output",
        "Question_gpt_summary_original":"The user is facing a challenge with the output type of a web service in Azure ML Studio. The experiment works fine in the portal but not as a web service, where it shows a single value numeric output instead of a data table type output. The user is seeking a solution to change the output type of the web service.",
        "Question_gpt_summary":"user face challeng output type web servic studio experi work fine portal web servic show singl valu numer output instead data tabl type output user seek solut chang output type web servic",
        "Answer_original_content":"classic web servic json output get provid data need",
        "Answer_preprocessed_content":"classic web servic json output get provid data",
        "Answer_gpt_summary_original":"the possible solution to the challenge of a user encountering issues with creating an experiment in studio that gives a data table type output when run as a web service is to make it a classic web service and check the json output. if the json output provides all the necessary data, then it can be used as a solution.",
        "Answer_gpt_summary":"possibl solut challeng user encount issu creat experi studio give data tabl type output run web servic classic web servic check json output json output provid necessari data solut"
    },
    {
        "Question_id":null,
        "Question_title":"How to use MLFlow models with managed endpoints and private pip",
        "Question_body":"How do you create a batch endpoint using an MLFlow with a private package installed via a private pip?\n\nI have tried multiple ways (see this github issue: https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/22441#issuecomment-1104082706) but I'm not really getting anywhere.",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1650549331407,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/821194\/how-to-use-mlflow-models-with-managed-endpoints-an.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-02T16:04:56.39Z",
                "Answer_score":0,
                "Answer_body":"Is this doable? I explored several times but all failed.\n\nCan Microsoft add documentation about that?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"us model manag endpoint privat pip creat batch endpoint privat packag instal privat pip tri multipl wai github issu http github com azur azur sdk python issu issuecom get",
        "Question_preprocessed_content":"us model manag endpoint privat pip creat batch endpoint privat packag instal privat pip tri multipl wai get",
        "Question_gpt_summary_original":"The user is facing challenges in creating a batch endpoint using MLFlow with a private package installed via a private pip. They have tried multiple ways but have not been successful.",
        "Question_gpt_summary":"user face challeng creat batch endpoint privat packag instal privat pip tri multipl wai success",
        "Answer_original_content":"doabl explor time fail microsoft add document",
        "Answer_preprocessed_content":"doabl explor time fail microsoft add document",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the user is asking if it is possible to create a batch endpoint using an  with a private package installed via a private pip and is requesting microsoft to add documentation about it.",
        "Answer_gpt_summary":"solut provid answer user ask possibl creat batch endpoint privat packag instal privat pip request microsoft add document"
    },
    {
        "Question_id":72058686.0,
        "Question_title":"How do I deploy a pre trained sklearn model on AWS sagemaker? (Endpoint stuck on creating)",
        "Question_body":"<p>To start with, I understand that this question has been asked multiple times but I haven't found the solution to my problem.<\/p>\n<p>So, to start with I have used joblib.dump to save a locally trained sklearn RandomForest. I then uploaded this to s3, made a folder called code and put in an inference script there, called inference.py.<\/p>\n<pre><code>import joblib\nimport json\nimport numpy\nimport scipy\nimport sklearn\nimport os\n\n&quot;&quot;&quot;\nDeserialize fitted model\n&quot;&quot;&quot;\ndef model_fn(model_dir):\n    model_path = os.path.join(model_dir, 'test_custom_model')\n    model = joblib.load(model_path)\n    return model\n\n&quot;&quot;&quot;\ninput_fn\n    request_body: The body of the request sent to the model.\n    request_content_type: (string) specifies the format\/variable type of the request\n&quot;&quot;&quot;\ndef input_fn(request_body, request_content_type):\n    if request_content_type == 'application\/json':\n        request_body = json.loads(request_body)\n        inpVar = request_body['Input']\n        return inpVar\n    else:\n        raise ValueError(&quot;This model only supports application\/json input&quot;)\n\n&quot;&quot;&quot;\npredict_fn\n    input_data: returned array from input_fn above\n    model (sklearn model) returned model loaded from model_fn above\n&quot;&quot;&quot;\ndef predict_fn(input_data, model):\n    return model.predict(input_data)\n\n&quot;&quot;&quot;\noutput_fn\n    prediction: the returned value from predict_fn above\n    content_type: the content type the endpoint expects to be returned. Ex: JSON, string\n&quot;&quot;&quot;\n\ndef output_fn(prediction, content_type):\n    res = int(prediction[0])\n    respJSON = {'Output': res}\n    return respJSON\n<\/code><\/pre>\n<p>Very simple so far.<\/p>\n<p>I also put this into the local jupyter sagemaker session<\/p>\n<p>all_files (folder)\ncode (folder)\ninference.py (python file)\ntest_custom_model (joblib dump of model)<\/p>\n<p>The script turns this folder all_files into a tar.gz file<\/p>\n<p>Then comes the main script that I ran on sagemaker:<\/p>\n<pre><code>import boto3\nimport json\nimport os\nimport joblib\nimport pickle\nimport tarfile\nimport sagemaker\nimport time\nfrom time import gmtime, strftime\nimport subprocess\nfrom sagemaker import get_execution_role\n\n#Setup\nclient = boto3.client(service_name=&quot;sagemaker&quot;)\nruntime = boto3.client(service_name=&quot;sagemaker-runtime&quot;)\nboto_session = boto3.session.Session()\ns3 = boto_session.resource('s3')\nregion = boto_session.region_name\nprint(region)\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\n#Bucket for model artifacts\ndefault_bucket = 'pretrained-model-deploy'\nmodel_artifacts = f&quot;s3:\/\/{default_bucket}\/test_custom_model.tar.gz&quot;\n\n#Build tar file with model data + inference code\nbashCommand = &quot;tar -cvpzf test_custom_model.tar.gz all_files&quot;\nprocess = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\noutput, error = process.communicate()\n\n#Upload tar.gz to bucket\nresponse = s3.meta.client.upload_file('test_custom_model.tar.gz', default_bucket, 'test_custom_model.tar.gz')\n\n# retrieve sklearn image\nimage_uri = sagemaker.image_uris.retrieve(\n    framework=&quot;sklearn&quot;,\n    region=region,\n    version=&quot;0.23-1&quot;,\n    py_version=&quot;py3&quot;,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n)\n\n#Step 1: Model Creation\nmodel_name = &quot;sklearn-test&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nprint(&quot;Model name: &quot; + model_name)\ncreate_model_response = client.create_model(\n    ModelName=model_name,\n    Containers=[\n        {\n            &quot;Image&quot;: image_uri,\n            &quot;ModelDataUrl&quot;: model_artifacts,\n        }\n    ],\n    ExecutionRoleArn=role,\n)\nprint(&quot;Model Arn: &quot; + create_model_response[&quot;ModelArn&quot;])\n\n#Step 2: EPC Creation - Serverless\nsklearn_epc_name = &quot;sklearn-epc&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nresponse = client.create_endpoint_config(\n   EndpointConfigName=sklearn_epc_name,\n   ProductionVariants=[\n        {\n            &quot;ModelName&quot;: model_name,\n            &quot;VariantName&quot;: &quot;sklearnvariant&quot;,\n            &quot;ServerlessConfig&quot;: {\n                &quot;MemorySizeInMB&quot;: 2048,\n                &quot;MaxConcurrency&quot;: 20\n            }\n        } \n    ]\n)\n\n# #Step 2: EPC Creation - Synchronous\n# sklearn_epc_name = &quot;sklearn-epc&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\n# endpoint_config_response = client.create_endpoint_config(\n#     EndpointConfigName=sklearn_epc_name,\n#     ProductionVariants=[\n#         {\n#             &quot;VariantName&quot;: &quot;sklearnvariant&quot;,\n#             &quot;ModelName&quot;: model_name,\n#             &quot;InstanceType&quot;: &quot;ml.m5.xlarge&quot;,\n#             &quot;InitialInstanceCount&quot;: 1\n#         },\n#     ],\n# )\n# print(&quot;Endpoint Configuration Arn: &quot; + endpoint_config_response[&quot;EndpointConfigArn&quot;])\n\n#Step 3: EP Creation\nendpoint_name = &quot;sklearn-local-ep&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\ncreate_endpoint_response = client.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=sklearn_epc_name,\n)\nprint(&quot;Endpoint Arn: &quot; + create_endpoint_response[&quot;EndpointArn&quot;])\n\n\n#Monitor creation\ndescribe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\nwhile describe_endpoint_response[&quot;EndpointStatus&quot;] == &quot;Creating&quot;:\n    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n    print(describe_endpoint_response)\n    time.sleep(15)\nprint(describe_endpoint_response)\n<\/code><\/pre>\n<p>Now, I mainly just want the serverless deployment but that fails after a while with this error message:<\/p>\n<pre><code>{'EndpointName': 'sklearn-local-ep2022-04-29-12-16-10', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-12-16-10', 'EndpointConfigName': 'sklearn-epc2022-04-29-12-16-03', 'EndpointStatus': 'Creating', 'CreationTime': datetime.datetime(2022, 4, 29, 12, 16, 10, 290000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 12, 16, 11, 52000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '1d25120e-ddb1-474d-9c5f-025c6be24383', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1d25120e-ddb1-474d-9c5f-025c6be24383', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '305', 'date': 'Fri, 29 Apr 2022 12:21:59 GMT'}, 'RetryAttempts': 0}}\n{'EndpointName': 'sklearn-local-ep2022-04-29-12-16-10', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-12-16-10', 'EndpointConfigName': 'sklearn-epc2022-04-29-12-16-03', 'EndpointStatus': 'Failed', 'FailureReason': 'Unable to successfully stand up your model within the allotted 180 second timeout. Please ensure that downloading your model artifacts, starting your model container and passing the ping health checks can be completed within 180 seconds.', 'CreationTime': datetime.datetime(2022, 4, 29, 12, 16, 10, 290000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 12, 22, 2, 68000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '59fb8ddd-9d45-41f5-9383-236a2baffb73', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '59fb8ddd-9d45-41f5-9383-236a2baffb73', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '559', 'date': 'Fri, 29 Apr 2022 12:22:15 GMT'}, 'RetryAttempts': 0}}\n<\/code><\/pre>\n<p>The real time deployment is just permanently stuck at creating.<\/p>\n<p>Cloudwatch has the following errors:\nError handling request \/ping<\/p>\n<p>AttributeError: 'NoneType' object has no attribute 'startswith'<\/p>\n<p>with traceback:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base_async.py&quot;, line 55, in handle\n    self.handle_request(listener_name, req, client, addr)\n<\/code><\/pre>\n<p>Copy paste has stopped working so I have attached an image of it instead.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hw80j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hw80j.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This is the error message I get:\nEndpoint Arn: arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-13-18-09\n{'EndpointName': 'sklearn-local-ep2022-04-29-13-18-09', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-13-18-09', 'EndpointConfigName': 'sklearn-epc2022-04-29-13-18-07', 'EndpointStatus': 'Creating', 'CreationTime': datetime.datetime(2022, 4, 29, 13, 18, 9, 548000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 13, 18, 13, 119000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'ef0e49ee-618e-45de-9c49-d796206404a4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'ef0e49ee-618e-45de-9c49-d796206404a4', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '306', 'date': 'Fri, 29 Apr 2022 13:18:24 GMT'}, 'RetryAttempts': 0}}<\/p>\n<p>These are the permissions I have associated with that role:<\/p>\n<pre><code>AmazonSageMaker-ExecutionPolicy\nSecretsManagerReadWrite\nAmazonS3FullAccess\nAmazonSageMakerFullAccess\nEC2InstanceProfileForImageBuilderECRContainerBuilds\nAWSAppRunnerServicePolicyForECRAccess\n<\/code><\/pre>\n<p>What am I doing wrong? I've tried different folder structures for the zip file, different accounts, all to no avail. I don't really want to use the model.deploy() method as I don't know how to use serverless with that, and it's also inconcistent between different model types (I'm trying to make a flexible deployment pipeline where different (xgb \/ sklearn) models can be deployed with minimal changes.<\/p>\n<p>Please send help, I'm very close to smashing my hair and tearing out my laptop, been struggling with this for a whole 4 days now.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1651238636737,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1651255570927,
        "Question_score":1.0,
        "Question_view_count":397.0,
        "Answer_body":"<p>I've solved this problem - I used sagemaker.model.model to load in the model data I already had and I called the deploy method on the aforementioned model object to deploy it. Further, I had the inference script and the model file in the same place as the notebook and directly called them, as this gave me an error earlier as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72058686",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1651509693110,
        "Question_original_content":"deploi pre train sklearn model endpoint stuck creat start understand question ask multipl time haven solut problem start joblib dump save local train sklearn randomforest upload folder call code infer script call infer import joblib import json import numpi import scipi import sklearn import deseri fit model def model model dir model path path join model dir test custom model model joblib load model path return model input request bodi bodi request sent model request content type string specifi format variabl type request def input request bodi request content type request content type applic json request bodi json load request bodi inpvar request bodi input return inpvar rais valueerror model support applic json input predict input data return arrai input model sklearn model return model load model def predict input data model return model predict input data output predict return valu predict content type content type endpoint expect return json string def output predict content type re int predict respjson output re return respjson simpl far local jupyt session file folder code folder infer python file test custom model joblib dump model script turn folder file tar file come main script ran import boto import json import import joblib import pickl import tarfil import import time time import gmtime strftime import subprocess import execut role setup client boto client servic runtim boto client servic runtim boto session boto session session boto session resourc region boto session region print region session session role execut role bucket model artifact default bucket pretrain model deploi model artifact default bucket test custom model tar build tar file model data infer code bashcommand tar cvpzf test custom model tar file process subprocess popen bashcommand split stdout subprocess pipe output error process commun upload tar bucket respons meta client upload file test custom model tar default bucket test custom model tar retriev sklearn imag imag uri imag uri retriev framework sklearn region region version version instanc type xlarg step model creation model sklearn test strftime gmtime print model model creat model respons client creat model modelnam model contain imag imag uri modeldataurl model artifact executionrolearn role print model arn creat model respons modelarn step epc creation serverless sklearn epc sklearn epc strftime gmtime respons client creat endpoint config endpointconfignam sklearn epc productionvari modelnam model variantnam sklearnvari serverlessconfig memorysizeinmb maxconcurr step epc creation synchron sklearn epc sklearn epc strftime gmtime endpoint config respons client creat endpoint config endpointconfignam sklearn epc productionvari variantnam sklearnvari modelnam model instancetyp xlarg initialinstancecount print endpoint configur arn endpoint config respons endpointconfigarn step creation endpoint sklearn local strftime gmtime creat endpoint respons client creat endpoint endpointnam endpoint endpointconfignam sklearn epc print endpoint arn creat endpoint respons endpointarn monitor creation endpoint respons client endpoint endpointnam endpoint endpoint respons endpointstatu creat endpoint respons client endpoint endpointnam endpoint print endpoint respons time sleep print endpoint respons mainli want serverless deploy fail error messag endpointnam sklearn local endpointarn arn aw east endpoint sklearn local endpointconfignam sklearn epc endpointstatu creat creationtim datetim datetim tzinfo tzlocal lastmodifiedtim datetim datetim tzinfo tzlocal responsemetadata requestid ddb cbe httpstatuscod httpheader amzn requestid ddb cbe content type applic amz json content length date fri apr gmt retryattempt endpointnam sklearn local endpointarn arn aw east endpoint sklearn local endpointconfignam sklearn epc endpointstatu fail failurereason unabl successfulli stand model allot second timeout ensur download model artifact start model contain pass ping health check complet second creationtim datetim datetim tzinfo tzlocal lastmodifiedtim datetim datetim tzinfo tzlocal responsemetadata requestid abaffb httpstatuscod httpheader amzn requestid abaffb content type applic amz json content length date fri apr gmt retryattempt real time deploy perman stuck creat cloudwatch follow error error handl request ping attributeerror nonetyp object attribut startswith traceback traceback recent file miniconda lib python site packag gunicorn worker base async line handl self handl request listen req client addr copi past stop work attach imag instead error messag endpoint arn arn aw east endpoint sklearn local endpointnam sklearn local endpointarn arn aw east endpoint sklearn local endpointconfignam sklearn epc endpointstatu creat creationtim datetim datetim tzinfo tzlocal lastmodifiedtim datetim datetim tzinfo tzlocal responsemetadata requestid efee httpstatuscod httpheader amzn requestid efee content type applic amz json content length date fri apr gmt retryattempt permiss associ role amazon executionpolici secretsmanagerreadwrit amazonsfullaccess amazonfullaccess ecinstanceprofileforimagebuilderecrcontainerbuild awsapprunnerservicepolicyforecraccess wrong tri differ folder structur zip file differ account avail want us model deploi method know us serverless inconcist differ model type try flexibl deploy pipelin differ xgb sklearn model deploi minim chang send help close smash hair tear laptop struggl dai",
        "Question_preprocessed_content":"deploi pre train sklearn model start understand question ask multipl time haven solut problem start save local train sklearn randomforest upload folder call code infer script call simpl far local jupyt session code script turn folder file come main script ran mainli want serverless deploy fail error messag real time deploy perman stuck creat cloudwatch follow error error handl request attributeerror nonetyp object attribut startswith traceback copi past stop work attach imag instead error messag endpoint arn endpointnam endpointarn endpointconfignam endpointstatu creat creationtim tzinfo tzlocal lastmodifiedtim tzinfo tzlocal responsemetadata retryattempt permiss associ role wrong tri differ folder structur zip file differ account avail want us method know us serverless inconcist differ model type model deploi minim chang send help close smash hair tear laptop struggl dai",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a pre-trained sklearn model on AWS Sagemaker. They have saved the model using joblib.dump and uploaded it to S3, along with an inference script. They have also created a tar.gz file with the model data and inference code and uploaded it to a bucket. The user is attempting to create a serverless endpoint configuration, but it fails with an error message indicating that the model cannot be started within the allotted 180-second timeout. The user has checked Cloudwatch logs and found an error handling request \/ping with an AttributeError. The user has tried different folder structures for the zip file and different accounts, but the issue persists. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user face challeng deploi pre train sklearn model save model joblib dump upload infer script creat tar file model data infer code upload bucket user attempt creat serverless endpoint configur fail error messag indic model start allot second timeout user check cloudwatch log error handl request ping attributeerror user tri differ folder structur zip file differ account issu persist user seek help resolv issu",
        "Answer_original_content":"solv problem model model load model data call deploi method aforement model object deploi infer script model file place notebook directli call gave error earlier",
        "Answer_preprocessed_content":"solv problem load model data call deploi method aforement model object deploi infer script model file place notebook directli call gave error earlier",
        "Answer_gpt_summary_original":"the solution to the problem of a pre-trained sklearn model getting stuck on creating while deploying on an endpoint is to use .model.model to load in the model data and call the deploy method on the model object. additionally, keeping the inference script and model file in the same place as the notebook can also help avoid errors.",
        "Answer_gpt_summary":"solut problem pre train sklearn model get stuck creat deploi endpoint us model model load model data deploi method model object addition keep infer script model file place notebook help avoid error"
    },
    {
        "Question_id":null,
        "Question_title":"Where is the confusion matrix saved on the dashboard?",
        "Question_body":"<p>I\u2019ve logged a confusion matrix like the below but I cannot find it listed anywhere on the dashboard of my project for any run. Indeed, I logged this for every step of the training (not every epoch). So where is the confusion matrix stored?<\/p>\n<pre><code class=\"lang-auto\">wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n                                y_true=target, preds=pred[0],\n                                class_names=class_names)})\n<\/code><\/pre>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1656694968455,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":73.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/where-is-the-confusion-matrix-saved-on-the-dashboard\/2689",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-01T22:19:29.260Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mesllo\">@mesllo<\/a> ,<\/p>\n<p>Would be happy to look into this for you. The matrix will be logged under the custom charts panel. Can you please provide the following:<\/p>\n<ul>\n<li>Wandb Version you are using<\/li>\n<li>Link to workspace where matrix isn\u2019t being logged<\/li>\n<li>Full traceback if available<\/li>\n<li>Also, if possible, a working colab with code example to take an attempt at replicating your work<\/li>\n<\/ul>\n<p>Thanks,<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-03T16:54:02.646Z",
                "Answer_body":"<p>I found out I had a bug in my code, now it is showing. However, I\u2019d like to show the counts for all steps rather than a single one. How I can use log() to aggregate all the counts for the confusion matrix like we do for loss and such?<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-15T01:11:09.015Z",
                "Answer_body":"<p>HI <a class=\"mention\" href=\"\/u\/mesllo\">@mesllo<\/a> , I\u2019m glad you discovered that bug that was impacting your chart. In regards to show counts for all steps, that\u2019s not currently doable directly. This would have to be done locally where you keep track of your class predictions per step, aggregate the results and then log the confusion matrix to wandb.<\/p>\n<p>Although this doesn\u2019t address your particular request, you can generate a step slider for your matrix, see <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/custom-charts#how-to-show-a-step-slider-in-a-custom-chart\">here<\/a>, and you can see your confusion matrix results per step.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-20T18:37:12.884Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mesllo\">@mesllo<\/a> ,<\/p>\n<p>Since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-18T18:37:28.434Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"confus matrix save dashboard iv log confus matrix like list dashboard project run log step train epoch confus matrix store log conf mat plot confus matrix prob true target pred pred class name class name",
        "Question_preprocessed_content":"confus matrix save dashboard iv log confus matrix like list dashboard project run log step train confus matrix store",
        "Question_gpt_summary_original":"The user has encountered a challenge in finding the confusion matrix that they have logged for every step of the training on the dashboard of their project. They are unsure where the confusion matrix is stored and cannot find it listed anywhere on the dashboard.",
        "Question_gpt_summary":"user encount challeng find confus matrix log step train dashboard project unsur confus matrix store list dashboard",
        "Answer_original_content":"mesllo happi look matrix log custom chart panel provid follow version link workspac matrix isnt log traceback avail possibl work colab code exampl attempt replic work thank bug code show like count step singl us log aggreg count confus matrix like loss mesllo glad discov bug impact chart regard count step that current doabl directli local track class predict step aggreg result log confus matrix doesnt address particular request gener step slider matrix confus matrix result step mesllo heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"happi look matrix log custom chart panel provid follow version link workspac matrix isnt log traceback avail possibl work colab code exampl attempt replic work thank bug code show like count step singl us log aggreg count confus matrix like loss glad discov bug impact chart regard count step that current doabl directli local track class predict step aggreg result log confus matrix doesnt address particular request gener step slider matrix confus matrix result step heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are:\n\n- the confusion matrix will be logged under the custom charts panel.\n- to show counts for all steps, the user needs to keep track of their class predictions per step, aggregate the results, and then log the confusion matrix.\n- the user can generate a step slider for their matrix to see the confusion matrix results per step.",
        "Answer_gpt_summary":"possibl solut extract answer confus matrix log custom chart panel count step user need track class predict step aggreg result log confus matrix user gener step slider matrix confus matrix result step"
    },
    {
        "Question_id":null,
        "Question_title":"What's the purpose of mlflow.models.FlavorBackend.serve API ?",
        "Question_body":"Hi,\n\nI was looking into the\u00a0mlflow.models.FlavorBackend.serve API. What is the purpose of this API? Has anyone used it?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1563949854000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":7.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/O_w-8r1UVmg",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-07-24T13:53:57",
                "Answer_body":"Hi Shevy, the purpose of the serve api is to start a (local) rest api server that can score model on user input. Flavors can define backend to implement or override model serving.\nYou can look at PyFuncBackend for an example of FlavorBackend implementation.\u00a0\n\n\n\n\n\n\n\n\nOn Wed, Jul 24, 2019 at 3:30 AM Shevy Mittal <shevy....@gslab.com> wrote:\n\nHi,\n\nI was looking into the\u00a0mlflow.models.FlavorBackend.serve API. What is the purpose of this API? Has anyone used it?\n\n\nConfidentiality Notice and Disclaimer: This email (including any attachments) contains information that may be confidential, privileged and\/or copyrighted. If you are not the intended recipient, please notify the sender immediately and destroy this email. Any unauthorized use of the contents of this email in any manner whatsoever, is strictly prohibited. If improper activity is suspected, all available information may be used by the sender for possible disciplinary action, prosecution, civil claim or any remedy or lawful purpose. Email transmission cannot be guaranteed to be secure or error-free, as information could be intercepted, lost, arrive late, or contain viruses. The sender is not liable whatsoever for damage resulting from the opening of this message and\/or the use of the information contained in this message and\/or attachments. Expressions in this email cannot be treated as opined by the sender company management \u2013 they are solely expressed by the sender unless authorized.\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/93952dc6-ef9b-4fe1-a340-d3c945dd05f7%40googlegroups.com."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"purpos model flavorbackend serv api look model flavorbackend serv api purpos api",
        "Question_preprocessed_content":"purpos api look api purpos api",
        "Question_gpt_summary_original":"The user is seeking information about the purpose of the mlflow.models.FlavorBackend.serve API and whether anyone has used it. No specific challenges are mentioned.",
        "Question_gpt_summary":"user seek inform purpos model flavorbackend serv api specif challeng mention",
        "Answer_original_content":"shevi purpos serv api start local rest api server score model user input flavor defin backend implement overrid model serv look pyfuncbackend exampl flavorbackend implement wed jul shevi mittal wrote look model flavorbackend serv api purpos api confidenti notic disclaim email includ attach contain inform confidenti privileg copyright intend recipi notifi sender immedi destroi email unauthor us content email manner whatsoev strictli prohibit improp activ suspect avail inform sender possibl disciplinari action prosecut civil claim remedi law purpos email transmiss guarante secur error free inform intercept lost arriv late contain virus sender liabl whatsoev damag result open messag us inform contain messag attach express email treat opin sender compani manag sole express sender author receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user efb dcddf googlegroup com",
        "Answer_preprocessed_content":"shevi purpos serv api start rest api server score model user input flavor defin backend implement overrid model serv look pyfuncbackend exampl flavorbackend implement wed jul shevi mittal wrote look api purpos api confidenti notic disclaim email contain inform confidenti privileg copyright intend recipi notifi sender immedi destroi email unauthor us content email manner whatsoev strictli prohibit improp activ suspect avail inform sender possibl disciplinari action prosecut civil claim remedi law purpos email transmiss guarante secur inform intercept lost arriv late contain virus sender liabl whatsoev damag result open messag us inform contain messag attach express email treat opin sender compani manag sole express sender author receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit",
        "Answer_gpt_summary_original":"Solution: \n- The purpose of the serve API is to start a (local) rest API server that can score model on user input.\n- Flavors can define backend to implement or override model serving.\n- PyFuncBackend can be used as an example of FlavorBackend implementation.",
        "Answer_gpt_summary":"solut purpos serv api start local rest api server score model user input flavor defin backend implement overrid model serv pyfuncbackend exampl flavorbackend implement"
    },
    {
        "Question_id":null,
        "Question_title":"Why do i get :NotLabeledDataset: There is no label column in",
        "Question_body":"Hello I am trying to make a classification model using the ml designer.\n\nI registert a dataset whit images. Maybe i did it wrong?\nI used the sample that Microsoft designer suplies densnet for image classification. I only changed the dataset whit one that i made.\n\nI have 5 classes and each one is in thier own subfolder i uploaded the folder. a file type.\n\ni keep getting the message",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1622732290603,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/421203\/why-do-i-get-notlabeleddataset-there-is-no-label-c.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-04T12:53:42.51Z",
                "Answer_score":0,
                "Answer_body":"Thank you for your response.\n\nThe train pytorch module.\nthis is the structure of the folder i uploaded",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-06-15T20:41:10.153Z",
                "Answer_score":0,
                "Answer_body":"I'm facing the same issue with a dataset folder setup with the same hierarchy as above using the Image Classification using DenseNet architecture.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-06-15T22:46:23.77Z",
                "Answer_score":0,
                "Answer_body":"Thanks all for your inquiry. After further review, I discovered that if you create a dataset by uploading images and registering it, it removes the structure and uploads all images into one folder. Please try to upload the images into separate folders in your blob container (..\/UI\/Folder\/Subfolder) and run the experiment again. I was able to resolve the issue with the steps mentioned above. Hope it helps!",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"notlabeleddataset label column hello try classif model design registert dataset whit imag mayb wrong sampl microsoft design supli densnet imag classif chang dataset whit class thier subfold upload folder file type get messag",
        "Question_preprocessed_content":"notlabeleddataset label column hello try classif model design registert dataset whit imag mayb wrong sampl microsoft design supli densnet imag classif chang dataset whit class thier subfold upload folder file type get messag",
        "Question_gpt_summary_original":"The user is encountering a challenge with creating a classification model using the ml designer due to the error message \"NotLabeledDataset: There is no label column.\" The user suspects that the issue may be related to registering the dataset incorrectly or using the wrong sample. The user has 5 classes with each in their own subfolder and uploaded the folder as a file type.",
        "Question_gpt_summary":"user encount challeng creat classif model design error messag notlabeleddataset label column user suspect issu relat regist dataset incorrectli wrong sampl user class subfold upload folder file type",
        "Answer_original_content":"thank respons train pytorch modul structur folder upload face issu dataset folder setup hierarchi imag classif densenet architectur thank inquiri review discov creat dataset upload imag regist remov structur upload imag folder try upload imag separ folder blob contain folder subfold run experi abl resolv issu step mention hope help",
        "Answer_preprocessed_content":"thank respons train pytorch modul structur folder upload face issu dataset folder setup hierarchi imag classif densenet architectur thank inquiri review discov creat dataset upload imag regist remov structur upload imag folder try upload imag separ folder blob contain run experi abl resolv issu step mention hope help",
        "Answer_gpt_summary_original":"the solution to the issue of receiving the error message \":notlabeleddataset: there is no label column in\" when attempting to create a classification model using a dataset of images is to upload the images into separate folders in the blob container (..\/ui\/folder\/subfolder) and run the experiment again. this will prevent the structure of the folder from being removed and all images being uploaded into one folder.",
        "Answer_gpt_summary":"solut issu receiv error messag notlabeleddataset label column attempt creat classif model dataset imag upload imag separ folder blob contain folder subfold run experi prevent structur folder remov imag upload folder"
    },
    {
        "Question_id":63571552.0,
        "Question_title":"joblib.dump() fails when saving model to temporary data store in AMLS",
        "Question_body":"<p>I am training a model using AMLS. I have a training pipeline in which step 1 trains a model then saves the output in temporary datastore model_folder using<\/p>\n<pre><code>os.makedirs(output_folder, exist_ok=True)\noutput_path = output_folder + &quot;\/model.pkl&quot;\njoblib.dump(value=model, filename=output_path)\n<\/code><\/pre>\n<p>Step 2 loads the model and registers it. The model folder is defined in the pipeline as<\/p>\n<pre><code>model_folder = PipelineData(&quot;model_folder&quot;, datastore=ws.get_default_datastore())\n<\/code><\/pre>\n<p>However, step 1 fails when it tries to save the model with the following ServiceError:<\/p>\n<p>Failed to upload outputs due to Exception: Microsoft.RelInfra.Common.Exceptions.OperationFailedException: Cannot upload output xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx. ---&gt; Microsoft.WindowsAzure.Storage.StorageException: This request is not authorized to perform this operation using this permission.<\/p>\n<p>How can I solve this? Earlier in my code I had no problem interacting with the default datastore using<\/p>\n<pre><code>default_ds = ws.get_default_datastore()\ndefault_ds.upload_files(...)\n<\/code><\/pre>\n<p>My <code>70_driver_log.txt<\/code> is as follows:<\/p>\n<pre><code>[2020-08-25T04:03:27.315114] Entering context manager injector.\n[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['train_word2vec.py', '--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10'])\nStarting the daemon thread to refresh tokens in background for process with pid = 113\nEntering Run History Context Manager.\nCurrent directory:  \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\nPreparing to call script [ train_word2vec.py ] with arguments: ['--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10']\nAfter variable expansion, calling script [ train_word2vec.py ] with arguments: ['--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10']\n\nScript type = None\n[nltk_data] Downloading package stopwords to \/root\/nltk_data...\n[nltk_data]   Unzipping corpora\/stopwords.zip.\n[nltk_data] Downloading package wordnet to \/root\/nltk_data...\n[nltk_data]   Unzipping corpora\/wordnet.zip.\nOUTPUT FOLDER: \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder\nLoading SQL data...\nLoading abbreviation data...\n\/azureml-envs\/azureml_xxxxx\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py:1783: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n  self.obj[item_labels[indexer[info_axis]]] = value\nPre-processing data...\nSuccesfully pre-processed the the text data\nTraining Word2Vec model...\nSaving the model...\nStarting the daemon thread to refresh tokens in background for process with pid = 113\n\n\nThe experiment completed successfully. Finalizing run...\n[2020-08-25T04:03:52.293994] TimeoutHandler __init__\n[2020-08-25T04:03:52.294149] TimeoutHandler __enter__\nCleaning up all outstanding Run operations, waiting 300.0 seconds\n2 items cleaning up...\nCleanup took 0.44109439849853516 seconds\n[2020-08-25T04:03:52.818991] TimeoutHandler __exit__\n2020\/08\/25 04:04:00 logger.go:293: Process Exiting with Code:  0\n<\/code><\/pre>\n<p>My arg parse arguments include<\/p>\n<pre><code>parser.add_argument('--output_folder', type=str, dest='output_folder', default=&quot;output_folder&quot;, help='output folder')\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":6,
        "Question_creation_time":1598325386190,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1598328703460,
        "Question_score":2.0,
        "Question_view_count":571.0,
        "Answer_body":"<p>Fixed this problem by adding my AMLS workspace to a 'storage blob data contributor' role in the AMLS default storage account. It seemly like usually this role is added by default, but it didn't happen in my case.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63571552",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1598745717623,
        "Question_original_content":"joblib dump fail save model temporari data store aml train model aml train pipelin step train model save output temporari datastor model folder makedir output folder exist true output path output folder model pkl joblib dump valu model filenam output path step load model regist model folder defin pipelin model folder pipelinedata model folder datastor default datastor step fail tri save model follow serviceerror fail upload output except microsoft relinfra common except operationfailedexcept upload output microsoft windowsazur storag storageexcept request author perform oper permiss solv earlier code problem interact default datastor default default datastor default upload file driver log txt follow enter context manag injector context manag injector command line option namespac inject projectpythonpath context manag projectpythonpath runhistori context manag runhistori trackusererror context manag trackusererror invoc train wordvec output folder mnt batch task share root job aiworkspac mount workspaceblobstor model folder model type train field task titl regex stopword remov token basic remov punctuat autocorrect lemmat word vector length model learn rate model min count model window num epoch start daemon thread refresh token background process pid enter run histori context manag current directori mnt batch task share root job aiworkspac mount workspaceblobstor prepar script train wordvec argument output folder mnt batch task share root job aiworkspac mount workspaceblobstor model folder model type train field task titl regex stopword remov token basic remov punctuat autocorrect lemmat word vector length model learn rate model min count model window num epoch variabl expans call script train wordvec argument output folder mnt batch task share root job aiworkspac mount workspaceblobstor model folder model type train field task titl regex stopword remov token basic remov punctuat autocorrect lemmat word vector length model learn rate model min count model window num epoch script type nltk data download packag stopword root nltk data nltk data unzip corpora stopword zip nltk data download packag wordnet root nltk data nltk data unzip corpora wordnet zip output folder mnt batch task share root job aiworkspac mount workspaceblobstor model folder load sql data load abbrevi data env lib python site packag panda core index settingwithcopywarn valu try set copi slice datafram try loc row index col index valu instead caveat document http panda pydata org panda doc stabl user guid index html return view versu copi self obj item label index info axi valu pre process data succesfulli pre process text data train wordvec model save model start daemon thread refresh token background process pid experi complet successfulli final run timeouthandl init timeouthandl enter clean outstand run oper wait second item clean cleanup took second timeouthandl exit logger process exit code arg pars argument includ parser add argument output folder type str dest output folder default output folder help output folder",
        "Question_preprocessed_content":"fail save model temporari data store aml train model aml train pipelin step train model save output temporari datastor step load model regist model folder defin pipelin step fail tri save model follow serviceerror fail upload output except upload output request author perform oper permiss solv earlier code problem interact default datastor follow arg pars argument includ",
        "Question_gpt_summary_original":"The user is encountering an issue with joblib.dump() failing to save a model to a temporary data store in AMLS. The error message indicates that the request is not authorized to perform the operation using the given permission. The user is seeking a solution to this issue.",
        "Question_gpt_summary":"user encount issu joblib dump fail save model temporari data store aml error messag indic request author perform oper given permiss user seek solut issu",
        "Answer_original_content":"fix problem ad aml workspac storag blob data contributor role aml default storag account seemli like usual role ad default happen case",
        "Answer_preprocessed_content":"fix problem ad aml workspac storag blob data contributor role aml default storag account seemli like usual role ad default happen case",
        "Answer_gpt_summary_original":"the solution to the issue with joblib.dump() failing when saving a model to a temporary data store in amls is to add the amls workspace to a 'storage blob data contributor' role in the amls default storage account. this role is usually added by default, but in some cases, it may need to be added manually.",
        "Answer_gpt_summary":"solut issu joblib dump fail save model temporari data store aml add aml workspac storag blob data contributor role aml default storag account role usual ad default case need ad manual"
    },
    {
        "Question_id":null,
        "Question_title":"XGBoost, cross-validation",
        "Question_body":"<p>I am checking out the <code>sigopt.xgboost<\/code> API.  How do I perform cross-validation with it?<\/p>\n<p>I think I see how to do it using <code>sigopt.create_experiment<\/code>, because that expects the training to happen external of the API call, and I can use a cross-validation technique that I am familiar with (e.g. <code>sklearn.model_selection.cross_val_score<\/code>).  However <code>sigopt.create_experiment<\/code> also requires me to do more work setting up parameter search ranges compare to <code>sigopt.xgboost<\/code>.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1644433403173,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":209.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.sigopt.com\/t\/xgboost-cross-validation\/92",
        "Tool":"SigOpt",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-11T18:18:51.389Z",
                "Answer_body":"<p>The <code>sigopt.xgboost<\/code> API was designed to follow the <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.training\"><code>xgboost.train<\/code><\/a> API. If you want to use a custom CV procedure, you will need to use <code>sigopt.create_experiment<\/code>. As you point out, this requires you defining the parameter space. If you are looking for guidance on how to construct the parameter space, you can reuse the defaults of <code>sigopt.xgboost<\/code> by accessing <a href=\"https:\/\/github.com\/sigopt\/sigopt-python\/blob\/master\/sigopt\/xgboost\/constants.py\">constants from sigopt.xgboost<\/a>.<\/p>",
                "Answer_score":21.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-11T19:23:51.889Z",
                "Answer_body":"<p>Eddie-SigOpt, thanks for the feedback.<\/p>\n<p>While the ability to do a \u201ccustom CV procedure\u201d would be nice, the ability to do <em>any<\/em> form of cross-validation (or other subsampling\/validation technique) seems important.  Otherwise it seems like the auto-tuning (HPO) will just maximize the training score, which will likely lead to overfitting.  Unless the experiment has some form of model validation happening behind the hood that I missed in the documentation?<\/p>\n<p>I guess I am thinking of something like <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#xgboost.cv\" rel=\"noopener nofollow ugc\">xgboost.cv<\/a>, except it could auto-tune instead of using a single set of hyperparameters.<\/p>",
                "Answer_score":6.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-11T19:56:58.693Z",
                "Answer_body":"<p>Yes, good point. If you do not provide any validation datasets in the <code>evals<\/code> argument, you will be optimizing for metrics computed on the training dataset which increases the likelihood of producing an overfit model. If you include entries in the <code>evals<\/code> argument of <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#xgboost.train\"><code>xgboost.train<\/code><\/a>, the first entry of the list will be the dataset used for optimization metric(s).<\/p>\n<p>On another note, if you would like to have a chat about the possibility of extending the existing <code>sigopt.xgboost<\/code> calls (which work with <code>xgboost.train<\/code>) to include <code>xgboost.cv<\/code>or more general CV workflows please reach me at <a href=\"mailto:eddie.mattia@intel.com\">eddie.mattia@intel.com<\/a> and we can schedule some time to discuss.<\/p>",
                "Answer_score":21.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-23T05:25:54.527Z",
                "Answer_body":"<p><code>evals<\/code>, I forgot about that one, that should definitely improve things over just maxing the training score.  Thanks for the tip.<\/p>\n<p>I think adding CV would be helpful.  Either something similar to the xgboost.cv, or perhaps allowing a callable to be passed into <code>dtrain<\/code> or <code>evals<\/code> which can be used to get splits\u2026 probably too much of hack.  I\u2019ll reach out if I get a chance.<br>\nThanks again!<\/p>",
                "Answer_score":45.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"xgboost cross valid check xgboost api perform cross valid think creat experi expect train happen extern api us cross valid techniqu familiar sklearn model select cross val score creat experi requir work set paramet search rang compar xgboost",
        "Question_preprocessed_content":"xgboost check api perform think expect train happen extern api us techniqu familiar requir work set paramet search rang compar",
        "Question_gpt_summary_original":"The user is trying to perform cross-validation with the sigopt.xgboost API but is facing challenges in setting up parameter search ranges compared to using sigopt.create_experiment.",
        "Question_gpt_summary":"user try perform cross valid xgboost api face challeng set paramet search rang compar creat experi",
        "Answer_original_content":"xgboost api design follow xgboost train api want us custom procedur need us creat experi point requir defin paramet space look guidanc construct paramet space reus default xgboost access constant xgboost eddi thank feedback abil custom procedur nice abil form cross valid subsampl valid techniqu import like auto tune hpo maxim train score like lead overfit experi form model valid happen hood miss document guess think like xgboost auto tune instead singl set hyperparamet ye good point provid valid dataset eval argument optim metric comput train dataset increas likelihood produc overfit model includ entri eval argument xgboost train entri list dataset optim metric note like chat possibl extend exist xgboost call work xgboost train includ xgboost cvor gener workflow reach eddi mattia intel com schedul time discuss eval forgot definit improv thing max train score thank tip think ad help similar xgboost allow callabl pass dtrain eval split probabl hack ill reach chanc thank",
        "Answer_preprocessed_content":"api design follow api want us custom procedur need us point requir defin paramet space look guidanc construct paramet space reus default access constant xgboost thank feedback abil custom procedur nice abil form import like maxim train score like lead overfit experi form model valid happen hood miss document guess think like instead singl set hyperparamet ye good point provid valid dataset argument optim metric comput train dataset increas likelihood produc overfit model includ entri argument entri list dataset optim metric note like chat possibl extend exist call includ gener workflow reach schedul time discuss forgot definit improv thing max train score thank tip think ad help similar allow callabl pass split probabl hack ill reach chanc thank",
        "Answer_gpt_summary_original":"possible solutions from the answer include: \n- using the .create_experiment method to perform custom cross-validation, but defining the parameter space is required.\n- reusing the defaults of .xgboost to construct the parameter space.\n- including entries in the evals argument of xgboost.train to optimize for metrics computed on validation datasets, which reduces the likelihood of producing an overfit model.\n- extending the existing .xgboost calls to include xgboost.cv or more general cv workflows.",
        "Answer_gpt_summary":"possibl solut answer includ creat experi method perform custom cross valid defin paramet space requir reus default xgboost construct paramet space includ entri eval argument xgboost train optim metric comput valid dataset reduc likelihood produc overfit model extend exist xgboost call includ xgboost gener workflow"
    },
    {
        "Question_id":63469762.0,
        "Question_title":"Weights&Biases Sweep Keras K-Fold Validation",
        "Question_body":"<p>I'm using Weights&amp;Biases Cloud-based sweeps with Keras.\nSo first i create a new Sweep within a W&amp;B Project with a config like following:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>description: LSTM Model\nmethod: random\nmetric:\n  goal: maximize\n  name: val_accuracy\nname: LSTM-Sweep\nparameters:\n  batch_size:\n    distribution: int_uniform\n    max: 128\n    min: 32\n  epochs:\n    distribution: constant\n    value: 200\n  node_size1:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size2:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size3:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size4:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size5:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  num_layers:\n    distribution: categorical\n    values:\n    - 1\n    - 2\n    - 3\n  optimizer:\n    distribution: categorical\n    values:\n    - Adam\n    - Adamax\n    - Adagrad\n  path:\n    distribution: constant\n    value: &quot;.\/path\/to\/data\/&quot;\nprogram: sweep.py\nproject: SLR\n<\/code><\/pre>\n<p>My <code>sweep.py<\/code> file looks something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># imports\ninit = wandb.init(project=&quot;my-project&quot;, reinit=True)\nconfig = wandb.config\n\ndef main():\n    skfold = StratifiedKFold(n_splits=5, \n    shuffle=True, random_state=7)\n    cvscores = []\n    group_id = wandb.util.generate_id()\n    X,y = # load data\n    i = 0\n    for train, test in skfold.split(X,y):\n        i=i+1\n        run = wandb.init(group=group_id, reinit=True, name=group_id+&quot;#&quot;+str(i))\n        model = # build model\n        model.fit([...], WandBCallback())\n        cvscores.append([...])\n        wandb.join()\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>Starting this with the <code>wandb agent<\/code> command within the folder of <code>sweep.py<\/code>.<\/p>\n<p>What i experienced with this setup is, that with the first wandb.init() call a new run is initialized. Okay, i could just remove that. But when calling wandb.init() for the second time it seems to lose track of the sweep it is running in. Online an empty run is listed in the sweep (because of the first wandb.init() call), all other runs are listed inside the project, but not in the sweep.<\/p>\n<p>My goal is to have a run for each fold of the k-Fold cross-validation. At least i thought this would be the right way of doing this.\nIs there a different approach to combine sweeps with keras k-fold cross validation?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597757510213,
        "Question_favorite_count":null,
        "Question_last_edit_time":1661849871016,
        "Question_score":2.0,
        "Question_view_count":1043.0,
        "Answer_body":"<p>We put together an example of how to accomplish k-fold cross validation:<\/p>\n<p><a href=\"https:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/wandb-sweeps\/sweeps-cross-validation\" rel=\"nofollow noreferrer\">https:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/wandb-sweeps\/sweeps-cross-validation<\/a><\/p>\n<p>The solution requires some contortions for the wandb library to spawn multiple jobs on behalf of a launched sweep job.<\/p>\n<p>The basic idea is:<\/p>\n<ul>\n<li>The agent requests a new set of parameters from the cloud hosted parameter server.  This is the run called <code>sweep_run<\/code> in the main function.<\/li>\n<li>Send information about what the folds should process over a multiprocessing queue to waiting processes<\/li>\n<li>Each spawned process logs to their own run, organized with group and job_type to enable auto-grouping in the UI<\/li>\n<li>When the process is finished, it sends the primary metric over a queue to the parent sweep run<\/li>\n<li>The sweep run reads metrics from the child runs and logs it to the sweep run so that the sweep can use that result to impact future parameter choices and\/or hyperband early termination optimizations<\/li>\n<\/ul>\n<p>Example visualizations of the sweep and k-fold grouping can be seen here:<\/p>\n<ul>\n<li>Sweep: <a href=\"https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/sweeps\/vp0fsvku\" rel=\"nofollow noreferrer\">https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/sweeps\/vp0fsvku<\/a><\/li>\n<li>K-fold Grouping: <a href=\"https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/groups\/vp0fsvku\" rel=\"nofollow noreferrer\">https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/groups\/vp0fsvku<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1599772735680,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63469762",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1598032113043,
        "Question_original_content":"weight bias sweep kera fold valid weight bias cloud base sweep kera creat new sweep project config like follow descript lstm model method random metric goal maxim val accuraci lstm sweep paramet batch size distribut int uniform max min epoch distribut constant valu node size distribut categor valu node size distribut categor valu node size distribut categor valu node size distribut categor valu node size distribut categor valu num layer distribut categor valu optim distribut categor valu adam adamax adagrad path distribut constant valu path data program sweep project slr sweep file look like import init init project project reinit true config config def main skfold stratifiedkfold split shuffl true random state cvscore group util gener load data train test skfold split run init group group reinit true group str model build model model fit callback cvscore append join main main start agent command folder sweep experienc setup init new run initi okai remov call init second time lose track sweep run onlin run list sweep init run list insid project sweep goal run fold fold cross valid thought right wai differ approach combin sweep kera fold cross valid",
        "Question_preprocessed_content":"weight bias sweep kera valid weight bias sweep kera creat new sweep project config like follow file look like start command folder experienc setup init new run initi okai remov call init second time lose track sweep run onlin run list sweep run list insid project sweep goal run fold thought right wai differ approach combin sweep kera cross valid",
        "Question_gpt_summary_original":"The user is facing challenges while using Weights&Biases Cloud-based sweeps with Keras for k-fold cross-validation. The user has created a new Sweep within a W&B Project with a specific configuration, and the sweep.py file is also set up correctly. However, the user is facing issues with the wandb.init() call, which initializes a new run and loses track of the sweep it is running in. The user's goal is to have a run for each fold of the k-Fold cross-validation, but they are unsure if this is the right approach. The user is seeking advice on how to combine sweeps with Keras k-fold cross-validation.",
        "Question_gpt_summary":"user face challeng weight bias cloud base sweep kera fold cross valid user creat new sweep project specif configur sweep file set correctli user face issu init initi new run lose track sweep run user goal run fold fold cross valid unsur right approach user seek advic combin sweep kera fold cross valid",
        "Answer_original_content":"exampl accomplish fold cross valid http github com exampl tree master exampl sweep sweep cross valid solut requir contort librari spawn multipl job behalf launch sweep job basic idea agent request new set paramet cloud host paramet server run call sweep run main function send inform fold process multiprocess queue wait process spawn process log run organ group job type enabl auto group process finish send primari metric queue parent sweep run sweep run read metric child run log sweep run sweep us result impact futur paramet choic hyperband earli termin optim exampl visual sweep fold group seen sweep http app jeffr exampl sweep cross valid sweep vpfsvku fold group http app jeffr exampl sweep cross valid group vpfsvku",
        "Answer_preprocessed_content":"exampl accomplish cross valid solut requir contort librari spawn multipl job behalf launch sweep job basic idea agent request new set paramet cloud host paramet server run call main function send inform fold process multiprocess queue wait process spawn process log run organ group enabl process finish send primari metric queue parent sweep run sweep run read metric child run log sweep run sweep us result impact futur paramet choic hyperband earli termin optim exampl visual sweep group seen sweep group",
        "Answer_gpt_summary_original":"the solution to the challenge of using  cloud-based sweeps with keras k-fold cross validation requires some contortions for the library to spawn multiple jobs on behalf of a launched sweep job. the solution involves requesting a new set of parameters from the cloud hosted parameter server, sending information about what the folds should process over a multiprocessing queue to waiting processes, logging each spawned process to their own run, and sending the primary metric over a queue to the parent sweep run. the sweep run reads metrics from the child runs and logs it to the sweep run so that the sweep can use that result to impact future parameter choices and\/or hyperband early termination optimizations. example visualizations of the sweep and k-fold grouping can be seen in the provided links.",
        "Answer_gpt_summary":"solut challeng cloud base sweep kera fold cross valid requir contort librari spawn multipl job behalf launch sweep job solut involv request new set paramet cloud host paramet server send inform fold process multiprocess queue wait process log spawn process run send primari metric queue parent sweep run sweep run read metric child run log sweep run sweep us result impact futur paramet choic hyperband earli termin optim exampl visual sweep fold group seen provid link"
    },
    {
        "Question_id":null,
        "Question_title":"Commercial usage of Google Cloud TTS",
        "Question_body":"Hi,I wish to use Google cloud's Wavenet TTS (TextToSpeech) for commercial use for my company. Can anyone please confirm whether it is allowed or not?RegardsUtkarsh Dubey",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1661107680000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":117.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Commercial-usage-of-Google-Cloud-TTS\/td-p\/458420\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-21T22:59:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"I am sure its not allowed, you can use it only if you choose to use for informative or project purpose, if you wanna use it for commercial purpose get it by being in touch with google cloud partner coordinator here.\u00a0\u00a0Enroll and proceed with further steps.\n\n- Nihal"
            },
            {
                "Answer_creation_time":"2022-08-22T00:04:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"What is covered under informative\/project use?\nBy commercial use, i meant we will be using it to develop voice-over for our videos. The videos will be used in marketing. Need confirmation on wether it allowed or not."
            },
            {
                "Answer_creation_time":"2022-08-30T10:54:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"I actually have this exact same question. Someone from chat support was meant to get back to me but I never heard back"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"commerci usag googl cloud tt wish us googl cloud wavenet tt texttospeech commerci us compani confirm allow regardsutkarsh dubei",
        "Question_preprocessed_content":"commerci usag googl cloud tt wish us googl cloud wavenet tt commerci us compani confirm allow regardsutkarsh dubei",
        "Question_gpt_summary_original":"The user is seeking confirmation on whether it is allowed to use Google Cloud's Wavenet TTS for commercial use for their company.",
        "Question_gpt_summary":"user seek confirm allow us googl cloud wavenet tt commerci us compani",
        "Answer_original_content":"sure allow us choos us inform project purpos wanna us commerci purpos touch googl cloud partner coordin enrol proce step nihal cover inform project us commerci us meant develop voic video video market need confirm wether allow actual exact question chat support meant heard",
        "Answer_preprocessed_content":"sure allow us choos us inform project purpos wanna us commerci purpos touch googl cloud partner coordin proce step nihal cover us commerci us meant develop video video market need confirm wether allow actual exact question chat support meant heard",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n- wavenet tts can only be used for informative or project purposes.\n- if the user wants to use it for commercial purposes, they need to get in touch with google cloud partner coordinator and enroll in further steps.\n- it is not allowed to use wavenet tts for commercial purposes without proper authorization.\n- the answer does not provide a clear definition of what is covered under informative\/project use.",
        "Answer_gpt_summary":"possibl solut answer wavenet tt inform project purpos user want us commerci purpos need touch googl cloud partner coordin enrol step allow us wavenet tt commerci purpos proper author answer provid clear definit cover inform project us"
    },
    {
        "Question_id":null,
        "Question_title":"Documentation: tutorial problem?",
        "Question_body":"<p>I understand this may not be fully related to dvc, but since the problem happens when following the tutorial at <a href=\"https:\/\/blog.dataversioncontrol.com\/data-version-control-tutorial-9146715eda46\" rel=\"nofollow noopener\">https:\/\/blog.dataversioncontrol.com\/data-version-control-tutorial-9146715eda46<\/a>. So here is my question:<\/p>\n<p>When following tutorial at the step in executing<\/p>\n<pre><code>dvc run -d data\/Posts.tsv -d code\/split_train_test.py         -d code\/conf.py         -o data\/Posts-test.tsv -o data\/Posts-train.tsv         python code\/split_train_test.py 0.33 20180319\n<\/code><\/pre>\n<p>it throws error<\/p>\n<pre><code>from ._sparsetools import csr_tocsc, csr_tobsr, csr_count_blocks, \\\nImportError: \/tmp\/_MEIUqCWxh\/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by \/usr\/lib\/python2.7\/dist-packages\/scipy\/sparse\/_sparsetools.x86_64-linux-gnu.so)\nFailed to run command: Stage 'Posts-test.tsv.dvc' cmd python code\/split_train_test.py 0.33 20180319 failed\n<\/code><\/pre>\n<p>I am not familiar with python, nor data science, but was just trying to evaluate if dvc fits our internal requirement so we can decide if going with dvc or not.<\/p>\n<p>How can I fix this error? Otherwise any even simpler version that can basically just show dataset, model are versioned so we can see the differences, say, between version 0.0.1 and 0.0.2 and its diff, or that kind of things?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1530112347491,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":686.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/documentation-tutorial-problem\/40",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-06-27T15:38:09.869Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jtodd5527\">@jtodd5527<\/a> !<\/p>\n<p>The error you are encountering seems to be the same as in <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/749\">https:\/\/github.com\/iterative\/dvc\/issues\/749<\/a> . The problem is that previous versions of dvc didn\u2019t preserve the shell you are running if it is not the default shell for your user. Could you please check that the shell you are running matches the default one for your user? I.e. these two commands should show the same shell:<\/p>\n<pre><code class=\"lang-auto\">$ echo $0\n# \/bin\/zsh\n$ grep $USER \/etc\/passwd\n# efiop:x:1000:1000:efiop:\/home\/efiop:\/bin\/zsh\n<\/code><\/pre>\n<p>If shells don\u2019t match, you could try running <code>chsh -s $(echo $0) $USER<\/code> to set the current shell as a default. After that you could try running your <code>dvc run<\/code> command once again. Please let us know if it worked for you.<\/p>\n<p>The fix for that issue has been merged into master and going to be released in 0.10.0(end of the next week).<\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":11.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-06-28T08:25:56.126Z",
                "Answer_body":"<p>Not sure if that matches or not<\/p>\n<pre><code>$ echo $0\nbash\n$ grep $USER \/etc\/passwd\njtodd:x:1000:1000:jtodd,,,:\/home\/jtodd:\/bin\/bash\n<\/code><\/pre>\n<p>Executing after setting with <code>chsh -s $(echo $0)<\/code>, it complains <code>chsh: bash is an invalid shell<\/code>. Setting <code>chsh -s \/bin\/$(echo $0)<\/code> works; however executing <code>dvc run ...<\/code> still throws <code>ImportError: \/tmp\/_MEIFQ9LXk\/libstdc++.so.6: version<\/code>GLIBCXX_3.4.21\u2019 not found`.<\/p>\n<p>I will also try the release next week maybe that\u2019d be fixed with that version. Thanks<\/p>",
                "Answer_score":26.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-06-28T11:28:19.774Z",
                "Answer_body":"<p>Looks like there is still a problem with the environment <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/frowning.png?v=5\" title=\":frowning:\" class=\"emoji\" alt=\":frowning:\"> Does running <code>python code\/split_train_test.py 0.33 20180319<\/code> without <code>dvc run<\/code> work for you?<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-06-28T12:21:55.624Z",
                "Answer_body":"<p>Oh. Look like it\u2019s merely my local environment problem. I was doing some other python tasks. And before this run, I removed .local and then just reinstall related packages. The execution works fine now.<\/p>\n<pre><code>dvc run -d data\/Posts.tsv -d code\/split_train_test.py         -d code\/conf.py         -o data\/Posts-test.tsv -o data\/Posts-train.tsv         python code\/split_train_test.py 0.33 20180319\nUsing 'Posts-test.tsv.dvc' as a stage file\nReproducing 'Posts-test.tsv.dvc':\n    python code\/split_train_test.py 0.33 20180319\nPositive size 2049, negative size 97951\n<\/code><\/pre>\n<p>Sorry for creating confusion. And really appreciate your assistance!<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-06-28T12:23:37.564Z",
                "Answer_body":"<p>No worries <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=5\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> Glad it resolved itself.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"document tutori problem understand fulli relat problem happen follow tutori http blog dataversioncontrol com data version control tutori eda question follow tutori step execut run data post tsv code split train test code conf data post test tsv data post train tsv python code split train test throw error sparsetool import csr tocsc csr tobsr csr count block importerror tmp meiuqcwxh libstdc version glibcxx requir usr lib python dist packag scipi spars sparsetool linux gnu fail run command stage post test tsv cmd python code split train test fail familiar python data scienc try evalu fit intern requir decid go fix error simpler version basic dataset model version differ version diff kind thing thank",
        "Question_preprocessed_content":"document tutori problem understand fulli relat problem happen follow tutori question follow tutori step execut throw error familiar python data scienc try evalu fit intern requir decid go fix error simpler version basic dataset model version differ version diff kind thing thank",
        "Question_gpt_summary_original":"The user encountered an error while following a tutorial on data version control (DVC) and is seeking help to fix the issue. The error occurred while executing a command in Python and the user is not familiar with Python or data science. The user is also looking for a simpler version of DVC that can show the differences between versions of a dataset or model.",
        "Question_gpt_summary":"user encount error follow tutori data version control seek help fix issu error occur execut command python user familiar python data scienc user look simpler version differ version dataset model",
        "Answer_original_content":"jtodd error encount http github com iter issu problem previou version didnt preserv shell run default shell user check shell run match default user command shell echo bin zsh grep user passwd efiop efiop home efiop bin zsh shell dont match try run chsh echo user set current shell default try run run command let know work fix issu merg master go releas end week thank ruslan sure match echo bash grep user passwd jtodd jtodd home jtodd bin bash execut set chsh echo complain chsh bash invalid shell set chsh bin echo work execut run throw importerror tmp meifqlxk libstdc versionglibcxx try releas week mayb thatd fix version thank look like problem environ run python code split train test run work look like mere local environ problem python task run remov local reinstal relat packag execut work fine run data post tsv code split train test code conf data post test tsv data post train tsv python code split train test post test tsv stage file reproduc post test tsv python code split train test posit size neg size sorri creat confus appreci assist worri glad resolv",
        "Answer_preprocessed_content":"error encount problem previou version didnt preserv shell run default shell user check shell run match default user command shell shell dont match try run set current shell default try run command let know work fix issu merg master go releas week thank ruslan sure match execut set complain set work execut throw try releas week mayb thatd fix version thank look like problem environ run work look like mere local environ problem python task run remov local reinstal relat packag execut work fine sorri creat confus appreci assist worri glad resolv",
        "Answer_gpt_summary_original":"the user encountered an error while following a tutorial to evaluate if data version control fits their internal requirements. the error was related to the shell not matching the default shell for the user. the solution was to check the shell and run chsh -s $(echo $0) $user to set the current shell as a default. the fix for the issue has been merged into master and will be released in 0.10.0. the user also encountered an import error, which was resolved by reinstalling related packages.",
        "Answer_gpt_summary":"user encount error follow tutori evalu data version control fit intern requir error relat shell match default shell user solut check shell run chsh echo user set current shell default fix issu merg master releas user encount import error resolv reinstal relat packag"
    },
    {
        "Question_id":null,
        "Question_title":"Storing the files to AWS S3",
        "Question_body":"<p>Is there a way we can save the tables to our own AWS S3 bucket ratherthan using W&amp;B storage.<br>\nCan we save all the .ckpt, .wand files that wandb logs to our AWS cloud storage.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1673440871244,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":29.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/storing-the-files-to-aws-s3\/3679",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-11T12:56:27.786Z",
                "Answer_body":"<p>Hi Sydney,<\/p>\n<p>Can you please help me understand how can it be done.\\<\/p>\n<p>Thanks &amp; Regards<\/p>\n<p>Prasanth Noel<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-11T14:02:26.002Z",
                "Answer_body":"<p>Hi Prasanth,<\/p>\n<p>Thanks for writing in! You can do that by using our reference artifacts, they allow you to track files saved outside the W&amp;B server. Also here you have a report showing an example on how to do this. Please let me know if this would be useful for you!<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-13T10:20:34.305Z",
                "Answer_body":"<p>Hi Prasanth,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"store file aw wai save tabl aw bucket ratherthan storag save ckpt wand file log aw cloud storag",
        "Question_preprocessed_content":"store file aw wai save tabl aw bucket ratherthan storag save ckpt wand file log aw cloud storag",
        "Question_gpt_summary_original":"The user is facing challenges with storing tables and files to their own AWS S3 bucket instead of using W&B storage. They are specifically looking for a way to save .ckpt and .wand files that are logged to their AWS cloud storage.",
        "Question_gpt_summary":"user face challeng store tabl file aw bucket instead storag specif look wai save ckpt wand file log aw cloud storag",
        "Answer_original_content":"sydnei help understand thank regard prasanth noel prasanth thank write refer artifact allow track file save outsid server report show exampl let know us best lui prasanth want follow support request heard let know assist issu resolv best lui",
        "Answer_preprocessed_content":"sydnei help understand thank regard prasanth noel prasanth thank write refer artifact allow track file save outsid server report show exampl let know us best lui prasanth want follow support request heard let know assist issu resolv best lui",
        "Answer_gpt_summary_original":"possible solutions: \n- use reference artifacts to track files saved outside the  server.\n- refer to the provided report for an example on how to store files to an aws s3 bucket.\n- follow up with the support team if further assistance is needed. \n\nsummary: the user can store their files to an aws s3 bucket by using reference artifacts or referring to the provided report. they can also follow up with the support team for further assistance.",
        "Answer_gpt_summary":"possibl solut us refer artifact track file save outsid server refer provid report exampl store file aw bucket follow support team assist need summari user store file aw bucket refer artifact refer provid report follow support team assist"
    },
    {
        "Question_id":50068941.0,
        "Question_title":"SageMaker Tensorflow - how to write my serving_input_fn()",
        "Question_body":"<p>I'm pretty new to Tensorflow and SageMaker and I'm trying to figure out how to write my <code>serving_input_fn()<\/code>. I've tried a number of ways to do it, but to no avail. <\/p>\n\n<p>my input function has 3 feature columns: <code>amount_normalized, x_month and y_month<\/code>:<\/p>\n\n<pre><code>def construct_feature_columns():\n    amount_normalized = tf.feature_column.numeric_column(key='amount_normalized')\n    x_month = tf.feature_column.numeric_column(key='x_month')\n    y_month = tf.feature_column.numeric_column(key='y_month')\n    return set([amount_normalized, x_month, y_month])\n<\/code><\/pre>\n\n<p>I want to be able to call my deployed model using something like <code>deployed_model.predict([1.23,0.3,0.8])<\/code> <\/p>\n\n<p>Where the first element is <code>amount_normalized<\/code>, second is <code>x_month<\/code> third is <code>y_month<\/code><\/p>\n\n<p>I've tried this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(params):\n    feature_placeholders = {\n      key : tf.placeholder(tf.float32, [None]) \\\n        for key in FEATURES\n    }\nreturn tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholders)()\n<\/code><\/pre>\n\n<p>But all I get is:\n<code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\".<\/code><\/p>\n\n<p>Any help would be <strong>really<\/strong> appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1524856485467,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1337.0,
        "Answer_body":"<p>Posting this here in case anyone else has this issue.<\/p>\n\n<p>After a bunch of trial and error I managed to solve my issue by writing my serving input function like this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(hyperparameters):\n    feature_spec = {\n        key : tf.FixedLenFeature(shape=[], dtype = tf.float32) \\\n          for key in FEATURES\n    }\n    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)()\n<\/code><\/pre>\n\n<p>I can then call my deployed model by passing in a hash:<\/p>\n\n<pre><code>deployed_model.predict({\"amount_normalized\": 2.3, \"x_month\": 0.2, \"y_month\": -0.3})\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50068941",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1525019634972,
        "Question_original_content":"tensorflow write serv input pretti new tensorflow try figur write serv input tri number wai avail input function featur column normal month month def construct featur column normal featur column numer column kei normal month featur column numer column kei month month featur column numer column kei month return set normal month month want abl deploi model like deploi model predict element normal second month month tri featur normal month month def serv input param featur placehold kei placehold float kei featur return estim export build raw serv input receiv featur placehold error occur modelerror call invokeendpoint oper receiv server error model messag help appreci",
        "Question_preprocessed_content":"tensorflow write pretti new tensorflow try figur write tri number wai avail input function featur column want abl deploi model like element second tri help appreci",
        "Question_gpt_summary_original":"The user is facing challenges in writing the serving_input_fn() for their Tensorflow model in SageMaker. They have tried different methods but have not been successful. The input function has three feature columns, and the user wants to call the deployed model using specific values for each column. The user has attempted to write the serving_input_fn() using placeholders, but they are encountering an error when calling the model.",
        "Question_gpt_summary":"user face challeng write serv input tensorflow model tri differ method success input function featur column user want deploi model specif valu column user attempt write serv input placehold encount error call model",
        "Answer_original_content":"post case issu bunch trial error manag solv issu write serv input function like featur normal month month def serv input hyperparamet featur spec kei fixedlenfeatur shape dtype float kei featur return estim export build pars serv input receiv featur spec deploi model pass hash deploi model predict normal month month",
        "Answer_preprocessed_content":"post case issu bunch trial error manag solv issu write serv input function like deploi model pass hash",
        "Answer_gpt_summary_original":"the solution to the challenge of writing a serving_input_fn() function for a tensorflow model with three feature columns is to define the feature columns as a list, create a feature specification dictionary, and use the build_parsing_serving_input_receiver_fn() function to return a serving input function. the deployed model can then be called by passing in a hash of the feature values.",
        "Answer_gpt_summary":"solut challeng write serv input function tensorflow model featur column defin featur column list creat featur specif dictionari us build pars serv input receiv function return serv input function deploi model call pass hash featur valu"
    },
    {
        "Question_id":36154971.0,
        "Question_title":"AZURE ML getting model weights",
        "Question_body":"<p>I have deployed a regression model on azure ML , is it possible to get the model weights\/coefficients of the model programatically from azure, rather than getting predicted value? .<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1458651323657,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":1044.0,
        "Answer_body":"<p>No. Currently we do not feature exporting weights from the models including with Azure Machine Learning. <\/p>\n\n<p>If you have a method for extracting weights from Python models, you may be able to work this out using the execute Python Script module.<\/p>\n\n<p>The primary purpose of Azure Machine Learning is to make deployable and scalable web services from the machine learning modules. Though the authoring experience for creating ML models is great, it is not intended to be a place to create and export models, but instead a place to create and operationalize your models. <\/p>\n\n<p><em>UPDATE<\/em> New features may make this answer outdated. <\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1495601794132,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36154971",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1458737377168,
        "Question_original_content":"get model weight deploi regress model possibl model weight coeffici model programat azur get predict valu",
        "Question_preprocessed_content":"get model weight deploi regress model possibl model model programat azur get predict valu",
        "Question_gpt_summary_original":"The user is facing a challenge in obtaining the model weights\/coefficients of a regression model deployed on Azure ML programmatically, instead of just getting the predicted value.",
        "Question_gpt_summary":"user face challeng obtain model weight coeffici regress model deploi programmat instead get predict valu",
        "Answer_original_content":"current featur export weight model includ method extract weight python model abl work execut python script modul primari purpos deploy scalabl web servic machin learn modul author experi creat model great intend place creat export model instead place creat operation model updat new featur answer outdat",
        "Answer_preprocessed_content":"current featur export weight model includ method extract weight python model abl work execut python script modul primari purpos deploy scalabl web servic machin learn modul author experi creat model great intend place creat export model instead place creat operation model updat new featur answer outdat",
        "Answer_gpt_summary_original":"there is currently no built-in way to export model weights\/coefficients from a deployed regression model in azure. however, if the user has a method for extracting weights from python models, they may be able to use the execute python script module to work it out. the primary purpose of azure is to create and operationalize machine learning models, not to create and export them. it is possible that new features may make this answer outdated in the future.",
        "Answer_gpt_summary":"current built wai export model weight coeffici deploi regress model azur user method extract weight python model abl us execut python script modul work primari purpos azur creat operation machin learn model creat export possibl new featur answer outdat futur"
    },
    {
        "Question_id":null,
        "Question_title":"Feature request: models only support single prediction column",
        "Question_body":"Many machine learning tasks involve input data in the form of scalars, vectors, matrices, and higher-order tensors. Similarly, the output predictions are often in the same format. The builtin NN regression model in Azure ML Designer only supports scalar predictions (single column of ground truth labels), but many tasks involve multiple columns. For example, you might be trying to predict mean and std dev, or a bounding box.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1594758597390,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/46949\/feature-request-models-only-support-single-predict.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-14T20:32:43.09Z",
                "Answer_score":0,
                "Answer_body":"Thanks for the feedback! We appreciate your input, I will forward your content to product group for reviewing. ^^\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":38.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"featur request model support singl predict column machin learn task involv input data form scalar vector matric higher order tensor similarli output predict format builtin regress model design support scalar predict singl column ground truth label task involv multipl column exampl try predict mean std dev bound box",
        "Question_preprocessed_content":"featur request model support singl predict column machin learn task involv input data form scalar vector matric tensor similarli output predict format builtin regress model design support scalar predict task involv multipl column exampl try predict mean std dev bound box",
        "Question_gpt_summary_original":"The user is facing a challenge with the Azure ML Designer's builtin NN regression model as it only supports single prediction columns, while many machine learning tasks involve multiple columns for input and output predictions. This limitation makes it difficult for the user to predict mean and standard deviation or bounding boxes.",
        "Question_gpt_summary":"user face challeng design builtin regress model support singl predict column machin learn task involv multipl column input output predict limit make difficult user predict mean standard deviat bound box",
        "Answer_original_content":"thank feedback appreci input forward content product group review regard yutong",
        "Answer_preprocessed_content":"thank feedback appreci input forward content product group review regard yutong",
        "Answer_gpt_summary_original":"there are no specific solutions mentioned in the answer. the user has provided feedback about a challenge they are facing with the builtin nn regression model in designer, which only supports scalar predictions. the answer acknowledges the feedback and states that it will be forwarded to the product group for reviewing.",
        "Answer_gpt_summary":"specif solut mention answer user provid feedback challeng face builtin regress model design support scalar predict answer acknowledg feedback state forward product group review"
    },
    {
        "Question_id":47847736.0,
        "Question_title":"AWS Sagemaker Neural Topic Model",
        "Question_body":"<p>What is underlying algorithm for Sagemaker's <a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ntm.html\" rel=\"nofollow noreferrer\">Neural Topic Model<\/a>? I have hard time googling for details, and the documentation doesn't mention any paper.<\/p>\n\n<p>Googling for 'neural topic model' doesn't exactly answer my question, since a couple of methods seems to be called that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1513441748563,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":222.0,
        "Answer_body":"<p>Seems like AWS SageMaker team answered the question, \n<a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=270493&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=270493&amp;tstart=0<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47847736",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1515260139876,
        "Question_original_content":"neural topic model underli algorithm neural topic model hard time googl detail document mention paper googl neural topic model exactli answer question coupl method call",
        "Question_preprocessed_content":"neural topic model underli algorithm neural topic model hard time googl detail document mention paper googl neural topic model exactli answer question coupl method call",
        "Question_gpt_summary_original":"The user is having difficulty finding information on the underlying algorithm for AWS Sagemaker's Neural Topic Model, as the documentation does not provide any paper or details, and searching for \"neural topic model\" yields multiple methods.",
        "Question_gpt_summary":"user have difficulti find inform underli algorithm neural topic model document provid paper detail search neural topic model yield multipl method",
        "Answer_original_content":"like team answer question http forum aw amazon com thread jspa threadid tstart",
        "Answer_preprocessed_content":"like team answer question",
        "Answer_gpt_summary_original":"possible solutions: the user can check the provided link to see if the amazon team has already answered their question about the neural topic model algorithm.",
        "Answer_gpt_summary":"possibl solut user check provid link amazon team answer question neural topic model algorithm"
    },
    {
        "Question_id":null,
        "Question_title":"Use GPU with Keras - Colab File Missing",
        "Question_body":"<p>Link: <a href=\"https:\/\/wandb.ai\/authors\/ayusht\/reports\/Use-GPUs-with-Keras--VmlldzoxNjEyNjE?_gl=1%2A1uwdrw1%2A_ga%2AMTA2NjE2OTcwOC4xNjYwMDgxMDY1%2A_ga_JH1SJHJQXJ%2AMTY2MDA4MTA2NC4xLjEuMTY2MDA4NTg2MC41Mg\" class=\"inline-onebox\">Weights &amp; Biases<\/a>\u2026<\/p>\n<h1>\n<a name=\"using-gpus-with-keras-a-tutorial-with-code-1\" class=\"anchor\" href=\"#using-gpus-with-keras-a-tutorial-with-code-1\"><\/a>Using GPUs With Keras: A Tutorial With Code<\/h1>\n<p>I am following this article to Monitor GPU and CPU performances but Colab file is missing. I wish to see this tutorial Can you help in this regard<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1660086055369,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":75.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/use-gpu-with-keras-colab-file-missing\/2879",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-12T16:42:51.213Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/alishafique\">@alishafique<\/a> thank you for reporting this broken link! Please find the Colab notebook of this article <a href=\"https:\/\/colab.research.google.com\/github\/ayulockin\/DLshots\/blob\/master\/Keras_GPU_Growth.ipynb\" rel=\"noopener nofollow ugc\">here<\/a>. Would this open for you?<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-19T13:28:20.719Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/alishafique\">@alishafique<\/a> were you able to open the Colab notebook from the link above? Let me know if I can help further with this.<\/p>",
                "Answer_score":10.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T00:53:20.012Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/alishafique\">@alishafique<\/a> , since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-24T00:53:35.643Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"us gpu kera colab file miss link gpu kera tutori code follow articl monitor gpu cpu perform colab file miss wish tutori help regard",
        "Question_preprocessed_content":"us gpu kera colab file miss link gpu kera tutori code follow articl monitor gpu cpu perform colab file miss wish tutori help regard",
        "Question_gpt_summary_original":"The user is facing a challenge in following an article on monitoring GPU and CPU performances using Keras. The Colab file required for the tutorial is missing, and the user is seeking assistance in accessing it.",
        "Question_gpt_summary":"user face challeng follow articl monitor gpu cpu perform kera colab file requir tutori miss user seek assist access",
        "Answer_original_content":"alishafiqu thank report broken link colab notebook articl open alishafiqu abl open colab notebook link let know help alishafiqu heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"thank report broken link colab notebook articl open abl open colab notebook link let know help heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the answer provides a link to a colab notebook that the user can use to complete the tutorial. if the user is still having issues, they can reach out for further help.",
        "Answer_gpt_summary":"answer provid link colab notebook user us complet tutori user have issu reach help"
    },
    {
        "Question_id":null,
        "Question_title":"How to get logs or print statements from SageMaker PyTorch deployed endpoint?",
        "Question_body":"I've deployed an extended Pytorch model as an endpoint and I'm trying to make inference requests to it. Problem is, the responses from the endpoint get timed out and CloudWatch logs show nothing beyond:\n\ntimestamp\tmessage\n1661544743589\tWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n1661544749569\tModel server started.\n\nNow in my inference.py file, which I provided as the entry point I've set logging as follows:\n\nimport logging\nimport sys\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.StreamHandler(sys.stdout))\nlogger.info(\"Loading file.\")\nprint(\"Loading file.\")\n\n\nI wish to see those logs\/prints. How can I accomplish that?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1661549640227,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":48.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU74MThjkyRVCtySw-DEozrQ\/how-to-get-logs-or-print-statements-from-sage-maker-py-torch-deployed-endpoint",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-30T16:47:09.082Z",
                "Answer_score":0,
                "Answer_body":"I've come across similar issues in the past of log messages not making it through to CloudWatch, and can suggest setting environment variable PYTHONUNBUFFERED=1 (discussed further here on StackOverflow wrt containerized Python in general).\n\nThe procedure for this may vary a little depending how you're creating your model, endpoint config & endpoint (e.g. direct boto3\/API calls, SageMaker SDK Estimator.deploy() or PyTorchModel). PyTorchModel should accept an env={\"PYTHONUNBUFFERED\": \"1\"} constructor argument for example.\n\nIf you are using the SageMaker Python SDK, do watch out that some methods (especially shortcuts like Estimator.deploy()) may re-use existing models & endpoint configs rather than re-creating each time they're run. Check you see the environment variable set in the SageMaker > Inference > Models > {Your Model Name} details page in AWS Console, and run DeleteModel first if needed to force an update!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-30T16:30:29.746Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nPlease note that you can view the logs under CloudWatch logs. There would be a generated log stream under your pytorch-inference tab when it is inService (Navigate to inference tab under SageMaker console > Endpoints > click on the endpoint name > locate \"view logs\"). The previous steps will take you to CloudWatch logs console. You then click on log groups and locate \/aws\/sagemaker\/Endpoints\/pytorch-inference-YYYY-MM-DD-HH-MM-SS-sss > AllTraffic\/i-instanceId. For example, for the code snippet you shared, If you add the lines of code below for logging purpose within the inference.py script):\n\nimport logging\nimport sys\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.StreamHandler(sys.stdout))\nlogger.info(\"Loading file.\")\nprint(\"Loading file. --> from print statement\")\n\n# rest of the inference script from here\n\n\nThe above will show up under AllTraffic\/instance-id (once the endpoint is inService) as the following:\n\n2022-08-30 15:48:42,183 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Loading file.\n2022-08-30 15:48:42,936 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Loading file. --> from print statement\n\n\nFrom above, you can see that print statements will show up as INFO level log.\n\nPlease see the link [1] for more details on how the log stream is captured for inference jobs. If you have logging statements within your input_fn or say within your predict_fn function, those statements will show up when there is a prediction\/scoring made. I hope the shared information is helpful.\n\nReference: [1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-logs-metrics.html#inference-pipeline-logs\n\nplease see @Alex_T answer for PYTHONUNBUFFERED=1 which forces print\/logging to stdout. If you are bringing your own container you can define this parameter in the Dockerfile.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"log print statement pytorch deploi endpoint deploi extend pytorch model endpoint try infer request problem respons endpoint time cloudwatch log timestamp messag warn sun reflect reflect getcallerclass support impact perform model server start infer file provid entri point set log follow import log import sy logger log getlogg logger setlevel log debug logger addhandl log streamhandl sy stdout logger info load file print load file wish log print accomplish",
        "Question_preprocessed_content":"log print statement pytorch deploi endpoint deploi extend pytorch model endpoint try infer request problem respons endpoint time cloudwatch log timestamp messag warn support impact perform model server start file provid entri point set log follow import log import sy logger print wish accomplish",
        "Question_gpt_summary_original":"The user has deployed a PyTorch model as an endpoint on SageMaker and is facing challenges in getting logs or print statements from the endpoint. The responses from the endpoint are getting timed out and the CloudWatch logs show only two messages. The user has set logging in the inference.py file but is unable to see the logs\/prints. The user is seeking guidance on how to view the logs\/prints.",
        "Question_gpt_summary":"user deploi pytorch model endpoint face challeng get log print statement endpoint respons endpoint get time cloudwatch log messag user set log infer file unabl log print user seek guidanc view log print",
        "Answer_original_content":"come similar issu past log messag make cloudwatch suggest set environ variabl pythonunbuff discuss stackoverflow wrt container python gener procedur vari littl depend creat model endpoint config endpoint direct boto api call sdk estim deploi pytorchmodel pytorchmodel accept env pythonunbuff constructor argument exampl python sdk watch method especi shortcut like estim deploi us exist model endpoint config creat time run check environ variabl set infer model model detail page aw consol run deletemodel need forc updat hello note view log cloudwatch log gener log stream pytorch infer tab inservic navig infer tab consol endpoint click endpoint locat view log previou step cloudwatch log consol click log group locat aw endpoint pytorch infer alltraff instanceid exampl code snippet share add line code log purpos infer script import log import sy logger log getlogg logger setlevel log debug logger addhandl log streamhandl sy stdout logger info load file print load file print statement rest infer script alltraff instanc endpoint inservic follow info model stdout com amazonaw mm wlm workerlifecycl load file info model stdout com amazonaw mm wlm workerlifecycl load file print statement print statement info level log link detail log stream captur infer job log statement input predict function statement predict score hope share inform help refer http doc aw amazon com latest infer pipelin log metric html infer pipelin log alex answer pythonunbuff forc print log stdout bring contain defin paramet dockerfil",
        "Answer_preprocessed_content":"come similar issu past log messag make cloudwatch suggest set environ variabl pythonunbuff procedur vari littl depend creat model endpoint config endpoint pytorchmodel pytorchmodel accept env pythonunbuff constructor argument exampl python sdk watch method exist model endpoint config time run check environ variabl set infer model detail page aw consol run deletemodel need forc updat hello note view log cloudwatch log gener log stream tab inservic previou step cloudwatch log consol click log group locat exampl code snippet share add line code log purpos script import log import sy logger print rest infer script follow load file load file print statement print statement info level log link detail log stream captur infer job log statement function statement hope share inform help refer answer pythonunbuff forc stdout bring contain defin paramet dockerfil",
        "Answer_gpt_summary_original":"possible solutions to the challenge of not being able to see logs or print statements from a pytorch deployed endpoint include setting the environment variable pythonunbuffered=1, checking that the environment variable is set in the inference > models > {your model name} details page in aws console, and viewing the logs under cloudwatch logs. additionally, adding logging statements within the input_fn or predict_fn function can help show logs when there is a prediction\/scoring made. the link provided in the answer gives more details on how the log stream is captured for inference jobs.",
        "Answer_gpt_summary":"possibl solut challeng abl log print statement pytorch deploi endpoint includ set environ variabl pythonunbuff check environ variabl set infer model model detail page aw consol view log cloudwatch log addition ad log statement input predict function help log predict score link provid answer give detail log stream captur infer job"
    },
    {
        "Question_id":null,
        "Question_title":"Strange out-of-memory behavior on Guild with XGBoost",
        "Question_body":"<p>Hi all, I\u2019m using Guild to manage and tune XGBoost models for a binary classification problem. My dataset is around 2MB and has around 20K rows of 15 features. My XGBClassifier has around 100 estimators, the max depth is 6, and the tree method is <code>gpu-hist<\/code>.<\/p>\n<p>When I run my program in VS Code it executes with no problems. When I run the same program in the command line with Guild, it also finishes without throwing any error. But when I look at the run in <code>guild view<\/code> or <code>guild runs<\/code>, it says that the run exited with an error status <code>3221226505<\/code>.<\/p>\n<p>Online sources say this generic python error is some form of out of memory error. However, this can\u2019t be the case as I monitored the RAM and VRAM usage while executing my program and they were both very low.<\/p>\n<p>When I switch the tree method to just <code>hist<\/code> (cpu-only training) and re-run the program, <code>guild view<\/code> now shows the run as <code>completed<\/code>.<\/p>\n<p>May I know if this is a bug? My GPU is a Quadro T1000, my XGBoost version is <code>1.6.2<\/code>, and here is part of the <code>guild check<\/code> output:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild_version:             0.8.1\npython_version:            3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]                            \nplatform:                  Windows 10 AMD64                                                                               \npsutil_version:            5.9.2\ntensorboard_version:       2.10.1\ncuda_version:              11.7\nnvidia_smi_version:        516.69\nlatest_guild_version:      0.8.1   \n<\/code><\/pre>\n<p>Thank you!<\/p>",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_creation_time":1665986502533,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":143.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/strange-out-of-memory-behavior-on-guild-with-xgboost\/931",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-17T17:23:57.599Z",
                "Answer_body":"<p>These resource-related issues can be tough to track down.<\/p>\n<p>Do you see this exit code consistently (i.e. every time) when you run with Guild? These OOM issues are sometimes intermittent.<\/p>\n<p>A solid way to test for this is to reboot your system (sorry, I know that\u2019s a common trope!) and then to immediately run the offending command in Guild. This would provide an environment that might have more available memory.<\/p>\n<p>If Guild consistently triggers this, you can disable some potential causes using a few environment variables:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">set GUILD_PLUGINS= \nset LOG_INIT_SKIP=1 \nset NO_RUN_OUTPUT=1\nguild run ...\n<\/code><\/pre>\n<p>With this , Guild is just passing through the command to Python and will not do any additional work. If there\u2019s a memory related issue with Guild, that should fix it. If that works, please let me know and we can further track the issue down.<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-18T08:52:28.675Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/garrett\">@garrett<\/a>, thanks for the suggestions!<\/p>\n<p>Yes, every time I use the <code>gpu_hist<\/code> tree method and run my experiment with Guild, the issue persists. (i.e. the exit code doesn\u2019t appear on the console output, but appears in <code>guild view<\/code> and <code>guild runs<\/code>. If I run the experiment with a hyperparameter optimizer, the error does get logged to the console though) Rebooting and running the command straightaway\/Setting the environment variables also didn\u2019t work.<\/p>\n<p>I put a print statement at the end of my program and ran it in VSCode as well as Guild. In VSCode, the program exits almost immediately after the print. In Guild, there is a significant delay of around 4 seconds after the print before the program exits.<\/p>\n<p>Perhaps the issue is in Guild\u2019s cleanup? Or is there an environment variable I can set to disable that too?<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-18T15:54:16.320Z",
                "Answer_body":"<p>You may be running into the same behavior when you run from the command line or from the VS Code terminal. Exit codes aren\u2019t printed anywhere so you could be getting a resource-related exit without knowing it, regardless of whether Guild is used.<\/p>\n<p>If you running from the Windows prompt (I recommend this, as opposed to a VS Code terminal, at least for this test), run this after running your code with Python directly (don\u2019t use Guild):<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">echo %errorlevel%\n<\/code><\/pre>\n<p>If you get a non-zero exit code here, it\u2019s your code \u2014 something\u2019s eating up something when you use gpu-hist.<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-19T00:21:14.325Z",
                "Answer_body":"<p>I just tried this. <code>echo %errorlevel%<\/code> returns <code>0<\/code> after I run the Python file directly in the command line.<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-19T15:22:11.352Z",
                "Answer_body":"<p>How are you running the operation with Guild? Are you running a Python script directly or are you using a Guild file with a <code>main<\/code> spec for the operation?<\/p>\n<p>You can remove Guild\u2019s Python support from the mix by running your operation this way:<\/p>\n<pre><code class=\"lang-yaml\">test:\n  exec: python .guild\/sourcecode\/test.py\n  output-scalars: off\n  plugins: off\n<\/code><\/pre>\n<p>This is as close to a pass-through to your code as possible.<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-26T01:54:31.063Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/garrett\">@garrett<\/a>, sorry for the late response!!<\/p>\n<p>I am using a Guild file in the operation-only format. My XGBoost operation looks like this (I only included 2 of my flags, I have a total of 12)<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">XGBClassifier:\n  description: Train XGBClassifier on dataset\n  main: XGBClassifier\n  flags:\n    N_ESTIMATORS:\n      type: int\n    MAX_DEPTH:\n      type: int\n  output-scalars:\n    AVG_ACC: 'AVG_ACC: (\\value)'\n    AVG_SENS: 'AVG_SENS: (\\value)'\n    AVG_SPEC: 'AVG_SPEC: (\\value)'\n<\/code><\/pre>\n<p>Then I run the experiment with<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild run XGBClassifier\n<\/code><\/pre>\n<p>After trying your suggestion, the run now shows up as \u2018completed\u2019 in <code>guild runs<\/code> . When I restore the <code>output-scalars<\/code> and remove <code>plugins: off<\/code>, there is also no issue. So the problem likely stems from <code>main: XGBClassifier<\/code> and the large number of flags.<\/p>\n<p>However, I also noticed that the console started to output my print statements in a very jerky manner once I implemented your suggestion. The text would print, but it would intermittently appear as a lot of lines at one shot, instead of coming out line by line.<\/p>\n<p>Thank you!<\/p>",
                "Answer_score":10.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-10T05:13:26.342Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/garrett\">@garrett<\/a> , sorry for the trouble, may I know if there\u2019s any update on this? Thank you very much!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"strang memori behavior xgboost manag tune xgboost model binari classif problem dataset row featur xgbclassifi estim max depth tree method gpu hist run program code execut problem run program command line finish throw error look run view run sai run exit error statu onlin sourc gener python error form memori error case monitor ram vram usag execut program low switch tree method hist cpu train run program view show run complet know bug gpu quadro xgboost version check output version python version default mar msc bit amd platform window amd psutil version tensorboard version cuda version nvidia smi version latest version thank",
        "Question_preprocessed_content":"strang behavior xgboost manag tune xgboost model binari classif problem dataset row featur xgbclassifi estim max depth tree method run program code execut problem run program command line finish throw error look run sai run exit error statu onlin sourc gener python error form memori error case monitor ram vram usag execut program low switch tree method program show run know bug gpu quadro xgboost version output thank",
        "Question_gpt_summary_original":"The user is encountering an error status 3221226505 when running an XGBoost model for a binary classification problem using Guild. The error is believed to be an out of memory error, but the RAM and VRAM usage were both low. The issue seems to be related to the tree method, as switching to cpu-only training allows the program to complete. The user is unsure if this is a bug and provides information about their GPU, XGBoost version, and Guild check output.",
        "Question_gpt_summary":"user encount error statu run xgboost model binari classif problem error believ memori error ram vram usag low issu relat tree method switch cpu train allow program complet user unsur bug provid inform gpu xgboost version check output",
        "Answer_original_content":"resourc relat issu tough track exit code consist time run oom issu intermitt solid wai test reboot sorri know that common trope immedi run offend command provid environ avail memori consist trigger disabl potenti caus environ variabl set plugin set log init skip set run output run pass command python addit work there memori relat issu fix work let know track issu garrett thank suggest ye time us gpu hist tree method run experi issu persist exit code doesnt appear consol output appear view run run experi hyperparamet optim error log consol reboot run command straightawai set environ variabl didnt work print statement end program ran vscode vscode program exit immedi print signific delai second print program exit issu cleanup environ variabl set disabl run behavior run command line code termin exit code arent print get resourc relat exit know regardless run window prompt recommend oppos code termin test run run code python directli dont us echo errorlevel non zero exit code code someth eat us gpu hist tri echo errorlevel return run python file directli command line run oper run python script directli file main spec oper remov python support mix run oper wai test exec python sourcecod test output scalar plugin close pass code possibl garrett sorri late respons file oper format xgboost oper look like includ flag total xgbclassifi descript train xgbclassifi dataset main xgbclassifi flag estim type int max depth type int output scalar avg acc avg acc valu avg sen avg sen valu avg spec avg spec valu run experi run xgbclassifi try suggest run show complet run restor output scalar remov plugin issu problem like stem main xgbclassifi larg number flag notic consol start output print statement jerki manner implement suggest text print intermitt appear lot line shot instead come line line thank garrett sorri troubl know there updat thank",
        "Answer_preprocessed_content":"issu tough track exit code consist run oom issu intermitt solid wai test reboot immedi run offend command provid environ avail memori consist trigger disabl potenti caus environ variabl pass command python addit work there memori relat issu fix work let know track issu thank suggest ye time us tree method run experi issu persist reboot run command environ variabl didnt work print statement end program ran vscode vscode program exit immedi print signific delai second print program exit issu cleanup environ variabl set disabl run behavior run command line code termin exit code arent print get exit know regardless run window prompt run run code python directli exit code code someth eat us tri return run python file directli command line run oper run python script directli file spec oper remov python support mix run oper wai close code possibl sorri late respons file format xgboost oper look like run experi try suggest run show complet restor remov issu problem like stem larg number flag notic consol start output print statement jerki manner implement suggest text print intermitt appear lot line shot instead come line line thank sorri troubl know there updat thank",
        "Answer_gpt_summary_original":"possible solutions to strange out-of-memory behavior when running an xgboost model with guild despite low ram and vram usage include rebooting the system and immediately running the offending command in guild, disabling potential causes using environment variables, and removing guild's python support from the mix. the issue may stem from the main xgboost operation and the large number of flags. it is also recommended to run the operation from the windows prompt and check for non-zero exit codes.",
        "Answer_gpt_summary":"possibl solut strang memori behavior run xgboost model despit low ram vram usag includ reboot immedi run offend command disabl potenti caus environ variabl remov python support mix issu stem main xgboost oper larg number flag recommend run oper window prompt check non zero exit code"
    },
    {
        "Question_id":null,
        "Question_title":"Matching Engine: Queries with filtering do not work as expected",
        "Question_body":"I tried to run this notebook: https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/matching_engi...\nI got an issue with the filtering step:\nLet's say glove100.json is:If I try to filter in this way:I do not get any response (empty result). If from the above code I remove the \"restrict block\", it works (of course without filtering).response:But, if I add a new vector by Vertex SDK for Python (link ), in this way:and then I try to filter in the same way above with filter class == \"3\", I get the right response.It seems like the allow_tokens are \"seen\" by vertex only when I insert a new vector by Vertex SDK and not when I specify them in the initial glove100.json.Moreover, if I update a datapoint where the filter did not work, for example id=1:The filter for class=1 starts to work.Is there a way to know what is actually stored in the index? I mean a kind of \"SELECT * FROM myindex\" in order to check embeddings and tokens stored.Any ideas on how to solve this issue?Thanks in advanceSpecificationsI tried from local and from workbench, the result is the same.- Version: Python 3.7.9\n- Platform: Matching Engine. zone: europe-west1",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1668065460000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":75.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Matching-Engine-Queries-with-filtering-do-not-work-as-expected\/td-p\/488086\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-11T14:02:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi,\n\nThank you for reaching out, you can create a public issue tracker for this question since it might be an unexpected behavior and you have a workaround to show for."
            },
            {
                "Answer_creation_time":"2022-11-15T02:58:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi\u00a0@ricconoel\u00a0,\nThank you for replying!\u00a0\n\nSince I do not see a \"Matching Engine\" section,\u00a0 in which one do you suggest to open an issue?"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"match engin queri filter work expect tri run notebook http github com googlecloudplatform vertex sampl blob main notebook commun match engi got issu filter step let glove json try filter wai respons result code remov restrict block work cours filter respons add new vector vertex sdk python link wai try filter wai filter class right respons like allow token seen vertex insert new vector vertex sdk specifi initi glove json updat datapoint filter work exampl filter class start work wai know actual store index mean kind select myindex order check embed token store idea solv issu thank advancespecificationsi tri local workbench result version python platform match engin zone europ west",
        "Question_preprocessed_content":"match engin queri filter work expect tri run notebook got issu filter step let try filter wai respons code remov restrict block work respons add new vector vertex sdk python wai try filter wai filter class right like seen vertex insert new vector vertex sdk specifi initi updat datapoint filter work exampl filter class start wai know actual store index mean kind select myindex order check embed token idea solv issu thank advancespecificationsi tri local workbench result version python platform match engin zone",
        "Question_gpt_summary_original":"The user encountered challenges with the filtering step while running a notebook on Matching Engine. The filtering did not work as expected when using the initial glove100.json file, but worked when a new vector was added using Vertex SDK for Python. The allow_tokens were only recognized by Vertex when a new vector was added, and not when specified in the initial file. Updating a datapoint where the filter did not work made the filter for that class work. The user is seeking a way to check what is stored in the index and looking for ideas to solve the issue.",
        "Question_gpt_summary":"user encount challeng filter step run notebook match engin filter work expect initi glove json file work new vector ad vertex sdk python allow token recogn vertex new vector ad specifi initi file updat datapoint filter work filter class work user seek wai check store index look idea solv issu",
        "Answer_original_content":"thank reach creat public issu tracker question unexpect behavior workaround ricconoel thank repli match engin section suggest open issu",
        "Answer_preprocessed_content":"thank reach creat public issu tracker question unexpect behavior workaround thank repli match engin section suggest open issu",
        "Answer_gpt_summary_original":"possible solution: the user can create a public issue tracker to report the unexpected behavior of queries with filtering on google cloud platform's . they can also provide a workaround for the issue.",
        "Answer_gpt_summary":"possibl solut user creat public issu tracker report unexpect behavior queri filter googl cloud platform provid workaround issu"
    },
    {
        "Question_id":63766714.0,
        "Question_title":"Run.get_context() gives the same run id",
        "Question_body":"<p>I am submitting the training through a script file. Following is the content of the <code>train.py<\/code> script. Azure ML is treating all these as one run (instead of run per alpha value as coded below) as <code>Run.get_context()<\/code> is returning the same Run id.<\/p>\n<p><strong>train.py<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.opendatasets import Diabetes\nfrom azureml.core import Run\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.externals import joblib\n\nimport math\nimport os\nimport logging\n\n# Load dataset\ndataset = Diabetes.get_tabular_dataset()\nprint(dataset.take(1))\n\ndf = dataset.to_pandas_dataframe()\ndf.describe()\n\n# Split X (independent variables) &amp; Y (target variable)\nx_df = df.dropna()      # Remove rows that have missing values\ny_df = x_df.pop(&quot;Y&quot;)    # Y is the label\/target variable\n\nx_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=66)\nprint('Original dataset size:', df.size)\nprint(&quot;Size after dropping 'na':&quot;, x_df.size)\nprint(&quot;Training split size: &quot;, x_train.size)\nprint(&quot;Test split size: &quot;, x_test.size)\n\n# Training\nalphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Define hyperparameters\n\n# Create and log interactive runs\n\noutput_dir = os.path.join(os.getcwd(), 'outputs')\n\nfor hyperparam_alpha in alphas:\n    # Get the experiment run context\n    run = Run.get_context()\n    print(&quot;Started run: &quot;, run.id)\n    run.log(&quot;train_split_size&quot;, x_train.size)\n    run.log(&quot;test_split_size&quot;, x_train.size)\n    run.log(&quot;alpha_value&quot;, hyperparam_alpha)\n\n    # Train\n    print(&quot;Train ...&quot;)\n    model = Ridge(hyperparam_alpha)\n    model.fit(X = x_train, y = y_train)\n    \n    # Predict\n    print(&quot;Predict ...&quot;)\n    y_pred = model.predict(X = x_test)\n\n    # Calculate &amp; log error\n    rmse = math.sqrt(mean_squared_error(y_true = y_test, y_pred = y_pred))\n    run.log(&quot;rmse&quot;, rmse)\n    print(&quot;rmse&quot;, rmse)\n\n    # Serialize the model to local directory\n    if not os.path.isdir(output_dir):\n        os.makedirs(output_dir, exist_ok=True) \n\n    print(&quot;Save model ...&quot;)\n    model_name = &quot;model_alpha_&quot; + str(hyperparam_alpha) + &quot;.pkl&quot; # Pickle file\n    file_path = os.path.join(output_dir, model_name)\n    joblib.dump(value = model, filename = file_path)\n\n    # Upload the model\n    run.upload_file(name = model_name, path_or_stream = file_path)\n\n    # Complete the run\n    run.complete()\n<\/code><\/pre>\n<p><strong>Experiments view<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/E6xAG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E6xAG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Authoring code (i.e. control plane)<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom azureml.core import Workspace, Experiment, RunConfiguration, ScriptRunConfig, VERSION, Run\n\nws = Workspace.from_config()\nexp = Experiment(workspace = ws, name = &quot;diabetes-local-script-file&quot;)\n\n# Create new run config obj\nrun_local_config = RunConfiguration()\n\n# This means that when we run locally, all dependencies are already provided.\nrun_local_config.environment.python.user_managed_dependencies = True\n\n# Create new script config\nscript_run_cfg = ScriptRunConfig(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    script = 'train.py',\n    run_config = run_local_config) \n\nrun = exp.submit(script_run_cfg)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1599411710610,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":1599438192360,
        "Question_score":3.0,
        "Question_view_count":2523.0,
        "Answer_body":"<h2>Short Answer<\/h2>\n<h3>Option 1: create child runs within run<\/h3>\n<p><code>run = Run.get_context()<\/code> assigns the run object of the run that you're currently in to <code>run<\/code>. So in every iteration of the hyperparameter search, you're logging to the same run. To solve this, you need to create child (or sub-) runs for each hyperparameter value. You can do this with <code>run.child_run()<\/code>. Below is the template for making this happen.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>run = Run.get_context()\n\nfor hyperparam_alpha in alphas:\n    # Get the experiment run context\n    run_child = run.child_run()\n    print(&quot;Started run: &quot;, run_child.id)\n    run_child.log(&quot;train_split_size&quot;, x_train.size)\n<\/code><\/pre>\n<p>On the <code>diabetes-local-script-file<\/code> Experiment page, you can see that Run <code>9<\/code> was the parent run and Runs <code>10-19<\/code> were the child runs if you click &quot;Include child runs&quot; page. There is also a &quot;Child runs&quot; tab on Run 9 details page.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5IMcz.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5IMcz.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Long answer<\/h2>\n<p>I highly recommend abstracting the hyperparameter search away from the data plane (i.e. <code>train.py<\/code>) and into the control plane (i.e. &quot;authoring code&quot;). This becomes especially valuable as training time increases and you can arbitrarily parallelize and also choose Hyperparameters more intelligently by using Azure ML's <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters\" rel=\"noreferrer\"><code>Hyperdrive<\/code><\/a>.<\/p>\n<h3>Option 2 Create runs from control plane<\/h3>\n<p>Remove the loop from your code, add the code like below (<a href=\"https:\/\/gist.github.com\/swanderz\/f5c0dc1aefc796d37f6bc3600f2ae3cd\" rel=\"noreferrer\">full data and control here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import argparse\nfrom pprint import pprint\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--alpha', type=float, default=0.5)\nargs = parser.parse_args()\nprint(&quot;all args:&quot;)\npprint(vars(args))\n\n# use the variable like this\nmodel = Ridge(args.alpha)\n<\/code><\/pre>\n<p>below is how to submit a single run using a script argument. To submit multiple runs, just use a loop in the control plane.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Define hyperparameters\n\nlist_rcs = [ScriptRunConfig(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    script = 'train.py',\n    arguments=['--alpha',a],\n    run_config = run_local_config) for a in alphas]\n\nlist_runs = [exp.submit(rc) for rc in list_rcs]\n\n<\/code><\/pre>\n<h3>Option 3 Hyperdrive (IMHO the recommended approach)<\/h3>\n<p>In this way you outsource the hyperparameter source to <code>Hyperdrive<\/code>. The UI will also report results exactly how you want them, and via the API you can easily download the best model.  Note you can't use this locally anymore and must use <code>AMLCompute<\/code>, but to me it is a worthwhile trade-off.<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters#configure-experiment\" rel=\"noreferrer\">This is a great overview<\/a>. Excerpt below (<a href=\"https:\/\/gist.github.com\/swanderz\/f5c0dc1aefc796d37f6bc3600f2ae3cd#file-hyperdrive-ipynb\" rel=\"noreferrer\">full code here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>param_sampling = GridParameterSampling( {\n        &quot;alpha&quot;: choice(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)\n    }\n)\n\nestimator = Estimator(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    entry_script = 'train.py',\n    compute_target=cpu_cluster,\n    environment_definition=Environment.get(workspace=ws, name=&quot;AzureML-Tutorial&quot;)\n)\n\nhyperdrive_run_config = HyperDriveConfig(estimator=estimator,\n                          hyperparameter_sampling=param_sampling, \n                          policy=None,\n                          primary_metric_name=&quot;rmse&quot;, \n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                          max_total_runs=10,\n                          max_concurrent_runs=4)\n\nrun = exp.submit(hyperdrive_run_config)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q1AhJ.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q1AhJ.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63766714",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1599436122230,
        "Question_original_content":"run context give run submit train script file follow content train script treat run instead run alpha valu code run context return run train opendataset import diabet core import run sklearn model select import train test split sklearn linear model import ridg sklearn metric import mean squar error sklearn extern import joblib import math import import log load dataset dataset diabet tabular dataset print dataset dataset panda datafram split independ variabl target variabl dropna remov row miss valu pop label target variabl train test train test train test split test size random state print origin dataset size size print size drop size print train split size train size print test split size test size train alpha defin hyperparamet creat log interact run output dir path join getcwd output hyperparam alpha alpha experi run context run run context print start run run run log train split size train size run log test split size train size run log alpha valu hyperparam alpha train print train model ridg hyperparam alpha model fit train train predict print predict pred model predict test calcul log error rmse math sqrt mean squar error true test pred pred run log rmse rmse print rmse rmse serial model local directori path isdir output dir makedir output dir exist true print save model model model alpha str hyperparam alpha pkl pickl file file path path join output dir model joblib dump valu model filenam file path upload model run upload file model path stream file path complet run run complet experi view author code control plane import core import workspac experi runconfigur scriptrunconfig version run workspac config exp experi workspac diabet local script file creat new run config obj run local config runconfigur mean run local depend provid run local config environ python user manag depend true creat new script config script run cfg scriptrunconfig sourc directori path join getcwd code script train run config run local config run exp submit script run cfg run wait complet output true",
        "Question_preprocessed_content":"give run submit train script file follow content script treat run return run experi view author code",
        "Question_gpt_summary_original":"The user is encountering a challenge where Azure ML is treating all runs as one run instead of one run per alpha value as coded, as Run.get_context() is returning the same Run id.",
        "Question_gpt_summary":"user encount challeng treat run run instead run alpha valu code run context return run",
        "Answer_original_content":"short answer option creat child run run run run context assign run object run current run iter hyperparamet search log run solv need creat child sub run hyperparamet valu run child run templat make happen run run context hyperparam alpha alpha experi run context run child run child run print start run run child run child log train split size train size diabet local script file experi page run parent run run child run click includ child run page child run tab run detail page long answer highli recommend abstract hyperparamet search awai data plane train control plane author code especi valuabl train time increas arbitrarili parallel choos hyperparamet intellig hyperdr option creat run control plane remov loop code add code like data control import argpars pprint import pprint parser argpars argumentpars parser add argument alpha type float default arg parser pars arg print arg pprint var arg us variabl like model ridg arg alpha submit singl run script argument submit multipl run us loop control plane alpha defin hyperparamet list rc scriptrunconfig sourc directori path join getcwd code script train argument alpha run config run local config alpha list run exp submit list rc option hyperdr imho recommend approach wai outsourc hyperparamet sourc hyperdr report result exactli want api easili download best model note us local anymor us amlcomput worthwhil trade great overview excerpt code param sampl gridparametersampl alpha choic estim estim sourc directori path join getcwd code entri script train comput target cpu cluster environ definit environ workspac tutori hyperdr run config hyperdriveconfig estim estim hyperparamet sampl param sampl polici primari metric rmse primari metric goal primarymetricgo maxim max total run max concurr run run exp submit hyperdr run config run wait complet output true",
        "Answer_preprocessed_content":"short answer option creat child run run assign run object run current iter hyperparamet search log run solv need creat child run hyperparamet valu templat make happen experi page run parent run run child run click includ child run page child run tab run detail page long answer highli recommend abstract hyperparamet search awai data plane control plane especi valuabl train time increas arbitrarili parallel choos hyperparamet intellig option creat run control plane remov loop code add code like submit singl run script argument submit multipl run us loop control plane option hyperdr wai outsourc hyperparamet sourc report result exactli want api easili download best model note us local anymor us worthwhil great overview excerpt",
        "Answer_gpt_summary_original":"the answer provides three possible solutions to the challenge of run.get_context() returning the same run id. the first solution is to create child runs within the run using run.child_run(). the second solution is to create runs from the control plane. the third solution is to use hyperdrive, which outsources the hyperparameter source to hyperdrive and reports results via the ui.",
        "Answer_gpt_summary":"answer provid possibl solut challeng run context return run solut creat child run run run child run second solut creat run control plane solut us hyperdr outsourc hyperparamet sourc hyperdr report result"
    },
    {
        "Question_id":null,
        "Question_title":"Getting OSError: [Errno 30] Read-only file system",
        "Question_body":"I am new to AzureML, I am trying to run the pipeline using parallelRunSteps and pipeline is getting submitted successfully but while running the pipeline it is throwing an above error not sure what would be the root cause of it.\n\nThe step I am following is\n\nCreating the workspace if does not exists\n\n\nFetching the datastore by specifying the storage account and other details\n\n\nUsing the from file dataset\n\n\nRegistering the dataset\n\n\nAfter registering fetching the dataset\n\n\nFetching\/Initialising Experiment\n\n\nFetching\/Initialising Environment\n\n\nAdding Private wheel file to pip package\n\n\nRegistering the packages to conda dependencies\n\n\nRegistering the Environment\n\n\nFetching\/Initialising the Compute Target\n\n\nInitialising the ParallelRunConfig\n\n\nInitialising the PipelineData as output data\n\n\nInitialising the ParallelRunStep\n\n\nFetching\/Initialising the Pipeline\n\n\nSubmitting the Pipeline\n\nThe above same technique I tried with different PythonScriptSteps instead of ParallelRunStep method.\n\nCreating the workspace if does not exists\n\n\nFetching the datastore by specifying the storage account and other details\n\n\nTabular Dataset\n\n\nsetting dataset name input\n\n\nFetching the Experiment\n\n\nFetching\/Initialising the Experiment\n\n\nFetching\/Initialising the Environment\n\n\nAdding Private wheel file to pip package\n\n\nRegistering the packages to conda dependencies\n\n\nRegistering the Environment\n\n\nFetching the ComputeTarget\n\n\nInitialising the PythonStepScript\n\n\nInitialising the Pipeline\n\n\nSubmitting the Pipeline\n\nWith PythonStepScripts it is working fine. Not able to understand what mistake I am doing while running ParallelRunStep method.",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1624894187850,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/454901\/getting-oserror-errno-30-read-only-file-system.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-06T08:12:22.683Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nHope your issue has been solved. We haven\u2019t heard from you on the last response and was just checking back to see if you have a resolution yet.\n\nThe workaround I have seen for the similar issue is to add \"tmp\" to the file path like filepath = '\/tmp\/' + key\n\nIn case if you have any resolution please do share that same with the community as it can be helpful to others . Please do let us know if you still have issue for it.\n\n\n\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-07-13T06:48:36.4Z",
                "Answer_score":0,
                "Answer_body":"Hi Yutong,\n\nSorry for the late reply was on leave.\n\nSharing the error message below\n\nTraceback (most recent call last):\nFile \"driver\/amlbi_main.py\", line 48, in <module>\nmain()\nFile \"driver\/amlbi_main.py\", line 44, in main\nJobStarter().start_job()\nFile \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/gmail\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/wd\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/driver\/job_starter.py\", line 50, in start_job\nself.setup(is_master=True)\nFile \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/gmail\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/wd\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/driver\/job_starter.py\", line 44, in setup\nLogConfig().config(args.logging_level, is_master=is_master)\nFile \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/gmail\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/wd\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/driver\/singleton_meta.py\", line 18, in call\ncls.instances[cls] = super(SingletonMeta, cls).call(args, *kwargs)\nFile \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/gmail\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/wd\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/driver\/log_config.py\", line 39, in init_\nself.log_dir = self.get_log_dir()\nFile \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/gmail\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/wd\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/driver\/log_config.py\", line 48, in get_log_dir\nworking_dir = RunContextFactory.get_context().working_dir\nFile \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/gmail\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/wd\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/driver\/run_context.py\", line 64, in working_dir\npth.mkdir(parents=True, exist_ok=True)\nFile \"\/azureml-envs\/azureml_91e342c44c0de9bc46808411bb1fed8e\/lib\/python3.6\/pathlib.py\", line 1226, in mkdir\nself._accessor.mkdir(self, mode)\nFile \"\/azureml-envs\/azureml_91e342c44c0de9bc46808411bb1fed8e\/lib\/python3.6\/pathlib.py\", line 387, in wrapped\nreturn strfunc(str(pathobj), *args)\nOSError: [Errno 30] Read-only file system: '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/gmail\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/mounts\/workspaceblobstore\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e'\n\nSorry not getting where to add file path like filepath = '\/tmp\/' + key can you some reference or example",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-19T21:21:49.037Z",
                "Answer_score":0,
                "Answer_body":"I am having the same issue\n\nI am creating a compute cluster and then mounting a Jupyter Lab mounted at the Workspace default datastore at this location:\n\n\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/{workspace_name}\/azureml\/{run_id.lower()}\/mounts\/\n\nI also uploaded Jupyter Notebooks to\n\n\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/{workspace_name}\/azureml\/{run_id.lower()}\/mounts\/workspaceblobstore\/\n\nI used to be able to run the Jupyter Notebooks and save the results on the mount, or being able to upload content using the Jupyter Lab, or duplicating or saving changes to the notebooks\n\nBut not anymore I am getting this error:\n\nUnexpected error while saving file: workspaceblobstore\/tao\/bpnet\/bpnet-Copy1.ipynb [Errno 30] Read-only file system: '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/ngc_aml_toolkit_ws_test2\/azureml\/tao-mrg-exp34_1634671669_282ea162\/mounts\/workspaceblobstore\/tao\/bpnet\/bpnet-Copy1.ipynb\n\nWhile trying to duplicate notebook bpnet.ipynb",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"get oserror errno read file new try run pipelin parallelrunstep pipelin get submit successfulli run pipelin throw error sure root caus step follow creat workspac exist fetch datastor specifi storag account detail file dataset regist dataset regist fetch dataset fetch initialis experi fetch initialis environ ad privat wheel file pip packag regist packag conda depend regist environ fetch initialis comput target initialis parallelrunconfig initialis pipelinedata output data initialis parallelrunstep fetch initialis pipelin submit pipelin techniqu tri differ pythonscriptstep instead parallelrunstep method creat workspac exist fetch datastor specifi storag account detail tabular dataset set dataset input fetch experi fetch initialis experi fetch initialis environ ad privat wheel file pip packag regist packag conda depend regist environ fetch computetarget initialis pythonstepscript initialis pipelin submit pipelin pythonstepscript work fine abl understand mistak run parallelrunstep method",
        "Question_preprocessed_content":"get oserror file new try run pipelin parallelrunstep pipelin get submit successfulli run pipelin throw error sure root caus step follow creat workspac exist fetch datastor specifi storag account detail file dataset regist dataset regist fetch dataset experi environ ad privat wheel file pip packag regist packag conda depend regist environ comput target initialis parallelrunconfig initialis pipelinedata output data initialis parallelrunstep pipelin submit pipelin techniqu tri differ pythonscriptstep instead parallelrunstep method creat workspac exist fetch datastor specifi storag account detail tabular dataset set dataset input fetch experi experi environ ad privat wheel file pip packag regist packag conda depend regist environ fetch computetarget initialis pythonstepscript initialis pipelin submit pipelin pythonstepscript work fine abl understand mistak run parallelrunstep method",
        "Question_gpt_summary_original":"The user is encountering an OSError with the error message \"Read-only file system\" while trying to run a pipeline using the ParallelRunSteps method in AzureML. The user has successfully submitted the pipeline but is unsure of the root cause of the error. The user has tried the same technique with different PythonScriptSteps instead of ParallelRunStep method and it is working fine.",
        "Question_gpt_summary":"user encount oserror error messag read file try run pipelin parallelrunstep method user successfulli submit pipelin unsur root caus error user tri techniqu differ pythonscriptstep instead parallelrunstep method work fine",
        "Answer_original_content":"hello hope issu solv havent heard respons check resolut workaround seen similar issu add tmp file path like filepath tmp kei case resolut share commun help let know issu regard yutong yutong sorri late repli leav share error messag traceback recent file driver amlbi main line main file driver amlbi main line main jobstart start job file mnt batch task share root job gmail bef aba afd bef aba afd driver job starter line start job self setup master true file mnt batch task share root job gmail bef aba afd bef aba afd driver job starter line setup logconfig config arg log level master master file mnt batch task share root job gmail bef aba afd bef aba afd driver singleton meta line cl instanc cl super singletonmeta cl arg kwarg file mnt batch task share root job gmail bef aba afd bef aba afd driver log config line init self log dir self log dir file mnt batch task share root job gmail bef aba afd bef aba afd driver log config line log dir work dir runcontextfactori context work dir file mnt batch task share root job gmail bef aba afd bef aba afd driver run context line work dir pth mkdir parent true exist true file env eccdebcbbfed lib python pathlib line mkdir self accessor mkdir self mode file env eccdebcbbfed lib python pathlib line wrap return strfunc str pathobj arg oserror errno read file mnt batch task share root job gmail bef aba afd mount workspaceblobstor bef aba afd sorri get add file path like filepath tmp kei refer exampl have issu creat comput cluster mount jupyt lab mount workspac default datastor locat mnt batch task share root job workspac run lower mount upload jupyt notebook mnt batch task share root job workspac run lower mount workspaceblobstor abl run jupyt notebook save result mount abl upload content jupyt lab duplic save chang notebook anymor get error unexpect error save file workspaceblobstor tao bpnet bpnet copi ipynb errno read file mnt batch task share root job ngc aml toolkit test tao mrg exp mount workspaceblobstor tao bpnet bpnet copi ipynb try duplic notebook bpnet ipynb",
        "Answer_preprocessed_content":"hello hope issu solv havent heard respons check resolut workaround seen similar issu add tmp file path like filepath kei case resolut share commun help let know issu regard yutong yutong sorri late repli leav share error messag traceback file line main file line main file line file line setup file line super file line file line file line file line mkdir mode file line wrap return strfunc arg oserror file sorri get add file path like filepath kei refer exampl have issu creat comput cluster mount jupyt lab mount workspac default datastor locat upload jupyt notebook abl run jupyt notebook save result mount abl upload content jupyt lab duplic save chang notebook anymor get error unexpect error save file file try duplic notebook",
        "Answer_gpt_summary_original":"the answer suggests adding \"tmp\" to the file path like filepath = '\/tmp\/' + key as a workaround for a similar issue. however, the context of the question is not clear, and there is no specific solution provided for the oserror encountered when attempting to run a pipeline using parallelrunsteps.",
        "Answer_gpt_summary":"answer suggest ad tmp file path like filepath tmp kei workaround similar issu context question clear specif solut provid oserror encount attempt run pipelin parallelrunstep"
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker inference on inf1 no opencv",
        "Question_body":"I am trying to deploy Pytorch model on ml.inf1.xlarge instance. Image: 301217895009.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-neo-pytorch:1.5.1-inf-py3 My python code using some oepncv functions, and when I am trying to run the infernce I got the following error: ModuleNotFoundError: No module named 'cv2'\n\nI tried to add opencv-python-headless to requirements.txt, but then I got another error ImportError: libgthread-2.0.so.0: cannot open shared object file\n\nHow I can use opencv with the ml.inf1 instances?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649704234930,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":123.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUG9fHLN3TNGbXjRmuwRBcA\/sagemaker-inference-on-inf-1-no-opencv",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-11T21:49:47.674Z",
                "Answer_score":0,
                "Answer_body":"When running Neo inference on Sagemaker, it\u2019s possible to now use the Deep Learning Containers (DLC) provided by AWS. It\u2019s also unnecessary to remain on Pytorch 1.5 when using the DLC images. In our latest released image, OpenCV is pre-installed along with a more recent version of AWS Neuron SDK. Here\u2019s a link to the latest docker image for Pytorch 1.10 + Neuron SDK 1.17: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.10\/py3\/sdk1.17.1\/Dockerfile.neuron#L83\n\nIf you\u2019re unable to easily move to the latest Pytorch+Neuron DLC, the dockerfile link may help with resolving installation errors of OpenCV into your container.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"infer inf opencv try deploi pytorch model inf xlarg instanc imag dkr ecr west amazonaw com neo pytorch inf python code oepncv function try run infernc got follow error modulenotfounderror modul name tri add opencv python headless requir txt got error importerror libgthread open share object file us opencv inf instanc",
        "Question_preprocessed_content":"infer inf opencv try deploi pytorch model instanc imag python code oepncv function try run infernc got follow error modulenotfounderror modul name tri add got error importerror open share object file us opencv instanc",
        "Question_gpt_summary_original":"The user is facing challenges while trying to deploy a Pytorch model on an ml.inf1.xlarge instance. The user's Python code uses some OpenCV functions, but when running the inference, the user encounters a \"ModuleNotFoundError: No module named 'cv2'\" error. The user attempted to add opencv-python-headless to requirements.txt, but then encountered another error \"ImportError: libgthread-2.0.so.0: cannot open shared object file\". The user is seeking a solution to use OpenCV with the ml.inf1 instances.",
        "Question_gpt_summary":"user face challeng try deploi pytorch model inf xlarg instanc user python code us opencv function run infer user encount modulenotfounderror modul name error user attempt add opencv python headless requir txt encount error importerror libgthread open share object file user seek solut us opencv inf instanc",
        "Answer_original_content":"run neo infer possibl us deep learn contain dlc provid aw unnecessari remain pytorch dlc imag latest releas imag opencv pre instal recent version aw neuron sdk here link latest docker imag pytorch neuron sdk http github com aw deep learn contain blob master pytorch infer docker sdk dockerfil neuron your unabl easili latest pytorch neuron dlc dockerfil link help resolv instal error opencv contain",
        "Answer_preprocessed_content":"run neo infer possibl us deep learn contain provid aw unnecessari remain pytorch dlc imag latest releas imag opencv recent version aw neuron sdk here link latest docker imag pytorch neuron sdk your unabl easili latest pytorch neuron dlc dockerfil link help resolv instal error opencv contain",
        "Answer_gpt_summary_original":"possible solutions to challenges encountered when deploying a pytorch model on an ml.inf1.xlarge instance include using the deep learning containers (dlc) provided by aws, which come with pre-installed opencv and a more recent version of aws neuron sdk. it is also unnecessary to remain on pytorch 1.5 when using the dlc images. if unable to move to the latest pytorch+neuron dlc, the dockerfile link provided may help with resolving installation errors of opencv into the container.",
        "Answer_gpt_summary":"possibl solut challeng encount deploi pytorch model inf xlarg instanc includ deep learn contain dlc provid aw come pre instal opencv recent version aw neuron sdk unnecessari remain pytorch dlc imag unabl latest pytorch neuron dlc dockerfil link provid help resolv instal error opencv contain"
    },
    {
        "Question_id":null,
        "Question_title":"Error - AttributeError: 'function' object has no attribute 'service_context'",
        "Question_body":"Hi all,\n\nI am trying out some code on Azure machine learning notebook however I keep getting this error as stated above.\nCan anyone please help\nI tried to re-login\n\n\n\n\n\nRegards\nLyon",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1638262419597,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/645756\/error-attributeerror-39function39-object-has-no-at.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-01T04:35:09.483Z",
                "Answer_score":0,
                "Answer_body":"Hi @romungi-MSFT,\n'\nI solved the problem already, it was because I did not include the parenthesis for the function Workspace.from_config()\n\n\n\n\n\nThanks again",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error attributeerror function object attribut servic context try code notebook get error state help tri login regard lyon",
        "Question_preprocessed_content":"error attributeerror function object attribut try code notebook get error state help tri regard lyon",
        "Question_gpt_summary_original":"The user is encountering an error while trying to run code on Azure machine learning notebook. The error message is \"AttributeError: 'function' object has no attribute 'service_context'\". The user has attempted to re-login but is still experiencing the issue.",
        "Question_gpt_summary":"user encount error try run code notebook error messag attributeerror function object attribut servic context user attempt login experienc issu",
        "Answer_original_content":"romungi msft solv problem includ parenthesi function workspac config thank",
        "Answer_preprocessed_content":"solv problem includ parenthesi function thank",
        "Answer_gpt_summary_original":"solution: the user lyon encountered an attributeerror when running code on a notebook. however, they were able to solve the problem by including the parenthesis for the function workspace.from_config().",
        "Answer_gpt_summary":"solut user lyon encount attributeerror run code notebook abl solv problem includ parenthesi function workspac config"
    },
    {
        "Question_id":null,
        "Question_title":"Why is sample rate optional only for FLAC or WAV file and not other formats?",
        "Question_body":"So for example at my work we are using WEBM_OPUS encoding, which from what I understand, specificies the sample rate in audio stream metadata itself? Yet from here: https:\/\/cloud.google.com\/speech-to-text\/docs\/basics#sample-rates it says the field is only optional  for FLAC or WAV formats.And indeed, when I try the GSTT API with some example code (Streaming Recognition and a WEBM_OPUS encoded at 48000 sample rate), the GSTT actually accepts sample rates other than 48000 - and depending on the recognition model, produces different results depending on the sample rate selected!",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1650520140000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":78.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Why-is-sample-rate-optional-only-for-FLAC-or-WAV-file-and-not\/td-p\/415704\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-06T13:50:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Did you try to put 8000 hz within your data that you are sending to speech-to-text? Answering your question those two files are optionals since they are the most commonly used."
            },
            {
                "Answer_creation_time":"2022-05-09T05:59:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"What \"two files\"? ..."
            },
            {
                "Answer_creation_time":"2022-05-09T06:55:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"I was refering to WAV and FLAC audio files"
            },
            {
                "Answer_creation_time":"2022-05-10T04:09:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Yes, it's optional for WAV and FLAC - but my question was about using WEBM_OPUS format..."
            },
            {
                "Answer_creation_time":"2022-05-13T12:19:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"It's because google uses auto defined sample rates that are the next ones.\u00a0Sample rate must be one of 8000 Hz, 12000 Hz, 16000 Hz, 24000 Hz, or 48000 Hz, also you can see this documentation here."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"sampl rate option flac wav file format exampl work webm opu encod understand specifici sampl rate audio stream metadata http cloud googl com speech text doc basic sampl rate sai field option flac wav format try gstt api exampl code stream recognit webm opu encod sampl rate gstt actual accept sampl rate depend recognit model produc differ result depend sampl rate select",
        "Question_preprocessed_content":"sampl rate option flac wav file format exampl work encod understand specifici sampl rate audio stream metadata sai field option flac wav try gstt api exampl code gstt actual accept sampl rate depend recognit model produc differ result depend sampl rate select",
        "Question_gpt_summary_original":"The user is facing challenges in understanding why the sample rate is optional only for FLAC or WAV formats and not for other formats like WEBM_OPUS. They are also experiencing issues with the GSTT API accepting sample rates other than 48000 and producing different results depending on the sample rate selected.",
        "Question_gpt_summary":"user face challeng understand sampl rate option flac wav format format like webm opu experienc issu gstt api accept sampl rate produc differ result depend sampl rate select",
        "Answer_original_content":"try data send speech text answer question file option commonli file refer wav flac audio file ye option wav flac question webm opu format googl us auto defin sampl rate on sampl rate document",
        "Answer_preprocessed_content":"try data send answer question file option commonli file refer wav flac audio file ye option wav flac question googl us auto defin sampl rate rate document",
        "Answer_gpt_summary_original":"possible solutions to the challenge with the google speech-to-text api include using the sample rates of 8000 hz, 12000 hz, 16000 hz, 24000 hz, or 48000 hz, which are the auto-defined sample rates used by google. the sample rate field is only optional for flac or wav formats, but it is required for other formats like webm_opus. the user can refer to the documentation for more information.",
        "Answer_gpt_summary":"possibl solut challeng googl speech text api includ sampl rate auto defin sampl rate googl sampl rate field option flac wav format requir format like webm opu user refer document inform"
    },
    {
        "Question_id":null,
        "Question_title":"Code: AuthorizationFailed",
        "Question_body":"Unit 4 of 7\nExercise - Back up an Azure virtual machine\nCreate a backup for Azure virtual machines\n\nI am unable to run the following command in cloud shell to set up the environment:\nRGROUP=$(az group create --name vmbackups --location westus2 --output tsv --query name)\n\nFollowing error pop up:\nERROR: (AuthorizationFailed) The client 'live.com#...... does not have authorization to perform action 'Microsoft.Resources\/subscriptions\/resourcegroups\/write' over scope '\/subscriptions\/.......\/resourcegroups\/vmbackups' or the scope is invalid. If access was recently granted, please refresh your credentials.\nCode: AuthorizationFailed\nMessage: The client 'live.com#l...... with object id '.......' does not have authorization to perform action 'Microsoft.Resources\/subscriptions\/resourcegroups\/write' over scope '\/subscriptions\/.....\/resourcegroups\/vmbackups' or the scope is invalid. If access was recently granted, please refresh your credentials.\n\nWhen I do refresh, sign out or sign in do not helps. Anybody has any idea what to do?\nThank you",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651917005187,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @Krisztian-8931\n\nWelcome to Microsoft Q&A community.\n\nHave you tried to do this first?\n\n\n\n\n\n\n\n\n\nCheers,\n\nPlease \"Accept the answer\" if the information helped you. This will help us and others in the community as well.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/840311\/code-authorizationfailed.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-07T10:07:44.643Z",
                "Answer_score":1,
                "Answer_body":"Hello @Krisztian-8931\n\nWelcome to Microsoft Q&A community.\n\nHave you tried to do this first?\n\n\n\n\n\n\n\n\n\nCheers,\n\nPlease \"Accept the answer\" if the information helped you. This will help us and others in the community as well.",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1651918064643,
        "Question_original_content":"code authorizationfail unit exercis azur virtual machin creat backup azur virtual machin unabl run follow command cloud shell set environ rgroup group creat vmbackup locat westu output tsv queri follow error pop error authorizationfail client live com author perform action microsoft resourc subscript resourcegroup write scope subscript resourcegroup vmbackup scope invalid access recent grant refresh credenti code authorizationfail messag client live com object author perform action microsoft resourc subscript resourcegroup write scope subscript resourcegroup vmbackup scope invalid access recent grant refresh credenti refresh sign sign help anybodi idea thank",
        "Question_preprocessed_content":"code authorizationfail unit exercis azur virtual machin creat backup azur virtual machin unabl run follow command cloud shell set environ rgroup follow error pop error client author perform action scope scope invalid access recent grant refresh credenti code authorizationfail messag client object author perform action scope scope invalid access recent grant refresh credenti refresh sign sign help anybodi idea thank",
        "Question_gpt_summary_original":"The user is facing an AuthorizationFailed error while trying to run a command to set up an environment for creating a backup for Azure virtual machines. The error message suggests that the user's credentials do not have authorization to perform the required action over the specified scope. The user has tried refreshing, signing out, and signing in, but the issue persists.",
        "Question_gpt_summary":"user face authorizationfail error try run command set environ creat backup azur virtual machin error messag suggest user credenti author perform requir action specifi scope user tri refresh sign sign issu persist",
        "Answer_original_content":"hello krisztian welcom microsoft commun tri cheer accept answer inform help help commun",
        "Answer_preprocessed_content":"hello welcom microsoft commun tri cheer accept answer inform help help commun",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the responder is simply welcoming the user to the microsoft q&a community and suggesting that they try a certain approach.",
        "Answer_gpt_summary":"solut provid answer respond simpli welcom user microsoft commun suggest try certain approach"
    },
    {
        "Question_id":73649262.0,
        "Question_title":"Create custom kernel via post-startup script in Vertex AI User Managed notebook",
        "Question_body":"<p>I am trying to use a post-startup script to create a Vertex AI User Managed Notebook whose Jupyter Lab has a dedicated virtual environment and corresponding computing kernel when first launched. I have had success creating the instance and then, as a second manual step from within the Jupyter Lab &gt; Terminal, running a bash script like so:<\/p>\n<pre><code>#!\/bin\/bash\ncd \/home\/jupyter\nmkdir -p env\ncd env\npython3 -m venv envName --system-site-packages\nsource envName\/bin\/activate\nenvName\/bin\/python3 -m pip install --upgrade pip\npython -m ipykernel install --user --name=envName\npip3 install geemap --user \npip3 install earthengine-api --user \npip3 install ipyleaflet --user \npip3 install folium --user \npip3 install voila --user \npip3 install jupyterlab_widgets\ndeactivate\njupyter labextension install --no-build @jupyter-widgets\/jupyterlab-manager jupyter-leaflet\njupyter lab build --dev-build=False --minimize=False\njupyter labextension enable @jupyter-widgets\/jupyterlab-manager\n<\/code><\/pre>\n<p>However, I have not had luck using this code as a post-startup script (being supplied through the console creation tools, as opposed to command line, thus far). When I open Jupyter Lab and look at the relevant structures, I find that there is no environment or kernel. Could someone please provide a working example that accomplishes my aim, or otherwise describe the order of build steps that one would follow?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1662640804323,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":85.0,
        "Answer_body":"<p>Post startup scripts run as root.\nWhen you run:<\/p>\n<pre><code>python -m ipykernel install --user --name=envName\n<\/code><\/pre>\n<p>Notebook is using current user which is <code>root<\/code> vs when you use Terminal, which is running as <code>jupyter<\/code> user.<\/p>\n<p>Option 1) Have 2 scripts:<\/p>\n<ul>\n<li>Script A. Contents specified in original post. Example: <code>gs:\/\/newsml-us-central1\/so73649262.sh<\/code><\/li>\n<li>Script B. Downloads script and execute it as <code>jupyter<\/code>. Example: <code>gs:\/\/newsml-us-central1\/so1.sh<\/code> and use it as post-startup script.<\/li>\n<\/ul>\n<pre><code>#!\/bin\/bash\n\nset -x\n\ngsutil cp gs:\/\/newsml-us-central1\/so73649262.sh \/home\/jupyter\nchown jupyter \/home\/jupyter\/so73649262.sh\nchmod a+x \/home\/jupyter\/so73649262.sh\nsu -c '\/home\/jupyter\/so73649262.sh' jupyter\n<\/code><\/pre>\n<p>Option 2) Create a file in bash using EOF. Write the contents into a single file and execute it as mentioned above.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73649262",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1662967888203,
        "Question_original_content":"creat custom kernel post startup script user manag notebook try us post startup script creat user manag notebook jupyt lab dedic virtual environ correspond comput kernel launch success creat instanc second manual step jupyt lab termin run bash script like bin bash home jupyt mkdir env env python venv envnam site packag sourc envnam bin activ envnam bin python pip instal upgrad pip python ipykernel instal user envnam pip instal geemap user pip instal earthengin api user pip instal ipyleaflet user pip instal folium user pip instal voila user pip instal jupyterlab widget deactiv jupyt labextens instal build jupyt widget jupyterlab manag jupyt leaflet jupyt lab build dev build fals minim fals jupyt labextens enabl jupyt widget jupyterlab manag luck code post startup script suppli consol creation tool oppos command line far open jupyt lab look relev structur environ kernel provid work exampl accomplish aim order build step follow",
        "Question_preprocessed_content":"creat custom kernel script user manag notebook try us script creat user manag notebook jupyt lab dedic virtual environ correspond comput kernel launch success creat instanc second manual step jupyt lab termin run bash script like luck code script open jupyt lab look relev structur environ kernel provid work exampl accomplish aim order build step follow",
        "Question_gpt_summary_original":"The user is facing challenges in creating a Vertex AI User Managed Notebook with a dedicated virtual environment and corresponding computing kernel using a post-startup script. Although the user has been successful in creating the instance and running a bash script manually, they have not been able to achieve the same result using the post-startup script supplied through the console creation tools. The user is seeking a working example or guidance on the order of build steps to accomplish their aim.",
        "Question_gpt_summary":"user face challeng creat user manag notebook dedic virtual environ correspond comput kernel post startup script user success creat instanc run bash script manual abl achiev result post startup script suppli consol creation tool user seek work exampl guidanc order build step accomplish aim",
        "Answer_original_content":"post startup script run root run python ipykernel instal user envnam notebook current user root us termin run jupyt user option script script content specifi origin post exampl newsml central script download script execut jupyt exampl newsml central us post startup script bin bash set gsutil newsml central home jupyt chown jupyt home jupyt chmod home jupyt home jupyt jupyt option creat file bash eof write content singl file execut mention",
        "Answer_preprocessed_content":"post startup script run root run notebook current user us termin run user option script script content specifi origin post exampl script download script execut exampl us script option creat file bash eof write content singl file execut mention",
        "Answer_gpt_summary_original":"the answer provides two possible solutions for a user encountering challenges in creating a custom kernel via a post-startup script in a user-managed notebook. the first solution involves having two scripts, one that contains the original contents and another that downloads and executes the first script as the jupyter user. the second solution involves creating a file in bash using eof, writing the contents into a single file, and executing it as mentioned in the first solution.",
        "Answer_gpt_summary":"answer provid possibl solut user encount challeng creat custom kernel post startup script user manag notebook solut involv have script contain origin content download execut script jupyt user second solut involv creat file bash eof write content singl file execut mention solut"
    },
    {
        "Question_id":null,
        "Question_title":"[Solved] How to create model comparison table",
        "Question_body":"<p>Hi all! In the YOLOv5 tutorial there is an excellent table comparing the model runs side by side. I\u2019m not sure how to create it - can anyone give any advice? The tutorial is located here:  <a href=\"https:\/\/wandb.ai\/glenn-jocher\/yolov5_tutorial\/reports\/YOLOv5-COCO128-Tutorial-Results--VmlldzozMDI5OTY\" class=\"inline-onebox\">Weights &amp; Biases<\/a><\/p>\n<p>and the table I\u2019m looking to recreate is this:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/0b8e2068bea1ab2b410430e5668fa688a2201fda.jpeg\" data-download-href=\"\/uploads\/short-url\/1EdKVW3ukTzAAcyxW6wBQuGijvY.jpeg?dl=1\" title=\"demo-model-comparison\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0b8e2068bea1ab2b410430e5668fa688a2201fda_2_690x365.jpeg\" alt=\"demo-model-comparison\" data-base62-sha1=\"1EdKVW3ukTzAAcyxW6wBQuGijvY\" width=\"690\" height=\"365\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0b8e2068bea1ab2b410430e5668fa688a2201fda_2_690x365.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0b8e2068bea1ab2b410430e5668fa688a2201fda_2_1035x547.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0b8e2068bea1ab2b410430e5668fa688a2201fda_2_1380x730.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0b8e2068bea1ab2b410430e5668fa688a2201fda_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">demo-model-comparison<\/span><span class=\"informations\">3062\u00d71624 365 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Thanks for the help!!<\/p>\n<p>EDIT: Figured it out by searching for the term \u201cdiff only\u201d that appears in the top left. If anyone is wondering, you create a new panel and select \u201cRun Comparer\u201d. Leaving this up in case anyone has the same question <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1652450684607,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":253.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/solved-how-to-create-model-comparison-table\/2416",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-13T14:11:28.454Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/iankelk\">@iankelk<\/a>!<\/p>\n<p>That\u2019s called a Run Comparer panel, and you can created it by clicking <code>+ Add Panel<\/code> in the top right of your workspace, and choosing Run Comparer.<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/run-comparer\">\n  <header class=\"source\">\n      \n\n      <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/run-comparer\" target=\"_blank\" rel=\"noopener\">docs.wandb.ai<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/www.gitbook.com\/cdn-cgi\/image\/height=640,width=1280,fit=contain,dpr=1,format=auto\/https%3A%2F%2F1039519455-files.gitbook.io%2F~%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252F-Lqya5RvLedGEWPhtkjU%252Fsocialpreview%252Fue7fU3AWqmkPrLvIA1sg%252Flogo.png%3Falt%3Dmedia%26token%3Dc5fd7a6b-2501-4ee8-9a85-bcef4bb40fdc\" class=\"thumbnail onebox-avatar\" width=\"500\" height=\"500\">\n\n<h3><a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/run-comparer\" target=\"_blank\" rel=\"noopener\">Run Comparer<\/a><\/h3>\n\n  <p>Compare metrics across multiple runs<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":21.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-13T14:12:24.243Z",
                "Answer_body":"<p>Thanks Scott! I found it almost immediately after posting by googling \u201cweights and biases diff only\u201d. Thanks for such an amazingly fast response!<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-12T14:13:17.412Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"solv creat model comparison tabl yolov tutori excel tabl compar model run sure creat advic tutori locat tabl look recreat demo model comparison thank help edit figur search term diff appear left wonder creat new panel select run compar leav case question",
        "Question_preprocessed_content":"creat model comparison tabl yolov tutori excel tabl compar model run sure creat advic tutori locat tabl look recreat thank help edit figur search term diff appear left wonder creat new panel select run compar leav case question",
        "Question_gpt_summary_original":"The user is seeking advice on how to create a model comparison table similar to the one in the YOLOv5 tutorial. They are unsure of how to create it and are asking for help. However, the user later updates the post stating that they have figured it out by searching for the term \"diff only\" and creating a new panel with the \"Run Comparer\" option.",
        "Question_gpt_summary":"user seek advic creat model comparison tabl similar yolov tutori unsur creat ask help user later updat post state figur search term diff creat new panel run compar option",
        "Answer_original_content":"iankelk that call run compar panel creat click add panel right workspac choos run compar doc run compar compar metric multipl run thank scott immedi post googl diff thank amazingli fast respons topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"that call run compar panel creat click right workspac choos run compar run compar compar metric multipl run thank scott immedi post googl diff thank amazingli fast respons topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the answer suggests that the user can create a model comparison table by using the \"run comparer panel\" feature in . the user can access this feature by clicking on the \"+ add panel\" button in the top right of their workspace and selecting \"run comparer.\" the feature allows the user to compare metrics across multiple runs.",
        "Answer_gpt_summary":"answer suggest user creat model comparison tabl run compar panel featur user access featur click add panel button right workspac select run compar featur allow user compar metric multipl run"
    },
    {
        "Question_id":null,
        "Question_title":"How to package custom prediction code and serve it using an Endpoint in Vertex AI ?",
        "Question_body":"Goal: serve prediction request from a Vertex AI Endpoint by executing custom prediction logic.Expected Workflow:1. Upload a pretrained image_quality.pb model (developed in a non vertex-ai pythonic environment) in a gcs bucket2. Port existing image inference logic into a container and serve the prediction functionality through a vertex AI endpoint. 3. Use Vertex AI api for logging and capturing metrics inside the  custom inference logic.4. Finally we want to pass a list of images (stored in another gcs bucket) to that endpoint.5. We also want to see the logs and metrics in tensorboard.Existing Vertex AI code samples provide examples for custom training , invoking model.batch_predict \/ endpoint.predict , but don't mention how to execute custom prediction code.It would be great if someone can provide guidelines and links to documents\/code in order to implement the above steps.Thanks  ",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635161760000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":204.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-package-custom-prediction-code-and-serve-it-using-an\/td-p\/173876\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-19T07:46:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Please find the following guides respective of the points\n\n1.\n[1] Import model:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/import-model\n[2] What cannot be migrated:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/start\/migrating-to-vertex-ai#migration-exceptions\n2.\n[3] Custom containers:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/containers-overview\n[4] https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-container\n3.\n[5] About metrics:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/monitoring-metrics\n4.\n[6] Passing list of images:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/create-dataset-api\n5.\n[7] Metrics in Tensorboard:\u00a0https:\/\/cloud.google.com\/architecture\/ml-on-gcp-best-practices?hl=en#use-vertex-tensorboard-to-visua...\n\n\nAs there is no existing unifying guide for these operations, I created a documentation feature request to have one, and asked the documentation team to post updates here."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"packag custom predict code serv endpoint goal serv predict request endpoint execut custom predict logic expect workflow upload pretrain imag qualiti model develop non vertex python environ gc bucket port exist imag infer logic contain serv predict function endpoint us api log captur metric insid custom infer logic final want pass list imag store gc bucket endpoint want log metric tensorboard exist code sampl provid exampl custom train invok model batch predict endpoint predict mention execut custom predict code great provid guidelin link document code order implement step thank",
        "Question_preprocessed_content":"packag custom predict code serv endpoint goal serv predict request endpoint execut custom predict workflow upload pretrain model gc bucket port exist imag infer logic contain serv predict function endpoint us api log captur metric insid custom infer final want pass list imag want log metric code sampl provid exampl custom train invok mention execut custom predict great provid guidelin link order implement",
        "Question_gpt_summary_original":"The user is facing challenges in packaging custom prediction code and serving it using an Endpoint in Vertex AI. They need to upload a pretrained model in a GCS bucket, port existing inference logic into a container, use Vertex AI API for logging and capturing metrics, and pass a list of images to the endpoint. Existing Vertex AI code samples provide examples for custom training but don't mention how to execute custom prediction code. The user is seeking guidelines and links to documents\/code to implement the required steps.",
        "Question_gpt_summary":"user face challeng packag custom predict code serv endpoint need upload pretrain model gc bucket port exist infer logic contain us api log captur metric pass list imag endpoint exist code sampl provid exampl custom train mention execut custom predict code user seek guidelin link document code implement requir step",
        "Answer_original_content":"follow guid respect point import model http cloud googl com vertex doc gener import model migrat http cloud googl com vertex doc start migrat vertex migrat except custom contain http cloud googl com vertex doc train contain overview http cloud googl com vertex doc train creat custom contain metric http cloud googl com vertex doc gener monitor metric pass list imag http cloud googl com vertex doc dataset creat dataset api metric tensorboard http cloud googl com architectur gcp best practic us vertex tensorboard visua exist unifi guid oper creat document featur request ask document team post updat",
        "Answer_preprocessed_content":"follow guid respect point import custom pass list metric exist unifi guid oper creat document featur request ask document team post updat",
        "Answer_gpt_summary_original":"the answer provides several guides for packaging custom prediction code and serving it using an endpoint. these include importing models, creating custom containers, monitoring metrics, and passing lists of images. the answer also mentions a documentation feature request for a unifying guide on these operations.",
        "Answer_gpt_summary":"answer provid guid packag custom predict code serv endpoint includ import model creat custom contain monitor metric pass list imag answer mention document featur request unifi guid oper"
    },
    {
        "Question_id":64211755.0,
        "Question_title":"How to upload packages to an instance in a Processing step in Sagemaker?",
        "Question_body":"<p>I have to do large scale feature engineering on some data. My current approach is to spin up an instance using <code>SKLearnProcessor<\/code> and then scale the job by choosing a larger instance size or increasing the number of instances. I require using some packages that are not installed on Sagemaker instances by default and so I want to install the packages using .whl files.<\/p>\n<p>Another hurdle is that the Sagemaker role does not have internet access.<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nsess = sagemaker.Session()\nsess.default_bucket()        \n\nregion = boto3.session.Session().region_name\n\nrole = get_execution_role()\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     sagemaker_session = sess,\n                                     instance_type=&quot;ml.t3.medium&quot;,\n                                     instance_count=1)\n\nsklearn_processor.run(code='script.py')\n<\/code><\/pre>\n<p><strong>Attempted resolutions:<\/strong><\/p>\n<ol>\n<li>Upload the packages to a CodeCommit repository and clone the repo into the SKLearnProcessor instances. Failed with error <code>fatal: could not read Username for 'https:\/\/git-codecommit.eu-west-1.amazonaws.com': No such device or address<\/code>. I tried cloning the repo into a sagemaker notebook instance and it works, so its not a problem with my script.<\/li>\n<li>Use a bash script to copy the packages from s3 using the CLI. The bash script I used is based off <a href=\"https:\/\/medium.com\/@shadidc\/installing-custom-python-package-to-sagemaker-notebook-b7b897f4f655\" rel=\"nofollow noreferrer\">this post<\/a>. But the packages never get copied, and an error message is not thrown.<\/li>\n<li>Also looked into using the package <code>s3fs<\/code> but it didn't seem suitable to copy the wheel files.<\/li>\n<\/ol>\n<p><strong>Alternatives<\/strong><\/p>\n<p>My client is hesitant to spin up containers from custom docker images. Any alternatives?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601912020397,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1071.0,
        "Answer_body":"<p><code>2. Use a bash script to copy the packages from s3 using the CLI. The bash script I used is based off this post. But the packages never get copied, and an error message is not thrown.<\/code><\/p>\n<p>This approach seems sound.<\/p>\n<p>You may be better off overriding the <code>command<\/code> field on the <code>SKLearnProcessor<\/code> to <code>\/bin\/bash<\/code>, run a bash script like <code>install_and_run_my_python_code.sh<\/code> that installs the wheel containing your python dependencies, then runs your main python entry point script.<\/p>\n<p>Additionally, instead of using AWS S3 calls to download your code in a script, you could use a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.ProcessingInput\" rel=\"nofollow noreferrer\">ProcessingInput<\/a> to download your code rather than doing this with AWS CLI calls in a bash script, which is what the <code>SKLearnProcessor<\/code> does to download your entry point <code>script.py<\/code> code across all the instances.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64211755",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1604006575952,
        "Question_original_content":"upload packag instanc process step larg scale featur engin data current approach spin instanc sklearnprocessor scale job choos larger instanc size increas number instanc requir packag instal instanc default want instal packag whl file hurdl role internet access import boto import import execut role sklearn process import sklearnprocessor sess session sess default bucket region boto session session region role execut role sklearn processor sklearnprocessor framework version role role session sess instanc type medium instanc count sklearn processor run code script attempt resolut upload packag codecommit repositori clone repo sklearnprocessor instanc fail error fatal read usernam http git codecommit west amazonaw com devic address tri clone repo notebook instanc work problem script us bash script copi packag cli bash script base post packag copi error messag thrown look packag sf suitabl copi wheel file altern client hesit spin contain custom docker imag altern",
        "Question_preprocessed_content":"upload packag instanc process step larg scale featur engin data current approach spin instanc scale job choos larger instanc size increas number instanc requir packag instal instanc default want instal packag whl file hurdl role internet access attempt resolut upload packag codecommit repositori clone repo sklearnprocessor instanc fail error tri clone repo notebook instanc work problem script us bash script copi packag cli bash script base post packag copi error messag thrown look packag suitabl copi wheel file altern client hesit spin contain custom docker imag altern",
        "Question_gpt_summary_original":"The user is facing challenges in uploading packages to an instance in a Processing step in Sagemaker. They require using packages that are not installed on Sagemaker instances by default and the Sagemaker role does not have internet access. The user attempted to upload the packages to a CodeCommit repository and clone the repo into the SKLearnProcessor instances, use a bash script to copy the packages from S3 using the CLI, and use the package s3fs, but none of these methods worked. The user is looking for alternatives to spinning up containers from custom docker images.",
        "Question_gpt_summary":"user face challeng upload packag instanc process step requir packag instal instanc default role internet access user attempt upload packag codecommit repositori clone repo sklearnprocessor instanc us bash script copi packag cli us packag sf method work user look altern spin contain custom docker imag",
        "Answer_original_content":"us bash script copi packag cli bash script base post packag copi error messag thrown approach sound better overrid command field sklearnprocessor bin bash run bash script like instal run python code instal wheel contain python depend run main python entri point script addition instead aw call download code script us processinginput download code aw cli call bash script sklearnprocessor download entri point script code instanc",
        "Answer_preprocessed_content":"approach sound better overrid field run bash script like instal wheel contain python depend run main python entri point script addition instead aw call download code script us processinginput download code aw cli call bash script download entri point code instanc",
        "Answer_gpt_summary_original":"possible solutions to the challenge of uploading packages to an instance in a processing step without internet access are: using a bash script to copy packages from s3 using the cli, overriding the command field on the sklearnprocessor to run a bash script that installs the wheel containing python dependencies and runs the main entry point script, and using a processinginput to download code instead of using aws cli calls in a bash script.",
        "Answer_gpt_summary":"possibl solut challeng upload packag instanc process step internet access bash script copi packag cli overrid command field sklearnprocessor run bash script instal wheel contain python depend run main entri point script processinginput download code instead aw cli call bash script"
    },
    {
        "Question_id":57946451.0,
        "Question_title":"Incremental learning with a built-in sagemaker algorithm",
        "Question_body":"<p>I am training the DeepAR AWS SageMaker built-in algorithm. With the sagemaker SDK, I can train the model with particular specified hyper-parameters:<\/p>\n\n<pre><code>estimator = sagemaker.estimator.Estimator(\n    sagemaker_session=sagemaker_session,\n    image_name=image_name,\n    role=role,\n    train_instance_count=1,\n    train_instance_type='ml.c4.2xlarge',\n    base_job_name='wfp-deepar',\n    output_path=join(s3_path, 'output')\n)\n\nestimator.set_hyperparameters(**{\n    'time_freq': 'M',\n    'epochs': '50',\n    'mini_batch_size': '96',\n    'learning_rate': '1E-3',\n    'context_length': '12',\n    'dropout_rate': 0,\n    'prediction_length': '12'\n})\n\nestimator.fit(inputs=data_channels, wait=True, job_name='wfp-deepar-job-level-5')\n<\/code><\/pre>\n\n<p>I would like to train the resulting model again with a <strong>smaller learning rate<\/strong>. I followed the incremental training method described here: <a href=\"https:\/\/docs.aws.amazon.com\/en_pv\/sagemaker\/latest\/dg\/incremental-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/en_pv\/sagemaker\/latest\/dg\/incremental-training.html<\/a>, but it does not work, apparently (according to the link), only two built-in models support incremental learning. <\/p>\n\n<p>Has anyone found a workaround for this so that they can train a built-in algorithm with a scheduled learning rate?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1568567773507,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":225.0,
        "Answer_body":"<p>Unfortunately, the SageMaker built-in DeepAR model doesn't support learning rate scheduling nor incremental learning.  If you want to implement learning rate plateau schedule on a DeepAR architecture I recommend to consider:<\/p>\n\n<ul>\n<li>using the open-source DeepAR implementation (<a href=\"https:\/\/gluon-ts.mxnet.io\/api\/gluonts\/gluonts.model.deepar.html\" rel=\"nofollow noreferrer\">code<\/a>, <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-neural-time-series-models-with-gluon-time-series\/\" rel=\"nofollow noreferrer\">demo<\/a>)<\/li>\n<li>or using the <a href=\"https:\/\/docs.aws.amazon.com\/forecast\/latest\/dg\/aws-forecast-recipe-deeparplus.html\" rel=\"nofollow noreferrer\">DeepAR+ algo of the Amazon Forecast service<\/a>, that features learning rate scheduling ability.<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57946451",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1568630636312,
        "Question_original_content":"increment learn built algorithm train deepar built algorithm sdk train model particular specifi hyper paramet estim estim estim session session imag imag role role train instanc count train instanc type xlarg base job wfp deepar output path join path output estim set hyperparamet time freq epoch mini batch size learn rate context length dropout rate predict length estim fit input data channel wait true job wfp deepar job level like train result model smaller learn rate follow increment train method describ http doc aw amazon com latest increment train html work appar accord link built model support increment learn workaround train built algorithm schedul learn rate",
        "Question_preprocessed_content":"increment learn algorithm train deepar algorithm sdk train model particular specifi like train result model smaller learn rate follow increment train method describ work appar model support increment learn workaround train algorithm schedul learn rate",
        "Question_gpt_summary_original":"The user is facing a challenge in training the DeepAR AWS SageMaker built-in algorithm with a smaller learning rate using the incremental training method. The user followed the method described in the documentation but found out that only two built-in models support incremental learning. The user is seeking a workaround to train the built-in algorithm with a scheduled learning rate.",
        "Question_gpt_summary":"user face challeng train deepar built algorithm smaller learn rate increment train method user follow method describ document built model support increment learn user seek workaround train built algorithm schedul learn rate",
        "Answer_original_content":"unfortun built deepar model support learn rate schedul increment learn want implement learn rate plateau schedul deepar architectur recommend consid open sourc deepar implement code demo deepar algo amazon forecast servic featur learn rate schedul abil",
        "Answer_preprocessed_content":"unfortun deepar model support learn rate schedul increment learn want implement learn rate plateau schedul deepar architectur recommend consid deepar implement deepar algo amazon forecast servic featur learn rate schedul abil",
        "Answer_gpt_summary_original":"possible solutions for implementing learning rate scheduling on a deepar architecture are to use the open-source deepar implementation or the deepar+ algorithm of the amazon forecast service. unfortunately, the built-in deepar model does not support learning rate scheduling or incremental learning.",
        "Answer_gpt_summary":"possibl solut implement learn rate schedul deepar architectur us open sourc deepar implement deepar algorithm amazon forecast servic unfortun built deepar model support learn rate schedul increment learn"
    },
    {
        "Question_id":null,
        "Question_title":"How to clean up remote storage?",
        "Question_body":"<p>Hello!<\/p>\n<p>I tried to run the following command<\/p>\n<pre><code class=\"lang-auto\">dvc gc -w -c -v -f\n<\/code><\/pre>\n<p>and expected to clear unnecessary data from the remote storage.<br>\nBut unfortunately, the command froze and nothing happens. And nothing happens on the remote storage<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1656865000345,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":132.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-clean-up-remote-storage\/1234",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-03T19:45:35.949Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/alexander\">@Alexander<\/a> . Could you try running with trace level (passing <code>-vv<\/code> flag) ? Does the command print anything before freezing?<\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-04T12:46:48.742Z",
                "Answer_body":"<p>No, nothing<\/p>\n<p>Last 5-6 lines are<\/p>\n<pre><code class=\"lang-auto\">2022-07-04 15:35:56,668 TRACE: Assuming '\/home\/amarkovich\/ml\/.dvc\/cache\/18\/2cda5747d04396db51ae146e9b427d.dir' is unchanged since it is read-only\n2022-07-04 15:35:56,668 TRACE: Assuming '\/home\/amarkovich\/ml\/.dvc\/cache\/a5\/e4c0c7a5025ed91d0d6755ffd88ca9.dir' is unchanged since it is read-only\n2022-07-04 15:35:56,668 TRACE: Assuming '\/home\/amarkovich\/ml\/.dvc\/cache\/a5\/e4c0c7a5025ed91d0d6755ffd88ca9.dir' is unchanged since it is read-only\n2022-07-04 15:35:56,800 DEBUG: Estimated remote size: 4864 files\n2022-07-04 15:35:56,950 DEBUG: Removing '\/home\/amarkovich\/ml\/.dvc\/cache\/1e\/.m4BxAHazk2YFz8fAJ2bhfw.tmp'\n2022-07-04 15:35:59,144 DEBUG: Estimated remote size: 221184 files\n                                                                                                    \n<\/code><\/pre>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-05T13:47:48.625Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/alexander\">@Alexander<\/a> Could you open a bug report and fill in the details in <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/new?assignees=&amp;labels=&amp;template=bug_report.md\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Sign in to GitHub \u00b7 GitHub<\/a><\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"clean remot storag hello tri run follow command expect clear unnecessari data remot storag unfortun command froze happen happen remot storag",
        "Question_preprocessed_content":"clean remot storag hello tri run follow command expect clear unnecessari data remot storag unfortun command froze happen happen remot storag",
        "Question_gpt_summary_original":"The user encountered a challenge while trying to clear unnecessary data from remote storage using the command \"dvc gc -w -c -v -f\". The command froze and did not clear any data from the remote storage.",
        "Question_gpt_summary":"user encount challeng try clear unnecessari data remot storag command command froze clear data remot storag",
        "Answer_original_content":"alexand try run trace level pass flag command print freez line trace assum home amarkovich cach cdaddbaeebd dir unchang read trace assum home amarkovich cach eccaedddffdca dir unchang read trace assum home amarkovich cach eccaedddffdca dir unchang read debug estim remot size file debug remov home amarkovich cach mbxahazkyfzfajbhfw tmp debug estim remot size file alexand open bug report detail sign github github",
        "Answer_preprocessed_content":"try run trace level command print freez line open bug report detail sign github github",
        "Answer_gpt_summary_original":"the answer suggests trying to run the command with trace level (-vv flag) to see if it prints anything before freezing. if there is no output, the user is advised to open a bug report and fill in the details on github. no specific solutions are provided for the freezing issue.",
        "Answer_gpt_summary":"answer suggest try run command trace level flag print freez output user advis open bug report detail github specif solut provid freez issu"
    },
    {
        "Question_id":null,
        "Question_title":"Amazon CloudWatch Metric ModelSetupTime not available",
        "Question_body":"The ModelSetupTime metric is not available for monitoring the serverless endpoint(https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html) in Amazon CloudWatch.Able to see only the below mentioned metrics Invocations Invocation5XXErrors Invocation4XXErrors ModelLatency OverheadLatency\n\nIs there any role\/configuration changes I have to do to view the ModelSetupTime ?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1665007033890,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":23.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUu2KU0TNvTu-2V6sOylZJsg\/amazon-cloud-watch-metric-model-setup-time-not-available",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-17T16:57:04.142Z",
                "Answer_score":0,
                "Answer_body":"Currently the metric is not published to CW.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"amazon cloudwatch metric modelsetuptim avail modelsetuptim metric avail monitor serverless endpoint http doc aw amazon com latest serverless endpoint html amazon cloudwatch abl mention metric invoc invocationxxerror invocationxxerror modellat overheadlat role configur chang view modelsetuptim",
        "Question_preprocessed_content":"amazon cloudwatch metric modelsetuptim avail modelsetuptim metric avail monitor serverless amazon mention metric invoc invoc xxerror invoc xxerror modellat overheadlat chang view modelsetuptim",
        "Question_gpt_summary_original":"The user is facing a challenge with the ModelSetupTime metric not being available for monitoring the serverless endpoint in Amazon CloudWatch. They are only able to see a limited set of metrics and are unsure if any role or configuration changes are required to view the ModelSetupTime metric.",
        "Question_gpt_summary":"user face challeng modelsetuptim metric avail monitor serverless endpoint amazon cloudwatch abl limit set metric unsur role configur chang requir view modelsetuptim metric",
        "Answer_original_content":"current metric publish",
        "Answer_preprocessed_content":"current metric publish",
        "Answer_gpt_summary_original":"there are no solutions available at the moment for the user's challenge with amazon cloudwatch. the modelsetuptime metric is not currently published to cw, so no role\/configuration changes can be made to view it.",
        "Answer_gpt_summary":"solut avail moment user challeng amazon cloudwatch modelsetuptim metric current publish role configur chang view"
    },
    {
        "Question_id":null,
        "Question_title":"Group Categorical Values in Azure ML Designer",
        "Question_body":"Hello,\nis there any possibility to do the same in Azure ML Studio designer as it is able in Azure ML Studio? I need to group categorical values, but in Azure ML Studio designer there option for Group Data into Bins. When I am trying to do it by chosing custom edges, it does not seem to work with data column which is categorical.\n\nEDIT\nI have rating 1-5, and I would like to make it in from 1-2-3-4-5 to 1-5. When creating bins I only can get categorized as 1 and 2.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1615818855177,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/315002\/group-categorical-values-in-azure-ml-designer.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-05T07:34:31.703Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nWe are sorry that have not heard from you. Azure ML Designer officially supports Group data into Bins with custom edge. I have highlighted the essential part below.\n\nAdd the Group Data Into Bins module to your pipeline in the designer. You can find this module in the category Data Transformation.\n\n\n\n\nConnect the dataset that has numerical data to bin. Quantization can be applied only to columns that contain numeric data.\n\nIf the dataset contains non-numeric columns, use the Select Columns in Dataset module to select a subset of columns to work with.\n\nSpecify the binning mode. The binning mode determines other parameters, so be sure to select the Binning mode option first. The following types of binning are supported:\n\nQuantiles: The quantile method assigns values to bins based on percentile ranks. This method is also known as equal height binning.\n\nEqual Width: With this option, you must specify the total number of bins. The values from the data column are placed in the bins such that each bin has the same interval between starting and ending values. As a result, some bins might have more values if data is clumped around a certain point.\n\nCustom Edges: You can specify the values that begin each bin. The edge value is always the lower boundary of the bin.\n\nFor example, assume you want to group values into two bins. One will have values greater than 0, and one will have values less than or equal to 0. In this case, for bin edges, you enter 0 in Comma-separated list of bin edges. The output of the module will be 1 and 2, indicating the bin index for each row value. Note that the comma-separated value list must be in an ascending order, such as 1, 3, 5, 7.\n\nNote\n\nEntropy MDL mode is defined in Studio (classic) and there's no corresponding open source package which can be leveraged to support in Designer yet.\n\nIf you're using the Quantiles and Equal Width binning modes, use the Number of bins option to specify how many bins, or quantiles, you want to create.\n\n\n\n\nFor Columns to bin, use the column selector to choose the columns that have the values you want to bin. Columns must be a numeric data type.\n\nThe same binning rule is applied to all applicable columns that you choose. If you need to bin some columns by using a different method, use a separate instance of the Group Data into Bins module for each set of columns.\n\nWarning\n\nIf you choose a column that's not an allowed type, a runtime error is generated. The module returns an error as soon as it finds any column of a disallowed type. If you get an error, review all selected columns. The error does not list all invalid columns.\n\nFor Output mode, indicate how you want to output the quantized values:\n\nAppend: Creates a new column with the binned values, and appends that to the input table.\n\nInplace: Replaces the original values with the new values in the dataset.\n\nResultOnly: Returns just the result columns.\n\nIf you select the Quantiles binning mode, use the Quantile normalization option to determine how values are normalized before sorting into quantiles. Note that normalizing values transforms the values but doesn't affect the final number of bins.\n\n\n\n\n\nThe following normalization types are supported:\n\nPercent: Values are normalized within the range [0,100].\n\nPQuantile: Values are normalized within the range [0,1].\n\nQuantileIndex: Values are normalized within the range [1,number of bins].\n\nIf you choose the Custom Edges option, enter a comma-separated list of numbers to use as bin edges in the Comma-separated list of bin edges text box.\n\n\n\n\nThe values mark the point that divides bins. For example, if you enter one bin edge value, two bins will be generated. If you enter two bin edge values, three bins will be generated.\n\nThe values must be sorted in the order that the bins are created, from lowest to highest.\n\nSelect the Tag columns as categorical option to indicate that the quantized columns should be handled as categorical variables.\n\n\n\n\nSubmit the pipeline.\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"group categor valu design hello possibl studio design abl studio need group categor valu studio design option group data bin try chose custom edg work data column categor edit rate like creat bin categor",
        "Question_preprocessed_content":"group categor valu design hello possibl studio design abl studio need group categor valu studio design option group data bin try chose custom edg work data column categor edit rate like creat bin categor",
        "Question_gpt_summary_original":"The user is facing challenges in grouping categorical values in Azure ML Studio designer as there is no option for Group Data into Bins for categorical data. The user attempted to group the data by choosing custom edges, but it did not work for the categorical data column. The user specifically wants to group a rating column from 1-5 to 1-2-3-4-5 to 1-5, but the binning option only categorizes it as 1 and 2.",
        "Question_gpt_summary":"user face challeng group categor valu studio design option group data bin categor data user attempt group data choos custom edg work categor data column user specif want group rate column bin option categor",
        "Answer_original_content":"hello sorri heard design offici support group data bin custom edg highlight essenti add group data bin modul pipelin design modul categori data transform connect dataset numer data bin quantiz appli column contain numer data dataset contain non numer column us select column dataset modul select subset column work specifi bin mode bin mode determin paramet sure select bin mode option follow type bin support quantil quantil method assign valu bin base percentil rank method known equal height bin equal width option specifi total number bin valu data column place bin bin interv start end valu result bin valu data clump certain point custom edg specifi valu begin bin edg valu lower boundari bin exampl assum want group valu bin valu greater valu equal case bin edg enter comma separ list bin edg output modul indic bin index row valu note comma separ valu list ascend order note entropi mdl mode defin studio classic correspond open sourc packag leverag support design quantil equal width bin mode us number bin option specifi bin quantil want creat column bin us column selector choos column valu want bin column numer data type bin rule appli applic column choos need bin column differ method us separ instanc group data bin modul set column warn choos column allow type runtim error gener modul return error soon find column disallow type error review select column error list invalid column output mode indic want output quantiz valu append creat new column bin valu append input tabl inplac replac origin valu new valu dataset resultonli return result column select quantil bin mode us quantil normal option determin valu normal sort quantil note normal valu transform valu affect final number bin follow normal type support percent valu normal rang pquantil valu normal rang quantileindex valu normal rang number bin choos custom edg option enter comma separ list number us bin edg comma separ list bin edg text box valu mark point divid bin exampl enter bin edg valu bin gener enter bin edg valu bin gener valu sort order bin creat lowest highest select tag column categor option indic quantiz column handl categor variabl submit pipelin regard yutong",
        "Answer_preprocessed_content":"hello sorri heard design offici support group data bin custom edg highlight essenti add group data bin modul pipelin design modul categori data transform connect dataset numer data bin quantiz appli column contain numer data dataset contain column us select column dataset modul select subset column work specifi bin mode bin mode determin paramet sure select bin mode option follow type bin support quantil quantil method assign valu bin base percentil rank method known equal height bin equal width option specifi total number bin valu data column place bin bin interv start end valu result bin valu data clump certain point custom edg specifi valu begin bin edg valu lower boundari bin exampl assum want group valu bin valu greater valu equal case bin edg enter list bin edg output modul indic bin index row valu note valu list ascend order note entropi mdl mode defin studio correspond open sourc packag leverag support design quantil equal width bin mode us number bin option specifi bin quantil want creat column bin us column selector choos column valu want bin column numer data type bin rule appli applic column choos need bin column differ method us separ instanc group data bin modul set column warn choos column allow type runtim error gener modul return error soon find column disallow type error review select column error list invalid column output mode indic want output quantiz valu append creat new column bin valu append input tabl inplac replac origin valu new valu dataset resultonli return result column select quantil bin mode us quantil normal option determin valu normal sort quantil note normal valu transform valu affect final number bin follow normal type support percent valu normal rang pquantil valu normal rang quantileindex valu normal rang choos custom edg option enter list number us bin edg list bin edg text box valu mark point divid bin exampl enter bin edg valu bin gener enter bin edg valu bin gener valu sort order bin creat lowest highest select tag column categor option indic quantiz column handl categor variabl submit pipelin regard yutong",
        "Answer_gpt_summary_original":"the answer provides possible solutions to the user's difficulty in grouping categorical values in studio designer. the solution involves adding the \"group data into bins\" module to the pipeline, selecting the appropriate binning mode (quantiles, equal width, or custom edges), specifying the number of bins, selecting the columns to bin, and choosing the output mode. the answer also warns about selecting columns that are not of the allowed type and provides options for normalizing values before sorting into quantiles.",
        "Answer_gpt_summary":"answer provid possibl solut user difficulti group categor valu studio design solut involv ad group data bin modul pipelin select appropri bin mode quantil equal width custom edg specifi number bin select column bin choos output mode answer warn select column allow type provid option normal valu sort quantil"
    },
    {
        "Question_id":60342645.0,
        "Question_title":"Azure ML inference pipeline deployment authorization token error",
        "Question_body":"<p>When deploying a real-time inferencing pipeline in Azure ML (as per <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy#deploy-the-real-time-endpoint\" rel=\"nofollow noreferrer\">this<\/a> tutorial), I receive the below error. I've tried forcibly logging out using OAuth. Tried creating a new Azure workspace but continue to receive the same error.<\/p>\n\n<p>It looks like the tenant id causing the problem is example.onmicrosoft.com (72f988bf-86f1-41af-91ab-2d7cd011db47)<\/p>\n\n<hr>\n\n<p><em>Deploy: Failed on step CreateServiceFromModels. Details: AzureML service API error. Error calling ServiceCreate: {\"code\":\"Unauthorized\",\"statusCode\":401,\"message\":\"Unauthorized\",\"details\":[{\"code\":\"EmptyOrInvalidToken\",\"message\":\"Error: Service invocation failed!\\r\\nRequest: GET <a href=\"https:\/\/management.azure.com\/subscriptions\/subscription_id\/resourceGroups\/dev-rg\/providers\/Microsoft.MachineLearningServices\/workspaces\/dev-ws\/providers\/Microsoft.Authorization\/permissions?api-version=2015-07-01\" rel=\"nofollow noreferrer\">https:\/\/management.azure.com\/subscriptions\/subscription_id\/resourceGroups\/dev-rg\/providers\/Microsoft.MachineLearningServices\/workspaces\/dev-ws\/providers\/Microsoft.Authorization\/permissions?api-version=2015-07-01<\/a>\\r\\nStatus Code: 401 Unauthorized\\r\\nReason Phrase: Unauthorized\\r\\nResponse Body: {\\\"error\\\":{\\\"code\\\":\\\"InvalidAuthenticationTokenTenant\\\",\\\"message\\\":\\\"The access token is from the wrong issuer '<a href=\"https:\/\/sts.windows.net\/72f988bf-86f1-41af-91ab-2d7cd011db47\/\" rel=\"nofollow noreferrer\">https:\/\/sts.windows.net\/72f988bf-86f1-41af-91ab-2d7cd011db47\/<\/a>'. It must match the tenant '<a href=\"https:\/\/sts.windows.net\/correct_tenant_id\/\" rel=\"nofollow noreferrer\">https:\/\/sts.windows.net\/correct_tenant_id\/<\/a>' associated with this subscription. Please use the authority (URL) '<a href=\"https:\/\/login.windows.net\/correct_tenant_id\" rel=\"nofollow noreferrer\">https:\/\/login.windows.net\/correct_tenant_id<\/a>' to get the token. Note, if the subscription is transferred to another tenant there i<\/em><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1582302156893,
        "Question_favorite_count":null,
        "Question_last_edit_time":1582395411243,
        "Question_score":2.0,
        "Question_view_count":481.0,
        "Answer_body":"<p>I appear to have had User Access Administrator role only (in addition to Classic Service Administrator). As soon as I added myself to the Owner role in the Access Control (IAM) section of the Azure Portal, the deployment succeeded.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60342645",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1582556597823,
        "Question_original_content":"infer pipelin deploy author token error deploi real time inferenc pipelin tutori receiv error tri forcibl log oauth tri creat new azur workspac continu receiv error look like tenant caus problem exampl onmicrosoft com fbf dcddb deploi fail step createservicefrommodel detail servic api error error call servicecr code unauthor statuscod messag unauthor detail code emptyorinvalidtoken messag error servic invoc fail nrequest http manag azur com subscript subscript resourcegroup dev provid microsoft machinelearningservic workspac dev provid microsoft author permiss api version nstatu code unauthor nreason phrase unauthor nrespons bodi error code invalidauthenticationtokenten messag access token wrong issuer http st window net fbf dcddb match tenant http st window net correct tenant associ subscript us author url http login window net correct tenant token note subscript transfer tenant",
        "Question_preprocessed_content":"infer pipelin deploy author token error deploi inferenc pipelin receiv error tri forcibl log oauth tri creat new azur workspac continu receiv error look like tenant caus problem deploi fail step createservicefrommodel detail servic api error error call servicecr code unauthor statuscod messag unauthor detail code emptyorinvalidtoken messag error servic invoc code phrase bodi access token wrong issuer match tenant associ subscript us author token note subscript transfer tenant",
        "Question_gpt_summary_original":"The user is encountering an error when deploying a real-time inferencing pipeline in Azure ML, receiving an \"Unauthorized\" error message with details indicating an issue with the authorization token. The error message suggests that the access token is from the wrong issuer and must match the correct tenant ID associated with the subscription. The user has attempted to resolve the issue by logging out and creating a new Azure workspace, but the error persists.",
        "Question_gpt_summary":"user encount error deploi real time inferenc pipelin receiv unauthor error messag detail indic issu author token error messag suggest access token wrong issuer match correct tenant associ subscript user attempt resolv issu log creat new azur workspac error persist",
        "Answer_original_content":"appear user access administr role addit classic servic administr soon ad owner role access control iam section azur portal deploy succeed",
        "Answer_preprocessed_content":"appear user access administr role soon ad owner role access control section azur portal deploy succeed",
        "Answer_gpt_summary_original":"solution: the user needs to add themselves to the owner role in the access control (iam) section of the azure portal to resolve the authorization token error when deploying a real-time inferencing pipeline.",
        "Answer_gpt_summary":"solut user need add owner role access control iam section azur portal resolv author token error deploi real time inferenc pipelin"
    },
    {
        "Question_id":58301879.0,
        "Question_title":"Unable to upload statsmodels 0.9rc1 python package in Azure ML studio",
        "Question_body":"<p>I'm not able to upload statsmodels 0.9rc1 python package in Azure ML studio for Time series analysis.<\/p>\n\n<p>I have downloaded <a href=\"https:\/\/files.pythonhosted.org\/packages\/df\/6f\/df6cf5faecd8082ee23916ff45d396dfee5a1f17aa275da7bab4f5c8926a\/statsmodels-0.9.0rc1-cp36-cp36m-win_amd64.whl\" rel=\"nofollow noreferrer\">statsmodels 0.9rc1<\/a>, unzipped contents and added statsmodels folder and model.pkl file to zip folder.<\/p>\n\n<p>But, while uploading to Microsoft Azure ML studio it says <strong>failed to build schema and visualization<\/strong><\/p>\n\n<p>I'm using this external package in <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts\" rel=\"nofollow noreferrer\">Execute Python script<\/a><\/p>\n\n<p>PS: I have succesfully uploaded packages like Adal, dateutils etc.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570616046857,
        "Question_favorite_count":null,
        "Question_last_edit_time":1570619708227,
        "Question_score":2.0,
        "Question_view_count":141.0,
        "Answer_body":"<p>I have switched to Azure Jupyter Notebook where I installed package using pip<\/p>\n\n<pre><code>!pip install statsmodels==0.9.0rc1\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58301879",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1573144655863,
        "Question_original_content":"unabl upload statsmodel python packag studio abl upload statsmodel python packag studio time seri analysi download statsmodel unzip content ad statsmodel folder model pkl file zip folder upload studio sai fail build schema visual extern packag execut python script succesfulli upload packag like adal dateutil",
        "Question_preprocessed_content":"unabl upload statsmodel python packag studio abl upload statsmodel python packag studio time seri analysi download statsmodel unzip content ad statsmodel folder file zip folder upload studio sai fail build schema visual extern packag execut python script succesfulli upload packag like adal dateutil",
        "Question_gpt_summary_original":"The user is facing challenges in uploading the statsmodels 0.9rc1 python package in Azure ML studio for time series analysis. Despite downloading and unzipping the contents, adding the statsmodels folder and model.pkl file to the zip folder, the user encountered an error message stating \"failed to build schema and visualization\" while uploading to Microsoft Azure ML studio. The user has successfully uploaded other packages like Adal and dateutils.",
        "Question_gpt_summary":"user face challeng upload statsmodel python packag studio time seri analysi despit download unzip content ad statsmodel folder model pkl file zip folder user encount error messag state fail build schema visual upload studio user successfulli upload packag like adal dateutil",
        "Answer_original_content":"switch azur jupyt notebook instal packag pip pip instal statsmodel",
        "Answer_preprocessed_content":"switch azur jupyt notebook instal packag pip",
        "Answer_gpt_summary_original":"the solution to the problem of being unable to upload the statsmodels 0.9rc1 python package in studio for time series analysis is to switch to azure jupyter notebook and install the package using pip with the command \"!pip install statsmodels==0.9.0rc1\".",
        "Answer_gpt_summary":"solut problem unabl upload statsmodel python packag studio time seri analysi switch azur jupyt notebook instal packag pip command pip instal statsmodel"
    },
    {
        "Question_id":null,
        "Question_title":"Scalars not getting saved",
        "Question_body":"<p>I\u2019m new to guild.ai and trying to use the guild.yml files to save all relevant information including scalars. The scalars are however not being saved.<\/p>\n<p>This is my code:<\/p>\n<pre><code>boston = load_boston()\nX = pd.DataFrame(boston.data, columns=boston.feature_names)\ny = boston.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nnamedf = ['X_train', 'X_test']\nnamenp = ['y_train', 'y_test']\n\nreg = Ridge(alpha=0.9)\nfitted = reg.fit(X, y)\n\nprint(\"score: %f\" % fitted.score(X, y))\n<\/code><\/pre>\n<p>and this is my guild.yml file:<\/p>\n<pre><code>ridge-regression:\n  description: fit ridge regression using boston data.\n  notebook: Checklist.ipynb\n  flags:\n    alpha:\n      description: alpha value in ridge regression\n      nb-replace: 'alpha=(\\d+)'\n  output-scalers:\n    score: 'score: (\\value)'\n<\/code><\/pre>\n<p>When I print out all information on the command line there are no scalars to be seen. Does anyone know why?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1611248492978,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":363.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/scalars-not-getting-saved\/527",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-21T17:47:53.850Z",
                "Answer_body":"<p>The problem is the spelling of <code>output-scalars<\/code> attribute - you have a typo there.<\/p>\n<p>It would be nice if Guild told you that but sadly you can mistype attribute names and Guild just ignores them, in many cases.<\/p>\n<p>You may run into other problems, related to the patterns. For those you can use the <code>--test-output-scalars<\/code> option to see how Guild applies the patterns for an operation. That would look like this:<\/p>\n<pre><code class=\"lang-command\">guild cat --output &lt;run ID to test&gt; | guild run &lt;op&gt; --test-output-scalars -\n<\/code><\/pre>\n<p>Note the use of <code>-<\/code> (dash) as the argument  to <code>--test-output-scalars<\/code>. That tells Guild to read from stdin. Otherwise you can use a filename containing output to test.<\/p>",
                "Answer_score":22.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-21T18:55:46.436Z",
                "Answer_body":"<p>Thank you for your reply! The misspelling was a one-off <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/smiley.png?v=9\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\"> The problem was that I was printing to the Jupyter notebook and not to the console\u2026<\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-21T19:41:04.112Z",
                "Answer_body":"<p>The code that you have in the original post looks okay to me. This <code>print(\"score: ...\")<\/code> line will print to the output for the cell but Guild will see this.<\/p>\n<p>How did you change your code to get this working?<\/p>",
                "Answer_score":6.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-25T17:07:49.423Z",
                "Answer_body":"<p>I added the line:<\/p>\n<pre><code>sys.stdout = open('\/dev\/stdout', 'w') \n<\/code><\/pre>\n<p>before printing the score. Guild didn\u2019t seem to be finding the output from the cell before.<\/p>",
                "Answer_score":6.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-26T12:26:55.892Z",
                "Answer_body":"<p>I\u2019m glad you have this working! This sounds like it may be a Guild bug. If it\u2019s convenient to reproduce this problem, could you paste the cell contents of the broken version here\u2014or share a simplified ipynb (e.g. via attachment, gist, etc.)?<\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"scalar get save new try us yml file save relev inform includ scalar scalar save code boston load boston datafram boston data column boston featur name boston target train test train test train test split test size namedf train test namenp train test reg ridg alpha fit reg fit print score fit score yml file ridg regress descript fit ridg regress boston data notebook checklist ipynb flag alpha descript alpha valu ridg regress replac alpha output scaler score score valu print inform command line scalar seen know",
        "Question_preprocessed_content":"scalar get save new try us yml file save relev inform includ scalar scalar save code yml file print inform command line scalar seen know",
        "Question_gpt_summary_original":"The user is facing a challenge with saving scalars using guild.yml files in guild.ai. The scalars are not being saved despite the user's efforts to include them in the code and guild.yml file. The user is seeking assistance in identifying the reason why the scalars are not being saved.",
        "Question_gpt_summary":"user face challeng save scalar yml file scalar save despit user effort includ code yml file user seek assist identifi reason scalar save",
        "Answer_original_content":"problem spell output scalar attribut typo nice told sadli mistyp attribut name ignor case run problem relat pattern us test output scalar option appli pattern oper look like cat output run test output scalar note us dash argument test output scalar tell read stdin us filenam contain output test thank repli misspel problem print jupyt notebook consol code origin post look okai print score line print output cell chang code work ad line sy stdout open dev stdout print score didnt find output cell glad work sound like bug conveni reproduc problem past cell content broken version hereor share simplifi ipynb attach gist",
        "Answer_preprocessed_content":"problem spell attribut typo nice told sadli mistyp attribut name ignor case run problem relat pattern us option appli pattern oper look like note us argument tell read stdin us filenam contain output test thank repli misspel problem print jupyt notebook consol code origin post look okai line print output cell chang code work ad line print score didnt find output cell glad work sound like bug conveni reproduc problem past cell content broken version hereor share simplifi ipynb",
        "Answer_gpt_summary_original":"the solution to the problem of saving scalars in the guild.yml file is to check for typos in the attribute names. the user can use the --test-output-scalars option to see how guild applies the patterns for an operation. the user can also use a filename containing output to test. another solution is to print to the console instead of the jupyter notebook. the user can add the line \"sys.stdout = open('\/dev\/stdout', 'w')\" before printing the score. if the problem persists, the user can share a simplified ipynb to reproduce the problem.",
        "Answer_gpt_summary":"solut problem save scalar yml file check typo attribut name user us test output scalar option appli pattern oper user us filenam contain output test solut print consol instead jupyt notebook user add line sy stdout open dev stdout print score problem persist user share simplifi ipynb reproduc problem"
    },
    {
        "Question_id":null,
        "Question_title":"Recreating wandb plots with matplotlib\/seaborn",
        "Question_body":"<p>I have been using w&amp;b for a few months and have had a great experience with it. However, I have had some trouble with including the diagram in my report. Under *more actions &gt; export panel \u2026 * there exist options for exporting the panel to PNG, SVG, PDF, and CSV. I want to export the panel to PDF with vectorized graphics and text but the PDF export seems to simply render the panel to PNG.<\/p>\n<p>What I have tried:<\/p>\n<ul>\n<li>exporting to SVG, but I was not able to convert this format to pdf or any other format suitable for my LaTeX report. I have been unable to use SVG files in my LaTeX report directly.<\/li>\n<li>creating a report of the panel and downloading the report as LaTeX, but again it renders the plots as PNG which is not desired in my case.<\/li>\n<\/ul>\n<p>Perhaps I am missing something, but as a final resort I have tried recreating the plots in wandb with the Export API in the following code snippet:<\/p>\n<pre><code class=\"lang-python\">import wandb\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\napi = wandb.Api()\n\n# Project is specified by &lt;entity\/project-name&gt;\nruns = api.runs(\"&lt;wandb-id&gt;\/&lt;entity-project-name&gt;\")\nhist_list = [] \nfor run in runs: \n    if not 'val\/loss' in run.summary:\n        continue\n\n    name = run.config['model']['_target_'].split('.')[-1]\n    hist = run.history(keys=['epoch', 'val\/loss'])\n    hist['name'] = name\n    hist_list.append(hist)\n\ndf = pd.concat(hist_list, ignore_index=True)\ndf = df.query(\"`val\/loss` != 'NaN'\")\n\nsns.lineplot(x=\"epoch\", y=\"val\/loss\", hue=\"name\", data=df)\nplt.show()\n<\/code><\/pre>\n<p>The script takes a long time to run (10 seconds) and comparing the output with the panel in the w&amp;b dashboard we have the following two plots<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/96ee4d13b86074c85b617696f6eb1306b1fde22a.png\" data-download-href=\"\/uploads\/short-url\/lxcb3jp5qj9Tu3wiJDqNxbIHY1Q.png?dl=1\" title=\"my fig\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/96ee4d13b86074c85b617696f6eb1306b1fde22a_2_690x343.png\" alt=\"my fig\" data-base62-sha1=\"lxcb3jp5qj9Tu3wiJDqNxbIHY1Q\" width=\"690\" height=\"343\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/96ee4d13b86074c85b617696f6eb1306b1fde22a_2_690x343.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/96ee4d13b86074c85b617696f6eb1306b1fde22a.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/96ee4d13b86074c85b617696f6eb1306b1fde22a.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/96ee4d13b86074c85b617696f6eb1306b1fde22a_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">my fig<\/span><span class=\"informations\">964\u00d7480 40.8 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/50ee331e0f5890b1feed33198e4e7e4b3e04f3aa.png\" data-download-href=\"\/uploads\/short-url\/bxWxmqbCJuvxst6H9WgGcA170lY.png?dl=1\" title=\"wandb fig\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/50ee331e0f5890b1feed33198e4e7e4b3e04f3aa_2_690x263.png\" alt=\"wandb fig\" data-base62-sha1=\"bxWxmqbCJuvxst6H9WgGcA170lY\" width=\"690\" height=\"263\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/50ee331e0f5890b1feed33198e4e7e4b3e04f3aa_2_690x263.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/50ee331e0f5890b1feed33198e4e7e4b3e04f3aa_2_1035x394.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/50ee331e0f5890b1feed33198e4e7e4b3e04f3aa_2_1380x526.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/50ee331e0f5890b1feed33198e4e7e4b3e04f3aa_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">wandb fig<\/span><span class=\"informations\">1516\u00d7580 69 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>The two plots have a noticeable difference. So I have two questions:<\/p>\n<ul>\n<li>Is it possible to export a panel to pdf with selectable text?<\/li>\n<li>If not, is there any reference for recreating the plots in wandb?<\/li>\n<\/ul>\n<p>Any help is appreciated <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p><strong>EDIT<\/strong><\/p>\n<p>I found a github thread that explains my problem a bit better: <a href=\"https:\/\/github.com\/wandb\/client\/issues\/1446#issuecomment-1029293004\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Custom Charts: Export Panel Feature \u00b7 Issue #1446 \u00b7 wandb\/client \u00b7 GitHub<\/a><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1650625685152,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":657.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/recreating-wandb-plots-with-matplotlib-seaborn\/2303",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-26T17:21:03.919Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kevjn\">@kevjn<\/a>, thank you for writing in and I\u2019m glad you\u2019re enjoying our product!<\/p>\n<p>Currently there isn\u2019t a way to export the panel with selectable text but I can put in a feature request around this if you would like? It sounds like this would solve most of your issues. I believe the issue here was the way that Vega wants to render charts but I can talk to our engineering team and see if there is a way around this.<\/p>\n<p>I don\u2019t have any specific references for recreating plots in Python but are you using custom charts? If so, you can access the Vega code that was used to create the chart in the UI and use that locally to recreate the chart.<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":12.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-27T09:39:52.203Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/nathank\">@nathank<\/a>, I am not using custom charts - the charts are rendered by default in the dashboard. Where can I access the Vega code and data used to populate the charts?<\/p>\n<p>I did get around my issue by exporting to CSV and doing some acrobatics with pandas and matplotlib, but I would prefer an easier way of doing it.<\/p>",
                "Answer_score":12.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-10T14:37:39.885Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/kevjn\">@kevjn<\/a> Unfortunately, only the custom charts are created with Vega and our standard charts are actually written in Javascript.<\/p>\n<p>I think using your acrobatics to make Matplotlib work will be the best workaround for now. I\u2019ve put in the feature request for exporting text- selectable PDF and can follow up with you if we are able to implement this feature.<\/p>\n<p>I apologize that I don\u2019t have a better workaround for you in the meantime.<\/p>",
                "Answer_score":2.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-09T14:38:03.434Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"recreat plot matplotlib seaborn month great experi troubl includ diagram report action export panel exist option export panel png svg pdf csv want export panel pdf vector graphic text pdf export simpli render panel png tri export svg abl convert format pdf format suitabl latex report unabl us svg file latex report directli creat report panel download report latex render plot png desir case miss final resort tri recreat plot export api follow code snippet import import panda import seaborn sn import matplotlib pyplot plt api api project specifi run api run hist list run run val loss run summari continu run config model target split hist run histori kei epoch val loss hist hist list append hist concat hist list ignor index true queri val loss nan sn lineplot epoch val loss hue data plt script take long time run second compar output panel dashboard follow plot fig fig plot notic differ question possibl export panel pdf select text refer recreat plot help appreci edit github thread explain problem bit better custom chart export panel featur issu client github",
        "Question_preprocessed_content":"recreat plot month great experi troubl includ diagram report action export panel exist option export panel png svg pdf csv want export panel pdf vector graphic text pdf export simpli render panel png tri export svg abl convert format pdf format suitabl latex report unabl us svg file latex report directli creat report panel download report latex render plot png desir case miss final resort tri recreat plot export api follow code snippet script take long time run compar output panel dashboard follow plot fig fig plot notic differ question possibl export panel pdf select text refer recreat plot help appreci edit github thread explain problem bit better custom chart export panel featur issu github",
        "Question_gpt_summary_original":"The user is having trouble exporting a panel from wandb to PDF with vectorized graphics and text. They have tried exporting to SVG and creating a report of the panel, but both options render the plots as PNG. As a final resort, they have tried recreating the plots in wandb using the Export API, but the output differs from the original panel. The user is seeking help to export the panel to PDF with selectable text or a reference for recreating the plots in wandb.",
        "Question_gpt_summary":"user have troubl export panel pdf vector graphic text tri export svg creat report panel option render plot png final resort tri recreat plot export api output differ origin panel user seek help export panel pdf select text refer recreat plot",
        "Answer_original_content":"kevjn thank write glad your enjoi product current isnt wai export panel select text featur request like sound like solv issu believ issu wai vega want render chart talk engin team wai dont specif refer recreat plot python custom chart access vega code creat chart us local recreat chart thank nate nathank custom chart chart render default dashboard access vega code data popul chart issu export csv acrobat panda matplotlib prefer easier wai kevjn unfortun custom chart creat vega standard chart actual written javascript think acrobat matplotlib work best workaround iv featur request export text select pdf follow abl implement featur apolog dont better workaround meantim topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"thank write glad your enjoi product current isnt wai export panel select text featur request like sound like solv issu believ issu wai vega want render chart talk engin team wai dont specif refer recreat plot python custom chart access vega code creat chart us local recreat chart thank nate custom chart chart render default dashboard access vega code data popul chart issu export csv acrobat panda matplotlib prefer easier wai unfortun custom chart creat vega standard chart actual written javascript think acrobat matplotlib work best workaround iv featur request export text select pdf follow abl implement featur apolog dont better workaround meantim topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- requesting a feature to export the panel with selectable text, which may solve most of the user's issues.\n- accessing the vega code used to create the chart in the ui and using that locally to recreate the chart, if the user is using custom charts.\n- using the workaround of exporting to csv and using pandas and matplotlib to recreate the chart, if the user is not using custom charts.\n- waiting for the feature request to be implemented, as there is no better workaround available at the moment.",
        "Answer_gpt_summary":"possibl solut answer request featur export panel select text solv user issu access vega code creat chart local recreat chart user custom chart workaround export csv panda matplotlib recreat chart user custom chart wait featur request implement better workaround avail moment"
    },
    {
        "Question_id":61415793.0,
        "Question_title":"Log metrics in PythonScriptStep",
        "Question_body":"<p>In my Azure ML pipeline I've got a PythonScriptStep that is crunching some data. I need to access the Azure ML Logger to track metrics in the step, so I'm trying to import get_azureml_logger but that's bombing out. I'm not sure what dependency I need to install via pip. <\/p>\n\n<p><code>from azureml.logging import get_azureml_logger<\/code><\/p>\n\n<p><code>ModuleNotFoundError: No module named 'azureml.logging'<\/code><\/p>\n\n<p>I came across a similar <a href=\"https:\/\/stackoverflow.com\/questions\/49438358\/azureml-logging-module-not-found\">post<\/a> but it deals with Azure Notebooks. Anyway, I tried adding that blob to my pip dependency, but it's failing with an Auth error.   <\/p>\n\n<pre><code>Collecting azureml.logging==1.0.79 [91m  ERROR: HTTP error 403 while getting\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n[0m91m  ERROR: Could not install requirement azureml.logging==1.0.79 from\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n(from -r \/azureml-environment-setup\/condaenv.g4q7suee.requirements.txt\n(line 3)) because of error 403 Client Error:\nServer failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. for url:\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n<\/code><\/pre>\n\n<p>I'm not sure how to move on this, all I need to do is to log metrics in the step.  <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1587755548897,
        "Question_favorite_count":null,
        "Question_last_edit_time":1587809992912,
        "Question_score":1.0,
        "Question_view_count":361.0,
        "Answer_body":"<p>Check out the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-track-experiments#option-2-use-scriptrunconfig\" rel=\"nofollow noreferrer\">ScriptRunConfig Section of the Monitor Azure ML experiment runs and metrics<\/a>. <code>ScriptRunConfig<\/code> works effectively the same as a <code>PythonScriptStep<\/code>.<\/p>\n\n<p>The idiom is generally to have the following in your the script of your <code>PythonScriptStep<\/code>:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\nrun.log('foo_score', \"bar\")\n<\/code><\/pre>\n\n<p>Side note: You don't need to change your environment dependencies to use this because <code>PythonScriptStep<\/code>s have <code>azureml-defaults<\/code> installed automatically as a dependency.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61415793",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1587756432003,
        "Question_original_content":"log metric pythonscriptstep pipelin got pythonscriptstep crunch data need access logger track metric step try import logger bomb sure depend need instal pip log import logger modulenotfounderror modul name log came similar post deal azur notebook tri ad blob pip depend fail auth error collect log error http error get http download blob core window net wheel latest log whl sig xnudtmb ffknhtarinbxyuqwaosxwgvgu bjk error instal requir log http download blob core window net wheel latest log whl sig xnudtmb ffknhtarinbxyuqwaosxwgvgu bjk environ setup condaenv gqsuee requir txt line error client error server fail authent request sure valu author header form correctli includ signatur url http download blob core window net wheel latest log whl sig xnudtmb ffknhtarinbxyuqwaosxwgvgu bjk sure need log metric step",
        "Question_preprocessed_content":"log metric pythonscriptstep pipelin got pythonscriptstep crunch data need access logger track metric step try import bomb sure depend need instal pip came similar post deal azur notebook tri ad blob pip depend fail auth error sure need log metric step",
        "Question_gpt_summary_original":"The user is facing challenges in accessing the Azure ML Logger to track metrics in a PythonScriptStep of their Azure ML pipeline. They are encountering a ModuleNotFoundError while trying to import get_azureml_logger and are unsure about the dependency they need to install via pip. They tried adding a blob to their pip dependency, but it failed with an Auth error. The user is unsure how to proceed and needs to log metrics in the step.",
        "Question_gpt_summary":"user face challeng access logger track metric pythonscriptstep pipelin encount modulenotfounderror try import logger unsur depend need instal pip tri ad blob pip depend fail auth error user unsur proce need log metric step",
        "Answer_original_content":"check scriptrunconfig section monitor experi run metric scriptrunconfig work effect pythonscriptstep idiom gener follow script pythonscriptstep core run import run run run context run log foo score bar note need chang environ depend us pythonscriptstep default instal automat depend",
        "Answer_preprocessed_content":"check scriptrunconfig section monitor experi run metric work effect idiom gener follow script note need chang environ depend us instal automat depend",
        "Answer_gpt_summary_original":"the answer suggests checking out the scriptrunconfig section of the monitor experiment runs and metrics as it works effectively the same as a pythonscriptstep. the solution is to have the following in the script of the pythonscriptstep: \"from .core.run import run run = run.get_context() run.log('foo_score', \"bar\")\". it is also noted that there is no need to change environment dependencies to use this because pythonscriptsteps have -defaults installed automatically as a dependency.",
        "Answer_gpt_summary":"answer suggest check scriptrunconfig section monitor experi run metric work effect pythonscriptstep solut follow script pythonscriptstep core run import run run run context run log foo score bar note need chang environ depend us pythonscriptstep default instal automat depend"
    },
    {
        "Question_id":70603137.0,
        "Question_title":"Is it possible to integrate AWS sagemaker and delta lake",
        "Question_body":"<p>Is it possible to integrate AWS sage maker and delta lake?<\/p>\n<p>thanks\nRamabadran<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1641448912433,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":174.0,
        "Answer_body":"<p>Yes, though it depends on what part of SageMaker you mean (Training, Notebook, Inference, etc).<\/p>\n<p>Last week, an integration between SageMaker and Delta Lake was documented here (custom docker in the SageMaker Processing API)<\/p>\n<p><a href=\"https:\/\/github.com\/eitansela\/sagemaker-delta-sharing-demo\/tree\/main\/delta_lake_bring_your_own_container_processing\" rel=\"nofollow noreferrer\">https:\/\/github.com\/eitansela\/sagemaker-delta-sharing-demo\/tree\/main\/delta_lake_bring_your_own_container_processing<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70603137",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1641839983127,
        "Question_original_content":"possibl integr delta lake possibl integr aw sage maker delta lake thank ramabadran",
        "Question_preprocessed_content":"possibl integr delta lake possibl integr aw sage maker delta lake thank ramabadran",
        "Question_gpt_summary_original":"The user is seeking information on whether it is possible to integrate AWS Sagemaker and Delta Lake. No specific challenges are mentioned.",
        "Question_gpt_summary":"user seek inform possibl integr delta lake specif challeng mention",
        "Answer_original_content":"ye depend mean train notebook infer week integr delta lake document custom docker process api http github com eitansela delta share demo tree main delta lake bring contain process",
        "Answer_preprocessed_content":"ye depend mean week integr delta lake document",
        "Answer_gpt_summary_original":"possible solution: it is possible to integrate aws sage maker and delta lake, depending on the specific part of the integration needed (training, notebook, inference, etc). an integration between the two was documented last week using a custom docker in the processing api, which can be found on github.",
        "Answer_gpt_summary":"possibl solut possibl integr aw sage maker delta lake depend specif integr need train notebook infer integr document week custom docker process api github"
    },
    {
        "Question_id":null,
        "Question_title":"CROSS VALIDATION WITH HYPER PARAMETER TUNING IN AZURE ML NOTEBOOK USING PYTHON SDK",
        "Question_body":"is there any way to use cross validation in azure notebook using python sdk, while using hyperdrive config for hyper parameter tuning?",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1624357454270,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/446761\/cross-validation-with-hyper-parameter-tuning-in-az.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-07T03:36:08.927Z",
                "Answer_score":0,
                "Answer_body":"@krishnakategaru-8347 Thanks, Please follow the document for Sampling the hyperparameter space.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters#sampling-the-hyperparameter-space",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"cross valid hyper paramet tune notebook python sdk wai us cross valid azur notebook python sdk hyperdr config hyper paramet tune",
        "Question_preprocessed_content":"cross valid hyper paramet tune notebook python sdk wai us cross valid azur notebook python sdk hyperdr config hyper paramet tune",
        "Question_gpt_summary_original":"The user is facing a challenge in implementing cross validation with hyper parameter tuning in Azure ML Notebook using Python SDK. They are seeking a solution to this problem.",
        "Question_gpt_summary":"user face challeng implement cross valid hyper paramet tune notebook python sdk seek solut problem",
        "Answer_original_content":"krishnakategaru thank follow document sampl hyperparamet space http doc microsoft com azur machin learn tune hyperparamet sampl hyperparamet space",
        "Answer_preprocessed_content":"thank follow document sampl hyperparamet space",
        "Answer_gpt_summary_original":"the answer suggests following the document for sampling the hyperparameter space to use cross validation in azure notebook using python sdk while using hyperdrive config for hyper parameter tuning. no other solutions are mentioned.",
        "Answer_gpt_summary":"answer suggest follow document sampl hyperparamet space us cross valid azur notebook python sdk hyperdr config hyper paramet tune solut mention"
    },
    {
        "Question_id":null,
        "Question_title":"Specifying AzureML output destination in SDK v2",
        "Question_body":"Hi. I have set up an AzureML pipeline with YAML components using the Python SDK (v2) with an attached blob store. However, it appears that the output destination is handled automatically by AzureML and so I can't specify where on the blob the pipeline writes its output. I want to configure the AzureML pipeline run using ADF, which involves moving some data to the blob, running the AzureML pipeline, and then moving some data from the blob to somewhere else. The trouble is that ADF doesn't get access to the AzureML output directory, and so it won't know where to look for the output file.\n\nI have tried to pass the output directory as an input rather than an output so that I can explicitly state where this should go. The directory, however, gets mounted as read only (quite sensibly by design, I trust) so that doesn't work. So I'm kind of running out of options.\n\nIs there any way for me specify the output path for an Azure ML SDK v2 pipeline in a similar way to how I would specify an input path? Alternatively, is there another way of solving this particular predicament of mine?\n\nI have looked through the notebooks (e.g. https:\/\/github.com\/Azure\/azureml-examples\/blob\/8a4070f55593c9641083784283b773f4f20955dd\/sdk\/jobs\/pipelines\/1a_pipeline_with_components_from_yaml\/pipeline_with_components_from_yaml.ipynb) and I can't find an example where people explicitly control the output destination (which seems odd).\n\nThoughts?",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1660825064220,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/972481\/specifying-azureml-output-destination-in-sdk-v2.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-10T10:39:09.38Z",
                "Answer_score":1,
                "Answer_body":"Any updates on this?\nI have the same problem, it seems you can specify the path in the output like\noutputs={\n\"output_path\": Output(type=\"uri_folder\", mode=\"rw_mount\", path=<path>),\n}\nIt doesnt throw an error, however the path is ignored anyways...",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"specifi output destin sdk set pipelin yaml compon python sdk attach blob store appear output destin handl automat specifi blob pipelin write output want configur pipelin run adf involv move data blob run pipelin move data blob troubl adf access output directori won know look output file tri pass output directori input output explicitli state directori get mount read sensibl design trust work kind run option wai specifi output path sdk pipelin similar wai specifi input path altern wai solv particular predica look notebook http github com azur exampl blob afcbffdd sdk job pipelin pipelin compon yaml pipelin compon yaml ipynb exampl peopl explicitli control output destin odd thought",
        "Question_preprocessed_content":"specifi output destin sdk set pipelin yaml compon python sdk attach blob store appear output destin handl automat specifi blob pipelin write output want configur pipelin run adf involv move data blob run pipelin move data blob troubl adf access output directori won know look output file tri pass output directori input output explicitli state directori get mount read work kind run option wai specifi output path sdk pipelin similar wai specifi input path altern wai solv particular predica look notebook exampl peopl explicitli control output destin thought",
        "Question_gpt_summary_original":"The user is facing challenges in specifying the output destination for an AzureML pipeline using the Python SDK (v2) with an attached blob store. The output destination is handled automatically by AzureML, and the user cannot specify where on the blob the pipeline writes its output. This is causing issues when configuring the AzureML pipeline run using ADF, as ADF doesn't get access to the AzureML output directory, and so it won't know where to look for the output file. The user has tried passing the output directory as an input, but it gets mounted as read-only. The user is looking for a way to specify the output path for an Azure ML SDK v2 pipeline or another way of solving this predicament.",
        "Question_gpt_summary":"user face challeng specifi output destin pipelin python sdk attach blob store output destin handl automat user specifi blob pipelin write output caus issu configur pipelin run adf adf access output directori won know look output file user tri pass output directori input get mount read user look wai specifi output path sdk pipelin wai solv predica",
        "Answer_original_content":"updat problem specifi path output like output output path output type uri folder mode mount path doesnt throw error path ignor anywai",
        "Answer_preprocessed_content":"updat problem specifi path output like output path doesnt throw error path ignor",
        "Answer_gpt_summary_original":"the answer suggests a possible solution to the problem of specifying an output destination in sdk v2 for an azure pipeline run using adf. the solution involves specifying the path in the output using a specific format, but it may not work as expected.",
        "Answer_gpt_summary":"answer suggest possibl solut problem specifi output destin sdk azur pipelin run adf solut involv specifi path output specif format work expect"
    },
    {
        "Question_id":null,
        "Question_title":"How do I fix the ImportError: cannot import name 'delayed'?",
        "Question_body":"I am using the LORAS package from pyloras for imbalanced learning, but it requires importing the delayed package. I used the pip.main() to install the delayed package and from delayed.delay imported delayed. However, I am still getting the following error: ImportError: cannot import name 'delayed'. I appreciate your help and suggestions.",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1664630157437,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1031590\/how-do-i-fix-the-importerror-cannot-import-name-39.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-10T05:07:54.997Z",
                "Answer_score":1,
                "Answer_body":"You cannot import a custom environment in execute python script. You can install some packages with execute python script, but it has limitations on packages that can be installed depending on the number of dependencies of the package. Glad you can use the notebook and work on your experiments now.\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"fix importerror import delai lora packag pylora imbalanc learn requir import delai packag pip main instal delai packag delai delai import delai get follow error importerror import delai appreci help suggest",
        "Question_preprocessed_content":"fix importerror import delai lora packag pylora imbalanc learn requir import delai packag instal delai packag import delai get follow error importerror import delai appreci help suggest",
        "Question_gpt_summary_original":"The user is encountering an ImportError while trying to import the 'delayed' package for imbalanced learning using the LORAS package from pyloras. Despite installing the package using pip.main() and importing it using from delayed.delay, the user is still unable to import the package and is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount importerror try import delai packag imbalanc learn lora packag pylora despit instal packag pip main import delai delai user unabl import packag seek help resolv issu",
        "Answer_original_content":"import custom environ execut python script instal packag execut python script limit packag instal depend number depend packag glad us notebook work experi answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"import custom environ execut python script instal packag execut python script limit packag instal depend number depend packag glad us notebook work experi answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"there are no specific solutions provided in the answer for the importerror issue with the loras package. however, it suggests that executing a python script has limitations on the packages that can be installed depending on the number of dependencies of the package. the answer also suggests using a notebook to work on experiments instead.",
        "Answer_gpt_summary":"specif solut provid answer importerror issu lora packag suggest execut python script limit packag instal depend number depend packag answer suggest notebook work experi instead"
    },
    {
        "Question_id":null,
        "Question_title":"Azure OpenAI advantages",
        "Question_body":"We have a customer that is interested in the Azure OpenAI Service and had a few question:\n\nWhat are the advantages for using Azure OpenAI vs. OpenAI API.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648125540293,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@App-4824 Thanks for the question. The Azure OpenAI team always works to make the latest models available in Azure as soon as they are available from OpenAI. This is an active area of work for the team to tighten release date for future models.\nThe Azure service is backed by an SLA (which typically a customer's primary need for high availability, low latency), and the support that comes along with Azure services. This is the first commercialized model of its kind from any public cloud provider. This is unique advantage to be first to market and to work closely with customer to adopt\/embrace these models for production use.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/785866\/azure-openai-advantages.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-24T14:21:38.047Z",
                "Answer_score":0,
                "Answer_body":"@App-4824 Thanks for the question. The Azure OpenAI team always works to make the latest models available in Azure as soon as they are available from OpenAI. This is an active area of work for the team to tighten release date for future models.\nThe Azure service is backed by an SLA (which typically a customer's primary need for high availability, low latency), and the support that comes along with Azure services. This is the first commercialized model of its kind from any public cloud provider. This is unique advantage to be first to market and to work closely with customer to adopt\/embrace these models for production use.",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1648131698047,
        "Question_original_content":"azur openai advantag custom interest azur openai servic question advantag azur openai openai api",
        "Question_preprocessed_content":"azur openai advantag custom interest azur openai servic question advantag azur openai openai api",
        "Question_gpt_summary_original":"The user is interested in the advantages of using Azure OpenAI compared to OpenAI API.",
        "Question_gpt_summary":"user interest advantag azur openai compar openai api",
        "Answer_original_content":"app thank question azur openai team work latest model avail azur soon avail openai activ area work team tighten releas date futur model azur servic back sla typic custom primari need high avail low latenc support come azur servic commerci model kind public cloud provid uniqu advantag market work close custom adopt embrac model product us",
        "Answer_preprocessed_content":"thank question azur openai team work latest model avail azur soon avail openai activ area work team tighten releas date futur model azur servic back sla support come azur servic commerci model kind public cloud provid uniqu advantag market work close custom model product us",
        "Answer_gpt_summary_original":"possible solutions mentioned in the answer are:\n\n- azure openai team works to make the latest models available in azure as soon as they are available from openai.\n- azure openai service is backed by an sla, which provides high availability and low latency.\n- azure openai service comes with support from azure services.\n- azure openai is the first commercialized model of its kind from any public cloud provider, which gives a unique advantage to be first to market and work closely with customers to adopt and embrace these models for production use. \n\nin summary, the advantages of using azure openai over openai api include the availability of the latest models, sla-backed service, support from azure services, and being the first commercialized model of its kind from any public cloud provider.",
        "Answer_gpt_summary":"possibl solut mention answer azur openai team work latest model avail azur soon avail openai azur openai servic back sla provid high avail low latenc azur openai servic come support azur servic azur openai commerci model kind public cloud provid give uniqu advantag market work close custom adopt embrac model product us summari advantag azur openai openai api includ avail latest model sla back servic support azur servic commerci model kind public cloud provid"
    },
    {
        "Question_id":70873792.0,
        "Question_title":"How to handle Sagemaker Batch Transform discarding a file with a failed model request",
        "Question_body":"<p>I have a large number of JSON requests for a model split across multiple files in an S3 bucket. I would like to use Sagemaker's Batch Transform feature to process all of these requests (I have done a couple of test runs using small amounts of data and the transform job succeeds). My main issue is here (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors<\/a>), specifically:<\/p>\n<blockquote>\n<p>If a batch transform job fails to process an input file because of a problem with the dataset, SageMaker marks the job as failed. If an input file contains a bad record, the transform job doesn't create an output file for that input file because doing so prevents it from maintaining the same order in the transformed data as in the input file. When your dataset has multiple input files, a transform job continues to process input files even if it fails to process one. The processed files still generate useable results.<\/p>\n<\/blockquote>\n<p>This is not preferable mainly because if 1 request fails (whether its a transient error, a malformmated request, or something wrong with the model container) in a file with a large number of requests, all of those requests will get discarded (even if all of them succeeded and the last one failed). I would ideally prefer Sagemaker to just write the output of the failed response to the file and keep going, rather than discarding the entire file.<\/p>\n<p>My question is, are there any suggestions to mitigating this issue? I was thinking about storing 1 request per file in S3, but this seems somewhat ridiculous? Even if I did this, is there a good way of seeing which requests specifically failed after the transform job finishes?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643261540133,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":357.0,
        "Answer_body":"<p>You've got the right idea: the fewer datapoints are in each file, the less likely a given file is to fail. The issue is that while you can pass a prefix with many files to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB\" rel=\"nofollow noreferrer\">CreateTransformJob<\/a>, partitioning one datapoint per file at least requires an S3 read per datapoint, plus a model invocation per datapoint, which is probably not great. Be aware also that <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=1000415&amp;tstart=0\" rel=\"nofollow noreferrer\">apparently there are hidden rate limits<\/a>.<\/p>\n<p>Here are a couple options:<\/p>\n<ol>\n<li><p>Partition into small-ish files, and plan on failures being rare. Hopefully, not many of your datapoints would actually fail. If you partition your dataset into e.g. 100 files, then a single failure only requires reprocessing 1% of your data. Note that Sagemaker has built-in retries, too, so most of the time failures should be caused by your data\/logic, not randomness on Sagemaker's side.<\/p>\n<\/li>\n<li><p>Deal with failures directly in your model. The same doc you quoted in your question also says:<\/p>\n<\/li>\n<\/ol>\n<blockquote>\n<p>If you are using your own algorithms, you can use placeholder text, such as ERROR, when the algorithm finds a bad record in an input file. For example, if the last record in a dataset is bad, the algorithm places the placeholder text for that record in the output file.<\/p>\n<\/blockquote>\n<p>Note that the reason Batch Transform does this whole-file failure is to maintain a 1-1 mapping between rows in the input and the output. If you can substitute the output for failed datapoints with an error message from inside your model, without actually causing the model itself to fail processing, Batch Transform will be happy.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1644528409280,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70873792",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1644424499008,
        "Question_original_content":"handl batch transform discard file fail model request larg number json request model split multipl file bucket like us batch transform featur process request coupl test run small amount data transform job succe main issu http doc aw amazon com latest batch transform html batch transform error specif batch transform job fail process input file problem dataset mark job fail input file contain bad record transform job creat output file input file prevent maintain order transform data input file dataset multipl input file transform job continu process input file fail process process file gener useabl result prefer mainli request fail transient error malformm request wrong model contain file larg number request request discard succeed fail ideal prefer write output fail respons file go discard entir file question suggest mitig issu think store request file somewhat ridicul good wai see request specif fail transform job finish",
        "Question_preprocessed_content":"handl batch transform discard file fail model request larg number json request model split multipl file bucket like us batch transform featur process request main issu specif batch transform job fail process input file problem dataset mark job fail input file contain bad record transform job creat output file input file prevent maintain order transform data input file dataset multipl input file transform job continu process input file fail process process file gener useabl result prefer mainli request fail file larg number request request discard ideal prefer write output fail respons file go discard entir file question suggest mitig issu think store request file somewhat ridicul good wai see request specif fail transform job finish",
        "Question_gpt_summary_original":"The user is facing challenges with Sagemaker Batch Transform feature while processing a large number of JSON requests split across multiple files in an S3 bucket. The main issue is that if one request fails in a file with multiple requests, all requests in that file get discarded. The user is looking for suggestions to mitigate this issue and is considering storing one request per file in S3, but is unsure if this is a practical solution and how to identify which requests failed after the transform job finishes.",
        "Question_gpt_summary":"user face challeng batch transform featur process larg number json request split multipl file bucket main issu request fail file multipl request request file discard user look suggest mitig issu consid store request file unsur practic solut identifi request fail transform job finish",
        "Answer_original_content":"got right idea fewer datapoint file like given file fail issu pass prefix file createtransformjob partit datapoint file requir read datapoint plu model invoc datapoint probabl great awar appar hidden rate limit coupl option partit small ish file plan failur rare hopefulli datapoint actual fail partit dataset file singl failur requir reprocess data note built retri time failur caus data logic random deal failur directli model doc quot question sai algorithm us placehold text error algorithm find bad record input file exampl record dataset bad algorithm place placehold text record output file note reason batch transform file failur maintain map row input output substitut output fail datapoint error messag insid model actual caus model fail process batch transform happi",
        "Answer_preprocessed_content":"got right idea fewer datapoint file like given file fail issu pass prefix file createtransformjob partit datapoint file requir read datapoint plu model invoc datapoint probabl great awar appar hidden rate limit coupl option partit file plan failur rare hopefulli datapoint actual fail partit dataset file singl failur requir reprocess data note retri time failur caus random deal failur directli model doc quot question sai algorithm us placehold text error algorithm find bad record input file exampl record dataset bad algorithm place placehold text record output file note reason batch transform failur maintain map row input output substitut output fail datapoint error messag insid model actual caus model fail process batch transform happi",
        "Answer_gpt_summary_original":"the answer suggests two possible solutions to mitigate the issue of batch transform discarding a file with a failed model request. the first solution is to partition the dataset into small-ish files, and plan on failures being rare. the second solution is to deal with failures directly in the model by using placeholder text, such as error, when the algorithm finds a bad record in an input file.",
        "Answer_gpt_summary":"answer suggest possibl solut mitig issu batch transform discard file fail model request solut partit dataset small ish file plan failur rare second solut deal failur directli model placehold text error algorithm find bad record input file"
    },
    {
        "Question_id":null,
        "Question_title":"Issue with data lake mounting in custom RStudio application Azure ML",
        "Question_body":"previously while creating a compute instance we were able to see RStudio application by default and we were able to mount\/access the data lake from RStudio.\n![compute creation][1]\n\n\n\n\n![Data lake mont][2]\n2. In current situation we are not able to access RStudio application by default.\n![234345-4.png][3]\n3.with the help of below link we are able to create custom RStudio application\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=azure-studio\n![custom RStudio app][4]\n\n4.In custom RStudio we are not able to mount\/access the data lake.\n\n![missing data lake][5]\n\nIs there way to mount\/access the data lake in custom RStudio app\n[1]: \/answers\/storage\/attachments\/234344-screenshot-2022-08-23-170502.png\n[2]: \/answers\/storage\/attachments\/234353-5.png\n[3]: \/answers\/storage\/attachments\/234345-4.png\n[4]: \/answers\/storage\/attachments\/234314-2.png\n[5]: \/answers\/storage\/attachments\/234361-3.png",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661155798650,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/976098\/install-rstudio-application-in-azure-ml-vm.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-23T02:41:16.113Z",
                "Answer_score":0,
                "Answer_body":"@alifshaikh-6049 Thanks for the question. Here is the snapshot for using the Rstudio from ml.azure.com\n\n\nHere is the sample to train Rmodel.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"issu data lake mount custom rstudio applic previous creat comput instanc abl rstudio applic default abl mount access data lake rstudio comput creation data lake mont current situat abl access rstudio applic default png help link abl creat custom rstudio applic http doc microsoft com azur machin learn creat manag comput instanc tab azur studio custom rstudio app custom rstudio abl mount access data lake miss data lake wai mount access data lake custom rstudio app answer storag attach screenshot png answer storag attach png answer storag attach png answer storag attach png answer storag attach png",
        "Question_preprocessed_content":"issu data lake mount custom rstudio applic previous creat comput instanc abl rstudio applic default abl data lake rstudio current situat abl access rstudio applic default help link abl creat custom rstudio applic custom rstudio abl data lake wai data lake custom rstudio app",
        "Question_gpt_summary_original":"The user is facing challenges with accessing the data lake from a custom RStudio application in Azure ML. Previously, they were able to access the data lake from the default RStudio application, but now they cannot access the default RStudio application. They have created a custom RStudio application using a provided link, but they are unable to mount\/access the data lake from it. The user is seeking a solution to mount\/access the data lake in the custom RStudio application.",
        "Question_gpt_summary":"user face challeng access data lake custom rstudio applic previous abl access data lake default rstudio applic access default rstudio applic creat custom rstudio applic provid link unabl mount access data lake user seek solut mount access data lake custom rstudio applic",
        "Answer_original_content":"alifshaikh thank question snapshot rstudio azur com sampl train rmodel",
        "Answer_preprocessed_content":"thank question snapshot rstudio sampl train rmodel",
        "Answer_gpt_summary_original":"the answer does not provide any solutions to the user's challenge with mounting\/accessing the data lake in a custom rstudio application. instead, it provides a snapshot for using rstudio from ml.azure.com and a sample to train an r model.",
        "Answer_gpt_summary":"answer provid solut user challeng mount access data lake custom rstudio applic instead provid snapshot rstudio azur com sampl train model"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML Endpoint Wont deploy on AKS with Azure ML Studio",
        "Question_body":"Got problem while deploying Azure ML real-time Endpoint on AKS with Azure ML Studio. It remains in Transitioning state indefinitely. Looked in AKS event logs and found that error is\n\nFailed to pull image\n\"viennaglobal.azurecr.io\/azureml\/azureml_f7815e0137b51ac1464986018a7cb849\":\n[rpc error: code = Unknown desc = Error response from daemon: Get\nhttps:\/\/viennaglobal.azurecr.io\/v2\/azureml\/azureml_f7815e0137b51ac1464986018a7cb849\/manifests\/latest:\nunauthorized: Application not registered with AAD., rpc error: code = Unknown\ndesc = Error response from daemon: Get\nhttps:\/\/viennaglobal.azurecr.io\/v2\/azureml\/azureml_f7815e0137b51ac1464986018a7cb849\/manifests\/latest:\nunauthorized: authentication required, visit https:\/\/aka.ms\/acr\/authorization\nfor more information.]\n\nIt repeat this action with no success. This Inference cluster created with Azure ML Dashboard, and it worked week ago without any unauthorized problems. Tried to create new inference clusters, endpoints but with no success",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1600005134313,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/93418\/azure-ml-endpoint-won39t-deploy-on-aks-with-azure.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-09-14T16:49:36.553Z",
                "Answer_score":0,
                "Answer_body":"@Eugene-3520 Thanks for the question.This is likely because you don't permission to deploy to AKS. Can you share your role definition?We have forwarded to the product team to check on this.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-assign-roles#frequently-asked-questions",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"endpoint wont deploi ak studio got problem deploi real time endpoint ak studio remain transit state indefinit look ak event log error fail pull imag viennaglob azurecr febacacb rpc error code unknown desc error respons daemon http viennaglob azurecr febacacb manifest latest unauthor applic regist aad rpc error code unknown desc error respons daemon http viennaglob azurecr febacacb manifest latest unauthor authent requir visit http aka acr author inform repeat action success infer cluster creat dashboard work week ago unauthor problem tri creat new infer cluster endpoint success",
        "Question_preprocessed_content":"endpoint wont deploi ak studio got problem deploi endpoint ak studio remain transit state indefinit look ak event log error fail pull imag rpc error code unknown desc error respons daemon unauthor applic regist rpc error code unknown desc error respons daemon unauthor authent requir visit repeat action success infer cluster creat dashboard work week ago unauthor problem tri creat new infer cluster endpoint success",
        "Question_gpt_summary_original":"The user is facing challenges while deploying an Azure ML real-time Endpoint on AKS with Azure ML Studio. The endpoint remains in a transitioning state indefinitely, and the AKS event logs show an error related to unauthorized access to the image. The user has tried creating new inference clusters and endpoints, but the issue persists.",
        "Question_gpt_summary":"user face challeng deploi real time endpoint ak studio endpoint remain transit state indefinit ak event log error relat unauthor access imag user tri creat new infer cluster endpoint issu persist",
        "Answer_original_content":"eugen thank question like permiss deploi ak share role definit forward product team check http doc microsoft com azur machin learn assign role frequent ask question",
        "Answer_preprocessed_content":"thank like permiss deploi ak share role definit forward product team check",
        "Answer_gpt_summary_original":"possible solutions: \n- check if the user has permission to deploy to aks and share the role definition. \n- the issue has been forwarded to the product team for further investigation.",
        "Answer_gpt_summary":"possibl solut check user permiss deploi ak share role definit issu forward product team investig"
    },
    {
        "Question_id":47921875.0,
        "Question_title":"Accessing files in Mongodb",
        "Question_body":"<p>I am using sacred package in python, this allows to keep track of computational experiments i'm running. sacred allows to add observer (<code>mongodb<\/code>) which stores all sorts of information regarding the experiment (<code>configuration<\/code>, <code>source files<\/code> etc).\n<code>sacred<\/code> allows to add artifacts to the db bt using <code>sacred.Experiment.add_artifact(PATH_TO_FILE).<\/code><\/p>\n\n<p>This command essentially adds the file to the DB.<\/p>\n\n<p>I'm using MongoDB compass, I can access the experiment information and see that an artifact has been added. it contains two fields:\n'<code>name<\/code>' and '<code>file_id<\/code>' which contains an <code>ObjectId<\/code>. (see image)<\/p>\n\n<p>I am attempting to access the stored file itself. i have noticed that under my db there is an additional <code>sub-db<\/code> called <code>fs.files<\/code> in it i can filter to find my <code>ObjectId<\/code> but it does not seem to allow me to access to content of the file itself.<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/SBg8m.png\" alt=\"object id under .files\"><\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/B7ymG.png\" alt=\"file_id under artifact\/object\"><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1513848655213,
        "Question_favorite_count":null,
        "Question_last_edit_time":1525602466710,
        "Question_score":1.0,
        "Question_view_count":2457.0,
        "Answer_body":"<p>MongoDB file storage is handled by \"GridFS\" which basically splits up files in chunks and stores them in a collection (fs.files).<\/p>\n\n<p>Tutorial to access: <a href=\"http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html\" rel=\"nofollow noreferrer\">http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47921875",
        "Tool":"Sacred",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1513855746928,
        "Question_original_content":"access file mongodb packag python allow track comput experi run allow add observ mongodb store sort inform experi configur sourc file allow add artifact experi add artifact path file command essenti add file mongodb compass access experi inform artifact ad contain field file contain objectid imag attempt access store file notic addit sub call file filter objectid allow access content file",
        "Question_preprocessed_content":"access file mongodb packag python allow track comput experi run allow add observ store sort inform experi allow add artifact command essenti add file mongodb compass access experi inform artifact ad contain field contain attempt access store file notic addit call filter allow access content file",
        "Question_gpt_summary_original":"The user is facing challenges in accessing files stored in MongoDB using the sacred package in Python. Although the user can add artifacts to the database, they are unable to access the content of the file itself. The user has identified an additional sub-database called \"fs.files\" under their database, but it does not allow them to access the content of the file.",
        "Question_gpt_summary":"user face challeng access file store mongodb packag python user add artifact databas unabl access content file user identifi addit sub databas call file databas allow access content file",
        "Answer_original_content":"mongodb file storag handl gridf basic split file chunk store collect file tutori access http api mongodb com python current exampl gridf html",
        "Answer_preprocessed_content":"mongodb file storag handl gridf basic split file chunk store collect tutori access",
        "Answer_gpt_summary_original":"the solution to accessing stored files in mongodb is to use \"gridfs,\" which splits up files into chunks and stores them in a collection called \"fs.files.\" a tutorial on how to access these files can be found at http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html.",
        "Answer_gpt_summary":"solut access store file mongodb us gridf split file chunk store collect call file tutori access file http api mongodb com python current exampl gridf html"
    },
    {
        "Question_id":null,
        "Question_title":"Run Time is inaccurate because of including upload time",
        "Question_body":"<p>Some runs will spend minutes because of my terrible network:<\/p>\n<pre><code class=\"lang-shell\">wandb: Waiting for W&amp;B process to finish... (success).\n<\/code><\/pre>\n<p>I find the Run Time column in UI will also contain the uploading time (by comparing with other runs\u2019 Run Time).<\/p>\n<p>My script is organized as follow:<\/p>\n<pre><code class=\"lang-python\">def main(config):\n    ...\n    wandb.init(wandb_config)\n    ...\n\nif __name__ == '__main__':\n    config = blabla\n    for p in [p1, p2, p3]:  # for loop to tune hyperparameters\n        config.param = p\n        main(config)\n<\/code><\/pre>\n<p>To fix this issue, should I use <code>wandb.finish<\/code> in the end of the <code>main()<\/code> function? As the <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/finish\">doc<\/a> of <code>wandb.finish<\/code> lists:<\/p>\n<blockquote>\n<p>Marks a run as finished, and finishes uploading all data.<\/p>\n<\/blockquote>\n<p>I worry about whether this func will kill my slow data uploading worker.<\/p>\n<p>Or any other solutions?<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1670249098960,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":83.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/run-time-is-inaccurate-because-of-including-upload-time\/3499",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-12-06T10:04:08.165Z",
                "Answer_body":"<p>I find the Run Time is also inaccurate when sync the offline-run instance. After sync, the Run-Time is about 1 min (seems to be the time spent on the sync command).<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-08T14:34:09.006Z",
                "Answer_body":"<p>Hi Yago!<\/p>\n<p>Looking into this.<\/p>\n<p>Cheers!<br>\nArtsiom<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-14T22:12:44.048Z",
                "Answer_body":"<p>Hi Yao!<\/p>\n<p>Sorry for the late response. I have consulted with a few others on my team and this seems to be a bug, considering the fact that you still get the wrong time using the offline mode.<\/p>\n<p>Would you send us some info on how to reproduce this so we can fix it asap?<\/p>\n<p>Cheers!<br>\nArtsiom<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-16T10:05:55.002Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"artsiom\" data-post=\"4\" data-topic=\"3499\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sea2.discourse-cdn.com\/business7\/user_avatar\/community.wandb.ai\/artsiom\/40\/1001_2.png\" class=\"avatar\"> artsiom:<\/div>\n<blockquote>\n<p>reproduce<\/p>\n<\/blockquote>\n<\/aside>\n<p>I think reproduce this issue is easy.<\/p>\n<ol>\n<li>Run wandb in offline mode.<\/li>\n<li>After the run is finish, use <code>wandb sync run_path<\/code> command to upload the run.<\/li>\n<li>The RunTime colum is several seconds (time the <code>sync<\/code> command spent) in the UI of uploaded run.<\/li>\n<\/ol>\n<p>My test code is like:<\/p>\n<pre><code class=\"lang-python\">os.environ['WANDB_MODE'] = 'offline'\nrun = wandb.init(...)\n...\nrun.finish()\n<\/code><\/pre>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-21T20:48:23.323Z",
                "Answer_body":"<p>Hi Yao!<\/p>\n<p>Thank you for the directions. I was able to reproduce it and will send it over to the engineers!<br>\nThe only real workaround for this is recording the run-times yourself and then logging them to wandb.<\/p>\n<p>Artsiom<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-27T21:36:19.734Z",
                "Answer_body":"<p>Hi Yao,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-03T15:30:48.984Z",
                "Answer_body":"<p>Hi Yao, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-14T10:06:10.411Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"run time inaccur includ upload time run spend minut terribl network wait process finish success run time column contain upload time compar run run time script organ follow def main config init config main config blabla loop tune hyperparamet config param main config fix issu us finish end main function doc finish list mark run finish finish upload data worri func kill slow data upload worker solut",
        "Question_preprocessed_content":"run time inaccur includ upload time run spend minut terribl network run time column contain upload time script organ follow fix issu us end function doc list mark run finish finish upload data worri func kill slow data upload worker solut",
        "Question_gpt_summary_original":"The user is facing challenges with inaccurate run time due to slow network upload time. The Run Time column in the UI includes the uploading time, which affects the accuracy of the data. The user is considering using wandb.finish to fix the issue but is concerned about the impact on the slow data uploading worker. The user is seeking alternative solutions to address the problem.",
        "Question_gpt_summary":"user face challeng inaccur run time slow network upload time run time column includ upload time affect accuraci data user consid finish fix issu concern impact slow data upload worker user seek altern solut address problem",
        "Answer_original_content":"run time inaccur sync offlin run instanc sync run time min time spent sync command yago look cheer artsiom yao sorri late respons consult team bug consid fact wrong time offlin mode send info reproduc fix asap cheer artsiom artsiom reproduc think reproduc issu easi run offlin mode run finish us sync run path command upload run runtim colum second time sync command spent upload run test code like environ mode offlin run init run finish yao thank direct abl reproduc send engin real workaround record run time log artsiom yao want follow support request heard let know assist issu resolv best yao heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"run time inaccur sync instanc sync min yago look cheer artsiom yao sorri late respons consult team bug consid fact wrong time offlin mode send info reproduc fix asap cheer artsiom artsiom reproduc think reproduc issu easi run offlin mode run finish us command upload run runtim colum second upload run test code like yao thank direct abl reproduc send engin real workaround record log artsiom yao want follow support request heard let know assist issu resolv best yao heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the user is experiencing inaccuracies in reported run time due to time spent uploading data. the answer suggests that this is a known bug and the only workaround is to record the run times manually and log them. the user is asked to provide information on how to reproduce the issue so that it can be fixed. the conversation ends with a request for the user to confirm if the issue has been resolved or if further assistance is needed.",
        "Answer_gpt_summary":"user experienc inaccuraci report run time time spent upload data answer suggest known bug workaround record run time manual log user ask provid inform reproduc issu fix convers end request user confirm issu resolv assist need"
    },
    {
        "Question_id":59392060.0,
        "Question_title":"Azure ML platform wrongly interprets uploaded dataset",
        "Question_body":"<p>I have a data set with about 7999 attributes and 39 labels, with 3339 total observations (resulting in 3339x8038 data set), and I'm trying to upload id to Azure ML platform.\nI've selected the 'type' as 'tabular', encoding as 'utf-8', no row skipping, and use header from first file.\nThe problem is, that the headers are still not included and the data is interpreted as string with 0s, 1s, and commas (see pic <a href=\"https:\/\/imgur.com\/a\/QdQNt1y\" rel=\"nofollow noreferrer\">https:\/\/imgur.com\/a\/QdQNt1y<\/a>)<\/p>\n\n<p>Am I missing something? For smaller data sets it seemed to work. My headers are A1, ... A7999 for the attributes, and L1, ... L39 for the labels.<\/p>\n\n<p>Thanks for help in advance.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1576672108560,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":49.0,
        "Answer_body":"<p>our system does our best guess over file settings when you try to create a dataset, but cannot guarantee perfect guesses in all cases. <\/p>\n\n<p>In such scenarios, you should be able to adjust the settings. We had a bug with the ability to change those settings, but rolled out a fix. Can you try to change those now?  <\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59392060",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1576813812910,
        "Question_original_content":"platform wrongli interpret upload dataset data set attribut label total observ result data set try upload platform select type tabular encod utf row skip us header file problem header includ data interpret string comma pic http imgur com qdqnty miss smaller data set work header attribut label thank help advanc",
        "Question_preprocessed_content":"platform wrongli interpret upload dataset data set attribut label total observ try upload platform select type tabular encod row skip us header file problem header includ data interpret string comma miss smaller data set work header attribut label thank help advanc",
        "Question_gpt_summary_original":"The user is facing challenges in uploading a large dataset with 7999 attributes and 39 labels to Azure ML platform. Despite selecting the 'type' as 'tabular', encoding as 'utf-8', and no row skipping, the headers are not included and the data is interpreted as a string with 0s, 1s, and commas. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user face challeng upload larg dataset attribut label platform despit select type tabular encod utf row skip header includ data interpret string comma user seek help resolv issu",
        "Answer_original_content":"best guess file set try creat dataset guarante perfect guess case scenario abl adjust set bug abil chang set roll fix try chang",
        "Answer_preprocessed_content":"best guess file set try creat dataset guarante perfect guess case scenario abl adjust set bug abil chang set roll fix try chang",
        "Answer_gpt_summary_original":"possible solutions: \n- adjust the file settings to correctly interpret the uploaded dataset.\n- check if the bug with the ability to change settings has been fixed and try to change the settings now.",
        "Answer_gpt_summary":"possibl solut adjust file set correctli interpret upload dataset check bug abil chang set fix try chang set"
    },
    {
        "Question_id":null,
        "Question_title":"Save trained model from AutoML\/Designer as pickle file to disk - Azure ML",
        "Question_body":"Hi,\n\nI want to save trained machine learning model as pickle file(.pkl) to disk which is trained in AutoML\/Designer.\n\nPlease let me know is there any way to do that?\n\nThanks",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1617893768667,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/349669\/save-trained-model-from-automldesigner-as-pickle-f.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-08T15:17:00.257Z",
                "Answer_score":0,
                "Answer_body":"Hi @Bhaskar11-9991\n\nRefer the below URL it may helps you.\nhttps:\/\/docs.microsoft.com\/en-us\/answers\/questions\/297882\/how-to-use-a-model-trained-by-azure-automl.html\n\n\n\n\nIf the Answer is helpful, please click Accept Answer and up-vote, this can be beneficial to other community members.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-08T17:20:45.26Z",
                "Answer_score":0,
                "Answer_body":"@Bhaskar11-9991 Thanks, Please follow this document: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-model-designer. Basically you can register a trained model in Designer bring it out with SDK\/CLI to deploy it.\n\nSharing a reference notebook from @Nicholas Moore: https:\/\/github.com\/nfmoore\/aml-designer-iot-edge\/blob\/main\/00-containerize-designer-model.ipynb.",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"save train model automl design pickl file disk want save train machin learn model pickl file pkl disk train automl design let know wai thank",
        "Question_preprocessed_content":"save train model pickl file disk want save train machin learn model pickl disk train let know wai thank",
        "Question_gpt_summary_original":"The user is facing a challenge in saving a trained machine learning model as a pickle file (.pkl) to disk, which was trained in AutoML\/Designer. They are seeking assistance in finding a way to accomplish this task.",
        "Question_gpt_summary":"user face challeng save train machin learn model pickl file pkl disk train automl design seek assist find wai accomplish task",
        "Answer_original_content":"bhaskar refer url help http doc microsoft com answer question us model train azur automl html answer help click accept answer vote benefici commun member bhaskar thank follow document http doc microsoft com azur machin learn deploi model design basic regist train model design bring sdk cli deploi share refer notebook nichola moor http github com nfmoor aml design iot edg blob main container design model ipynb",
        "Answer_preprocessed_content":"refer url help answer help click accept answer benefici commun member thank follow document basic regist train model design bring deploi share refer notebook moor",
        "Answer_gpt_summary_original":"the answer suggests referring to a url that may help in saving a trained machine learning model as a pickle file to disk that was trained in automl\/designer. additionally, the answer provides a document link that explains how to deploy the model designer. the answer also shares a reference notebook from nicholas moore that can be helpful.",
        "Answer_gpt_summary":"answer suggest refer url help save train machin learn model pickl file disk train automl design addition answer provid document link explain deploi model design answer share refer notebook nichola moor help"
    },
    {
        "Question_id":null,
        "Question_title":"Convert to tabular from file dataset",
        "Question_body":"Yesterday I ran the \u2018consume\u2019 code to pull the data onto the cluster and it seemed to work but I still can\u2019t use it from ML designer as it\u2019s a file dataset rather than a tabular dataset. I\u2019ve tried a couple of ways of converting it without success, my next avenue of exploration is to use the SDK rather than trying to do it through the console",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617956083027,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/350780\/convert-to-tabular-from-file-dataset.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-09T20:14:35.61Z",
                "Answer_score":0,
                "Answer_body":"Hi, Designer only supports tabular dataset, hence you'd need to use SDK to work with file dataset. Based on the documentation, there's no method for converting file dataset to tabular dataset. Is your file in csv format? Perhaps you can create a datastore and connect to your blob storage. Then, use the import data module in designer to connect to your datastore(data path in blob storage). You can also create tabular dataset from datastore. Let me know if you have further questions, thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"convert tabular file dataset yesterdai ran consum code pull data cluster work us design file dataset tabular dataset iv tri coupl wai convert success avenu explor us sdk try consol",
        "Question_preprocessed_content":"convert tabular file dataset yesterdai ran consum code pull data cluster work us design file dataset tabular dataset iv tri coupl wai convert success avenu explor us sdk try consol",
        "Question_gpt_summary_original":"The user is facing challenges in converting a file dataset to a tabular dataset, which is preventing them from using it in ML designer. They have attempted to convert it without success and are now considering using the SDK as an alternative method.",
        "Question_gpt_summary":"user face challeng convert file dataset tabular dataset prevent design attempt convert success consid sdk altern method",
        "Answer_original_content":"design support tabular dataset need us sdk work file dataset base document method convert file dataset tabular dataset file csv format creat datastor connect blob storag us import data modul design connect datastor data path blob storag creat tabular dataset datastor let know question thank",
        "Answer_preprocessed_content":"design support tabular dataset need us sdk work file dataset base document method convert file dataset tabular dataset file csv format creat datastor connect blob storag us import data modul design connect datastor creat tabular dataset datastor let know question thank",
        "Answer_gpt_summary_original":"possible solutions from the answer include using the sdk to work with file datasets, creating a datastore and connecting to blob storage, using the import data module in designer to connect to the datastore, and creating a tabular dataset from the datastore. however, there is no method for directly converting a file dataset to a tabular dataset in designer.",
        "Answer_gpt_summary":"possibl solut answer includ sdk work file dataset creat datastor connect blob storag import data modul design connect datastor creat tabular dataset datastor method directli convert file dataset tabular dataset design"
    },
    {
        "Question_id":null,
        "Question_title":"Atlas Integration",
        "Question_body":"Hi guys, me again ;)\n\n\nAre you guys planning on integrating Atlas for keeping hold of which version of a model is deployed for which business case?\n\n\nCheers,\nD.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1529644466000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":39.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/YRUxFOeiP4U",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-06-27T03:35:28",
                "Answer_body":"On Friday, June 22, 2018 at 11:14:26 AM UTC+2, Daan Gerits wrote:\nAre you guys planning on integrating Atlas for keeping hold of which version of a model is deployed for which business case?\n\n\n\n\n+1\n\n\n\nI'd cheer for the feature of \"tracking which version of a model is deployed for which business case\". (Whether it's done with Atlas or not)\n\n\n--\nEmre"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"atla integr gui gui plan integr atla keep hold version model deploi busi case cheer",
        "Question_preprocessed_content":"atla integr gui gui plan integr atla keep hold version model deploi busi case cheer",
        "Question_gpt_summary_original":"The user is inquiring about the integration of Atlas to keep track of the deployed version of a model for different business cases.",
        "Question_gpt_summary":"user inquir integr atla track deploi version model differ busi case",
        "Answer_original_content":"fridai june utc daan gerit wrote gui plan integr atla keep hold version model deploi busi case cheer featur track version model deploi busi case atla emr",
        "Answer_preprocessed_content":"fridai june utc daan gerit wrote gui plan integr atla keep hold version model deploi busi case cheer featur track version model deploi busi case emr",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut mention discuss"
    },
    {
        "Question_id":68533916.0,
        "Question_title":"what is features and how to interpret in RFormula",
        "Question_body":"<p>I am trying to understand what an RFormula is in MLflow or spark.<\/p>\n<p>I have found these:<\/p>\n<p><a href=\"https:\/\/george-jen.gitbook.io\/data-science-and-apache-spark\/rformula\" rel=\"nofollow noreferrer\">https:\/\/george-jen.gitbook.io\/data-science-and-apache-spark\/rformula<\/a>\n<a href=\"https:\/\/spark.apache.org\/docs\/latest\/api\/python\/reference\/api\/pyspark.ml.feature.RFormula.html\" rel=\"nofollow noreferrer\">https:\/\/spark.apache.org\/docs\/latest\/api\/python\/reference\/api\/pyspark.ml.feature.RFormula.html<\/a><\/p>\n<p>but still cannot understand how to interpret an RFormula fully. I am not sure how to interpret the below table\n<a href=\"https:\/\/i.stack.imgur.com\/guLyU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/guLyU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>based on the formula &quot;y ~ x+ s&quot;, y is related to x and s, but in the table when y=0 and x=0 and s =a (i.e. third row), then the features is [0,1] and label is 0, so how shall I interpret this.<\/p>\n<p>I have found <a href=\"https:\/\/stackoverflow.com\/questions\/61290042\/spark-rformula-interpretation\">this<\/a> but still cannot understand my way through this problem.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1627319315683,
        "Question_favorite_count":null,
        "Question_last_edit_time":1627371880352,
        "Question_score":0.0,
        "Question_view_count":78.0,
        "Answer_body":"<p>So your label is y. You parse x and s in rformula.<\/p>\n<p>x stays the same:<\/p>\n<pre><code>+-----------+---+\n|      x    | x |\n+-----------+---+\n|     1.0   |1.0|\n|     2.0   |2.0|\n|     0.0   |0.0|\n+-----------+---+\n<\/code><\/pre>\n<p>s:<\/p>\n<pre><code>+-----------+---+\n|       s   | s |\n+-----------+---+\n|       a   |1.0|\n|       b   |0.0|\n|       a   |1.0|\n+-----------+---+\n<\/code><\/pre>\n<p>I hope I could answer you question.\nRformula just converts the strings, standarize them and parse them into a vector.<\/p>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1627334932660,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68533916",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1627334164712,
        "Question_original_content":"featur interpret rformula try understand rformula spark http georg jen gitbook data scienc apach spark rformula http spark apach org doc latest api python refer api pyspark featur rformula html understand interpret rformula fulli sure interpret tabl base formula relat tabl row featur label shall interpret understand wai problem",
        "Question_preprocessed_content":"featur interpret rformula try understand rformula spark understand interpret rformula fulli sure interpret tabl base formula relat tabl featur label shall interpret understand wai problem",
        "Question_gpt_summary_original":"The user is having difficulty understanding how to interpret an RFormula in MLflow or Spark, specifically in relation to a table that shows features and labels based on the formula \"y ~ x+ s\". The user is confused about how to interpret the features and label values when y=0, x=0, and s=a. Despite finding resources and a related Stack Overflow post, the user is still struggling to understand the problem.",
        "Question_gpt_summary":"user have difficulti understand interpret rformula spark specif relat tabl show featur label base formula user confus interpret featur label valu despit find resourc relat stack overflow post user struggl understand problem",
        "Answer_original_content":"label pars rformula stai hope answer question rformula convert string standar pars vector",
        "Answer_preprocessed_content":"label pars rformula stai hope answer question rformula convert string standar pars vector",
        "Answer_gpt_summary_original":"there are no explicit solutions provided in the answer. the answer provides an explanation of how rformula works in apache spark and gives an example of how to parse the formula.",
        "Answer_gpt_summary":"explicit solut provid answer answer provid explan rformula work apach spark give exampl pars formula"
    },
    {
        "Question_id":69522401.0,
        "Question_title":"Compare String with list of strings in bash",
        "Question_body":"<p>I am trying to compare the service with a list of available service names, if service is found in the list then do update the service otherwise deploy the service.\nBut below condition only deploying new service even when service available in list variable?<\/p>\n<pre><code>SERVNAME=ner\nSERVICE=$(az ml service list -g $(ml_rg) --workspace-name $(ml_ws) --model-name $(model_name) --query &quot;[].name&quot;)\n\nif [[ &quot;$SERVNAME&quot; == &quot;$SERVICE&quot; ]];\nthen\n   echo &quot;Service Found: $(SERVNAME) and updating the service&quot;\n   az ml service update --name $(AKS_DEPLOYMENT_NAME) \\\n          --model '$(MODEL_NAME):$(MODEL_VERSION)' \\\n          --dc aksDeploymentConfig.json \\\n          --ic inferenceConfig.json \\\n          -e $(ml_env_name) --ev $(ml_env_version) \\\n          -g $(ml_rg) --workspace-name $(ml_ws) -v ;\nelse\n   echo &quot;Service Not found and starting deploying new service&quot;\n   az ml model deploy --name $(AKS_DEPLOYMENT_NAME) --model \\\n   '$(MODEL_NAME):$(MODEL_VERSION)' \\\n          --compute-target $(ml_aks_name) \\\n          --ic inferenceConfig.json \\\n          -e $(ml_env_name) --ev $(ml_env_version) \\\n          --dc aksDeploymentConfig.json \\\n          -g $(ml_rg) --workspace-name $(ml_ws) \\\n          --overwrite -v ;\nfi\n<\/code><\/pre>\n<p>Example list<\/p>\n<pre><code>SERVNAME=&quot;ner&quot;\nSERVICE=[ &quot;ner&quot;, &quot;aks-gpu-ner-0306210907&quot;, &quot;aks-gpu-ner-30012231&quot;, &quot;aks-gpu-ner-1305211336&quot;]\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1633936791707,
        "Question_favorite_count":null,
        "Question_last_edit_time":1633953732907,
        "Question_score":1.0,
        "Question_view_count":1153.0,
        "Answer_body":"<p>Assuming the <code>az ml<\/code> command returns a json array string and you want to\ncheck if the array includes the value of variable <code>SERVNAME<\/code>, would you\nplease try:<\/p>\n<pre><code>SERVNAME=&quot;ner&quot;\nSERVICE='[ &quot;ner&quot;, &quot;aks-gpu-ner-0306210907&quot;, &quot;aks-gpu-ner-30012231&quot;, &quot;aks-gpu-ner-1305211336&quot;]'\n\nif [[ $SERVICE =~ &quot;\\&quot;$SERVNAME\\&quot;&quot; ]]; then\n    echo &quot;Service Found&quot;\n    # put your command here to update the service\nelse\n    echo &quot;Service Not Found&quot;\n    # put your command here to deploy new service\nfi\n<\/code><\/pre>\n<p>The regex operator <code>$SERVICE =~ &quot;\\&quot;$SERVNAME\\&quot;&quot;<\/code> matches if the string <code>$SERVICE<\/code>\ncontains the substring <code>$SERVNAME<\/code> enclosed with double quotes.<\/p>\n<p>If <code>jq<\/code> is available, you could also say:<\/p>\n<pre><code>result=$(echo &quot;$SERVICE&quot; | jq --arg var &quot;$SERVNAME&quot; '. | index($var)')\nif [[ $result != &quot;null&quot; ]]; then\n    echo &quot;Service Found&quot;\nelse\n    echo &quot;Service Not Found&quot;\nfi\n<\/code><\/pre>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1634278832727,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69522401",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1634000233752,
        "Question_original_content":"compar string list string bash try compar servic list avail servic name servic list updat servic deploi servic condit deploi new servic servic avail list variabl servnam ner servic servic list workspac model model queri servnam servic echo servic servnam updat servic servic updat ak deploy model model model version aksdeploymentconfig json inferenceconfig json env env version workspac echo servic start deploi new servic model deploi ak deploy model model model version comput target ak inferenceconfig json env env version aksdeploymentconfig json workspac overwrit exampl list servnam ner servic ner ak gpu ner ak gpu ner ak gpu ner",
        "Question_preprocessed_content":"compar string list string bash try compar servic list avail servic name servic list updat servic deploi servic condit deploi new servic servic avail list variabl exampl list",
        "Question_gpt_summary_original":"The user is attempting to compare a service name with a list of available service names in a bash script. If the service is found in the list, the script should update the service, otherwise, it should deploy a new service. However, the script is only deploying a new service even when the service is available in the list variable.",
        "Question_gpt_summary":"user attempt compar servic list avail servic name bash script servic list script updat servic deploi new servic script deploi new servic servic avail list variabl",
        "Answer_original_content":"assum command return json arrai string want check arrai includ valu variabl servnam try servnam ner servic ner ak gpu ner ak gpu ner ak gpu ner servic servnam echo servic command updat servic echo servic command deploi new servic regex oper servic servnam match string servic contain substr servnam enclos doubl quot avail result echo servic arg var servnam index var result null echo servic echo servic",
        "Answer_preprocessed_content":"assum command return json arrai string want check arrai includ valu variabl try regex oper match string contain substr enclos doubl quot avail",
        "Answer_gpt_summary_original":"the answer suggests two possible solutions to the challenge of comparing a string with a list of strings in bash. the first solution involves using the regex operator to check if the list variable contains the value of the variable being compared. the second solution involves using the jq command to check if the list variable contains the value of the variable being compared. both solutions provide a way to determine if a service is available in the list variable and take appropriate action based on the result.",
        "Answer_gpt_summary":"answer suggest possibl solut challeng compar string list string bash solut involv regex oper check list variabl contain valu variabl compar second solut involv command check list variabl contain valu variabl compar solut provid wai determin servic avail list variabl appropri action base result"
    },
    {
        "Question_id":null,
        "Question_title":"how to share project to my team",
        "Question_body":"I want to share my whole project to my team in CLI, how to do that, can\u2019t find any document about that.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1669425813897,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1105193\/how-to-share-project-to-my-team.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-26T10:40:35.147Z",
                "Answer_score":0,
                "Answer_body":"Hello @sursalaterrano-8475\n\nThanks for reaching out to us, which version you are working on?\n\nFor Azure Machine Learning CLI V1, you should use az ml workspace share commands\n\nFor Azure Machine Learning CLI V2, you should use az role assignment create commands\n\nPlease refer to below document -\n\nhttps:\/\/learn.microsoft.com\/en-us\/cli\/azure\/ml\/workspace?view=azure-cli-latest\n\nhttps:\/\/learn.microsoft.com\/en-us\/cli\/azure\/ml(v1)\/workspace?view=azure-cli-latest\n\nI hope this helps.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"share project team want share project team cli document",
        "Question_preprocessed_content":"share project team want share project team cli document",
        "Question_gpt_summary_original":"The user is facing challenges in sharing their entire project with their team through CLI and is unable to find any relevant documentation on how to do so.",
        "Question_gpt_summary":"user face challeng share entir project team cli unabl relev document",
        "Answer_original_content":"hello sursalaterrano thank reach version work cli us workspac share command cli us role assign creat command refer document http learn microsoft com cli azur workspac view azur cli latest http learn microsoft com cli azur workspac view azur cli latest hope help regard yutong kindli accept answer feel help support commun thank lot",
        "Answer_preprocessed_content":"hello thank reach version work cli us workspac share command cli us role assign creat command refer document hope help regard yutong kindli accept answer feel help support commun thank lot",
        "Answer_gpt_summary_original":"possible solutions to share a project with a team using the command line interface are provided in the answer. for cli v1, the user should use \"az ml workspace share\" commands, and for cli v2, they should use \"az role assignment create\" commands. the answer also includes links to relevant documentation for both versions.",
        "Answer_gpt_summary":"possibl solut share project team command line interfac provid answer cli user us workspac share command cli us role assign creat command answer includ link relev document version"
    },
    {
        "Question_id":null,
        "Question_title":"D365 Demand Forecasting - Can we connect to new Azure ML Service instead of a classic studio service?",
        "Question_body":"There is a D365 forecasting option that allows to connect to a azure ml classic studio service. Can we connect from D365 to the new Azure ML Service? I couldnt find any documentation about this, any pointers please.Thanks.",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1606959642257,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/183990\/d365-demand-forecasting-can-we-connect-to-new-azur.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-04T08:31:11.253Z",
                "Answer_score":0,
                "Answer_body":"@SriramNarayanan-6939 you can deploy a real-time endpoint in Azure Machine Learning designer and get the REST endpoint\/token by following this doc: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"demand forecast connect new servic instead classic studio servic forecast option allow connect classic studio servic connect new servic document pointer thank",
        "Question_preprocessed_content":"demand forecast connect new servic instead classic studio servic forecast option allow connect classic studio servic connect new servic document pointer",
        "Question_gpt_summary_original":"The user is facing a challenge in finding documentation on whether it is possible to connect D365 to the new Azure ML Service instead of the classic studio service for forecasting.",
        "Question_gpt_summary":"user face challeng find document possibl connect new servic instead classic studio servic forecast",
        "Answer_original_content":"sriramnarayanan deploi real time endpoint design rest endpoint token follow doc http doc microsoft com azur machin learn tutori design automobil price deploi",
        "Answer_preprocessed_content":"deploi endpoint design rest follow doc",
        "Answer_gpt_summary_original":"possible solution: the user can deploy a real-time endpoint in designer and get the rest endpoint\/token by following the documentation provided in the answer.",
        "Answer_gpt_summary":"possibl solut user deploi real time endpoint design rest endpoint token follow document provid answer"
    },
    {
        "Question_id":null,
        "Question_title":"Explicitly define pipeline execution order",
        "Question_body":"<p>Is there a way to specify a dependency between two pipeline stages except for a <code>outs<\/code> and <code>deps<\/code> combination? My use case is to run two stages consecutive, although they don\u2019t read \/ write files, that could be tracked by DVC, but depend on each other in a different way, e.g. by one stage writing a value to an external database and the other one reading it again.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645444562953,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":129.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/explicitly-define-pipeline-execution-order\/1073",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-22T02:20:47.287Z",
                "Answer_body":"<p>The only way to define stage dependencies is via <code>outs<\/code> and <code>deps<\/code> files.<\/p>\n<p>For your situation, you could just write an empty\/temporary file as an <code>out<\/code> for the first stage, and also mark it as <code>cache: false<\/code> (since you don\u2019t actually need DVC to care about the contents of the file).<\/p>\n<p>Then in the second stage you could add the empty file as a <code>dep<\/code>, and then also mark the stage as <code>always_changed: true<\/code> so that the second stage is always executed (even though the content of the empty file won\u2019t ever be modified)<\/p>",
                "Answer_score":16.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"explicitli defin pipelin execut order wai specifi depend pipelin stage out dep combin us case run stage consecut dont read write file track depend differ wai stage write valu extern databas read",
        "Question_preprocessed_content":"explicitli defin pipelin execut order wai specifi depend pipelin stage combin us case run stage consecut dont read write file track depend differ wai stage write valu extern databas read",
        "Question_gpt_summary_original":"The user is facing a challenge in specifying a dependency between two pipeline stages, which do not read or write files that can be tracked by DVC. The stages depend on each other in a different way, such as one stage writing a value to an external database and the other stage reading it again. The user is seeking a way to run these stages consecutively without using the <code>outs<\/code> and <code>deps<\/code> combination.",
        "Question_gpt_summary":"user face challeng specifi depend pipelin stage read write file track stage depend differ wai stage write valu extern databas stage read user seek wai run stage consecut out dep combin",
        "Answer_original_content":"wai defin stage depend out dep file situat write temporari file stage mark cach fals dont actual need care content file second stage add file dep mark stage chang true second stage execut content file wont modifi",
        "Answer_preprocessed_content":"wai defin stage depend file situat write file stage mark second stage add file mark stage second stage execut",
        "Answer_gpt_summary_original":"possible solution: write an empty\/temporary file as an out for the first stage, mark it as cache: false, and add the empty file as a dep in the second stage. also, mark the second stage as always_changed: true so that it is always executed.",
        "Answer_gpt_summary":"possibl solut write temporari file stage mark cach fals add file dep second stage mark second stage chang true execut"
    },
    {
        "Question_id":null,
        "Question_title":"Passing arguments, defined in guild.yml file when using Python's argparse",
        "Question_body":"<p>Guild version 0.7.0<\/p>\n<p>I noticed that flags, defined in guild.yml are not passed normally as arguments to config.yml if I use argparse in my Python code. It seems it wants to pass them python arguments even though flags-dest and flags-import are off.<\/p>\n<p>My workflow usually consists of defining main hyperparameters in the config.yml file, but for choosing which gpu to run it on (in my testing phase) I use argparse. Is there a way to combine those two?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1598602229021,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":882.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/passing-arguments-defined-in-guild-yml-file-when-using-pythons-argparse\/329",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-08-28T14:11:29.421Z",
                "Answer_body":"<p>In 0.7 flags are independent of support for configuration files. In a future release we\u2019ll promote config files as a proper flags interface. It will also be possible to configure multiple interfaces to direct some flags to one destination and other flags to another. Alas none of this is available yet.<\/p>\n<p>You can sill get this done though. Use the config dependency type: <a href=\"https:\/\/my.guild.ai\/t\/dependencies\/162#configuration-files\" class=\"inline-onebox\">Dependencies<\/a>.<\/p>\n<p>Here\u2019s an example:<\/p>\n<aside class=\"onebox githubblob\">\n  <header class=\"source\">\n      <a href=\"https:\/\/github.com\/guildai\/guildai\/blob\/master\/examples\/dependencies\/guild.yml#L38-L46\" target=\"_blank\" rel=\"noopener\">github.com<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <h4><a href=\"https:\/\/github.com\/guildai\/guildai\/blob\/master\/examples\/dependencies\/guild.yml#L38-L46\" target=\"_blank\" rel=\"noopener\">guildai\/guildai\/blob\/master\/examples\/dependencies\/guild.yml#L38-L46<\/a><\/h4>\n<pre class=\"onebox\"><code class=\"lang-yml\"><ol class=\"start lines\" start=\"38\" style=\"counter-reset: li-counter 37 ;\">\n<li>config:<\/li>\n<li>  description: Configuration file dependency<\/li>\n<li>  main: guild.pass<\/li>\n<li>  requires:<\/li>\n<li>    - config: config.yml<\/li>\n<li>  flags:<\/li>\n<li>    lr: null<\/li>\n<li>    batch-size: null<\/li>\n<li>    dropout: null<\/li>\n<\/ol><\/code><\/pre>\n\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>You want to specify <code>args<\/code> for <code>flag-dest<\/code>. By default this will pass all flags as command line options. You can disable that for specific flags using <code>arg-skip: yes<\/code>. You can do this for every arg that you don\u2019t parse with argparse.  To skip all flags by default, set <code>default-flag-arg-skip<\/code> to <code>yes<\/code> for the operation.<\/p>\n<pre><code class=\"lang-yaml\">op:\n  flags-dest: args\n  default-flag-arg-skip: yes\n  flags:\n    # Add your flag defs used for config.yml\n    gpu:\n      arg-skip: no\n  requires:\n    - config: config.yml\n<\/code><\/pre>",
                "Answer_score":14.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"pass argument defin yml file python argpars version notic flag defin yml pass normal argument config yml us argpars python code want pass python argument flag dest flag import workflow usual consist defin main hyperparamet config yml file choos gpu run test phase us argpars wai combin",
        "Question_preprocessed_content":"pass argument defin yml file python argpars version notic flag defin yml pass normal argument us argpars python code want pass python argument workflow usual consist defin main hyperparamet file choos gpu run us argpars wai combin",
        "Question_gpt_summary_original":"The user is facing challenges in passing flags defined in guild.yml as arguments to config.yml when using argparse in Python code. The user typically defines main hyperparameters in config.yml but uses argparse to choose which GPU to run on during testing. The user is seeking a way to combine these two approaches.",
        "Question_gpt_summary":"user face challeng pass flag defin yml argument config yml argpars python code user typic defin main hyperparamet config yml us argpars choos gpu run test user seek wai combin approach",
        "Answer_original_content":"flag independ support configur file futur releas promot config file proper flag interfac possibl configur multipl interfac direct flag destin flag ala avail sill us config depend type depend here exampl github com blob master exampl depend yml config descript configur file depend main pass requir config config yml flag null batch size null dropout null want specifi arg flag dest default pass flag command line option disabl specif flag arg skip ye arg dont pars argpars skip flag default set default flag arg skip ye oper flag dest arg default flag arg skip ye flag add flag def config yml gpu arg skip requir config config yml",
        "Answer_preprocessed_content":"flag independ support configur file futur releas promot config file proper flag interfac possibl configur multipl interfac direct flag destin flag ala avail sill us config depend type depend here exampl want specifi default pass flag command line option disabl specif flag arg dont pars argpars skip flag default set oper",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- in a future release, config files will be promoted as a proper flags interface, and it will be possible to configure multiple interfaces to direct some flags to one destination and other flags to another.\n- use the config dependency type: dependencies, and specify args for flag-dest. by default, this will pass all flags as command line options. you can disable that for specific flags using arg-skip: yes. you can do this for every arg that you don't parse with argparse. to skip all flags by default, set default-flag-arg-skip to yes for the operation.",
        "Answer_gpt_summary":"possibl solut answer futur releas config file promot proper flag interfac possibl configur multipl interfac direct flag destin flag us config depend type depend specifi arg flag dest default pass flag command line option disabl specif flag arg skip ye arg pars argpars skip flag default set default flag arg skip ye oper"
    },
    {
        "Question_id":58755708.0,
        "Question_title":"Sagemaker Notebook Instance Type Recommendation",
        "Question_body":"<p>I will be running ml models on a pretty large dataset. It is about 15 gb, with 200 columns and 4.3 million rows. I'm wondering what the best Notebook instance type is for this kind of dataset in AWS Sagemaker.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573155038577,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":2326.0,
        "Answer_body":"<p><strong>For choosing a SageMaker hosted notebook type:<\/strong><\/p>\n\n<p>Do you plan to do all of your preprocessing of your data in-memory on the notebook, or do you plan to orchestrate ETL with external services? <\/p>\n\n<p>If you're planning to load the dataset into memory on the notebook instance for exploration\/preprocessing, the primary bottleneck here would be ensuring the instance has enough memory for your dataset. This would require at least the 16gb types (<em>.xlarge<\/em>) (full list of ML instance types <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"noreferrer\">available here<\/a>). Further, depending on how compute intensive your pre-processing is, and your desired pre-processing completion time, you can opt for a compute optimized instance (<em>c4, c5<\/em>) to speed this up.<\/p>\n\n<hr>\n\n<p><strong>For the training job, specifically:<\/strong><\/p>\n\n<p>Using the Amazon SageMaker SDK, your training data will be loaded and distributed to the training cluster, allowing your training job to be completely separate from the instance your hosted notebook is running on.<\/p>\n\n<p>Figuring out the ideal instance type for training will depend on whether your algorithm of choice\/training job is memory, CPU, or IO bound. Since your dataset will likely be loaded onto your training cluster from S3, the instance you choose for your hosted notebook will have no bearing on the speed of your training job.<\/p>\n\n<hr>\n\n<p><strong>Broadly:<\/strong>\nWhen it comes to SageMaker notebooks, the best practice is to use your notebook as a \"puppeteer\" or orchestrator, that calls out to external services (AWS Glue or Amazon EMR for preprocessing, SageMaker for training, S3 for storage, etc). It is best to treat them as ephemeral forms of compute\/storage for building and kicking off your experiment pipeline.<\/p>\n\n<p>This will allow you to more closely pair compute, storage, and hosting resources\/services with the demands for your workload, ultimately resulting in the best bang for your buck by not having you pay for latent or unused resources.<\/p>\n\n<hr>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58755708",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1573157762368,
        "Question_original_content":"notebook instanc type recommend run model pretti larg dataset column million row wonder best notebook instanc type kind dataset",
        "Question_preprocessed_content":"notebook instanc type recommend run model pretti larg dataset column million row wonder best notebook instanc type kind dataset",
        "Question_gpt_summary_original":"The user is seeking a recommendation for the best AWS Sagemaker Notebook instance type to use for running machine learning models on a large dataset of 15 GB, 200 columns, and 4.3 million rows.",
        "Question_gpt_summary":"user seek recommend best notebook instanc type us run machin learn model larg dataset column million row",
        "Answer_original_content":"choos host notebook type plan preprocess data memori notebook plan orchestr etl extern servic plan load dataset memori notebook instanc explor preprocess primari bottleneck ensur instanc memori dataset requir type xlarg list instanc type avail depend comput intens pre process desir pre process complet time opt comput optim instanc speed train job specif sdk train data load distribut train cluster allow train job complet separ instanc host notebook run figur ideal instanc type train depend algorithm choic train job memori cpu bound dataset like load train cluster instanc choos host notebook bear speed train job broadli come notebook best practic us notebook puppet orchestr call extern servic aw glue amazon emr preprocess train storag best treat ephemer form comput storag build kick experi pipelin allow close pair comput storag host resourc servic demand workload ultim result best bang buck have pai latent unus resourc",
        "Answer_preprocessed_content":"choos host notebook type plan preprocess data notebook plan orchestr etl extern servic plan load dataset memori notebook instanc primari bottleneck ensur instanc memori dataset requir type depend comput intens desir complet time opt comput optim instanc speed train job specif sdk train data load distribut train cluster allow train job complet separ instanc host notebook run figur ideal instanc type train depend algorithm job memori cpu bound dataset like load train cluster instanc choos host notebook bear speed train job broadli come notebook best practic us notebook puppet orchestr call extern servic best treat ephemer form build kick experi pipelin allow close pair comput storag host demand workload ultim result best bang buck have pai latent unus resourc",
        "Answer_gpt_summary_original":"the answer suggests that for a dataset of 15 gb, with 200 columns and 4.3 million rows, the user should choose a notebook instance type with at least 16gb memory. depending on the compute intensity of the pre-processing, a compute optimized instance can be chosen to speed up the process. for the training job, the instance type will depend on whether the algorithm is memory, cpu, or io bound. it is recommended to use the notebook as an orchestrator and call out to external services for pre-processing and training. this will allow the user to more closely pair compute, storage, and hosting resources\/services with the demands for their workload, ultimately resulting in the best bang for their buck by not having to pay for latent or unused resources.",
        "Answer_gpt_summary":"answer suggest dataset column million row user choos notebook instanc type memori depend comput intens pre process comput optim instanc chosen speed process train job instanc type depend algorithm memori cpu bound recommend us notebook orchestr extern servic pre process train allow user close pair comput storag host resourc servic demand workload ultim result best bang buck have pai latent unus resourc"
    },
    {
        "Question_id":null,
        "Question_title":"Recover deleted Vertex AI resources",
        "Question_body":"Hi,In order to save on billing, I deleted most of the resources in the data sources, workbench, pipelines in Vertex AI.Is there a way I can recover them??",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1647987780000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":210.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Recover-deleted-Vertex-AI-resources\/td-p\/405934\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-22T22:23:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi,\n\nIn order to save on billing, I deleted most of the resources in the data sources, workbench, pipelines in Vertex AI.\n\nIs there a way I can recover them??"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"recov delet resourc order save bill delet resourc data sourc workbench pipelin wai recov",
        "Question_preprocessed_content":"recov delet resourc order save bill delet resourc data sourc workbench pipelin wai recov",
        "Question_gpt_summary_original":"The user deleted resources in data sources, workbench, and pipelines in Vertex AI to save on billing. They are now looking for a way to recover these deleted resources.",
        "Question_gpt_summary":"user delet resourc data sourc workbench pipelin save bill look wai recov delet resourc",
        "Answer_original_content":"order save bill delet resourc data sourc workbench pipelin wai recov",
        "Answer_preprocessed_content":"order save bill delet resourc data sourc workbench pipelin wai recov",
        "Answer_gpt_summary_original":"possible solutions to recover deleted resources from data sources, workbench, and pipelines are not mentioned in the answer.",
        "Answer_gpt_summary":"possibl solut recov delet resourc data sourc workbench pipelin mention answer"
    },
    {
        "Question_id":null,
        "Question_title":"How does one have high disk utilization in pytorch?",
        "Question_body":"<p>I saw that being mentioned here <a href=\"https:\/\/youtu.be\/G7GH0SeNBMA?t=1141\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">\ud83d\udd25 Integrate Weights &amp; Biases with PyTorch - YouTube<\/a> so I was curious - how do we have the data loaders in pytorch to have high disk utilization e.g. is increasing the batch size, num_workers the way to go or something else?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1631575114171,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":256.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-does-one-have-high-disk-utilization-in-pytorch\/553",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-16T16:19:58.530Z",
                "Answer_body":"<p>So, the idea I had in mind when I said that was that there are three categories of time-consuming operations:<\/p>\n<ol>\n<li>Getting data off disk<\/li>\n<li>Running data\/network logic in Python on CPU<\/li>\n<li>Running data\/network operations in CUDA on GPU<\/li>\n<\/ol>\n<p>Roughly, the fraction of available resources used by these 3 categories is tracked by three system metrics in wandb: Disk Utilization, CPU Utilization, and GPU Utilization.<\/p>\n<p>Again roughly, you\u2019re squeezing every last drop of juice out of your hardware when all three of those are maximized. In every second, bits are being read from disk while the CPU is moving forward in the compute graph and the GPU is executing the operation, all at full capacity.<\/p>\n<p>This is probably not achievable for every problem, but that\u2019s what I had in mind when I was talking about the system metrics in that video.<\/p>\n<p>Looking back, I think parts 1+2 are less important than I did at the time. The real killer for GPU-accelerated tensor workloads like DNN training is that the GPU is sitting idle \u2013 it\u2019s waiting on a disk read or waiting on the <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/sloth.png?v=10\" title=\":sloth:\" class=\"emoji\" alt=\":sloth:\"> Python layer. The optimizations to fix this aren\u2019t always <em>in<\/em> the GPU (maybe you need to write a faster file loading strategy, with cacheing or multiprocessing; maybe you need to move more logic out of pure Python and into your tensor library) but they tend to show up as low GPU Utilization.<\/p>",
                "Answer_score":92.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-20T18:02:05.359Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"high disk util pytorch saw mention integr pytorch youtub curiou data loader pytorch high disk util increas batch size num worker wai",
        "Question_preprocessed_content":"high disk util pytorch saw mention integr pytorch youtub curiou data loader pytorch high disk util increas batch size wai",
        "Question_gpt_summary_original":"The user is seeking information on how to increase disk utilization in PyTorch, specifically in regards to data loaders. They are curious if increasing batch size or num_workers is the best approach or if there are other methods to achieve this.",
        "Question_gpt_summary":"user seek inform increas disk util pytorch specif regard data loader curiou increas batch size num worker best approach method achiev",
        "Answer_original_content":"idea mind said categori time consum oper get data disk run data network logic python cpu run data network oper cuda gpu roughli fraction avail resourc categori track metric disk util cpu util gpu util roughli your squeez drop juic hardwar maxim second bit read disk cpu move forward comput graph gpu execut oper capac probabl achiev problem that mind talk metric video look think part import time real killer gpu acceler tensor workload like dnn train gpu sit idl wait disk read wait python layer optim fix arent gpu mayb need write faster file load strategi cach multiprocess mayb need logic pure python tensor librari tend low gpu util topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"idea mind said categori oper get data disk run logic python cpu run oper cuda gpu roughli fraction avail resourc categori track metric disk util cpu util gpu util roughli your squeez drop juic hardwar maxim second bit read disk cpu move forward comput graph gpu execut oper capac probabl achiev problem that mind talk metric video look think part import time real killer tensor workload like dnn train gpu sit idl wait disk read wait python layer optim fix arent gpu tend low gpu util topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"possible solutions to increase disk utilization in pytorch when using data loaders include optimizing file loading strategies with caching or multiprocessing, moving more logic out of pure python and into the tensor library, and ensuring that all three system metrics (disk utilization, cpu utilization, and gpu utilization) are maximized to squeeze every last drop of juice out of the hardware. the real challenge for gpu-accelerated tensor workloads like dnn training is that the gpu may sit idle while waiting on a disk read or waiting on the python layer.",
        "Answer_gpt_summary":"possibl solut increas disk util pytorch data loader includ optim file load strategi cach multiprocess move logic pure python tensor librari ensur metric disk util cpu util gpu util maxim squeez drop juic hardwar real challeng gpu acceler tensor workload like dnn train gpu sit idl wait disk read wait python layer"
    },
    {
        "Question_id":null,
        "Question_title":"databricks to power bi",
        "Question_body":"Hello community , i'm trying to save stream data in delta lake and i want to push this data to power bi for real time insights , however when i try to connect databricks to power bi i get an empty table , someone can help me please ??\u00a0\nif there's another alternative it would be grateful\u00a0\nthank you",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1621248613000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":9.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/GvkoTw0sL4U",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-17T11:47:47",
                "Answer_body":"Hello Nadine,\n\n\nYou might want to post this on the delta-lake user group or Delta Lake slack channel. You can join both here:\u00a0https:\/\/delta.io\/\n\u00a0\n\n\n\nThanks,\nJules\n\n\n\n\n\n\n\u2013\u2013\n\nThe Best Ideas are Simple\n\nJules S. Damji\n\nSr. Developer Advocate\n\nDatabricks, Inc.\n\nju...@databricks.com\n\n(510) 304-7686\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/dc8ee8ff-3f04-423c-a446-96b7f250a3e5n%40googlegroups.com."
            },
            {
                "Answer_creation_time":"2021-05-17T11:50:14",
                "Answer_body":"Ok , thank you\u00a0\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"databrick power hello commun try save stream data delta lake want push data power real time insight try connect databrick power tabl help altern grate thank",
        "Question_preprocessed_content":"databrick power hello commun try save stream data delta lake want push data power real time insight try connect databrick power tabl help altern grate thank",
        "Question_gpt_summary_original":"The user is facing challenges in connecting Databricks to Power BI to save stream data in Delta Lake and obtain real-time insights. Despite attempting to connect the two platforms, the user is encountering an empty table and is seeking assistance or alternative solutions.",
        "Question_gpt_summary":"user face challeng connect databrick power save stream data delta lake obtain real time insight despit attempt connect platform user encount tabl seek assist altern solut",
        "Answer_original_content":"hello nadin want post delta lake user group delta lake slack channel join http delta thank jule best idea simpl jule damji develop advoc databrick databrick com receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user dceeff bfaen googlegroup com thank",
        "Answer_preprocessed_content":"hello nadin want post user group delta lake slack channel join thank jule best idea simpl jule damji develop advoc databrick receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit thank",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_id":null,
        "Question_title":"Local run vs remote run dependencies",
        "Question_body":"<p>I have run some operations on remotes successfully in the past. However, there was always a some discrepancy between the imports for local and remote runs that I needed to fix by trial and error.<\/p>\n<p>In my current setup, I switched from flags as global variables in the training script to flags in config.yml files. And I\u2019m unable to make it work on remotes.<\/p>\n<p><strong>Project structure<\/strong><br>\nProject:<\/p>\n<ul>\n<li>[some folders]<\/li>\n<li>datasets \u2192 module, contains data loaders + their config.yml files<\/li>\n<li>zoo \u2192 guild Home for local runs<\/li>\n<li>models \u2192 model definitions<\/li>\n<li>\n<ul>\n<li>guild.yml<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>abstract_model.py<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>conv_lstm \u2192 model I want to run<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>\n<ul>\n<li>model.py \u2192 model definition<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>\n<ul>\n<li>train.py \u2192 training script<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>\n<ul>\n<li>config.yml \u2192 flags<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p><strong>Guild file<\/strong><\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\"># Standard convolutional LSTM\n- model: conv_lstm\n  description: Convolutional LSTM\n  operations:\n    train_local:\n      description: Train Convolutional LSTM\n      sourcecode:\n        - conv_lstm\/train.py\n        - conv_lstm\/model.py\n        - abstract_model.py\n      requires:\n        - config: conv_lstm\/config.yml\n        - file: ..\/datasets\/\n      main: conv_lstm\/train\n      flags-dest: config:conv_lstm\/config.yml\n      flags-import: all\n      flags:\n        epochs: 100\n        dataset_args:\n          - dataset_name: ucsd\n            batch_size: 2\n      output-scalars:\n        train_loss: 'Train mse: (\\value)'\n        test_acc: 'Test mse: (\\value)'\n    train_remote:\n      description: Train Convolutional LSTM on remote\n      sourcecode:\n        - conv_lstm\/train.py\n        - conv_lstm\/model.py\n        - abstract_model.py\n      requires:\n        - config: conv_lstm\/config.yml\n        - file: ..\/datasets\/\n      main: conv_lstm\/train\n      flags-dest: config:conv_lstm\/config.yml\n      flags-import: all\n      flags:\n        optimizer: Adam\n        loss: mse\n        learning_rate: 0.001\n        epochs: 100\n        dev: True\n        gpus: [7]\n        dataset_args:\n          - dataset_name: ucsd\n            batch_size: 2\n            train_path: ~\/data\/ucsd\/UCSDped1\/Train\/\n            test_path: ~\/data\/ucsd\/UCSDped1\/Test\/\n      output-scalars:\n        train_loss: 'Train mse: (\\value)'\n        test_acc: 'Test mse: (\\value)'\n<\/code><\/pre>\n<p><strong>Training script<\/strong><\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">sys.path.append('..\/')\nsys.path.append('..\/..\/datasets')\n# Tensorflow logging level\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n\nimport tensorflow as tf\nimport yaml\nfrom model import ConvLSTM\nfrom datasets.data_loader import DataLoader\n\n\n# Load the model configuration\nclass Config(object):\n    def __init__(self, filename):\n        self.__dict__.update(yaml.safe_load(open(filename)))\n\n\nconfig = Config(\"config.yml\")\n\n(...)\n<\/code><\/pre>\n<p><strong>Current situation &amp; error<\/strong><br>\nI\u2019m able to run \u2018conv_lstm:train_local\u2019 without any issues, and everything works as expected. However, almost the same configuration, with a few flags changed, fails to run on remote.<\/p>\n<p>Issue 1: I cannot see any evidence of the config.yml file being copied to the remote<br>\nIssue 2: the remote run fails to find the main training script, even though it works locally.<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild -H \/home\/bleporowski\/Projects\/mad\/zoo run conv_lstm:train_remote --remote [remote_name] --gpus 7\nYou are about to run conv_lstm:train_remote as a batch (1 trial) on [remote_name]\n  dataset_args: [{batch_size: 2, dataset_name: ucsd, test_path: ~\/data\/ucsd\/UCSDped1\/Test\/, train_path: ~\/data\/ucsd\/UCSDped1\/Train\/}]\n  dev: yes\n  epochs: 100\n  gpus: [7]\n  learning_rate: 0.001\n  loss: mse\n  optimizer: Adam\nContinue? (Y\/n) y\nBuilding package\npackage src: \/home\/bleporowski\/Projects\/mad\/models\npackage dist: \/tmp\/guild-remote-stage-eq7ahi7e\nrunning clean\nremoving 'build\/lib' (and everything under it)\nremoving 'build\/bdist.linux-x86_64' (and everything under it)\n'build\/scripts-3.8' does not exist -- can't clean it\nremoving 'build'\nrunning bdist_wheel\nrunning build\nrunning build_py\npackage init file '\/home\/bleporowski\/Projects\/mad\/models\/__init__.py' not found (or not a regular file)\ncreating build\ncreating build\/lib\ncreating build\/lib\/conv_lstm\ncopying \/home\/bleporowski\/Projects\/mad\/models\/abstract_model.py -&gt; build\/lib\/conv_lstm\ncopying \/home\/bleporowski\/Projects\/mad\/models\/guild.yml -&gt; build\/lib\/conv_lstm\ninstalling to build\/bdist.linux-x86_64\/wheel\nrunning install\nrunning install_lib\ncreating build\/bdist.linux-x86_64\ncreating build\/bdist.linux-x86_64\/wheel\ncreating build\/bdist.linux-x86_64\/wheel\/conv_lstm\ncopying build\/lib\/conv_lstm\/guild.yml -&gt; build\/bdist.linux-x86_64\/wheel\/conv_lstm\ncopying build\/lib\/conv_lstm\/abstract_model.py -&gt; build\/bdist.linux-x86_64\/wheel\/conv_lstm\nrunning install_egg_info\nrunning egg_info\nwriting conv_lstm.egg-info\/PKG-INFO\nwriting dependency_links to conv_lstm.egg-info\/dependency_links.txt\nwriting entry points to conv_lstm.egg-info\/entry_points.txt\nwriting namespace_packages to conv_lstm.egg-info\/namespace_packages.txt\nwriting top-level names to conv_lstm.egg-info\/top_level.txt\nreading manifest file 'conv_lstm.egg-info\/SOURCES.txt'\nwriting manifest file 'conv_lstm.egg-info\/SOURCES.txt'\nCopying conv_lstm.egg-info to build\/bdist.linux-x86_64\/wheel\/conv_lstm-0.0.0-py3.8.egg-info\nrunning install_scripts\ncreating build\/bdist.linux-x86_64\/wheel\/conv_lstm-0.0.0.dist-info\/WHEEL\ncreating '\/tmp\/guild-remote-stage-eq7ahi7e\/conv_lstm-0.0.0-py2.py3-none-any.whl' and adding 'build\/bdist.linux-x86_64\/wheel' to it\nadding 'conv_lstm\/abstract_model.py'\nadding 'conv_lstm\/guild.yml'\nadding 'conv_lstm-0.0.0.dist-info\/METADATA'\nadding 'conv_lstm-0.0.0.dist-info\/PACKAGE'\nadding 'conv_lstm-0.0.0.dist-info\/WHEEL'\nadding 'conv_lstm-0.0.0.dist-info\/entry_points.txt'\nadding 'conv_lstm-0.0.0.dist-info\/namespace_packages.txt'\nadding 'conv_lstm-0.0.0.dist-info\/top_level.txt'\nadding 'conv_lstm-0.0.0.dist-info\/RECORD'\nremoving build\/bdist.linux-x86_64\/wheel\nInitializing remote run\nCopying package\nsending incremental file list\nconv_lstm-0.0.0-py2.py3-none-any.whl\n\nsent 3,558 bytes  received 35 bytes  1,437.20 bytes\/sec\ntotal size is 3,424  speedup is 0.95\nInstalling package and its dependencies\nProcessing .\/conv_lstm-0.0.0-py2.py3-none-any.whl\nInstalling collected packages: conv-lstm\nSuccessfully installed conv-lstm-0.0.0\nStarting conv_lstm:train_remote on charybdis as 8a26ca399039412fb31c7791d293b507\nWARNING: [Errno 2] No such file or directory: 'conv_lstm\/config.yml'\nWARNING: [Errno 2] No such file or directory: 'conv_lstm\/config.yml'\nWARNING: cannot import flags from conv_lstm\/train: No module named conv_lstm\/train\nWARNING: cannot import flags from conv_lstm\/train: No module named conv_lstm\/train\nINFO: [guild] Running trial 05afef0858f74b4198af160c6d904e2e: conv-lstm\/conv_lstm:train_remote (dataset_args={batch_size: 2, dataset_name: ucsd, test_path: ~\/data\/ucsd\/UCSDped1\/Test\/, train_path: ~\/data\/ucsd\/UCSDped1\/Train\/}, dev=yes, epochs=100, gpus=7, learning_rate=0.001, loss=mse, optimizer=Adam)\nINFO: [guild] Resolving config:conv_lstm\/config.yml dependency\nERROR: [guild] Trial 05afef0858f74b4198af160c6d904e2e exited with an error: (1) run failed because a dependency was not met: could not resolve 'config:conv_lstm\/config.yml' in config:conv_lstm\/config.yml resource: cannot find source file 'conv_lstm\/config.yml'\nRun 8a26ca399039412fb31c7791d293b507 stopped with a status of 'completed'\n\n<\/code><\/pre>\n<p>Do remote runs require everything to become a module, with \u2018<strong>init<\/strong>.py\u2019? Or should the guild file be in a different location?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1645549217715,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":198.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/local-run-vs-remote-run-dependencies\/812",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-03T17:17:20.121Z",
                "Answer_body":"<p>Sorry for the late reply here! I thought someone had replied to this but I had my messages crossed. Taking a look now.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-03T17:49:58.131Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cptpirx\">@CptPirx<\/a>,<\/p>\n<p>We think your issue is the lack of a data-files entry: <a href=\"https:\/\/my.guild.ai\/t\/packages\/223\" class=\"inline-onebox\">Packages<\/a><\/p>\n<p>Garrett and I discussed the unintuitive behavior that explicitly listed sourcecode and files are not being included. We\u2019ll file a feature request for straightening this out, so that the data-files entry is only necessary for files not explicitly listed in other fields.<\/p>\n<p>EDIT: issue filed at <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/323\" class=\"inline-onebox\">Package explicitly listed sourcecode and config entries without data-files \u00b7 Issue #323 \u00b7 guildai\/guildai \u00b7 GitHub<\/a><\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-07T16:00:50.556Z",
                "Answer_body":"<p>After adding appropriate data-files entries in the package, as suggested, everything works.<\/p>\n<p>Thank you both for quick help, as always  <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
                "Answer_score":51.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-07T16:05:40.637Z",
                "Answer_body":"<p>I\u2019ll echo <a class=\"mention\" href=\"\/u\/msarahan\">@msarahan<\/a> with a mea culpa \u2014 Guild should figure that stuff out and not require the extra work. We\u2019ll get that cleaned up! Thanks for your patience.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"local run remot run depend run oper remot successfulli past discrep import local remot run need fix trial error current setup switch flag global variabl train script flag config yml file unabl work remot project structur project folder dataset modul contain data loader config yml file zoo home local run model model definit yml abstract model conv lstm model want run model model definit train train script config yml flag file standard convolut lstm model conv lstm descript convolut lstm oper train local descript train convolut lstm sourcecod conv lstm train conv lstm model abstract model requir config conv lstm config yml file dataset main conv lstm train flag dest config conv lstm config yml flag import flag epoch dataset arg dataset ucsd batch size output scalar train loss train mse valu test acc test mse valu train remot descript train convolut lstm remot sourcecod conv lstm train conv lstm model abstract model requir config conv lstm config yml file dataset main conv lstm train flag dest config conv lstm config yml flag import flag optim adam loss mse learn rate epoch dev true gpu dataset arg dataset ucsd batch size train path data ucsd ucsdp train test path data ucsd ucsdp test output scalar train loss train mse valu test acc test mse valu train script sy path append sy path append dataset tensorflow log level environ cpp min log level import tensorflow import yaml model import convlstm dataset data loader import dataload load model configur class config object def init self filenam self dict updat yaml safe load open filenam config config config yml current situat error abl run conv lstm train local issu work expect configur flag chang fail run remot issu evid config yml file copi remot issu remot run fail main train script work local home bleporowski project mad zoo run conv lstm train remot remot remot gpu run conv lstm train remot batch trial remot dataset arg batch size dataset ucsd test path data ucsd ucsdp test train path data ucsd ucsdp train dev ye epoch gpu learn rate loss mse optim adam continu build packag packag src home bleporowski project mad model packag dist tmp remot stage eqahi run clean remov build lib remov build bdist linux build script exist clean remov build run bdist wheel run build run build packag init file home bleporowski project mad model init regular file creat build creat build lib creat build lib conv lstm copi home bleporowski project mad model abstract model build lib conv lstm copi home bleporowski project mad model yml build lib conv lstm instal build bdist linux wheel run instal run instal lib creat build bdist linux creat build bdist linux wheel creat build bdist linux wheel conv lstm copi build lib conv lstm yml build bdist linux wheel conv lstm copi build lib conv lstm abstract model build bdist linux wheel conv lstm run instal egg info run egg info write conv lstm egg info pkg info write depend link conv lstm egg info depend link txt write entri point conv lstm egg info entri point txt write namespac packag conv lstm egg info namespac packag txt write level name conv lstm egg info level txt read manifest file conv lstm egg info sourc txt write manifest file conv lstm egg info sourc txt copi conv lstm egg info build bdist linux wheel conv lstm egg info run instal script creat build bdist linux wheel conv lstm dist info wheel creat tmp remot stage eqahi conv lstm whl ad build bdist linux wheel ad conv lstm abstract model ad conv lstm yml ad conv lstm dist info metadata ad conv lstm dist info packag ad conv lstm dist info wheel ad conv lstm dist info entri point txt ad conv lstm dist info namespac packag txt ad conv lstm dist info level txt ad conv lstm dist info record remov build bdist linux wheel initi remot run copi packag send increment file list conv lstm whl sent byte receiv byte byte sec total size speedup instal packag depend process conv lstm whl instal collect packag conv lstm successfulli instal conv lstm start conv lstm train remot charybdi acafbcdb warn errno file directori conv lstm config yml warn errno file directori conv lstm config yml warn import flag conv lstm train modul name conv lstm train warn import flag conv lstm train modul name conv lstm train info run trial afeffbafcde conv lstm conv lstm train remot dataset arg batch size dataset ucsd test path data ucsd ucsdp test train path data ucsd ucsdp train dev ye epoch gpu learn rate loss mse optim adam info resolv config conv lstm config yml depend error trial afeffbafcde exit error run fail depend met resolv config conv lstm config yml config conv lstm config yml resourc sourc file conv lstm config yml run acafbcdb stop statu complet remot run requir modul init file differ locat",
        "Question_preprocessed_content":"local run remot run depend run oper remot successfulli past discrep import local remot run need fix trial error current setup switch flag global variabl train script flag file unabl work remot project structur project folder dataset modul contain data loader file zoo home local run model model definit yml model want run model definit train script flag file train script current situat error abl run issu work expect configur flag chang fail run remot issu evid file copi remot issu remot run fail main train script work local remot run requir modul file differ locat",
        "Question_gpt_summary_original":"The user is facing challenges with running operations on remotes due to discrepancies between the imports for local and remote runs. They switched from flags as global variables in the training script to flags in config.yml files, but are unable to make it work on remotes. The user is able to run 'conv_lstm:train_local' without any issues, but almost the same configuration, with a few flags changed, fails to run on remote. The user is facing two issues: they cannot see any evidence of the config.yml file being copied to the remote, and the remote run fails to find the main training script, even though it works locally.",
        "Question_gpt_summary":"user face challeng run oper remot discrep import local remot run switch flag global variabl train script flag config yml file unabl work remot user abl run conv lstm train local issu configur flag chang fail run remot user face issu evid config yml file copi remot remot run fail main train script work local",
        "Answer_original_content":"sorri late repli thought repli messag cross take look cptpirx think issu lack data file entri packag garrett discuss unintuit behavior explicitli list sourcecod file includ file featur request straighten data file entri necessari file explicitli list field edit issu file packag explicitli list sourcecod config entri data file issu github ad appropri data file entri packag suggest work thank quick help ill echo msarahan mea culpa figur stuff requir extra work clean thank patienc",
        "Answer_preprocessed_content":"sorri late repli thought repli messag cross take look think issu lack entri packag garrett discuss unintuit behavior explicitli list sourcecod file includ file featur request straighten entri necessari file explicitli list field edit issu file packag explicitli list sourcecod config entri issu github ad appropri entri packag suggest work thank quick help ill echo mea culpa figur stuff requir extra work clean thank patienc",
        "Answer_gpt_summary_original":"the solution to the user's issue with discrepancies between local and remote runs is to add a data-files entry in the package. this will resolve the issues with the config.yml file and the main training script. the user was able to resolve the issue by adding the appropriate data-files entries in the package. the team will file a feature request to straighten out the unintuitive behavior of explicitly listed sourcecode and files not being included. the team will work on cleaning up the issue so that the data-files entry is only necessary for files not explicitly listed in other fields.",
        "Answer_gpt_summary":"solut user issu discrep local remot run add data file entri packag resolv issu config yml file main train script user abl resolv issu ad appropri data file entri packag team file featur request straighten unintuit behavior explicitli list sourcecod file includ team work clean issu data file entri necessari file explicitli list field"
    },
    {
        "Question_id":null,
        "Question_title":"web app model deployment",
        "Question_body":"I am deploying my prediction model as web app for first time in Azure through github actions step.\nI can see the deployment process is successful in github actions window. When I browse my app from azure window, it is not connecting to my web app. There is some issue. but the app is up & running.\n\nmy subscription id - 06facb88-7723-4e1e-82cd-774f082c46d5\n\nAttached is the sample images.\n\n\n\n\n[1]: \/answers\/storage\/attachments\/79718-1.png\n\n[2]: \/answers\/storage\/attachments\/79719-2.png\n\n[3]: \/answers\/storage\/attachments\/79720-3.png",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616240230297,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/323724\/web-app-model-deployment.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-22T10:07:54.367Z",
                "Answer_score":0,
                "Answer_body":"@23174178 Thanks for the question. Can you please add more details about the prediction model that you are trying and error details to check. Can you please confirm are you using the Designer?\n\nCurrently App Service and Azure functions(preview) as deployment target via Azure ML is currently documented as a recipe to facilitate the model packaging story for customers with affinity to Web Apps. Please follow the doc to deploy to Azure App services and doc on choosing Inference compute target here (AKS for real-time and AMLCompute for batch).",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"web app model deploy deploi predict model web app time azur github action step deploy process success github action window brows app azur window connect web app issu app run subscript facb fcd attach sampl imag answer storag attach png answer storag attach png answer storag attach png",
        "Question_preprocessed_content":"web app model deploy deploi predict model web app time azur github action step deploy process success github action window brows app azur window connect web app issu app run subscript attach sampl imag",
        "Question_gpt_summary_original":"The user is facing challenges in deploying their prediction model as a web app for the first time in Azure through Github actions. Although the deployment process is successful in Github actions window, the web app is not connecting when browsed from the Azure window. The app is up and running, but there seems to be an issue. The user has provided their subscription ID and attached sample images.",
        "Question_gpt_summary":"user face challeng deploi predict model web app time azur github action deploy process success github action window web app connect brows azur window app run issu user provid subscript attach sampl imag",
        "Answer_original_content":"thank question add detail predict model try error detail check confirm design current app servic azur function preview deploy target current document recip facilit model packag stori custom affin web app follow doc deploi azur app servic doc choos infer comput target ak real time amlcomput batch",
        "Answer_preprocessed_content":"thank question add detail predict model try error detail check confirm design current app servic azur function deploy target current document recip facilit model packag stori custom affin web app follow doc deploi azur app servic doc choos infer comput target",
        "Answer_gpt_summary_original":"possible solutions from the answer include adding more details about the prediction model and error details, confirming if the designer is being used, following the documentation to deploy to azure app services, and choosing the appropriate inference compute target (aks for real-time and amlcompute for batch).",
        "Answer_gpt_summary":"possibl solut answer includ ad detail predict model error detail confirm design follow document deploi azur app servic choos appropri infer comput target ak real time amlcomput batch"
    },
    {
        "Question_id":64286191.0,
        "Question_title":"How to add keyboard shortcuts to AWS Ground Truth labeler UI?",
        "Question_body":"<p>I'm using AWS Sagemaker Ground Truth for a Custom Labeling Task that involves editing bounding boxes and their labels.  Ground Truth's UI has built-in keyboard shortcuts for doing things like choosing the label for a box, but it seems to lack shortcuts for other built-in UI elements like &quot;No adjustments needed&quot; or the &quot;Submit&quot; button.<\/p>\n<p>Is there a way to add such shortcuts?  I've looked at the crowd-html-elements for customizing the appearance of the page, but can't find anything in there about keyboard shortcuts.  It doesn't even look like crowd-button or crowd-icon-button support specifying a shortcut as an attribute.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1602271699933,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":412.0,
        "Answer_body":"<p>Could try something like:<\/p>\n<pre><code>document.addEventListener('keydown', function(event) {\n  if (event.shiftKey &amp;&amp; event.keyCode === 13) {\n    document.getElementsByTagName('crowd-bounding-box')[0].shadowRoot.getElementById('nothing-to-adjust').querySelector('label').click();\n  }\n});\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64286191",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1605831050088,
        "Question_original_content":"add keyboard shortcut aw ground truth label ground truth custom label task involv edit bound box label ground truth built keyboard shortcut thing like choos label box lack shortcut built element like adjust need submit button wai add shortcut look crowd html element custom appear page keyboard shortcut look like crowd button crowd icon button support specifi shortcut attribut",
        "Question_preprocessed_content":"add keyboard shortcut aw ground truth label ground truth custom label task involv edit bound box label ground truth keyboard shortcut thing like choos label box lack shortcut element like adjust need submit button wai add shortcut look custom appear page keyboard shortcut look like support specifi shortcut attribut",
        "Question_gpt_summary_original":"The user is facing challenges in using AWS Sagemaker Ground Truth for a Custom Labeling Task as the UI lacks keyboard shortcuts for certain built-in UI elements such as \"No adjustments needed\" or the \"Submit\" button. The user has tried to customize the appearance of the page using crowd-html-elements but couldn't find anything related to keyboard shortcuts.",
        "Question_gpt_summary":"user face challeng ground truth custom label task lack keyboard shortcut certain built element adjust need submit button user tri custom appear page crowd html element couldn relat keyboard shortcut",
        "Answer_original_content":"try like document addeventlisten keydown function event event shiftkei event keycod document getelementsbytagnam crowd bound box shadowroot getelementbyid adjust queryselector label click",
        "Answer_preprocessed_content":"try like",
        "Answer_gpt_summary_original":"the answer suggests adding a keyboard shortcut to the aws ground truth labeler ui by using a javascript code that listens for a specific key combination and then triggers a click event on the desired ui element. specifically, the code provided targets the \"no adjustments needed\" label using the tag name and id of the relevant html elements.",
        "Answer_gpt_summary":"answer suggest ad keyboard shortcut aw ground truth label javascript code listen specif kei combin trigger click event desir element specif code provid target adjust need label tag relev html element"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML: Creating a Siamese network, I have the reference data stored in one blob storage and the user input stored in another blob storage. How can I connect the two blob storage to my batch endpoint. If not, is there a workaround?",
        "Question_body":"Azure ML: Creating a Siamese network, I have the reference data stored in one blob storage and the user input stored in another blob storage. How can I connect the two blob storage to my batch endpoint. If not, is there a workaround? The setup looks like this.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662886746297,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1002087\/azure-ml-creating-a-siamese-network-i-have-the-ref.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-12T12:04:20.93Z",
                "Answer_score":0,
                "Answer_body":"@SamarjeetSinghPatil-6739 I think this should be possible where your input could be a different storage account and your output could be the default datastore.\n\n export OUTPUT_FILE_NAME=predictions_`echo $RANDOM`.csv\n JOB_NAME=$(az ml batch-endpoint invoke --name $ENDPOINT_NAME --input https:\/\/pipelinedata.blob.core.windows.net\/sampledata\/mnist --input-type uri_folder --output-path azureml:\/\/datastores\/workspaceblobstore\/paths\/$ENDPOINT_NAME --set output_file_name=$OUTPUT_FILE_NAME --mini-batch-size 20 --instance-count 5 --query name -o tsv)\n\n\n\nRef: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-batch-endpoint#configure-the-output-location-and-overwrite-settings\n\nPlease refer some other scenarios that you can set with the input and output path while using the batch endpoint invoke command.\n\n az ml batch-endpoint invoke --help",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"creat siames network refer data store blob storag user input store blob storag connect blob storag batch endpoint workaround creat siames network refer data store blob storag user input store blob storag connect blob storag batch endpoint workaround setup look like",
        "Question_preprocessed_content":"creat siames network refer data store blob storag user input store blob storag connect blob storag batch endpoint workaround creat siames network refer data store blob storag user input store blob storag connect blob storag batch endpoint workaround setup look like",
        "Question_gpt_summary_original":"The user is facing a challenge in connecting two separate blob storages, one containing reference data and the other containing user input, to their batch endpoint while creating a Siamese network in Azure ML. They are seeking a workaround if a direct connection is not possible.",
        "Question_gpt_summary":"user face challeng connect separ blob storag contain refer data contain user input batch endpoint creat siames network seek workaround direct connect possibl",
        "Answer_original_content":"samarjeetsinghpatil think possibl input differ storag account output default datastor export output file predict echo random csv job batch endpoint invok endpoint input http pipelinedata blob core window net sampledata mnist input type uri folder output path datastor workspaceblobstor path endpoint set output file output file mini batch size instanc count queri tsv ref http doc microsoft com azur machin learn us batch endpoint configur output locat overwrit set refer scenario set input output path batch endpoint invok command batch endpoint invok help",
        "Answer_preprocessed_content":"think possibl input differ storag account output default datastor export invok tsv ref refer scenario set input output path batch endpoint invok command invok",
        "Answer_gpt_summary_original":"the answer suggests that it is possible to connect two blob storages to a batch endpoint by setting the input as a different storage account and the output as the default datastore. the answer also provides a reference to microsoft's documentation on how to configure the output location and overwrite settings. additionally, the answer suggests referring to other scenarios that can be set with the input and output path while using the batch endpoint invoke command.",
        "Answer_gpt_summary":"answer suggest possibl connect blob storag batch endpoint set input differ storag account output default datastor answer provid refer microsoft document configur output locat overwrit set addition answer suggest refer scenario set input output path batch endpoint invok command"
    },
    {
        "Question_id":null,
        "Question_title":"GOCD Plugin for MLflow",
        "Question_body":"Hi All,\n\n\nWe have built a new GoCD plugin[1] which works with mlflow and helps us in continuous delivery of models.\n\nHope Y'all find this useful.\u00a0\n\n\n[1] https:\/\/github.com\/indix\/mlflow-gocd\/\n\n\nBest,\nKrishna Sangeeth",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1541984493000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":22.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/CIihDdCS014",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-11-20T21:44:38",
                "Answer_body":"This is very cool, Krishna; thanks for sharing! We also just saw your blog post at https:\/\/stacktoheap.com\/blog\/2018\/11\/19\/mlflow-model-repository-ci-cd\/.\n\nIf you\u2019d like, we could feature this blog on mlflow.org or link to the plugin in the docs somewhere; please let us know whether you\u2019d be interested.\n\nMatei\n\n\ue5d3\n> --\n> You received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n> To unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n> To post to this group, send email to mlflow...@googlegroups.com.\n> To view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CADub9hP%3DtTiJ6uMLL6pauzq_BorF26OD0mvp%3D9gzMhOkVuZesw%40mail.gmail.com.\n> For more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2018-11-21T01:43:34",
                "Answer_body":"<Reposting after joining group>\n\n\nHi Matei,\n\n\nThanks for the response.\n\n\nWe would definitely be happy if you can feature the blog and\/or the plugin. Let us know if you need anything from us.\n\n\nThanks\nManoj\n\n\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"gocd plugin built new gocd plugin work help continu deliveri model hope us http github com indix gocd best krishna sangeeth",
        "Question_preprocessed_content":"gocd plugin built new gocd plugin work help continu deliveri model hope us best krishna sangeeth",
        "Question_gpt_summary_original":"The user has encountered challenges in implementing continuous delivery of models using mlflow and GoCD, which led them to develop a new GoCD plugin that works with mlflow.",
        "Question_gpt_summary":"user encount challeng implement continu deliveri model gocd led develop new gocd plugin work",
        "Answer_original_content":"cool krishna thank share saw blog post http stacktoheap com blog model repositori youd like featur blog org link plugin doc let know youd interest matei receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user cadubhp dttijumllpauzq borfodmvp dgzmhokvuzesw mail gmail com option visit http group googl com optout matei thank respons definit happi featur blog plugin let know need thank manoj",
        "Answer_preprocessed_content":"cool krishna thank share saw blog post youd like featur blog org link plugin doc let know youd interest matei receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit matei thank respons definit happi featur blog plugin let know need thank manoj",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut mention discuss"
    },
    {
        "Question_id":null,
        "Question_title":"MLflow tracking remote mysql database connection failed",
        "Question_body":"I have setup mysql database and used mflow_set_tracking_uri to setup connection to the database. It runs fine from the local machine. However when i run the same file from another machine It does not connect to the database.\nmlflow.set_tracking_uri(\"mysql+pymysql:\/\/mlflow:*****@10.32.129.62:3306\/mlflow_tracking\")",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1569564671000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":8.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/wBN1p_C0KhI",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-09-27T14:23:08",
                "Answer_body":"Hi Ali, thx for posting. Does it show any error message?\u00a0\n\n\n\n\n\n\nOn Fri, Sep 27, 2019 at 3:11 AM babar ali <bac...@gmail.com> wrote:\n\nI have setup mysql database and used mflow_set_tracking_uri to setup connection to the database. It runs fine from the local machine. However when i run the same file from another machine It does not connect to the database.\nmlflow.set_tracking_uri(\"mysql+pymysql:\/\/mlflow:*****@10.32.129.62:3306\/mlflow_tracking\")\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/a5064446-ec2a-4868-9993-c007b3f5fad4%40googlegroups.com."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"track remot mysql databas connect fail setup mysql databas mflow set track uri setup connect databas run fine local machin run file machin connect databas set track uri mysql pymysql track",
        "Question_preprocessed_content":"track remot mysql databas connect fail setup mysql databas setup connect databas run fine local machin run file machin connect databas",
        "Question_gpt_summary_original":"The user is facing a challenge with connecting to a remote MySQL database using MLflow tracking. The connection works fine on the local machine, but fails when attempting to connect from another machine.",
        "Question_gpt_summary":"user face challeng connect remot mysql databas track connect work fine local machin fail attempt connect machin",
        "Answer_original_content":"ali thx post error messag fri sep babar ali wrote setup mysql databas mflow set track uri setup connect databas run fine local machin run file machin connect databas set track uri mysql pymysql track receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user eca cbffad googlegroup com",
        "Answer_preprocessed_content":"ali thx post error messag fri sep babar ali wrote setup mysql databas setup connect databas run fine local machin run file machin connect databas receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_id":66277250.0,
        "Question_title":"Reading multiple CSV files from S3 using Python with Boto3",
        "Question_body":"<p>I am able to read multiple csv files from S3 bucket with boto3 in python and finally combine those files in single dataframe in pandas.However, in some of the folders there are some empty files which results in the error &quot;No columns to parse from file&quot;. Can we skip those empty files in the below codes?<\/p>\n<pre><code>s3 = boto3.resource('s3')\nbucket = s3.Bucket('testbucket')\n\nprefix_objs = bucket.objects.filter(Prefix=&quot;extracted\/abc&quot;)\n\n    prefix_df = []\n\nfor obj in prefix_objs:\n    key = obj.key\n    body = obj.get()['Body'].read()\n    temp = pd.read_csv(io.BytesIO(body),header=None, encoding='utf8',sep=',')        \n    prefix_df.append(temp)\n<\/code><\/pre>\n<p>I have used this ans [https:\/\/stackoverflow.com\/questions\/52855221\/reading-multiple-csv-files-from-s3-bucket-with-boto3][1]<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1613736268983,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":3048.0,
        "Answer_body":"<pre><code>s3 = boto3.resource('s3')\nbucket = s3.Bucket('testbucket')\n\nprefix_objs = bucket.objects.filter(Prefix=&quot;extracted\/abc&quot;)\n\nprefix_df = []\n\nfor obj in prefix_objs:\n    try:\n        key = obj.key\n        body = obj.get()['Body'].read()\n        temp = pd.read_csv(io.BytesIO(body),header=None, encoding='utf8',sep=',')        \n        prefix_df.append(temp)\n    except:\n        continue\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66277250",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1613737607800,
        "Question_original_content":"read multipl csv file python boto abl read multipl csv file bucket boto python final combin file singl datafram panda folder file result error column pars file skip file code boto resourc bucket bucket testbucket prefix obj bucket object filter prefix extract abc prefix obj prefix obj kei obj kei bodi obj bodi read temp read csv bytesio bodi header encod utf sep prefix append temp an http stackoverflow com question read multipl csv file bucket boto",
        "Question_preprocessed_content":"read multipl csv file python boto abl read multipl csv file bucket boto python final combin file singl datafram folder file result error column pars file skip file code an",
        "Question_gpt_summary_original":"The user is facing a challenge while reading multiple CSV files from an S3 bucket using Python with Boto3. The issue is caused by some empty files in certain folders, resulting in an error message \"No columns to parse from file\". The user is seeking a solution to skip these empty files in the code.",
        "Question_gpt_summary":"user face challeng read multipl csv file bucket python boto issu caus file certain folder result error messag column pars file user seek solut skip file code",
        "Answer_original_content":"boto resourc bucket bucket testbucket prefix obj bucket object filter prefix extract abc prefix obj prefix obj try kei obj kei bodi obj bodi read temp read csv bytesio bodi header encod utf sep prefix append temp continu",
        "Answer_preprocessed_content":"",
        "Answer_gpt_summary_original":"the solution to the problem of encountering an error while attempting to read multiple csv files from an s3 bucket using boto3 in python and combine them into a single dataframe in pandas due to empty files in some of the folders is to use the provided code. the code uses boto3 to access the s3 bucket, filters the objects based on a prefix, reads the csv files using pandas, and appends them to a list. any errors encountered during the process are ignored.",
        "Answer_gpt_summary":"solut problem encount error attempt read multipl csv file bucket boto python combin singl datafram panda file folder us provid code code us boto access bucket filter object base prefix read csv file panda append list error encount process ignor"
    },
    {
        "Question_id":null,
        "Question_title":"Real-time inferencing with azure ml model and send output to databricks",
        "Question_body":"Hi,\nI've built and registered a basic ml model in azure ml studio and I will get an aks cluster for the purpose of doing real-time inferencing with the model. So our pipipline would be we get the real time data from eventhub in databricks and store it into datalake(datastore) near real-time then azure ml model will get the input data from datalake, inference the data and generate output and send the output back to the databricks. and finally we want to visualize the output in powerbi.\nBut I'm just wondering if it's possible to do it as I'm new to azure ml. I was thinking to use stream analytics but seems like I can't get an approval from my company for using stream analytics for this pipeline.\n\nAnyone who could help me on this would be every much appreciated. and I'd appreicate it if you could tell me how to connct azure ml model and databricks using API as well.\n\nThank you!",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1644441202260,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/729370\/real-time-inferencing-with-azure-ml-model-and-send.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-10T08:45:17.373Z",
                "Answer_score":0,
                "Answer_body":"@jingyeongyu-5737 Thanks, In Azure Machine Learning, you can write output data directly to Azure Blob Storage, Azure Data Lake Storage Gen 1, Azure Data Lake Storage Gen 2, Azure FileShare without going through extra DataTransferStep. Learn how to use OutputFileDatasetConfig to achieve that with sample notebooks here.**",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"real time inferenc model send output databrick built regist basic model studio ak cluster purpos real time inferenc model pipiplin real time data eventhub databrick store datalak datastor near real time model input data datalak infer data gener output send output databrick final want visual output powerbi wonder possibl new think us stream analyt like approv compani stream analyt pipelin help appreci appreic tell connct model databrick api thank",
        "Question_preprocessed_content":"inferenc model send output databrick built regist basic model studio ak cluster purpos inferenc model pipiplin real time data eventhub databrick store datalak near model input data datalak infer data gener output send output databrick final want visual output powerbi wonder possibl new think us stream analyt like approv compani stream analyt pipelin help appreci appreic tell connct model databrick api thank",
        "Question_gpt_summary_original":"The user is facing challenges in connecting their Azure ML model with Databricks to perform real-time inferencing and send the output back to Databricks for visualization in PowerBI. They are unsure if this is possible and are seeking help on how to connect the two platforms using API. Additionally, the user is unable to use Stream Analytics for this pipeline due to company restrictions.",
        "Question_gpt_summary":"user face challeng connect model databrick perform real time inferenc send output databrick visual powerbi unsur possibl seek help connect platform api addition user unabl us stream analyt pipelin compani restrict",
        "Answer_original_content":"jingyeongyu thank write output data directli azur blob storag azur data lake storag gen azur data lake storag gen azur fileshar go extra datatransferstep learn us outputfiledatasetconfig achiev sampl notebook",
        "Answer_preprocessed_content":"thank write output data directli azur blob storag azur data lake storag gen azur data lake storag gen azur fileshar go extra datatransferstep learn us outputfiledatasetconfig achiev sampl notebook",
        "Answer_gpt_summary_original":"the answer suggests that the user can write output data directly to various azure storage options without going through extra data transfer steps. the user can learn how to use outputfiledatasetconfig to achieve this with sample notebooks provided in the link.",
        "Answer_gpt_summary":"answer suggest user write output data directli azur storag option go extra data transfer step user learn us outputfiledatasetconfig achiev sampl notebook provid link"
    },
    {
        "Question_id":60839279.0,
        "Question_title":"how can i use tensorboard with aws sagemaker tensorflow?",
        "Question_body":"<p>i have started a sagemaker job:<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\nmytraining= TensorFlow(entry_point='model.py',\n                        role=role,\n                        train_instance_count=1,\n                        train_instance_type='ml.p2.xlarge',\n                        framework_version='2.0.0',\n                        py_version='py3',\n                        distributions={'parameter_server'{'enabled':False}})\n\ntraining_data_uri ='s3:\/\/path\/to\/my\/data'\nmytraining.fit(training_data_uri,run_tensorboard_locally=True)\n<\/code><\/pre>\n\n<p>using <code>run_tesorboard_locally=True<\/code> gave me<\/p>\n\n<pre><code>Tensorboard is not supported with script mode. You can run the following command: tensorboard --logdir None --host localhost --port 6006 This can be run from anywhere with access to the S3 URI used as the logdir.\n<\/code><\/pre>\n\n<p>It seems like i cant use it script mode, but I can access the logs of tensorboard in s3? But where are the logs in s3?<\/p>\n\n<pre><code>def _parse_args():\n    parser = argparse.ArgumentParser()\n\n    # Data, model, and output directories\n    # model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.\n    parser.add_argument('--model_dir', type=str)\n    parser.add_argument('--sm-model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))\n    parser.add_argument('--hosts', type=list, default=json.loads(os.environ.get('SM_HOSTS')))\n    parser.add_argument('--current-host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n\n    return parser.parse_known_args()\n\nif __name__ == \"__main__\":\n    args, unknown = _parse_args()\n\n    train_data, train_labels = load_training_data(args.train)\n    eval_data, eval_labels = load_testing_data(args.train)\n\n    mymodel= model(train_data, train_labels, eval_data, eval_labels)\n\n    if args.current_host == args.hosts[0]:\n        mymodel.save(os.path.join(args.sm_model_dir, '000000002\/model.h5'))\n<\/code><\/pre>\n\n<p>similiar question is here :<a href=\"https:\/\/stackoverflow.com\/questions\/53713660\/tensorboard-without-callbacks-for-keras-docker-image-in-sagemaker\">stack<\/a><\/p>\n\n<p>EDIT i tried this new config but it doesnt work.<\/p>\n\n<pre><code> tensorboard_output_config = TensorBoardOutputConfig( s3_output_path='s3:\/\/PATH\/to\/my\/bucket')\n\nmytraining= TensorFlow(entry_point='model.py',\n                        role=role,\n                        train_instance_count=1,\n                        train_instance_type='ml.p2.xlarge',\n                        framework_version='2.0.0',\n                        py_version='py3',\n                        distributions={'parameter_server': {'enabled':False}},\n                        tensorboard_output_config=tensorboard_output_config)\n<\/code><\/pre>\n\n<p>i added the callback in my model.py script that is actually what i use without sagemaker. As logdir i defined the default dir, where the TensoboardOutputConfig writes the data.. but it doesnt work. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_TensorBoardOutputConfig.html\" rel=\"nofollow noreferrer\">docs<\/a> I also used it without the callback.<\/p>\n\n<pre><code> tensorboardCallback = tf.keras.callbacks.TensorBoard(\n        log_dir='\/opt\/ml\/output\/tensorboard',\n        histogram_freq=0,\n        # batch_size=32,ignored tf.2.0\n        write_graph=True,\n        write_grads=False,\n        write_images=False,\n        embeddings_freq=0,\n        embeddings_layer_names=None,\n        embeddings_metadata=None,\n        embeddings_data=None,\n        update_freq='batch') \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1585083712870,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1585093210283,
        "Question_score":4.0,
        "Question_view_count":1011.0,
        "Answer_body":"<p>Difficult to debug what the exact root cause is in your case, but following steps worked for me. I started tensorboard inside the notebook instance manually.<\/p>\n<ol>\n<li><p>Followed guide on <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_debugger.html#capture-real-time-tensorboard-data-from-the-debugging-hook\" rel=\"noreferrer\">sagemaker debugging<\/a> to configure the <code>S3<\/code> output path for tensorboard logs.<\/p>\n<pre><code>from sagemaker.debugger import TensorBoardOutputConfig\n\ntensorboard_output_config = TensorBoardOutputConfig(\n       s3_output_path = 's3:\/\/bucket-name\/tensorboard_log_folder\/'\n)\n\nestimator = TensorFlow(entry_point='train.py',\n               source_dir='.\/',\n               model_dir=model_dir,\n               output_path= output_dir,\n               train_instance_type=train_instance_type,\n               train_instance_count=1,\n               hyperparameters=hyperparameters,\n               role=sagemaker.get_execution_role(),\n               base_job_name='Testing-TrainingJob',\n               framework_version='2.2',\n               py_version='py37',\n               script_mode=True,\n               tensorboard_output_config=tensorboard_output_config)\n\nestimator.fit(inputs)\n<\/code><\/pre>\n<\/li>\n<li><p>Start the tensorboard with the <code>S3<\/code> location provided above via a terminal on the notebook instance.<\/p>\n<pre><code>$ tensorboard --logdir 's3:\/\/bucket-name\/tensorboard_log_folder\/'\n<\/code><\/pre>\n<\/li>\n<li><p>Access the board via URL with <code>\/proxy\/6006\/<\/code>. You need to update the notebook instance details in the following URL.<\/p>\n<pre><code>https:\/\/myinstance.notebook.us-east-1.sagemaker.aws\/proxy\/6006\/\n<\/code><\/pre>\n<\/li>\n<\/ol>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60839279",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1604514854867,
        "Question_original_content":"us tensorboard tensorflow start job tensorflow import tensorflow mytrain tensorflow entri point model role role train instanc count train instanc type xlarg framework version version distribut paramet server enabl fals train data uri path data mytrain fit train data uri run tensorboard local true run tesorboard local true gave tensorboard support script mode run follow command tensorboard logdir host localhost port run access uri logdir like us script mode access log tensorboard log def pars arg parser argpars argumentpars data model output directori model dir pass default path default bucket parser add argument model dir type str parser add argument model dir type str default environ model dir parser add argument train type str default environ channel train parser add argument host type list default json load environ host parser add argument current host type str default environ current host return parser pars known arg main arg unknown pars arg train data train label load train data arg train eval data eval label load test data arg train mymodel model train data train label eval data eval label arg current host arg host mymodel save path join arg model dir model similiar question stack edit tri new config doesnt work tensorboard output config tensorboardoutputconfig output path path bucket mytrain tensorflow entri point model role role train instanc count train instanc type xlarg framework version version distribut paramet server enabl fals tensorboard output config tensorboard output config ad callback model script actual us logdir defin default dir tensoboardoutputconfig write data doesnt work doc callback tensorboardcallback kera callback tensorboard log dir opt output tensorboard histogram freq batch size ignor write graph true write grad fals write imag fals embed freq embed layer name embed metadata embed data updat freq batch",
        "Question_preprocessed_content":"us tensorboard tensorflow start job gave like us script mode access log tensorboard log similiar question stack edit tri new config doesnt work ad callback script actual us logdir defin default dir tensoboardoutputconfig write doesnt work doc callback",
        "Question_gpt_summary_original":"The user is facing challenges in using TensorBoard with AWS SageMaker TensorFlow. They are unable to use TensorBoard in script mode and are trying to access the logs of TensorBoard in S3, but are unsure of where the logs are located. They have tried using TensorBoardOutputConfig and adding a callback in their model.py script, but it has not worked. They have also provided code snippets for reference.",
        "Question_gpt_summary":"user face challeng tensorboard tensorflow unabl us tensorboard script mode try access log tensorboard unsur log locat tri tensorboardoutputconfig ad callback model script work provid code snippet refer",
        "Answer_original_content":"difficult debug exact root caus case follow step work start tensorboard insid notebook instanc manual follow guid debug configur output path tensorboard log debugg import tensorboardoutputconfig tensorboard output config tensorboardoutputconfig output path bucket tensorboard log folder estim tensorflow entri point train sourc dir model dir model dir output path output dir train instanc type train instanc type train instanc count hyperparamet hyperparamet role execut role base job test trainingjob framework version version script mode true tensorboard output config tensorboard output config estim fit input start tensorboard locat provid termin notebook instanc tensorboard logdir bucket tensorboard log folder access board url proxi need updat notebook instanc detail follow url http myinstanc notebook east aw proxi",
        "Answer_preprocessed_content":"difficult debug exact root caus case follow step work start tensorboard insid notebook instanc manual follow guid debug configur output path tensorboard log start tensorboard locat provid termin notebook instanc access board url need updat notebook instanc detail follow url",
        "Answer_gpt_summary_original":"the answer suggests starting tensorboard manually, configuring the s3 output path for tensorboard logs, and starting tensorboard with the s3 location provided via a terminal on the notebook instance. the user can then access the board via a url with \/proxy\/6006\/ and update the notebook instance details in the url.",
        "Answer_gpt_summary":"answer suggest start tensorboard manual configur output path tensorboard log start tensorboard locat provid termin notebook instanc user access board url proxi updat notebook instanc detail url"
    },
    {
        "Question_id":null,
        "Question_title":"Jupyterlab page is blank",
        "Question_body":"In my Workspace, after creating a compute, when I click on Jupyterlab , I get a blank page. When I click on Jupyter, the notebook work fine.\n\nWhats the issue",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1598559753367,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/79393\/jupyterlab-page-is-blank.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-08-28T20:00:16.237Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nThanks for reaching out to us. I can confirm that it works well in my laptop as following:\n\n\n\n\n\n\nCould you please share your region so I can reproduce more accurately? Thanks.\n\nRegards,\nYutong",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":2.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"jupyterlab page blank workspac creat comput click jupyterlab blank page click jupyt notebook work fine what issu",
        "Question_preprocessed_content":"jupyterlab page blank workspac creat comput click jupyterlab blank page click jupyt notebook work fine what issu",
        "Question_gpt_summary_original":"The user is encountering a problem where the Jupyterlab page appears blank after creating a compute in their Workspace. However, the Jupyter notebook works fine when clicked.",
        "Question_gpt_summary":"user encount problem jupyterlab page appear blank creat comput workspac jupyt notebook work fine click",
        "Answer_original_content":"thank reach confirm work laptop follow share region reproduc accur thank regard yutong",
        "Answer_preprocessed_content":"thank reach confirm work laptop follow share region reproduc accur thank regard yutong",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer to the issue of a blank jupyterlab page after creating a compute. the responder is asking for more information about the user's region to reproduce the issue accurately.",
        "Answer_gpt_summary":"solut provid answer issu blank jupyterlab page creat comput respond ask inform user region reproduc issu accur"
    },
    {
        "Question_id":null,
        "Question_title":"Machine Learning\u306b\u3064\u3044\u3066\u306e\u8cea\u554f",
        "Question_body":"\u63b2\u984c\u306e\u4ef6\u306b\u3064\u304d\u307e\u3057\u3066\u3001\u73fe\u5728Machine Learning\u3092\u4f7f\u7528\u3057\u3066\u6a5f\u68b0\u5b66\u7fd2\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\u305d\u3053\u3067\u8cea\u554f\u306b\u306a\u308b\u306e\u3067\u3059\u304c\u3001\u30c7\u30b6\u30a4\u30ca\u30fc\u6a5f\u80fd\u3092\u4f7f\u7528\u3057\u3066\u5b66\u7fd2\u7d50\u679c\u3092CSV\u3067\u30a8\u30af\u30b9\u30dd\u30fc\u30c8\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u306e\u3067\u3059\u304c\u3001\nExport Data\u30e2\u30c7\u30eb\u3067CSV\u5f62\u5f0f\u306b\u8a2d\u5b9a\u3057\u3066\u3044\u3066\u3082CSV\u3067\u306f\u306a\u3044\u5f62\u5f0f\u3067\u5171\u6709\u305b\u308c\u3066\u3057\u307e\u3046\u306e\u3067\u3059\u304c\u3001\u539f\u56e0\u304c\u308f\u304b\u3089\u306a\u3044\u72b6\u6cc1\u3067\u3059\u3002\n\u3054\u6559\u793a\u306e\u307b\u3069\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1631251067517,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@63862379 Are you referring to the export data module of the designer from ml.azure.com?\nI think I understand the issue, Are you seeing that the .csv format of file is not listed on the blob storage?\n\nSince the input is a dataframe directory to export module the output format selected should still be the format you selected, in this case CSV. The file name extension only might be missing. You can still open the csv file in excel and it will recognize the delimiters and headers so you can convert it into excel files.\n\nYou can also avoid this by providing the .csv extension in the path itself in export settings and file will be exported as a csv file directly.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/546760\/machine-learning%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E3%81%AE%E8%B3%AA%E5%95%8F.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-10T10:15:10.237Z",
                "Answer_score":0,
                "Answer_body":"@63862379 Are you referring to the export data module of the designer from ml.azure.com?\nI think I understand the issue, Are you seeing that the .csv format of file is not listed on the blob storage?\n\nSince the input is a dataframe directory to export module the output format selected should still be the format you selected, in this case CSV. The file name extension only might be missing. You can still open the csv file in excel and it will recognize the delimiters and headers so you can convert it into excel files.\n\nYou can also avoid this by providing the .csv extension in the path itself in export settings and file will be exported as a csv file directly.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1631268910236,
        "Question_original_content":"machin learn machin learn csv export datacsvcsv",
        "Question_preprocessed_content":"machin learn machin learn csv export datacsvcsv",
        "Question_gpt_summary_original":"The user is facing challenges with exporting machine learning results in CSV format using the designer function. Despite setting the Export Data model to CSV format, the shared file is not in CSV format, and the user is unsure of the cause of the issue.",
        "Question_gpt_summary":"user face challeng export machin learn result csv format design function despit set export data model csv format share file csv format user unsur caus issu",
        "Answer_original_content":"refer export data modul design azur com think understand issu see csv format file list blob storag input datafram directori export modul output format select format select case csv file extens miss open csv file excel recogn delimit header convert excel file avoid provid csv extens path export set file export csv file directli forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_preprocessed_content":"refer export data modul design think understand issu see csv format file list blob storag input datafram directori export modul output format select format select case csv file extens miss open csv file excel recogn delimit header convert excel file avoid provid csv extens path export set file export csv file directli forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_gpt_summary_original":"possible solutions from the answer are: \n1. the user can still open the csv file in excel and convert it into excel files.\n2. the user can provide the .csv extension in the path itself in export settings and the file will be exported as a csv file directly.",
        "Answer_gpt_summary":"possibl solut answer user open csv file excel convert excel file user provid csv extens path export set file export csv file directli"
    },
    {
        "Question_id":null,
        "Question_title":"WinError 1314 when trying to do a grid search",
        "Question_body":"<p>Hi,<br>\nI\u2019m having trouble when trying to do a search with guild on a Windows 10 machine.<\/p>\n<p>I took the example code fromt the tutorial<\/p>\n<pre><code>import numpy as np\n\n# Hyperparameters\nx = 0.1\nnoise = 0.1\n\n# Simulated training loss\nloss = (np.sin(5 * x) * (1 - np.tanh(x ** 2)) + np.random.randn() * noise)\n\nprint(\"loss: %f\" % loss)\n<\/code><\/pre>\n<p>I\u2019m running it from console with<\/p>\n<pre><code>guild run test.py x=\"[1,0,-1]\"\n<\/code><\/pre>\n<p>When executing i get an error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File \"c:\\users\\x\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"c:\\users\\x\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_main.py\", line 41, in &lt;module&gt;\n    main()\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_main.py\", line 27, in main\n    batch_util.handle_trials(batch_run, trials)\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 73, in handle_trials\n    _run_trials(batch_run, trials)\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 135, in _run_trials\n    trial_runs = _init_trial_runs(batch_run, trials)\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 144, in _init_trial_runs\n    return [init_trial_run(batch_run, trial) for trial in trials]\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 144, in &lt;listcomp&gt;\n    return [init_trial_run(batch_run, trial) for trial in trials]\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 149, in init_trial_run\n    _link_to_trial(batch_run, run)\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 168, in _link_to_trial\n    os.symlink(rel_trial_path, trial_link)\nOSError: [WinError 1314] A required privilege is not held by the client.\n\n '..\\\\749cba372c474a1598aac11e2d4fb902' -&gt; 'C:\\\\Users\\\\x\\\\Anaconda3\\\\.guild\\\\runs\\\\045f73f210fb463f814f3334fd30825d\\\\749cba372c474a1598aac11e2d4fb902'\n<\/code><\/pre>\n<p>I don\u2019t have administrator permissions on the machine and I\u2019m probably not going to get them.<\/p>\n<p>Installing it via pip with --User flag doesnt fix it, either.<\/p>\n<p>Can I fix this somehow without having to run as admin?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1642757463278,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":612.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/winerror-1314-when-trying-to-do-a-grid-search\/800",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-02T20:06:07.651Z",
                "Answer_body":"<p>Hello! Unfortunately Guild requires symbolic link permissions. If you can\u2019t run as Admin you\u2019ll need to get those permissions. That\u2019s a pain I realize \u2014 I wish this was easier. I would reach out to your system admin to get those setup.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"winerror try grid search have troubl try search window machin took exampl code fromt tutori import numpi hyperparamet nois simul train loss loss sin tanh random randn nois print loss loss run consol run test execut error traceback recent file user appdata local program python python lib runpi line run modul main return run code code main global file user appdata local program python python lib runpi line run code exec code run global file user appdata roam python python site packag batch main line main file user appdata roam python python site packag batch main line main batch util handl trial batch run trial file user appdata roam python python site packag batch util line handl trial run trial batch run trial file user appdata roam python python site packag batch util line run trial trial run init trial run batch run trial file user appdata roam python python site packag batch util line init trial run return init trial run batch run trial trial trial file user appdata roam python python site packag batch util line return init trial run batch run trial trial trial file user appdata roam python python site packag batch util line init trial run link trial batch run run file user appdata roam python python site packag batch util line link trial symlink rel trial path trial link oserror winerror requir privileg held client cbacaaacedfb user anaconda run fffbfffdd cbacaaacedfb dont administr permiss machin probabl go instal pip user flag doesnt fix fix have run admin",
        "Question_preprocessed_content":"winerror try grid search have troubl try search window machin took exampl code fromt tutori run consol execut error dont administr permiss machin probabl go instal pip flag doesnt fix fix have run admin",
        "Question_gpt_summary_original":"The user is encountering an error (WinError 1314) when trying to perform a search with guild on a Windows 10 machine. The error message suggests that the user does not have the required privileges to perform the operation. The user does not have administrator permissions on the machine and is unable to install it via pip with the --User flag. The user is seeking a solution to fix the issue without having to run as admin.",
        "Question_gpt_summary":"user encount error winerror try perform search window machin error messag suggest user requir privileg perform oper user administr permiss machin unabl instal pip user flag user seek solut fix issu have run admin",
        "Answer_original_content":"hello unfortun requir symbol link permiss run admin youll need permiss that pain realiz wish easier reach admin setup",
        "Answer_preprocessed_content":"hello unfortun requir symbol link permiss run admin youll need permiss that pain realiz wish easier reach admin setup",
        "Answer_gpt_summary_original":"possible solutions: \n- obtain symbolic link permissions to run guild as an administrator.\n- contact the system administrator to set up the necessary permissions.",
        "Answer_gpt_summary":"possibl solut obtain symbol link permiss run administr contact administr set necessari permiss"
    },
    {
        "Question_id":69000752.0,
        "Question_title":"Amazon Sagemaker: write your own inference",
        "Question_body":"<p>I was evaluating what is needed to write your own Estimator in Sagemaker. I was following this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\/container\" rel=\"nofollow noreferrer\">here<\/a> and it's well explained and quite simple.<\/p>\n<p>My question is regarding the inference <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py\" rel=\"nofollow noreferrer\">here<\/a>. I see an example in which we can feed the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/e648e9a6f596263c7683635d1a55f1729b08277d\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py#L60\" rel=\"nofollow noreferrer\">invocations endpoint<\/a> a CSV. What if I want to just post a string or even individual parameters? What's the best practise for that? I see there is a condition like:<\/p>\n<pre><code>if flask.request.content_type == &quot;text\/csv&quot;:\n<\/code><\/pre>\n<p>Should we add more like those to support different formats or should we create a new endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630420918507,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":168.0,
        "Answer_body":"<p>You need to add support for more content types.<\/p>\n<p>Since you would like to pass a string or a parameter, I suggest you add support for &quot;application\/json&quot; MIME media type (<a href=\"https:\/\/stackoverflow.com\/questions\/477816\/what-is-the-correct-json-content-type\">What is the correct JSON content type?<\/a>). Then your users will call the API with a Json that you can parse and extract parameters from in the backend.<\/p>\n<p>For example, if you have two parameters <code>age<\/code> and <code>gender<\/code> you want to pass to your model. You can put them in the following Json datastructure:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n &quot;age&quot;: ...,\n &quot;gender&quot;: ...\n}\n<\/code><\/pre>\n<p>Then add support for loading the Json and extracting the parameters in the backend as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if flask.request.content_type == &quot;application\/json&quot;:\n    data = flask.request.data.decode(&quot;utf-8&quot;)\n    data = json.loads(data)\n    parameter1 = data['age']\n    parameter2 = data['gender']\n    ...\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1631097716312,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69000752",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1631096060112,
        "Question_original_content":"write infer evalu need write estim follow exampl explain simpl question infer exampl feed invoc endpoint csv want post string individu paramet best practis condit like flask request content type text csv add like support differ format creat new endpoint",
        "Question_preprocessed_content":"write infer evalu need write estim follow exampl explain simpl question infer exampl feed invoc endpoint csv want post string individu paramet best practis condit like add like support differ format creat new endpoint",
        "Question_gpt_summary_original":"The user is evaluating how to write their own Estimator in Amazon Sagemaker and is following a well-explained example. However, they have a question regarding the inference and how to post a string or individual parameters instead of a CSV. They are unsure whether to add more conditions to support different formats or create a new endpoint.",
        "Question_gpt_summary":"user evalu write estim follow explain exampl question infer post string individu paramet instead csv unsur add condit support differ format creat new endpoint",
        "Answer_original_content":"need add support content type like pass string paramet suggest add support applic json mime media type correct json content type user api json pars extract paramet backend exampl paramet ag gender want pass model follow json datastructur ag gender add support load json extract paramet backend follow flask request content type applic json data flask request data decod utf data json load data paramet data ag paramet data gender",
        "Answer_preprocessed_content":"need add support content type like pass string paramet suggest add support mime media type user api json pars extract paramet backend exampl paramet want pass model follow json datastructur add support load json extract paramet backend follow",
        "Answer_gpt_summary_original":"the solution to the challenge of writing a custom estimator to post a string or individual parameters to the invocations endpoint is to add support for more content types. the suggestion is to add support for \"application\/json\" mime media type, which allows users to call the api with a json that can be parsed and parameters extracted from in the backend. the example given is to put the parameters in a json data structure and load it in the backend to extract the parameters.",
        "Answer_gpt_summary":"solut challeng write custom estim post string individu paramet invoc endpoint add support content type suggest add support applic json mime media type allow user api json pars paramet extract backend exampl given paramet json data structur load backend extract paramet"
    },
    {
        "Question_id":69338516.0,
        "Question_title":"how to train and deploy YOLOv5 on aws sagemaker",
        "Question_body":"<p>I want to train YOLOv5 on aws sagemaker also deploy the model on sagemaker itself,need to know about entrypoint python script as well. how can I build a pipeline for this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1632686467053,
        "Question_favorite_count":null,
        "Question_last_edit_time":1632754422356,
        "Question_score":0.0,
        "Question_view_count":2510.0,
        "Answer_body":"<p>This official AWS <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/speed-up-yolov4-inference-to-twice-as-fast-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">blog post<\/a> has information on how to deploy YOLOv4. I wonder if you can use it as a guide and change the model to v5.<\/p>\n<p>If not, there is a 3rd party implementation of YOLOv5 <a href=\"https:\/\/github.com\/HKT-SSA\/yolov5-on-sagemaker\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69338516",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1632853607047,
        "Question_original_content":"train deploi yolov want train yolov deploi model need know entrypoint python script build pipelin",
        "Question_preprocessed_content":"train deploi yolov want train yolov deploi model need know entrypoint python script build pipelin",
        "Question_gpt_summary_original":"The user is facing challenges in training and deploying YOLOv5 on AWS Sagemaker. They are specifically seeking information on the entrypoint python script and how to build a pipeline for this process.",
        "Question_gpt_summary":"user face challeng train deploi yolov specif seek inform entrypoint python script build pipelin process",
        "Answer_original_content":"offici aw blog post inform deploi yolov wonder us guid chang model parti implement yolov",
        "Answer_preprocessed_content":"offici aw blog post inform deploi yolov wonder us guid chang model parti implement yolov",
        "Answer_gpt_summary_original":"possible solutions for training and deploying yolov5 on a platform include using the official aws blog post on deploying yolov4 as a guide and modifying it for yolov5, or using a third-party implementation of yolov5.",
        "Answer_gpt_summary":"possibl solut train deploi yolov platform includ offici aw blog post deploi yolov guid modifi yolov parti implement yolov"
    },
    {
        "Question_id":null,
        "Question_title":"Azure for students showing no usage despite using it",
        "Question_body":"Hello,\n\nI've created an azure for students account wiht $100 free credit and started using Azure Notebooks to train some ML models. I've created a GPU instance which costs $1.20\/hr. I've been using it for at least 1.5h now and what's weird is that no usage is being shown on my dashboard, and on the sponsorship page it's showing that it is not active and that I haven't used any of my credit:\n\n\n\n\nOn the other hand when I go to my subscriptions it says it's active:\n\nIs something wrong or does it take a while to see usage statistics\/credit spending?\n\nThanks in advance.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1662893566663,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Hi,\n\nUsually it is every 4 hours the data\/cost is updated so check after sometime, you can check and download the data by using and following the steps over here - download-azure-daily-usage\n\n\n\n\n\n==\nPlease \"Accept the answer\" if the information helped you. This will help us and others in the community as well.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1002201\/azure-for-students-showing-no-usage-despite-using.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-11T13:58:24.207Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nUsually it is every 4 hours the data\/cost is updated so check after sometime, you can check and download the data by using and following the steps over here - download-azure-daily-usage\n\n\n\n\n\n==\nPlease \"Accept the answer\" if the information helped you. This will help us and others in the community as well.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-09-12T17:18:02.6Z",
                "Answer_score":0,
                "Answer_body":"Hi Ak,\n\nI will suggest you raise a support ticket for this issue how-to-create-azure-support-request\n\n\n\n\n\n==\nPlease \"Accept the answer\" if the information helped you. This will help us and others in the community as well.",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1662904704207,
        "Question_original_content":"azur student show usag despit hello creat azur student account wiht free credit start azur notebook train model creat gpu instanc cost weird usag shown dashboard sponsorship page show activ haven credit hand subscript sai activ wrong usag statist credit spend thank advanc",
        "Question_preprocessed_content":"azur student show usag despit hello creat azur student account wiht free credit start azur notebook train model creat gpu instanc cost weird usag shown dashboard sponsorship page show activ haven credit hand subscript sai activ wrong usag spend thank advanc",
        "Question_gpt_summary_original":"The user has encountered a challenge with their Azure for students account, where they have been using Azure Notebooks to train ML models on a GPU instance that costs $1.20\/hr, but no usage is being shown on their dashboard and the sponsorship page shows that it is not active and they haven't used any credit. However, the subscriptions page shows that it is active, and the user is unsure if something is wrong or if it takes time to see usage statistics\/credit spending.",
        "Question_gpt_summary":"user encount challeng azur student account azur notebook train model gpu instanc cost usag shown dashboard sponsorship page show activ haven credit subscript page show activ user unsur wrong take time usag statist credit spend",
        "Answer_original_content":"usual hour data cost updat check check download data follow step download azur daili usag accept answer inform help help commun",
        "Answer_preprocessed_content":"usual hour updat check check download data follow step accept answer inform help help commun",
        "Answer_gpt_summary_original":"the possible solution to the challenge with the azure for students account is to wait for the data\/cost to update every 4 hours and check again later. the user can also download the data and check it by following the steps provided in the answer. the user is also encouraged to accept the answer if it helped them, which will benefit the community.",
        "Answer_gpt_summary":"possibl solut challeng azur student account wait data cost updat hour check later user download data check follow step provid answer user encourag accept answer help benefit commun"
    },
    {
        "Question_id":69269073.0,
        "Question_title":"How to assign two or more time series identifier columns in Vertex AI Tabular Forecasting",
        "Question_body":"<p>I was wondering if it is possible to have more than one time series identifier column in the model? Let's assume I'd like to create a forecast at a product and store level (which the documentation suggests should be possible).<\/p>\n<p>If I select product as the series identifier, the only options I have left for store is either a covariate or an attribute and neither is applicable in this scenario.<\/p>\n<p>Would concatenating product and store and using the individual product and store code values for that concatenated ID as attributes be a solution? It doesn't feel right, but I can't see any other option - am I missing something?<\/p>\n<p>Note: I understand that this feature of Vertex AI is currently in preview and that because of that the options may be limited.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1632228068913,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":269.0,
        "Answer_body":"<p>There isn't an alternate way to assign 2 or more <strong>Time Series Identifiers<\/strong> in the <strong>Forecasting Model<\/strong> on <strong>Vertex AI<\/strong>. The &quot;<strong>Forecasting model<\/strong>&quot; is in the &quot;<strong>Preview<\/strong>&quot; <a href=\"https:\/\/cloud.google.com\/products#product-launch-stages\" rel=\"nofollow noreferrer\">Product launch stage<\/a>, as you are aware, with all consequences of that fact the options are limited. Please refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/bp-tabular#data_preparation_best_practices\" rel=\"nofollow noreferrer\">doc<\/a> for more information about the best practices for data preparation to train the forecasting model.<\/p>\n<p>As a workaround, the two columns can be concatenated and assigned a Time Series Identifier on that concatenated column, as you have mentioned in the question. This way, the concatenated column carries more contextual information into the training of the model.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1632482492492,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69269073",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1632376096768,
        "Question_original_content":"assign time seri identifi column tabular forecast wonder possibl time seri identifi column model let assum like creat forecast product store level document suggest possibl select product seri identifi option left store covari attribut applic scenario concaten product store individu product store code valu concaten attribut solut feel right option miss note understand featur current preview option limit",
        "Question_preprocessed_content":"assign time seri identifi column tabular forecast wonder possibl time seri identifi column model let assum like creat forecast product store level select product seri identifi option left store covari attribut applic scenario concaten product store individu product store code valu concaten attribut solut feel right option miss note understand featur current preview option limit",
        "Question_gpt_summary_original":"The user is facing a challenge in assigning two or more time series identifier columns in Vertex AI Tabular Forecasting. They want to create a forecast at a product and store level, but the only options available for store are either a covariate or an attribute, which are not applicable in this scenario. The user is considering concatenating product and store and using the individual product and store code values for that concatenated ID as attributes, but they are unsure if this is the right solution. The user acknowledges that this feature is currently in preview and the options may be limited.",
        "Question_gpt_summary":"user face challeng assign time seri identifi column tabular forecast want creat forecast product store level option avail store covari attribut applic scenario user consid concaten product store individu product store code valu concaten attribut unsur right solut user acknowledg featur current preview option limit",
        "Answer_original_content":"isn altern wai assign time seri identifi forecast model forecast model preview product launch stage awar consequ fact option limit refer doc inform best practic data prepar train forecast model workaround column concaten assign time seri identifi concaten column mention question wai concaten column carri contextu inform train model",
        "Answer_preprocessed_content":"isn altern wai assign time seri identifi forecast model forecast model preview product launch stage awar consequ fact option limit refer doc inform best practic data prepar train forecast model workaround column concaten assign time seri identifi concaten column mention question wai concaten column carri contextu inform train model",
        "Answer_gpt_summary_original":"the answer suggests that there is no alternate way to assign two or more time series identifier columns in the forecasting model. however, a possible solution is to concatenate the two columns and assign a time series identifier on the concatenated column. this workaround can provide more contextual information to the model during training. the answer also recommends referring to a document for best practices in data preparation for training the forecasting model.",
        "Answer_gpt_summary":"answer suggest altern wai assign time seri identifi column forecast model possibl solut concaten column assign time seri identifi concaten column workaround provid contextu inform model train answer recommend refer document best practic data prepar train forecast model"
    },
    {
        "Question_id":null,
        "Question_title":"Debugging and profiling guild",
        "Question_body":"<p>I sometimes see <code>guild<\/code> taking a long time to start a run compared to just running the command that I get from <code>--print-cmd<\/code>. I realise this is because <code>guild<\/code> has to resolve dependencies etc., but I would like to understand if there is an easy way to debug and especially profile what steps \/ operations that is expensive in the <code>guild<\/code> command.<\/p>\n<p>I am aware of the <code>guild --debug<\/code> flag, but in my particular case it doesn\u2019t provide much info about what is taking a long time.<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1607965561395,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":512.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/debugging-and-profiling-guild\/500",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-15T19:47:26.311Z",
                "Answer_body":"<p>Run your command with <code>PROFILE=1<\/code> env var like this:<\/p>\n<pre><code class=\"lang-command\">PROFILE=1 guild run ...\n<\/code><\/pre>\n<p>You\u2019ll get a couple of Python profile stats written.<\/p>\n<p>A nice way to view these files is with <a href=\"https:\/\/jiffyclub.github.io\/snakeviz\/\">SnakeViz<\/a>. Guild prints the instructions for running <code>snakeviz<\/code> when you run with the profile flag.<\/p>\n<p>If you need help interpreting anything just attach both stat files and I\u2019ll take a look.<\/p>\n<p>The shorter you can make your code the better. Otherwise the startup cost\/time will be overshadowed by the actual run time.<\/p>",
                "Answer_score":3.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-15T22:33:34.706Z",
                "Answer_body":"<p>That\u2019s pretty sweet!<\/p>\n<p>I\u2019ve profiled the run that takes a long time. You can find the results <a href=\"https:\/\/drive.google.com\/file\/d\/1-OBwznTVA_KtbJGZo8yUgi8RLIEUPaMo\/view?usp=sharing\" rel=\"noopener nofollow ugc\">here<\/a>.<\/p>\n<p>As you can see it takes a looong time. This happened after I did a big refactoring of my <code>guild<\/code> file into multiple <code>guild<\/code> files using inheritance.<\/p>\n<p>Is the above happening because it is looking for source code in the root dir?<\/p>",
                "Answer_score":7.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-15T22:55:26.025Z",
                "Answer_body":"<p>It looks like you have a directory with a lot of files - over 1M. Guild is example those files to see if they\u2019re candidates for source code copy. By default Guild only looks at I think around 100 files unless you\u2019ve configured the <code>sourcecode<\/code> attr for the operation.<\/p>\n<p>You can see what\u2019s going on by running:<\/p>\n<pre><code class=\"lang-command\">guild run &lt;op&gt; --test-sourcecode\n<\/code><\/pre>\n<p>This should take all that time but you\u2019ll see where the files are.<\/p>\n<p>You can remove a directory from consideration (Guild won\u2019t scan it) this way:<\/p>\n<pre><code class=\"lang-yaml\">op:\n  sourcecode:\n    - exclude:\n        dir: &lt;dir containing lots of files&gt;\n<\/code><\/pre>",
                "Answer_score":2.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-15T23:30:50.935Z",
                "Answer_body":"<p>So I have the <code>sourcecode<\/code> attribute specified, but I guess not it in the right way.<\/p>\n<p>My current folder structure looks like this:<\/p>\n<pre><code>training\/\nscripts\/\nguild\/\n   flags\/\n           classification.yml\n           common.yml\n           segmentation.yml\n    base_model.yml\n    classification_model.yml\n    segmentation_model.yml\n    utils.yml      \nguild.yml\n<\/code><\/pre>\n<p>My main <code>guild.yml<\/code> looks like this:<\/p>\n<pre><code>- include: guild\/segmentation_model.yml\n- include: guild\/classification_model.yml\n<\/code><\/pre>\n<p>These two <code>guild<\/code> files in turn looks like this:<\/p>\n<pre><code>guild\/segmentation_model.yml\n-----------------------------\n- include:\n    - base_model.yml\n    - utils.yml\n    - flags\/segmentation.yml\n    - flags\/common.yml\n\n- model: segmentation_model\n  sourcecode:\n    - scripts\n    - training\n    - guild.yml\n  extends:\n    - base_model\n    - utils\n  operations:\n    convert_to_onnx:\n      flags:\n        $include: onnx_flags_segmentation\n    train:\n      flags:\n        batch_size: 1\n        $include:\n          - segmentation_flags\n          - train_flags\n          - common_flags\n    test:\n      flags:\n        $include:\n          - common_flags\n          - test_flags\n<\/code><\/pre>\n<pre><code>guild\/classification_model.yml\n-------------------------------\n- include:\n    - base_model.yml\n    - utils.yml\n    - flags\/classification.yml\n    - flags\/common.yml\n\n- model: classification_model\n  sourcecode:\n    - scripts\n    - training\n    - guild.yml\n  extends:\n    - base_model\n    - utils\n  operations:\n    train:\n      flags:\n        batch_size: 0\n        $include:\n          - classification_flags\n          - train_flags\n          - common_flags\n    convert_to_onnx:\n      flags:\n        $include: onnx_flags_classification\n<\/code><\/pre>\n<p>The <code>base_model.yml<\/code> looks like this:<\/p>\n<pre><code>base_model.yml\n--------------------------------\n- config: base_model\n  sourcecode:\n    select:\n      - scripts\n      - training\n      - guild.yml\n  operations:\n    train:\n      main: scripts\/training\/train_model --input_database ...\n      requires: prepared_data\n    test:\n      main: scripts\/training\/test_model --input_database ...\n      requires:\n        - operation: train\n        - prepared_data\n  resources:\n    prepared_data:\n      sources: ...\n<\/code><\/pre>\n<p>The command<\/p>\n<pre><code>guild run classification_model:train\n<\/code><\/pre>\n<p>Is what takes a long time. The interesting thing is that the <code>sourcecode<\/code> directory in the run directory only contains the sourcecode that I have specified.<\/p>\n<pre><code>$ ls ~\/...\/.guild\/runs\/5d239a67d97d4bd4952e2b1cc2b10083\/.guild\/sourcecode\/\nguild.yml  scripts  training\n<\/code><\/pre>",
                "Answer_score":7.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-15T23:34:01.459Z",
                "Answer_body":"<p>It\u2019s the scanning\/testing of a large number of files that\u2019s taking time.<\/p>\n<p>What does this command reveal?<\/p>\n<pre><code class=\"lang-command\">guild run classification_model:train --test-sourcecode\n<\/code><\/pre>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-15T23:35:24.171Z",
                "Answer_body":"<p>It scans through the entire directory:<\/p>\n<pre><code>training\/\ndata\/\n3rd_party_lib\/\nscripts\/\nguild\/\nguild.yml\n<\/code><\/pre>\n<p>So also <code>data<\/code> and <code>3rd_part_lib<\/code> which are the heavy folders.<\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-15T23:37:28.584Z",
                "Answer_body":"<p>Refer the example I provided above. You need to explicitly exclude any directories containing large numbers of files - unless you want those scanned for consideration as source code files. This is what\u2019s taking time. The code snippet above will address that.<\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-16T00:39:39.430Z",
                "Answer_body":"<p>I see - thank you! It works now <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":46.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"debug profil take long time start run compar run command print cmd realis resolv depend like understand easi wai debug especi profil step oper expens command awar debug flag particular case doesnt provid info take long time",
        "Question_preprocessed_content":"debug profil take long time start run compar run command realis resolv depend like understand easi wai debug especi profil step oper expens command awar flag particular case doesnt provid info take long time",
        "Question_gpt_summary_original":"The user is facing challenges with the long start time of the guild command due to dependency resolution. They are looking for an easy way to debug and profile the expensive steps\/operations in the guild command, but the guild --debug flag does not provide much information in their particular case.",
        "Question_gpt_summary":"user face challeng long start time command depend resolut look easi wai debug profil expens step oper command debug flag provid inform particular case",
        "Answer_original_content":"run command profil env var like profil run youll coupl python profil stat written nice wai view file snakeviz print instruct run snakeviz run profil flag need help interpret attach stat file ill look shorter code better startup cost time overshadow actual run time that pretti sweet iv profil run take long time result take lng time happen big refactor file multipl file inherit happen look sourc code root dir look like directori lot file exampl file theyr candid sourc code copi default look think file youv configur sourcecod attr oper what go run run test sourcecod time youll file remov directori consider wont scan wai sourcecod exclud dir sourcecod attribut specifi guess right wai current folder structur look like train script flag classif yml common yml segment yml base model yml classif model yml segment model yml util yml yml main yml look like includ segment model yml includ classif model yml file turn look like segment model yml includ base model yml util yml flag segment yml flag common yml model segment model sourcecod script train yml extend base model util oper convert onnx flag includ onnx flag segment train flag batch size includ segment flag train flag common flag test flag includ common flag test flag classif model yml includ base model yml util yml flag classif yml flag common yml model classif model sourcecod script train yml extend base model util oper train flag batch size includ classif flag train flag common flag convert onnx flag includ onnx flag classif base model yml look like base model yml config base model sourcecod select script train yml oper train main script train train model input databas requir prepar data test main script train test model input databas requir oper train prepar data resourc prepar data sourc command run classif model train take long time interest thing sourcecod directori run directori contain sourcecod specifi run daddbdebccb sourcecod yml script train scan test larg number file that take time command reveal run classif model train test sourcecod scan entir directori train data parti lib script yml data lib heavi folder refer exampl provid need explicitli exclud directori contain larg number file want scan consider sourc code file what take time code snippet address thank work",
        "Answer_preprocessed_content":"run command env var like youll coupl python profil stat written nice wai view file snakeviz print instruct run run profil flag need help interpret attach stat file ill look shorter code better startup overshadow actual run time that pretti sweet iv profil run take long time result take lng time happen big refactor file multipl file inherit happen look sourc code root dir look like directori lot file exampl file theyr candid sourc code copi default look think file youv configur attr oper what go run time youll file remov directori consider wai attribut specifi guess right wai current folder structur look like main look like file turn look like look like command take long time interest thing directori run directori contain sourcecod specifi larg number file that take time command reveal scan entir directori heavi folder refer exampl provid need explicitli exclud directori contain larg number file want scan consider sourc code file what take time code snippet address thank work",
        "Answer_gpt_summary_original":"the solution to debug and profile expensive steps and operations when running a guild command is to run the command with the profile=1 environment variable. this will generate python profile stats that can be viewed with snakeviz. it is recommended to make the code as short as possible to reduce startup time. if the command takes a long time, it may be due to guild scanning a large number of files. to address this, directories containing large numbers of files can be excluded using the sourcecode attribute. the command \"guild run <op> --test-sourcecode\" can be used to see where the files are and exclude them.",
        "Answer_gpt_summary":"solut debug profil expens step oper run command run command profil environ variabl gener python profil stat view snakeviz recommend code short possibl reduc startup time command take long time scan larg number file address directori contain larg number file exclud sourcecod attribut command run test sourcecod file exclud"
    },
    {
        "Question_id":null,
        "Question_title":"Voice\/language options during conversion of long text files to speech",
        "Question_body":"The voice\/language options during conversion of long text files to speech. Can anyone help with the doc\/sample for the same.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1627934100000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":375.0,
        "Answer_body":"Howdy Ram56.\u00a0 Could you perhaps elaborate on what you are looking for?\u00a0 We'll be delighted to try and assist.\u00a0\u00a0\n\nHere is the home page for the GCP Text To Speech materials with links to docs:\n\nhttps:\/\/cloud.google.com\/text-to-speech\n\nI fully realize that is a fluffy response ... so if you can add a little more detail to the voice\/language query in your question, we'll get back to you ASAP.\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Voice-language-options-during-conversion-of-long-text-files-to\/td-p\/165947\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-03T15:53:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Howdy Ram56.\u00a0 Could you perhaps elaborate on what you are looking for?\u00a0 We'll be delighted to try and assist.\u00a0\u00a0\n\nHere is the home page for the GCP Text To Speech materials with links to docs:\n\nhttps:\/\/cloud.google.com\/text-to-speech\n\nI fully realize that is a fluffy response ... so if you can add a little more detail to the voice\/language query in your question, we'll get back to you ASAP.\n\nView solution in original post"
            },
            {
                "Answer_creation_time":"2021-08-03T15:53:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Howdy Ram56.\u00a0 Could you perhaps elaborate on what you are looking for?\u00a0 We'll be delighted to try and assist.\u00a0\u00a0\n\nHere is the home page for the GCP Text To Speech materials with links to docs:\n\nhttps:\/\/cloud.google.com\/text-to-speech\n\nI fully realize that is a fluffy response ... so if you can add a little more detail to the voice\/language query in your question, we'll get back to you ASAP."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1628005980000,
        "Question_original_content":"voic languag option convers long text file speech voic languag option convers long text file speech help doc sampl",
        "Question_preprocessed_content":"option convers long text file speech option convers long text file speech help",
        "Question_gpt_summary_original":"The user is seeking assistance with voice and language options for converting long text files to speech and is requesting a sample or documentation to help with this task.",
        "Question_gpt_summary":"user seek assist voic languag option convert long text file speech request sampl document help task",
        "Answer_original_content":"howdi ram elabor look delight try assist home page gcp text speech materi link doc http cloud googl com text speech fulli realiz fluffi respons add littl voic languag queri question asap view solut origin post",
        "Answer_preprocessed_content":"howdi ram elabor look delight try assist home page gcp text speech materi link doc fulli realiz fluffi respons add littl queri question asap view solut origin post",
        "Answer_gpt_summary_original":"there are no specific solutions provided in the answer. however, the answer suggests visiting the home page for gcp text-to-speech materials, which may have information on voice and language options. the user is also encouraged to provide more detail on their query to receive a more helpful response.",
        "Answer_gpt_summary":"specif solut provid answer answer suggest visit home page gcp text speech materi inform voic languag option user encourag provid queri receiv help respons"
    },
    {
        "Question_id":null,
        "Question_title":"Connection Error 403",
        "Question_body":"Hi, I keep on getting this error while trying to access the Twitter API using the TwitterClient. The Client builds successfully but then it shows this error when I run: dotnet run",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1663611187617,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1014310\/connction-error-403.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-19T20:27:36.457Z",
                "Answer_score":1,
                "Answer_body":"This is not an Azure error, right?\n\nThis is the developer forum https:\/\/developer.microsoft.com\/en-us\/community\/\nAnd this is the .NET forum https:\/\/techcommunity.microsoft.com\/t5\/net\/ct-p\/dotnet",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":36.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"connect error get error try access twitter api twittercli client build successfulli show error run dotnet run",
        "Question_preprocessed_content":"connect error get error try access twitter api twittercli client build successfulli show error run dotnet run",
        "Question_gpt_summary_original":"The user is encountering a connection error 403 while trying to access the Twitter API using the TwitterClient. The client builds successfully but shows the error when the user runs \"dotnet run\".",
        "Question_gpt_summary":"user encount connect error try access twitter api twittercli client build successfulli show error user run dotnet run",
        "Answer_original_content":"azur error right develop forum http develop microsoft com commun net forum http techcommun microsoft com net dotnet",
        "Answer_preprocessed_content":"azur error right develop forum net forum",
        "Answer_gpt_summary_original":"the answer does not provide any solutions to the connection error 403 while accessing the twitter api using the twitterclient. instead, it redirects the user to the developer forum and .net forum of microsoft.",
        "Answer_gpt_summary":"answer provid solut connect error access twitter api twittercli instead redirect user develop forum net forum microsoft"
    },
    {
        "Question_id":50611864.0,
        "Question_title":"Using the AWS SageMaker create-training-job command: type Error",
        "Question_body":"<p>I'm trying to use the AWS sagemaker cli to run the create-training-job command. Here is my command:<\/p>\n\n<pre><code>aws sagemaker create-training-job \\\n--training-job-name $(DEPLOYMENT_NAME)-$(BUILD_ID) \\\n--hyper-parameters file:\/\/sagemaker\/hyperparameters.json \\\n--algorithm-specification TrainingImage=$(IMAGE_NAME),\\\nTrainingInputMode=\"File\" \\\n--role-arn $(ROLE) \\\n--input-data-config ChannelName=training,DataSource={S3DataSource={S3DataType=S3Prefix,S3Uri=$(S3_INPUT),S3DataDistributionType=FullyReplicated}},ContentType=string,CompressionType=None,RecordWrapperType=None \\\n--output-data-config S3OutputPath=$(S3_OUTPUT) \\\n--resource-config file:\/\/sagemaker\/train-resource-config.json \\\n--stopping-condition file:\/\/sagemaker\/stopping-conditions.json \n<\/code><\/pre>\n\n<p>and here is the error:<\/p>\n\n<pre><code>Parameter validation failed:\nInvalid type for parameter InputDataConfig[0].DataSource.S3DataSource, value: S3DataType=S3Prefix, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nInvalid type for parameter InputDataConfig[1].DataSource.S3DataSource, value: S3Uri=s3:\/\/hs-machine-learning-processed-production\/inbound-autotag\/data, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nInvalid type for parameter InputDataConfig[2].DataSource.S3DataSource, value: S3DataDistributionType=FullyReplicated, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nmake: *** [train] Error 255\n<\/code><\/pre>\n\n<p>The error is happening with the <code>--input-data-config<\/code> flag. I'm trying to use the Shorthand Syntax so I can inject some variables (the capitalized words). Haalp!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1527707311910,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":1130.0,
        "Answer_body":"<p>So, your input config is not correctly formatted. \nCheckout the sample json here:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html<\/a><\/p>\n\n<pre><code># look at the format of input-data-config, it is a dictionary\n  \"InputDataConfig\": [ \n      { \n         \"ChannelName\": \"string\",\n         \"CompressionType\": \"string\",\n         \"ContentType\": \"string\",\n         \"DataSource\": { \n            \"FileSystemDataSource\": { \n               \"DirectoryPath\": \"string\",\n               \"FileSystemAccessMode\": \"string\",\n               \"FileSystemId\": \"string\",\n               \"FileSystemType\": \"string\"\n            },\n            \"S3DataSource\": { \n               \"AttributeNames\": [ \"string\" ],\n               \"S3DataDistributionType\": \"string\",\n               \"S3DataType\": \"string\",\n               \"S3Uri\": \"string\"\n            }\n         },\n         \"InputMode\": \"string\",\n         \"RecordWrapperType\": \"string\",\n         \"ShuffleConfig\": { \n            \"Seed\": number\n         }\n      }\n   ]\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50611864",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1567514274983,
        "Question_original_content":"creat train job command type error try us cli run creat train job command command creat train job train job deploy build hyper paramet file hyperparamet json algorithm specif trainingimag imag traininginputmod file role arn role input data config channelnam train datasourc sdatasourc sdatatyp sprefix suri input sdatadistributiontyp fullyrepl contenttyp string compressiontyp recordwrappertyp output data config soutputpath output resourc config file train resourc config json stop condit file stop condit json error paramet valid fail invalid type paramet inputdataconfig datasourc sdatasourc valu sdatatyp sprefix type valid type invalid type paramet inputdataconfig datasourc sdatasourc valu suri machin learn process product inbound autotag data type valid type invalid type paramet inputdataconfig datasourc sdatasourc valu sdatadistributiontyp fullyrepl type valid type train error error happen input data config flag try us shorthand syntax inject variabl capit word haalp",
        "Question_preprocessed_content":"command type error try us cli run command command error error happen flag try us shorthand syntax inject variabl haalp",
        "Question_gpt_summary_original":"The user is encountering an error while using the AWS SageMaker create-training-job command. The error is related to the <code>--input-data-config<\/code> flag and is caused by invalid types for the parameters <code>DataSource.S3DataSource<\/code>, <code>S3Uri<\/code>, and <code>S3DataDistributionType<\/code>. The user is trying to use the Shorthand Syntax to inject variables into the command.",
        "Question_gpt_summary":"user encount error creat train job command error relat input data config flag caus invalid type paramet datasourc sdatasourc suri sdatadistributiontyp user try us shorthand syntax inject variabl command",
        "Answer_original_content":"input config correctli format checkout sampl json http doc aw amazon com latest api createtrainingjob html look format input data config dictionari inputdataconfig channelnam string compressiontyp string contenttyp string datasourc filesystemdatasourc directorypath string filesystemaccessmod string filesystemid string filesystemtyp string sdatasourc attributenam string sdatadistributiontyp string sdatatyp string suri string inputmod string recordwrappertyp string shuffleconfig seed number",
        "Answer_preprocessed_content":"input config correctli format checkout sampl json",
        "Answer_gpt_summary_original":"the solution to the type error encountered when using the create-training-job command with the --input-data-config flag is to ensure that the input config is correctly formatted. the user can refer to the sample json provided in the aws documentation and ensure that the input-data-config is a dictionary with the correct format.",
        "Answer_gpt_summary":"solut type error encount creat train job command input data config flag ensur input config correctli format user refer sampl json provid aw document ensur input data config dictionari correct format"
    },
    {
        "Question_id":null,
        "Question_title":"How to manage files on remote repository",
        "Question_body":"<p>Hi there,<\/p>\n<p>First of all thanks for the great tool that really help manage ML practice!<br>\nRight now I\u2019m setup repo in my local pc share folder and tried dvc for a while, my main usage now is to datafile versioning<\/p>\n<p>My problem is how to handle the growth of files in the repository. It is one of showstopper for me to promote to team or larger scale usage \u2026<\/p>\n<p>If there is any mechanism to support keeping track latest 5 versions of file \u201cX\u201d or any statistic could tell how many files we have, how many version we save in repo? Checked tracked data files on repo they are compressed with hashed filename. It is hard to convert to back filename for analytics.<\/p>\n<p>Any tool or better way to handle that? thanks in advance<\/p>\n<p>Donald<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1588035461752,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":659.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-manage-files-on-remote-repository\/372",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-04-28T01:46:07.905Z",
                "Answer_body":"<p>Hi Donald,<\/p>\n<p>I\u2019m just confused whether your question refers to the files in your local cache or in a remote storage (backup of the cache) but no problem. The short answer is that the only tool built-in DVC to manage cache\/storage is the <code>dvc gc<\/code> command, AFAIK: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/gc\">https:\/\/dvc.org\/doc\/command-reference\/gc<\/a><\/p>\n<p>It usually garbage collects the local project cache, but applies to the DVC remote instead when using the <code>--cloud<\/code> option. The other 2 options I think could help you are <code>--all-branches<\/code> and <code>--all-tags flags<\/code>, which tell DVC to keep all files associated to any Git branch or tag, respectively.<\/p>\n<blockquote>\n<p>If there is any mechanism to support keeping track latest 5 versions of file \u201cX\u201d?<\/p>\n<\/blockquote>\n<p>I don\u2019t think so. Would you mind sending a request so that <code>dvc gc<\/code> supports a new option to specify a number of recent versions to keep for all or certain data files? You can do so in <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/new\/choose\">https:\/\/github.com\/iterative\/dvc\/issues\/new\/choose<\/a><\/p>\n<blockquote>\n<p>statistic could tell how many files we have, how many version we save in repo?<\/p>\n<\/blockquote>\n<p>Same here, DVC doesn\u2019t have a tool to report this right now although like you discovered, it could be discovered by exploring the commit history of each DVC-file \u2014 that shouldn\u2019t be too difficult! But feel free to open another feature request for that, or to continue this conversation by asking in <a href=\"http:\/\/dvc.org\/chat\">dvc.org\/chat<\/a>, we\u2019re pretty responsive over there.<\/p>\n<p>Thanks<\/p>",
                "Answer_score":23.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-04-29T04:07:30.124Z",
                "Answer_body":"<p>Thanks Jorgeorpinal reply and advise.<\/p>\n<p>I\u2019m referring remote storage\u2026e.g. hadoop, google-cloud etc\u2026 yes and gc seem only help on locally cache\u2026without those tool. I\u2019m a bit hestiate to provide an storage estimation on remote server for costing\u2026<\/p>\n<p>I will send request to suggest location, many thanks <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":7.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-04-29T04:23:03.897Z",
                "Answer_body":"<p>Sounds good. But please note that <code>dvc gc<\/code> works on remote storage using the <code>--cloud<\/code> option, like I mentioned. But it doesn\u2019t have the specific capabilities you need directly.<\/p>\n<p>Please let us know if you have further questions here or in the chat, and looking forward to see your feature request on GH!<\/p>",
                "Answer_score":17.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"manag file remot repositori thank great tool help manag practic right setup repo local share folder tri main usag datafil version problem handl growth file repositori showstopp promot team larger scale usag mechan support keep track latest version file statist tell file version save repo check track data file repo compress hash filenam hard convert filenam analyt tool better wai handl thank advanc donald",
        "Question_preprocessed_content":"manag file remot repositori thank great tool help manag practic right setup repo local share folder tri main usag datafil version problem handl growth file repositori showstopp promot team larger scale usag mechan support keep track latest version file statist tell file version save repo check track data file repo compress hash filenam hard convert filenam analyt tool better wai handl thank advanc donald",
        "Question_gpt_summary_original":"The user is facing challenges in managing the growth of files in their remote repository, which is hindering their ability to promote it to a larger team or scale usage. They are seeking a mechanism to keep track of the latest versions of specific files or obtain statistics on the number of files and versions saved in the repository. The hashed filenames of the tracked data files are making it difficult to convert them back to their original filenames for analytics. The user is requesting any tools or better ways to handle these challenges.",
        "Question_gpt_summary":"user face challeng manag growth file remot repositori hinder abil promot larger team scale usag seek mechan track latest version specif file obtain statist number file version save repositori hash filenam track data file make difficult convert origin filenam analyt user request tool better wai handl challeng",
        "Answer_original_content":"donald confus question refer file local cach remot storag backup cach problem short answer tool built manag cach storag command afaik http org doc command refer usual garbag collect local project cach appli remot instead cloud option option think help branch tag flag tell file associ git branch tag respect mechan support keep track latest version file dont think mind send request support new option specifi number recent version certain data file http github com iter issu new choos statist tell file version save repo doesnt tool report right like discov discov explor commit histori file shouldnt difficult feel free open featur request continu convers ask org chat pretti respons thank thank jorgeorpin repli advis refer remot storage hadoop googl cloud ye help local cachewithout tool bit hestiat provid storag estim remot server cost send request suggest locat thank sound good note work remot storag cloud option like mention doesnt specif capabl need directli let know question chat look forward featur request",
        "Answer_preprocessed_content":"donald confus question refer file local cach remot storag problem short answer tool manag command afaik usual garbag collect local project cach appli remot instead option option think help tell file associ git branch tag respect mechan support keep track latest version file dont think mind send request support new option specifi number recent version certain data file statist tell file version save repo doesnt tool report right like discov discov explor commit histori file shouldnt difficult feel free open featur request continu convers ask pretti respons thank thank jorgeorpin repli advis refer remot hadoop ye help local cachewithout tool bit hestiat provid storag estim remot server cost send request suggest locat thank sound good note work remot storag option like mention doesnt specif capabl need directli let know question chat look forward featur request",
        "Answer_gpt_summary_original":"possible solutions mentioned in the answer are: \n- use the gc command to manage cache\/storage, which can be applied to remote storage using the --cloud option.\n- use the --all-branches and --all-tags flags to keep all files associated with any git branch or tag, respectively.\n- send a request to suggest a new option for gc to specify a number of recent versions to keep for all or certain data files.\n- explore the commit history of each file to determine the number of files and versions saved in the repository.\n- use the chat to ask further questions or request additional features.",
        "Answer_gpt_summary":"possibl solut mention answer us command manag cach storag appli remot storag cloud option us branch tag flag file associ git branch tag respect send request suggest new option specifi number recent version certain data file explor commit histori file determin number file version save repositori us chat ask question request addit featur"
    },
    {
        "Question_id":null,
        "Question_title":"Best practices of project management for multiple users on Polyaxon v1",
        "Question_body":"Hello all, I\u2019m curious about the best practices of project management for multiple users on Polyaxon v1. Polyaxon v0 allows multiple users to have the same name project, but a project seems to be global under an organization on Polyaxon v1. The ideas I came up with are the following. Do we have other approaches?\n\nAdd username as a prefix or suffix to a project name\nUse username as a tag in a project",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619642544000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":6.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1306",
        "Tool":"Polyaxon",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-29T18:21:00Z",
                "Answer_score":1,
                "Answer_body":"Let's make this Discussion a home for sharing and discussing patterns users are following to:\n\nOrganize and manage their projects\nSettings for CI\/CD\/CT\nEnable the rest of the community to share their thoughts on this important part of developing and scaling their workflows with Polyaxon\nEnable auditing and access to protected data\nGo from experimentation to deployment\nEnable reproducibility\nEnable collaboration\n\nTo set a template of how to share a pattern and to keep the Discussion organized:\n\nLet's make each comment describe a pattern\nLet's use threads to comment and ask questions about a specific pattern\nLet's follow, loosely, the template below\n\nNote: Some of the patterns might be converted to tutorials or added as guides in the scheduling strategies or deployment strategies in the documentation.\n\nTemplate\n\n## Overview\n\n> A description of the process or the pattern\n\n## Local project structure\n\n> Folders and files structure\n\n## Writing\/Authoring polayxonfiles\n\n> YAML vs Python\n> Using presets?\n> Using `pathRef`, `UrlRef`, `HubRef`, or a built-in component\n> Component vs Operation\n> Forking public hub components?\n\n## Development\n\n> How do you submit files to the API\/Scheduler during devlopement?\n> Do you use git or upload?\n> Do you require rebuilding images?\n> Do you generally delete test files and test runs?\n> How do you manage tags?\n> Does Polyaxon need some extra tags management on the project\/org level?\n> Does Polyaxon need some special tags: like `prod`\/`stage`\/`testing` similar to the model and the component versions?\n> Does Polyaxon need some special dropdown to allow users to easily switch the view to prod\/test, and the space should be preserved throughout the browsing experience?\n\n## Testing\n\n> How do you test your files and the logic you are running?\n> Do you declare a test data as a second connection and do you manually need to deploy a polyaxonfile with that subset\/test data?\n> Do you run the logic with NO_OP or OFFLINE?\n\n## Deployment\n\n> How do you handle polyaxonfiles discovery?\n> Do you write descriptions and do you deploy with new descriptions for each run? \n> How do you handle building images?\n> Do you submit polyaxonfiles with external systems?\n\n## Presets\n\n> Do you write override files?\n> Do you save them globally or on each repo?\n> Do you use presets to switch between test and prod?\n\n## Agents or Multi-deployment clusters\n\n> How do you manage multi-environments?\n> How do you manage data access?\n> How do you manage access to production connections?\n> Do you mount specific configmaps\/variables for production?\n\n## Limitations\n\n> Are their any limitations with this pattern?\n> Do you feel like Polyaxon can expose some abstractions to simplify this pattern?"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0,
        "Question_closed_time":null,
        "Question_original_content":"best practic project manag multipl user hello curiou best practic project manag multipl user allow multipl user project project global organ idea came follow approach add usernam prefix suffix project us usernam tag project",
        "Question_preprocessed_content":"best practic project manag multipl user hello curiou best practic project manag multipl user allow multipl user project project global organ idea came follow approach add usernam prefix suffix project us usernam tag project",
        "Question_gpt_summary_original":"The user is seeking advice on the best practices for project management on Polyaxon v1 when dealing with multiple users. They have noticed that projects on v1 seem to be global under an organization, unlike v0 where multiple users can have the same project name. The user has suggested adding a username as a prefix or suffix to a project name or using the username as a tag in a project, but they are open to other approaches.",
        "Question_gpt_summary":"user seek advic best practic project manag deal multipl user notic project global organ unlik multipl user project user suggest ad usernam prefix suffix project usernam tag project open approach",
        "Answer_original_content":"let discuss home share discuss pattern user follow organ manag project set enabl rest commun share thought import develop scale workflow enabl audit access protect data experiment deploy enabl reproduc enabl collabor set templat share pattern discuss organ let comment pattern let us thread comment ask question specif pattern let follow loos templat note pattern convert tutori ad guid schedul strategi deploy strategi document templat overview descript process pattern local project structur folder file structur write author polayxonfil yaml python preset pathref urlref hubref built compon compon oper fork public hub compon develop submit file api schedul devlop us git upload requir rebuild imag gener delet test file test run manag tag need extra tag manag project org level need special tag like prod stage test similar model compon version need special dropdown allow user easili switch view prod test space preserv brows experi test test file logic run declar test data second connect manual need deploi file subset test data run logic offlin deploy handl file discoveri write descript deploi new descript run handl build imag submit file extern system preset write overrid file save global repo us preset switch test prod agent multi deploy cluster manag multi environ manag data access manag access product connect mount specif configmap variabl product limit limit pattern feel like expos abstract simplifi pattern",
        "Answer_preprocessed_content":"let discuss home share discuss pattern user follow organ manag project set enabl rest commun share thought import develop scale workflow enabl audit access protect data experiment deploy enabl reproduc enabl collabor set templat share pattern discuss organ let comment pattern let us thread comment ask question specif pattern let follow loos templat note pattern convert tutori ad guid schedul strategi deploy strategi document templat overview descript process pattern local project structur folder file structur polayxonfil yaml python preset compon compon oper fork public hub compon develop submit file devlop us git upload requir rebuild imag gener delet test file test run manag tag need extra tag manag level need special tag like similar model compon version need special dropdown allow user easili switch view space preserv brows experi test test file logic run declar test data second connect manual need deploi file data run logic offlin deploy handl file discoveri write descript deploi new descript run handl build imag submit file extern system preset write overrid file save global repo us preset switch test prod agent cluster manag manag data access manag access product connect mount specif product limit limit pattern feel like expos abstract simplifi pattern",
        "Answer_gpt_summary_original":"There are no solutions provided in this discussion. The discussion is focused on sharing and discussing patterns for organizing and managing projects, enabling collaboration, and ensuring reproducibility. The participants are encouraged to share their thoughts on various aspects of project management, such as local project structure, writing\/authoring Polyaxon files, development, testing, deployment, presets, agents or multi-deployment clusters, and limitations. The discussion provides a template for sharing patterns and encourages the use of threads to comment and ask questions about specific patterns.",
        "Answer_gpt_summary":"solut provid discuss discuss focus share discuss pattern organ manag project enabl collabor ensur reproduc particip encourag share thought aspect project manag local project structur write author file develop test deploy preset agent multi deploy cluster limit discuss provid templat share pattern encourag us thread comment ask question specif pattern"
    },
    {
        "Question_id":null,
        "Question_title":"Multiple params.yaml",
        "Question_body":"<p>Is it possible to have more than one params.yaml? If so, how should they be organised and named?<\/p>\n<p>The context is I am considering of breaking up my dvc.yaml to multiple pipelines and would like to do the same for the params as well. Is the answer to save a params.yaml at the same place where the pipeline for dvc.yaml will live? something like pipelines\/pip1\/dvc.yaml,params.yaml and pipelines\/pip2\/dvc.yaml,params.yaml<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1618304562199,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1043.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-params-yaml\/720",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-14T08:09:46.380Z",
                "Answer_body":"<p>You can have multiple parameters files but only one file named <code>params.yaml<\/code> per directory.<\/p>\n<p>Any valid YAML, JSON, TOML, or Python file can be a params file. It\u2019s up to you how to organize or name them. A good way to think about them is as any other config file in your project (so probably git-tracked).<\/p>\n<p>Please see <a href=\"https:\/\/dvc.org\/doc\/command-reference\/params\">https:\/\/dvc.org\/doc\/command-reference\/params<\/a> and <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#parameter-dependencies\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#parameter-dependencies<\/a> for more details.<\/p>\n<aside class=\"quote no-group\" data-username=\"nsorros\" data-post=\"1\" data-topic=\"720\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/nsorros\/40\/104_2.png\" class=\"avatar\"> nsorros:<\/div>\n<blockquote>\n<p>Is the answer to save a params.yaml at the same place where the pipeline for dvc.yaml will live?<\/p>\n<\/blockquote>\n<\/aside>\n<p>That\u2019s a straightforward approach, yes <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/+1.png?v=9\" title=\":+1:\" class=\"emoji\" alt=\":+1:\"><\/p>",
                "Answer_score":32.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-14T09:11:19.946Z",
                "Answer_body":"<p>Thanks for the quick response. I assume the suggestion then is to break up the pipelines currently in one dvc.yaml into separate pipelines within their own directory and create a params.yaml at the same directory as the dvc.yaml that defines each pipeline.<\/p>\n<p>Let me know if I understood it wrong.<\/p>",
                "Answer_score":17.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-14T12:25:47.228Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/nsorros\">@nsorros<\/a> Yes, this is a viable approach. Remember, that if you are using params file that is not named <code>params.yaml<\/code>, you need to address those params properly. For example if you have file <code>other_params.yaml<\/code> and want to use <code>foo<\/code> param from inside, you need to address it with <code>-p other_params.yaml:foo<\/code> when defining the stage.<\/p>",
                "Answer_score":7.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"multipl param yaml possibl param yaml organis name context consid break yaml multipl pipelin like param answer save param yaml place pipelin yaml live like pipelin pip yaml param yaml pipelin pip yaml param yaml",
        "Question_preprocessed_content":"multipl possibl organis name context consid break yaml multipl pipelin like param answer save place pipelin yaml live like",
        "Question_gpt_summary_original":"The user is facing a challenge of organizing and naming multiple params.yaml files while breaking up their dvc.yaml into multiple pipelines. They are unsure of the best way to save the params.yaml files and are considering saving them in the same location as the corresponding dvc.yaml files.",
        "Question_gpt_summary":"user face challeng organ name multipl param yaml file break yaml multipl pipelin unsur best wai save param yaml file consid save locat correspond yaml file",
        "Answer_original_content":"multipl paramet file file name param yaml directori valid yaml json toml python file param file organ good wai think config file project probabl git track http org doc command refer param http org doc user guid project structur pipelin file paramet depend detail nsorro answer save param yaml place pipelin yaml live that straightforward approach ye thank quick respons assum suggest break pipelin current yaml separ pipelin directori creat param yaml directori yaml defin pipelin let know understood wrong nsorro ye viabl approach rememb param file name param yaml need address param properli exampl file param yaml want us foo param insid need address param yaml foo defin stage",
        "Answer_preprocessed_content":"multipl paramet file file name directori valid yaml json toml python file param file organ good wai think config file project detail nsorro answer save place pipelin yaml live that straightforward approach ye thank quick respons assum suggest break pipelin current yaml separ pipelin directori creat directori yaml defin pipelin let know understood wrong ye viabl approach rememb param file name need address param properli exampl file want us param insid need address defin stage",
        "Answer_gpt_summary_original":"the answer suggests that it is possible to have multiple parameters files, but only one file named params.yaml per directory. the user can organize or name them as they wish, but it is recommended to treat them like any other config file in the project. the suggested approach is to break up the pipelines currently in one .yaml into separate pipelines within their own directory and create a params.yaml at the same directory as the .yaml that defines each pipeline. if the user is using a params file that is not named params.yaml, they need to address those params properly.",
        "Answer_gpt_summary":"answer suggest possibl multipl paramet file file name param yaml directori user organ wish recommend treat like config file project suggest approach break pipelin current yaml separ pipelin directori creat param yaml directori yaml defin pipelin user param file name param yaml need address param properli"
    },
    {
        "Question_id":null,
        "Question_title":"Getting Authenticatin error for python script step in pipeline",
        "Question_body":"I am trying to create a azure pipeline for learning purpose. I followed the steps mentioned in this notebook:\nhttps:\/\/github.com\/MicrosoftLearning\/mslearn-dp100\/blob\/main\/08%20-%20Create%20a%20Pipeline.ipynb\n\n\n\n     #%% connect to a workspace from config\n # use service principal authentication - https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb\n    \n azure_svppwd = os.environ.get(\"AZUREML_PASSWD\") # use os.environ[\"AZUREML_PASSWD\"] = ' from azure service client secret\"\n auth_pwd = ServicePrincipalAuthentication(\n     tenant_id=\"7da05296-d70e-4398-8a3d-f113e0dad997\",\n     service_principal_id=\"57bac230-cbb8-409e-be64-5c4516d40771\",\n     service_principal_password=azure_svppwd)\n ws = Workspace.from_config('config',auth=auth_pwd)    \n print(\"Found workspace {} at location {}\".format(ws.name, ws.location))    \n # get the data from datasets\n    \n in_data = ws.datasets.get('testparquet') # dataset is registered through Azure UI which refers to some parquet files in another container\n    \n #%% create environment\n env = Environment.from_conda_specification(name='azenv',file_path='envspec.yml')\n env.register(ws)  # Save the environment for future use and retreival\n #%% create compute target - compute clusters\n try:\n     comp_cluster = ComputeTarget(ws,'amltryoutcluster')\n     print('amltryoutcluster found existing and will be used')\n except 'ComputeTargetException':\n        \n     comp_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3',min_nodes=0,max_nodes=6)\n     # vm_size in above line indicates the type of computation powe we need\n     comp_cluster = ComputeTarget.create(ws,'amltryoutcluster',comp_config)\n     comp_cluster.wait_for_completion(show_output=True)\n #%% Create pipeline data object and steps\n prep_data = PipelineData(name=\"pipedata\",datastore=ws.get_default_datastore())\n #%% Create pipeline steps\n #%% pipeline configuration\n pipe_config = RunConfiguration()\n pipe_config.target = comp_cluster\n pipe_config.environment= Environment.get(ws,\"azenv\")\n prep_step = PythonScriptStep(name=\"prep\",\n                              script_name=\"prep_script.py\",             \n                             source_directory='.\/',                \n                              arguments=['--ipdata',in_data.as_named_input(\"inparquet\"),\n                                         '--oppath',prep_data],\n                              outputs=[prep_data],\n                              compute_target = comp_cluster,\n                              runconfig =pipe_config,\n                              allow_reuse=True)\n train_step = PythonScriptStep(name='train',\n                               script_name='train_script.py',\n                               source_directory='.\/',\n                               arguments=['--prepdata',prep_data],\n                               inputs=[prep_data],\n                               compute_target=comp_cluster,\n                               runconfig=pipe_config,\n                               allow_reuse=True)\n # get experiment and run pipeline\n # Construct the pipeline\n pipeline_steps = [prep_step, train_step]\n pipeline = Pipeline(workspace=ws, steps=pipeline_steps,default_datastore='parquet_ingestion')\n exp =Experiment(ws,'amltryout')\n pipeline_run = exp.submit(pipeline, regenerate_outputs=True)\n\n\n\nIn the prep_script.py, I have the following :\n\n from azureml.core import Experiment,Workspace,Datastore,Dataset\n import pandas as pd\n import argparse\n from azureml.core import Run\n from sklearn import preprocessing\n import os\n    \n parse = argparse.ArgumentParser()\n parse.add_argument('--ipdata')\n parse.add_argument('--oppath')\n args = parse.parse_args()\n save_folder = args.oppath\n    \n run = Run.get_context()\n    \n #https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/work-with-data-in-aml\/5-using-datasets\n pqdata = run.input_datasets['inparquet'].to_pandas_dataframe()\n # ws = run.experiment.workspace\n # pqdata = Dataset.get_by_id(ws, id='testparquet')\n run.log(\"count\",pqdata.count())\n    \n sel_data = pqdata[['title','country','salary']]\n str_enc = preprocessing.LabelEncoder()\n    \n sel_data['title','country'] = str_enc.fit_transform(sel_data['title','country']) \n    \n    \n os.makedirs(save_folder, exist_ok=True)\n save_path = os.path.join(save_folder,'data_prep.csv')\n sel_data.to_csv(save_path)\n    \n run.complete()\n\n\n\n\nAlso some steps in train_script , but when I submit the pipeline it fails at prep step with the following error :\n\nAuthentication failed for Container Registry: 3495c636784040b2ae0f5c8e7ee4133a.azurecr.io.\n\nAm I missing something here?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1623207954850,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/427884\/getting-authenticatin-error-for-python-script-step.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-09T13:11:50.927Z",
                "Answer_score":1,
                "Answer_body":"@RaghuvarranVH-5191 Thanks for the question. Can you please add more details about the Docker Container Registry that you are trying and share error log snapshot to check.\nTo Initialize Workspace, Please follow the below doc.\nInitialize a workspace object from persisted configuration.\n\n ws = Workspace.from_config()\n print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n\n\n\nFor Azure machine learning pipelines quickstart please follow the below notebook.\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-getting-started.ipynb",
                "Answer_comment_count":5,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"get authenticatin error python script step pipelin try creat azur pipelin learn purpos follow step mention notebook http github com microsoftlearn mslearn blob main creat pipelin ipynb connect workspac config us servic princip authent http github com azur machinelearningnotebook blob master us manag servic authent authent ipynb azur svppwd environ passwd us environ passwd azur servic client secret auth pwd serviceprincipalauthent tenant fedad servic princip bac cbb servic princip password azur svppwd workspac config config auth auth pwd print workspac locat format locat data dataset data dataset testparquet dataset regist azur refer parquet file contain creat environ env environ conda specif azenv file path envspec yml env regist save environ futur us retreiv creat comput target comput cluster try comp cluster computetarget amltryoutclust print amltryoutclust exist computetargetexcept comp config amlcomput provis configur size standard min node max node size line indic type comput pow need comp cluster computetarget creat amltryoutclust comp config comp cluster wait complet output true creat pipelin data object step prep data pipelinedata pipedata datastor default datastor creat pipelin step pipelin configur pipe config runconfigur pipe config target comp cluster pipe config environ environ azenv prep step pythonscriptstep prep script prep script sourc directori argument ipdata data name input inparquet oppath prep data output prep data comput target comp cluster runconfig pipe config allow reus true train step pythonscriptstep train script train script sourc directori argument prepdata prep data input prep data comput target comp cluster runconfig pipe config allow reus true experi run pipelin construct pipelin pipelin step prep step train step pipelin pipelin workspac step pipelin step default datastor parquet ingest exp experi amltryout pipelin run exp submit pipelin regener output true prep script follow core import experi workspac datastor dataset import panda import argpars core import run sklearn import preprocess import pars argpars argumentpars pars add argument ipdata pars add argument oppath arg pars pars arg save folder arg oppath run run context http doc microsoft com learn modul work data aml dataset pqdata run input dataset inparquet panda datafram run experi workspac pqdata dataset testparquet run log count pqdata count sel data pqdata titl countri salari str enc preprocess labelencod sel data titl countri str enc fit transform sel data titl countri makedir save folder exist true save path path join save folder data prep csv sel data csv save path run complet step train script submit pipelin fail prep step follow error authent fail contain registri cbaefceeea azurecr miss",
        "Question_preprocessed_content":"get authenticatin error python script step pipelin try creat azur pipelin learn purpos follow step mention notebook connect workspac config us servic princip authent us azur servic client secret serviceprincipalauthent print data dataset dataset regist azur refer parquet file contain creat environ env save environ futur us retreiv creat comput target comput cluster try computetarget print computetargetexcept line indic type comput pow need creat pipelin data object step creat pipelin step pipelin configur runconfigur pythonscriptstep runconfig pythonscriptstep experi run pipelin construct pipelin pipelin pipelin exp experi follow core import experi workspac datastor dataset import panda import argpars core import run sklearn import preprocess import pars arg run pqdata pqdata testparquet pqdata step submit pipelin fail prep step follow error authent fail contain registri miss",
        "Question_gpt_summary_original":"The user is encountering an authentication error while trying to create an Azure pipeline for learning purposes. The user followed the steps mentioned in a notebook and is using service principal authentication. The pipeline involves creating an environment, compute target, pipeline data object, and steps. The error occurs in the prep_script.py file, where the user is trying to access a dataset and perform some data preprocessing. The error message indicates that authentication failed for the container registry.",
        "Question_gpt_summary":"user encount authent error try creat azur pipelin learn purpos user follow step mention notebook servic princip authent pipelin involv creat environ comput target pipelin data object step error occur prep script file user try access dataset perform data preprocess error messag indic authent fail contain registri",
        "Answer_original_content":"raghuvarranvh thank question add detail docker contain registri try share error log snapshot check initi workspac follow doc initi workspac object persist configur workspac config print resourc group locat subscript sep pipelin quickstart follow notebook http github com azur machinelearningnotebook blob master us machin learn pipelin intro pipelin aml pipelin get start ipynb",
        "Answer_preprocessed_content":"thank question add detail docker contain registri try share error log snapshot check initi workspac follow doc initi workspac object persist configur sep pipelin quickstart follow notebook",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer to the authentication error encountered when attempting to create an azure pipeline. the answer requests for more details about the docker container registry and error log snapshot to check. it also provides a link to initialize a workspace object from persisted configuration and a notebook for pipelines quickstart.",
        "Answer_gpt_summary":"solut provid answer authent error encount attempt creat azur pipelin answer request detail docker contain registri error log snapshot check provid link initi workspac object persist configur notebook pipelin quickstart"
    },
    {
        "Question_id":71308112.0,
        "Question_title":"How to return a float with 'text\/csv' as \"Content-Type\" from SageMaker endpoint that uses custom inference code?",
        "Question_body":"<p>I am trying to return output(or predictions) from a SageMaker endpoint with 'text\/csv' or 'text\/csv; charset=utf-8' as &quot;Content-Type&quot;. I have tried multiple ways, but the sagemaker always returns with 'text\/html; charset=utf-8' as the &quot;Content-Type&quot;, and I would like SageMaker to return 'text\/csv' or 'text\/csv; charset=utf-8'.<\/p>\n<p>Here's the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#process-output\" rel=\"nofollow noreferrer\"><code>output_fn<\/code><\/a> from my inference-code:<\/p>\n<pre><code>** my other code **\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return output_float\n<\/code><\/pre>\n<p>above function returns number with float data-type and I got error(in cloudwatch logs) that  this function should only be returning string, tuple, dict or Respoonse instance.<\/p>\n<p>So, here are all the different ways I have tried to have SageMaker return my number with 'text\/csv' but only receives 'text\/html; charset=utf-8'<\/p>\n<ol>\n<li><code>return json.dumps(output_float)<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float}&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float},&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float}\\n&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float},\\n&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li>by using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\" rel=\"nofollow noreferrer\"><code>sagemaker.serializers.CSVSerializer<\/code><\/a> like this:\n<pre><code>from sagemaker.serializers import CSVSerializer\ncsv_serialiser = CSVSerializer(content_type='text\/csv')\n\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return csv_serialiser.serialize(output_float)\n<\/code><\/pre>\nI got <code>'NoneType' object has no attribute 'startswith'<\/code> error with this.<\/li>\n<li>as a tuple: <code>return (output_float,)<\/code>. I haven't noted down what this did, but it sure didn't return the number with 'text\/csv' as &quot;Content-Type&quot;.<\/li>\n<li>made changes in my model object to return a float on calling <code>.predict_proba<\/code> on my model-object and deployed it from sagemaker studio without using any custom inference code and deployed this from SageMaker studio. but got this error after sending a request to the endpoint: <code>'NoneType' object has no attribute 'startswith'<\/code>, but at my side, when I pass proper inputs to the unpicked model and call .predict_proba, i get float as expected.<\/li>\n<li>by returning <a href=\"https:\/\/tedboy.github.io\/flask\/generated\/generated\/flask.Response.html#flask.Response\" rel=\"nofollow noreferrer\"><code>flask.Response<\/code><\/a> like this:\n<pre><code>from flask import Response\n\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return Response(response=output_float, status=200, headers={'Content-Type':'text\/csv; charset=utf-8'})\n<\/code><\/pre>\nbut, I got some IndexError with this(I didn't notedown the traceback.)<\/li>\n<\/ol>\n<p>Some other info:<\/p>\n<ol>\n<li>Model I am using is completely outside of SageMaker, not from a training job or anything like that.<\/li>\n<li>All of the aforementioned endpoints have been deployed entirely from aws-cli with relevant .json files, except the one in point-8 above.<\/li>\n<\/ol>\n<p>How do I return number with &quot;text\/csv&quot; as content-type from sagemaker? I need my output in &quot;text\/csv&quot; content-type specifically for Model-Quality-Monitor. How do I do this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1646135687137,
        "Question_favorite_count":null,
        "Question_last_edit_time":1646652069630,
        "Question_score":0.0,
        "Question_view_count":461.0,
        "Answer_body":"<p>From the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py#L88\" rel=\"nofollow noreferrer\">scikit_bring_your_own<\/a> example, I suggest testing by setting the Response as follows:<\/p>\n<pre><code>return flask.Response(response= output_float, status=200, mimetype=&quot;text\/csv&quot;)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71308112",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1646685903080,
        "Question_original_content":"return float text csv content type endpoint us custom infer code try return output predict endpoint text csv text csv charset utf content type tri multipl wai return text html charset utf content type like return text csv text csv charset utf output infer code code def output predict content type text csv code return output float function return number float data type got error cloudwatch log function return string tupl dict respoons instanc differ wai tri return number text csv receiv text html charset utf return json dump output float sent text html charset utf return output float sent text html charset utf return output float sent text html charset utf return output float sent text html charset utf return output float sent text html charset utf serial csvserial like serial import csvserial csv serialis csvserial content type text csv def output predict content type text csv code return csv serialis serial output float got nonetyp object attribut startswith error tupl return output float haven note sure return number text csv content type chang model object return float call predict proba model object deploi studio custom infer code deploi studio got error send request endpoint nonetyp object attribut startswith pass proper input unpick model predict proba float expect return flask respons like flask import respons def output predict content type text csv code return respons respons output float statu header content type text csv charset utf got indexerror notedown traceback info model complet outsid train job like aforement endpoint deploi entir aw cli relev json file point return number text csv content type need output text csv content type specif model qualiti monitor",
        "Question_preprocessed_content":"return float endpoint us custom infer code try return output endpoint tri multipl wai return like return function return number float got error function return string tupl dict respoons instanc differ wai tri return number receiv sent sent sent sent sent like got error tupl haven note sure return number chang model object return float call deploi studio custom infer code deploi studio got error send request endpoint pass proper input unpick model float expect return like got indexerror info model complet outsid train job like aforement endpoint deploi entir relev json file return number need output specif",
        "Question_gpt_summary_original":"The user is trying to return output from a SageMaker endpoint with 'text\/csv' or 'text\/csv; charset=utf-8' as \"Content-Type\", but SageMaker always returns with 'text\/html; charset=utf-8' as the \"Content-Type\". The user has tried multiple ways to return the number with 'text\/csv', but none of them worked. The user has also encountered errors such as \"NoneType\" object has no attribute 'startswith' and IndexError. The user needs the output in 'text\/csv' content-type specifically for Model-Quality-Monitor.",
        "Question_gpt_summary":"user try return output endpoint text csv text csv charset utf content type return text html charset utf content type user tri multipl wai return number text csv work user encount error nonetyp object attribut startswith indexerror user need output text csv content type specif model qualiti monitor",
        "Answer_original_content":"scikit bring exampl suggest test set respons follow return flask respons respons output float statu mimetyp text csv",
        "Answer_preprocessed_content":"exampl suggest test set respons follow",
        "Answer_gpt_summary_original":"the solution suggested for returning a float with 'text\/csv' as \"content-type\" from an endpoint that uses custom inference code is to set the response as follows: return flask.response(response= output_float, status=200, mimetype=\"text\/csv\").",
        "Answer_gpt_summary":"solut suggest return float text csv content type endpoint us custom infer code set respons follow return flask respons respons output float statu mimetyp text csv"
    },
    {
        "Question_id":null,
        "Question_title":"Use parameters with include flags",
        "Question_body":"<p>I\u2019m trying to use parameters to set portions of flags, but it seems that the parameters don\u2019t affect flags which have been imported. Here\u2019s a simple case:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">- config: config\n  flags:\n    flag-key: \"{{param-key}}\"\n\n- model: model\n  params:\n    param-key: param-value\n  operations:\n    op:\n      main: path\n      flags:\n        $include: config\n<\/code><\/pre>\n<p>This results in flag:<br>\nflag-key: \u2018{{param-key}}\u2019<br>\nwhereas I\u2019d want it to be  the same result as if I declared the flag directly in the operation config:<br>\nflag-key: param-value<\/p>\n<p>In my specific use-case I\u2019m also getting the parameters from a config which this model extends, in case that affects the issue. Essentially the setup is that we have all parameters in a single config for organization, and there\u2019s many models\/operations which each use one of a few different sets of flags, and all flag sets reference the parameters.  Any recommendations or workarounds would be appreciated, thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1667442665957,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":64.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/use-parameters-with-include-flags\/952",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-10T21:41:25.875Z",
                "Answer_body":"<p>Found a workaround of using yaml anchors instead of $include.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"us paramet includ flag try us paramet set portion flag paramet dont affect flag import here simpl case config config flag flag kei param kei model model param param kei param valu oper main path flag includ config result flag flag kei param kei want result declar flag directli oper config flag kei param valu specif us case get paramet config model extend case affect issu essenti setup paramet singl config organ there model oper us differ set flag flag set refer paramet recommend workaround appreci thank",
        "Question_preprocessed_content":"us paramet includ flag try us paramet set portion flag paramet dont affect flag import here simpl case result flag want result declar flag directli oper config specif get paramet config model extend case affect issu essenti setup paramet singl config organ there us differ set flag flag set refer paramet recommend workaround appreci thank",
        "Question_gpt_summary_original":"The user is facing challenges in using parameters to set portions of flags as the parameters do not affect flags that have been imported. The user wants the flag to have the same value as if it was declared directly in the operation config. The user is also getting parameters from a config that the model extends, which may affect the issue. The user is looking for recommendations or workarounds to solve this problem.",
        "Question_gpt_summary":"user face challeng paramet set portion flag paramet affect flag import user want flag valu declar directli oper config user get paramet config model extend affect issu user look recommend workaround solv problem",
        "Answer_original_content":"workaround yaml anchor instead includ",
        "Answer_preprocessed_content":"workaround yaml anchor instead includ",
        "Answer_gpt_summary_original":"the solution to the user's challenge of using parameters to set portions of flags is to use yaml anchors instead of $include.",
        "Answer_gpt_summary":"solut user challeng paramet set portion flag us yaml anchor instead includ"
    },
    {
        "Question_id":null,
        "Question_title":"MLFlow Get Run information in R API and NGINX issue",
        "Question_body":"Hi all,\n\n\nGreat work on mlflow, we have been using it to run many experiments.\n\n\nI am trying to get a list of runs for a specific experiment. I am hosting mlflow in a container running on an EC2 instance in AWS.\u00a0\n\n\n\nI am using the R API.\n\n\n# set tracking URI\nmlflow_set_tracking_uri(ML_FLOW_TRACKING_URI)\n\n\n# init MLflow client object\nclient <- mlflow:::mlflow_client(ML_FLOW_TRACKING_URI)\n\n\n# extract experimment information\nd_exp <- mlflow:::mlflow_client_get_experiment(client, EXPERIMENT_ID)\n\n\n\nNow when I use this on experiments with few runs (<30) I get back the data in a named list with the data being stores in d_exp$runs\n\n\nHowever, for experiments with many runs I receive a cURL error as follow:\n\n\nError in curl::curl_fetch_memory(url, handle = handle) :\u00a0\n\u00a0 Timeout was reached: Operation timed out after 1003 milliseconds with 0 bytes received\n\n\nHas anyone had a similar\u00a0problem and happened to fix it? Or is this an issue with the API?\n\n\nNGINX Issue\n\n\nAlso another issue I have is sending requests to mlflow when I secure it with SSL certs using nGINX (with authentication) as a reverse proxy.\n\n\nIs there an example implementation on how to get this setup and working? I saw that there is a HostCreds class in the source code, but how do I initialise this?\n\n\nCurrent fix has been to whitelist the IP address, however this is obviously not a long-term solution.\n\n\nCheers,\n--\n\nVivek Katial\nData Scientist\n\nLevel 1, 155 Karangahape Road, Auckland Central,\u00a01010\nvivek....@quantiful.co.nz | \u00a00210435892\nwww.quantiful.co.nz",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1541534149000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":31.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/6nWaATd4uTw",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-11-08T13:13:45",
                "Answer_body":"It looks like the default timeout in the R API is 1 second (per\u00a0this code). This seems relatively aggressive, especially for a remote server. I think you can change this timeout by using\n\n\noptions(\"mlflow.rest.timeout\" = 60)\n\n\nto set it to 60 seconds, for example.\n\n\nRegarding the TLS stuff, let me add Tomas -- I know he was working on improving the R client support for authenticating against remote servers.\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CABGg2YG0C8uPjWmg6G2B2rU3AZeJpM%2B-i5QGc4XfA7y1o_1NuQ%40mail.gmail.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"run inform api nginx issu great work run experi try list run specif experi host contain run instanc aw api set track uri set track uri flow track uri init client object client client flow track uri extract experim inform exp client experi client experi us experi run data name list data store exp run experi run receiv curl error follow error curl curl fetch memori url handl handl timeout reach oper time millisecond byte receiv similarproblem happen fix issu api nginx issu issu send request secur ssl cert nginx authent revers proxi exampl implement setup work saw hostcr class sourc code initialis current fix whitelist address obvious long term solut cheer vivek katial data scientist level karangahap road auckland central vivek quanti quanti",
        "Question_preprocessed_content":"run inform api nginx issu great work run experi try list run specif experi host contain run instanc aw api set track uri init client object client extract experim inform us experi run data name list data store experi run receiv curl error follow error handl handl timeout reach oper time millisecond byte receiv similarproblem happen fix issu api nginx issu issu send request secur ssl cert nginx revers proxi exampl implement setup work saw hostcr class sourc code initialis current fix whitelist address obvious solut cheer vivek katial data scientist level karangahap road auckland central",
        "Question_gpt_summary_original":"The user is encountering two challenges with MLFlow. Firstly, when using the R API to get a list of runs for an experiment with many runs, they receive a cURL error due to a timeout. Secondly, when securing MLFlow with SSL certs using NGINX as a reverse proxy, they are unable to send requests and are seeking an example implementation to resolve the issue. The user has temporarily fixed the issue by whitelisting the IP address, but this is not a long-term solution.",
        "Question_gpt_summary":"user encount challeng firstli api list run experi run receiv curl error timeout secondli secur ssl cert nginx revers proxi unabl send request seek exampl implement resolv issu user temporarili fix issu whitelist address long term solut",
        "Answer_original_content":"look like default timeout api second perthi code rel aggress especi remot server think chang timeout option rest timeout set second exampl tl stuff let add toma know work improv client support authent remot server receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user cabggygcupjwmggbruazejpm iqgcxfayo nuq mail gmail com option visit http group googl com optout",
        "Answer_preprocessed_content":"look like default timeout api second rel aggress especi remot server think chang timeout set second exampl tl stuff let add toma know work improv client support authent remot server receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit",
        "Answer_gpt_summary_original":"Solution 1: Change the timeout value in the R API to a higher value, such as 60 seconds, using the \"options\" function.\n\nSolution 2: Tomas is working on improving the R client support for authenticating against remote servers, which may help with the TLS issue.\n\nThere are no long-term solutions mentioned, but a temporary fix was to whitelist the address.",
        "Answer_gpt_summary":"solut chang timeout valu api higher valu second option function solut toma work improv client support authent remot server help tl issu long term solut mention temporari fix whitelist address"
    },
    {
        "Question_id":73058582.0,
        "Question_title":"Access denied for aws public sagemaker xgboost registry",
        "Question_body":"<p>I am trying to pull a prebuilt xgboost image from the public aws xgboost registry specified here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ecr-us-west-2.html#xgboost-us-west-2.title\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ecr-us-west-2.html#xgboost-us-west-2.title<\/a>, however whenever I run the sagemaker pipeline I get the error:<\/p>\n<pre><code>ClientError: Failed to invoke sagemaker:CreateModelPackage. \nError Details: Access denied for registry ID: 246618743249, repository name: sagemaker-xgboost. \nPlease check if your ECR image exists and has proper pull permissions for SageMaker.\n<\/code><\/pre>\n<p>Here is the attached role boundary I am using to run the pipeline:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Action&quot;: [\n                &quot;codebuild:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;codepipeline:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;events:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;logs:CreateLogGroup&quot;,\n                &quot;logs:CreateLogStream&quot;,\n                &quot;logs:DescribeLogStreams&quot;,\n                &quot;logs:DescribeLogGroups&quot;,\n                &quot;logs:PutLogEvents&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;iam:PassRole&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:iam::xxxxxxxxxxxx:role\/ml-*&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;ecr:*&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:ecr:us-west-2:246618743249:repository\/246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;ecr:GetAuthorizationToken&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;*&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>and below is the attached policies for the role:<\/p>\n<pre><code>{\n    &quot;Statement&quot;: [\n        {\n            &quot;Action&quot;: &quot;ecr:*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Sid&quot;: &quot;&quot;\n        }\n    ],\n    &quot;Version&quot;: &quot;2012-10-17&quot;\n}\n<\/code><\/pre>\n<p>plus the AWSCodePipelineFullAccess, AWSCodeBuildAdminAccess, and AmazonSageMakerFullAccess managed policies.<\/p>\n<p>Why can't I access the image\/why am I getting this error? As you can see I gave my role full permissions for the ecr registry in the boundary, and full permissions for ecr in the attached policy.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1658354562373,
        "Question_favorite_count":null,
        "Question_last_edit_time":1658354892732,
        "Question_score":1.0,
        "Question_view_count":61.0,
        "Answer_body":"<p>I had to change the boundary to be this: <code> arn:aws:ecr:us-west-2:246618743249:repository\/sagemaker-xgboost<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73058582",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1658508649572,
        "Question_original_content":"access deni aw public xgboost registri try pull prebuilt xgboost imag public aw xgboost registri specifi http doc aw amazon com latest ecr west html xgboost west titl run pipelin error clienterror fail invok createmodelpackag error detail access deni registri repositori xgboost check ecr imag exist proper pull permiss attach role boundari run pipelin version statement action codebuild resourc effect allow action codepipelin resourc effect allow action event resourc effect allow action log createloggroup log createlogstream log describelogstream log describeloggroup log putlogev resourc effect allow action iam passrol resourc arn aw iam role effect allow action ecr resourc arn aw ecr west repositori dkr ecr west amazonaw com xgboost effect allow action ecr getauthorizationtoken resourc effect allow attach polici role statement action ecr effect allow resourc sid version plu awscodepipelinefullaccess awscodebuildadminaccess amazonfullaccess manag polici access imag get error gave role permiss ecr registri boundari permiss ecr attach polici",
        "Question_preprocessed_content":"access deni aw public xgboost registri try pull prebuilt xgboost imag public aw xgboost registri specifi run pipelin error attach role boundari run pipelin attach polici role plu awscodepipelinefullaccess awscodebuildadminaccess amazonfullaccess manag polici access get error gave role permiss ecr registri boundari permiss ecr attach polici",
        "Question_gpt_summary_original":"The user is encountering an \"Access denied\" error when trying to pull a prebuilt xgboost image from the public AWS xgboost registry. The error message suggests that the user does not have proper pull permissions for SageMaker. The user has provided the role boundary and policies, which include full permissions for the ECR registry, but the issue persists.",
        "Question_gpt_summary":"user encount access deni error try pull prebuilt xgboost imag public aw xgboost registri error messag suggest user proper pull permiss user provid role boundari polici includ permiss ecr registri issu persist",
        "Answer_original_content":"chang boundari arn aw ecr west repositori xgboost",
        "Answer_preprocessed_content":"chang boundari",
        "Answer_gpt_summary_original":"the solution to the \"access denied\" error when pulling a prebuilt xgboost image from the public aws xgboost registry is to change the boundary to \"arn:aws:ecr:us-west-2:246618743249:repository\/-xgboost\".",
        "Answer_gpt_summary":"solut access deni error pull prebuilt xgboost imag public aw xgboost registri chang boundari arn aw ecr west repositori xgboost"
    },
    {
        "Question_id":null,
        "Question_title":"ModuleNotFoundError: No module named 'azure.ai'",
        "Question_body":"I am getting the following error message\n\nModuleNotFoundError: No module named 'azure.ai'\n\n\n\n\nin Azure machine learning studio...when i try to run sample azureml-in-a-day.ipynb",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1668204745757,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1086028\/modulenotfounderror-no-module-named-39azureai39.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-15T19:33:55.33Z",
                "Answer_score":1,
                "Answer_body":"Hello @Antonymstephen\n\nSorry for your experience and thanks for reaching out to us, I am able to reproduce your issue with my new create compute-cpu.\n\nThere are two workaround working well for myself, please have a try and let me know if that works for you or not -\n\n1.Copy the whole Tutorial file as this guidance - https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/quickstart-run-notebooks#clone-tutorials-folder\n\n\n\n\n2.Easy install the azureml in your compute at the terminal under your root user.\n\n pip install azureml\n\n\n\nJust in case, I encounter pyarrow error after that, the resolution is uninstall the pyarrow 4.0 and install pyarrow 3.0.0 instead as below:\n\n pip uninstall pyarrow\n        \n pip install pyarrow==3.0.0\n\n\n\nI have forwarded this bug to product group and hope to make this process smoother. Please let me know how is things going and I am willing to help more.\n\n\n\n\nRegards,\nYutong\n\nPlease kindly accept the answer if you feel this is helpful. Thank you.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"modulenotfounderror modul name azur get follow error messag modulenotfounderror modul name azur studio try run sampl dai ipynb",
        "Question_preprocessed_content":"modulenotfounderror modul name get follow error messag modulenotfounderror modul name try run sampl",
        "Question_gpt_summary_original":"The user is encountering a \"ModuleNotFoundError\" error message in Azure machine learning studio while trying to run a sample notebook called \"azureml-in-a-day.ipynb\". The error message specifically states that there is no module named \"azure.ai\".",
        "Question_gpt_summary":"user encount modulenotfounderror error messag studio try run sampl notebook call dai ipynb error messag specif state modul name azur",
        "Answer_original_content":"hello antonymstephen sorri experi thank reach abl reproduc issu new creat comput cpu workaround work try let know work copi tutori file guidanc http learn microsoft com azur machin learn quickstart run notebook clone tutori folder easi instal comput termin root user pip instal case encount pyarrow error resolut uninstal pyarrow instal pyarrow instead pip uninstal pyarrow pip instal pyarrow forward bug product group hope process smoother let know thing go will help regard yutong kindli accept answer feel help thank",
        "Answer_preprocessed_content":"hello sorri experi thank reach abl reproduc issu new creat workaround work try let know work copi tutori file guidanc easi instal comput termin root user pip instal case encount pyarrow error resolut uninstal pyarrow instal pyarrow instead pip uninstal pyarrow pip instal forward bug product group hope process smoother let know thing go will help regard yutong kindli accept answer feel help thank",
        "Answer_gpt_summary_original":"the answer suggests two possible solutions for the modulenotfounderror issue encountered when running a sample notebook in azure studio. the first solution is to copy the entire tutorial file, and the second solution is to install the missing module using pip. additionally, if there is a pyarrow error, the solution is to uninstall pyarrow 4.0 and install pyarrow 3.0.0 instead. the answer also mentions that the bug has been forwarded to the product group for resolution.",
        "Answer_gpt_summary":"answer suggest possibl solut modulenotfounderror issu encount run sampl notebook azur studio solut copi entir tutori file second solut instal miss modul pip addition pyarrow error solut uninstal pyarrow instal pyarrow instead answer mention bug forward product group resolut"
    },
    {
        "Question_id":null,
        "Question_title":"ERROR Abnormal program exit",
        "Question_body":"<p>I am using wandb version 0.13.5 on python 3.6.9 (Linux kernel version: 4.14.281-212.502.amzn2.x86_64). I have a problem running <code>wandb.init()<\/code> with the following error.<\/p>\n<blockquote>\n<p>Traceback (most recent call last):<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_init.py\u201d, line 1075, in init<br>\nwi.setup(kwargs)<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_init.py\u201d, line 165, in setup<br>\nself._wl = wandb_setup.setup()<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py\u201d, line 312, in setup<br>\nret = _setup(settings=settings)<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py\u201d, line 307, in _setup<br>\nwl = _WandbSetup(settings=settings)<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py\u201d, line 293, in <strong>init<\/strong><br>\n_WandbSetup._instance = _WandbSetup__WandbSetup(settings=settings, pid=pid)<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py\u201d, line 106, in <strong>init<\/strong><br>\nself._setup()<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py\u201d, line 234, in _setup<br>\nself._setup_manager()<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py\u201d, line 266, in _setup_manager<br>\n_use_grpc=use_grpc, settings=self._settings<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_manager.py\u201d, line 108, in <strong>init<\/strong><br>\nself._service.start()<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/service\/service.py\u201d, line 112, in start<br>\nself._launch_server()<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/service\/service.py\u201d, line 108, in _launch_server<br>\nassert ports_found<br>\nAssertionError<br>\nwandb: ERROR Abnormal program exit<br>\nproc exited with 1<\/p>\n<\/blockquote>\n<blockquote>\n<hr>\n<p>AssertionError                            Traceback (most recent call last)<br>\n\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)<br>\n1074         wi = _WandbInit()<br>\n \u2192 1075         wi.setup(kwargs)<br>\n1076         except_exit = wi.settings._except_exit<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_init.py in setup(self, kwargs)<br>\n164<br>\n \u2192 165         self._wl = wandb_setup.setup()<br>\n166         # Make sure we have a logger setup (might be an early logger)<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py in setup(settings)<br>\n311 def setup(settings=None) \u2192 Optional[\u201c_WandbSetup\u201d]:<br>\n \u2192 312     ret = _setup(settings=settings)<br>\n313     return ret<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py in _setup(settings, _reset)<br>\n306         return<br>\n \u2192 307     wl = _WandbSetup(settings=settings)<br>\n308     return wl<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py in <strong>init<\/strong>(self, settings)<br>\n292             return<br>\n \u2192 293         _WandbSetup._instance = _WandbSetup__WandbSetup(settings=settings, pid=pid)<br>\n294<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py in <strong>init<\/strong>(self, pid, settings, environ)<br>\n105         self._check()<br>\n \u2192 106         self._setup()<br>\n107<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py in _setup(self)<br>\n233     def _setup(self):<br>\n \u2192 234         self._setup_manager()<br>\n235<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py in _setup_manager(self)<br>\n265         self._manager = wandb_manager._Manager(<br>\n \u2192 266             _use_grpc=use_grpc, settings=self._settings<br>\n267         )<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_manager.py in <strong>init<\/strong>(self, settings, _use_grpc)<br>\n107         if not token:<br>\n \u2192 108             self._service.start()<br>\n109             host = \u201clocalhost\u201d<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/service\/service.py in start(self)<br>\n111     def start(self) \u2192 None:<br>\n \u2192 112         self._launch_server()<br>\n113<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/service\/service.py in _launch_server(self)<br>\n107             ports_found = self._wait_for_ports(fname, proc=internal_proc)<br>\n \u2192 108             assert ports_found<br>\n109             self._internal_proc = internal_proc<\/p>\n<p>AssertionError:<\/p>\n<p>The above exception was the direct cause of the following exception:<\/p>\n<p>Exception                                 Traceback (most recent call last)<br>\n in <br>\n1 import wandb<br>\n----&gt; 2 wandb.init()<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)<br>\n1114             if except_exit:<br>\n1115                 os._exit(-1)<br>\n \u2192 1116             raise Exception(\u201cproblem\u201d) from error_seen<br>\n1117     return run<\/p>\n<p>Exception: problem<\/p>\n<\/blockquote>\n<p>I tried downgrading the wandb version to 0.9.7 but the problem still the same. Could you please help me solve this error?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1669091571015,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":582.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/error-abnormal-program-exit\/3448",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-24T18:03:29.592Z",
                "Answer_body":"<p>Hi Raveerat,<\/p>\n<p>Thanks for writing in and sorry that you are experiencing this! Could you try running <code>wandb login --relogin<\/code> and introducing you API key? Also, is this happening in colab and Jupiter notebook too?<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-29T09:05:07.995Z",
                "Answer_body":"<p>Hi Raveerat,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":10.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-21T04:33:16.803Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error abnorm program exit version python linux kernel version amzn problem run init follow error traceback recent file usr local lib python dist packag sdk init line init setup kwarg file usr local lib python dist packag sdk init line setup self setup setup file usr local lib python dist packag sdk setup line setup ret setup set set file usr local lib python dist packag sdk setup line setup setup set set file usr local lib python dist packag sdk setup line init setup instanc setup setup set set pid pid file usr local lib python dist packag sdk setup line init self setup file usr local lib python dist packag sdk setup line setup self setup manag file usr local lib python dist packag sdk setup line setup manag us grpc us grpc set self set file usr local lib python dist packag sdk manag line init self servic start file usr local lib python dist packag sdk servic servic line start self launch server file usr local lib python dist packag sdk servic servic line launch server assert port assertionerror error abnorm program exit proc exit assertionerror traceback recent usr local lib python dist packag sdk init init job type dir config project entiti reinit tag group note magic config exclud kei config includ kei anonym mode allow val chang resum forc tensorboard sync tensorboard monitor gym save code set init setup kwarg exit set exit usr local lib python dist packag sdk init setup self kwarg self setup setup sure logger setup earli logger usr local lib python dist packag sdk setup setup set def setup set option setup ret setup set set return ret usr local lib python dist packag sdk setup setup set reset return setup set set return usr local lib python dist packag sdk setup init self set return setup instanc setup setup set set pid pid usr local lib python dist packag sdk setup init self pid set environ self check self setup usr local lib python dist packag sdk setup setup self def setup self self setup manag usr local lib python dist packag sdk setup setup manag self self manag manag manag us grpc us grpc set self set usr local lib python dist packag sdk manag init self set us grpc token self servic start host localhost usr local lib python dist packag sdk servic servic start self def start self self launch server usr local lib python dist packag sdk servic servic launch server self port self wait port fname proc intern proc assert port self intern proc intern proc assertionerror except direct caus follow except except traceback recent import init usr local lib python dist packag sdk init init job type dir config project entiti reinit tag group note magic config exclud kei config includ kei anonym mode allow val chang resum forc tensorboard sync tensorboard monitor gym save code set exit exit rais except problem error seen return run except problem tri downgrad version problem help solv error",
        "Question_preprocessed_content":"error abnorm program exit version python problem run follow error traceback file line init file line setup file line setup ret file line file line init pid pid file line init file line file line file line init file line start file line assert assertionerror error abnorm program exit proc exit assertionerror traceback dir config project entiti reinit tag group note magic anonym mode resum forc tensorboard set setup sure logger setup setup def setup ret return ret return return init return pid pid init def init token host localhost start def start assert assertionerror except direct caus follow except except traceback import init dir config project entiti reinit tag group note magic anonym mode resum forc tensorboard set rais except return run except problem tri downgrad version problem help solv error",
        "Question_gpt_summary_original":"The user is encountering an error when running wandb.init() and is receiving an AssertionError message. They have tried downgrading the wandb version to 0.9.7 but the problem persists. The user is seeking assistance in resolving the error.",
        "Question_gpt_summary":"user encount error run init receiv assertionerror messag tri downgrad version problem persist user seek assist resolv error",
        "Answer_original_content":"raveerat thank write sorri experienc try run login relogin introduc api kei happen colab jupit notebook best lui raveerat want follow support request heard let know assist issu resolv best lui topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"raveerat thank write sorri experienc try run introduc api kei happen colab jupit notebook best lui raveerat want follow support request heard let know assist issu resolv best lui topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no clear solutions provided in the answer. the responder suggests trying to run \"login --relogin\" and introducing an api key, and asks if the issue is happening in colab and jupiter notebook as well. the responder also follows up with the user to see if their issue has been resolved or if they need further assistance.",
        "Answer_gpt_summary":"clear solut provid answer respond suggest try run login relogin introduc api kei ask issu happen colab jupit notebook respond follow user issu resolv need assist"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML Text Classification Import Text Files",
        "Question_body":"Hi,\n\nI'm trying to create an Azure Machine Learning model to classify text files. I have hundreds of text files that have been organized into a subfolder named its correct label. Similar to how you train Image classification.\n\nHow would I get this data into a data set. I have been trying to use the python sdk since I was able to successfully get the Image classification to work.\n\nThanks,\nKyle",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1659040614790,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@DomsohnKyle-2610 Thanks for the question. Here is the sample to import text files and explore azure ml text classification.\n\nhttps:\/\/github.com\/microsoft\/nlp-recipes\/blob\/master\/examples\/text_classification\/tc_bert_azureml.ipynb",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/946690\/azure-ml-text-classification-import-text-files.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-04T04:07:43.577Z",
                "Answer_score":0,
                "Answer_body":"@DomsohnKyle-2610 Thanks for the question. Here is the sample to import text files and explore azure ml text classification.\n\nhttps:\/\/github.com\/microsoft\/nlp-recipes\/blob\/master\/examples\/text_classification\/tc_bert_azureml.ipynb",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1659586063576,
        "Question_original_content":"text classif import text file try creat model classifi text file hundr text file organ subfold name correct label similar train imag classif data data set try us python sdk abl successfulli imag classif work thank kyle",
        "Question_preprocessed_content":"text classif import text file try creat model classifi text file hundr text file organ subfold name correct label similar train imag classif data data set try us python sdk abl successfulli imag classif work thank kyle",
        "Question_gpt_summary_original":"The user is facing challenges in importing text files into Azure Machine Learning to create a text classification model. They have organized the text files into subfolders based on their correct label and are seeking guidance on how to get this data into a dataset using the Python SDK.",
        "Question_gpt_summary":"user face challeng import text file creat text classif model organ text file subfold base correct label seek guidanc data dataset python sdk",
        "Answer_original_content":"domsohnkyl thank question sampl import text file explor text classif http github com microsoft nlp recip blob master exampl text classif bert ipynb",
        "Answer_preprocessed_content":"thank question sampl import text file explor text classif",
        "Answer_gpt_summary_original":"the answer provides a sample code to import text files and explore text classification using the python sdk.",
        "Answer_gpt_summary":"answer provid sampl code import text file explor text classif python sdk"
    },
    {
        "Question_id":70379395.0,
        "Question_title":"Vertex AI - Viewing Pipeline Output",
        "Question_body":"<p>I have followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/schedule-cloud-scheduler\" rel=\"nofollow noreferrer\">this<\/a> tutorial to create my first scheduled Vertex AI Pipeline to run every minute. The only thing it does is prints <code>&quot;Hello, &lt;any-greet-string&gt;&quot;<\/code> and also returns this same string. I can see that it is running because the last run time updates and the last run result is &quot;Success&quot; every time.<\/p>\n<p>My question is very simple: Where can I see this string printed and the output of my pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1639659363547,
        "Question_favorite_count":null,
        "Question_last_edit_time":1639726542407,
        "Question_score":2.0,
        "Question_view_count":346.0,
        "Answer_body":"<p>The output of the <code>print()<\/code> statements in the pipeline can be found in &quot;Cloud Logging&quot; with the appropriate filters. To check logs for each component in the pipeline, click on the respective component in the console and click &quot;<strong>VIEW LOGS<\/strong>&quot; in the right pane. A new pane with the logs will open in the pipeline page which will allow us to see the output of the component. Refer to the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I ran a sample <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#4\" rel=\"nofollow noreferrer\">pipeline<\/a> from this codelab, <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#0\" rel=\"nofollow noreferrer\">Intro to Vertex Pipelines<\/a> and below is the output for one of the <code>print()<\/code> statements in the pipeline.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<br \/>\n<h4>UPDATE:<\/h4>\n<p>Every component in a pipeline run is deployed as an individual Vertex AI custom job. Corresponding to the sample pipeline consisting 3 components, there are 3 entries in the &quot;CUSTOM JOBS&quot; section as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Therefore, to view the logs on the run level, we would need to query the log entries with the respective <code>job_id<\/code>s of the pipeline components and the <code>job_id<\/code> of the Cloud Scheduler job. The query would look like this.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.labels.job_id=(&quot;JOB_ID_1&quot; OR &quot;JOB_ID_2&quot; [OR &quot;JOB_ID_N&quot;...])\nseverity&gt;=DEFAULT\n<\/code><\/pre>\n<p>If there are no simultaneous pipeline runs, a simpler query like below can be used.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.type=(&quot;cloud_scheduler_job&quot; OR &quot;ml_job&quot;)\nseverity&gt;=DEFAULT\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1639750456896,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70379395",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1639667359983,
        "Question_original_content":"view pipelin output follow tutori creat schedul pipelin run minut thing print hello return string run run time updat run result success time question simpl string print output pipelin",
        "Question_preprocessed_content":"view pipelin output follow tutori creat schedul pipelin run minut thing print return string run run time updat run result success time question simpl string print output pipelin",
        "Question_gpt_summary_original":"The user has successfully created a scheduled Vertex AI Pipeline that prints a greeting string and returns the same string. However, the user is facing a challenge in locating the output of the pipeline and the printed string.",
        "Question_gpt_summary":"user successfulli creat schedul pipelin print greet string return string user face challeng locat output pipelin print string",
        "Answer_original_content":"output print statement pipelin cloud log appropri filter check log compon pipelin click respect compon consol click view log right pane new pane log open pipelin page allow output compon refer screenshot ran sampl pipelin codelab intro vertex pipelin output print statement pipelin updat compon pipelin run deploi individu custom job correspond sampl pipelin consist compon entri custom job section shown view log run level need queri log entri respect job id pipelin compon job cloud schedul job queri look like resourc label job job job job sever default simultan pipelin run simpler queri like resourc type cloud schedul job job sever default",
        "Answer_preprocessed_content":"output statement pipelin cloud log appropri filter check log compon pipelin click respect compon consol click view log right pane new pane log open pipelin page allow output compon refer screenshot ran sampl pipelin codelab intro vertex pipelin output statement pipelin updat compon pipelin run deploi individu custom job correspond sampl pipelin consist compon entri custom job section shown view log run level need queri log entri respect pipelin compon cloud schedul job queri look like simultan pipelin run simpler queri like",
        "Answer_gpt_summary_original":"possible solutions to view the output of a scheduled pipeline that prints and returns a string are: \n1. check the cloud logging with appropriate filters to find the output of the print() statements in the pipeline. \n2. click on the respective component in the console and click \"view logs\" in the right pane to check logs for each component in the pipeline. \n3. query the log entries with the respective job_ids of the pipeline components and the job_id of the cloud scheduler job to view the logs on the run level. \n4. use a simpler query like resource.type=(\"cloud_scheduler_job\" or \"ml_job\") severity>=default if there are no simultaneous pipeline runs.",
        "Answer_gpt_summary":"possibl solut view output schedul pipelin print return string check cloud log appropri filter output print statement pipelin click respect compon consol click view log right pane check log compon pipelin queri log entri respect job id pipelin compon job cloud schedul job view log run level us simpler queri like resourc type cloud schedul job job sever default simultan pipelin run"
    },
    {
        "Question_id":58815367.0,
        "Question_title":"How to solve the error with deploying a model in aws sagemaker?",
        "Question_body":"<p>I have to deploy a custom keras model in AWS Sagemaker. I have a created a notebook instance and I have the following files:<\/p>\n\n<pre><code>AmazonSagemaker-Codeset16\n   -ann\n      -nginx.conf\n      -predictor.py\n      -serve\n      -train.py\n      -wsgi.py\n   -Dockerfile\n<\/code><\/pre>\n\n<p>I now open the AWS terminal and build the docker image and push the image in the ECR repository. Then I open a new jupyter python notebook and try to fit the model and deploy the same. The training is done correctly but while deploying I get the following error:<\/p>\n\n<blockquote>\n  <p>\"Error hosting endpoint sagemaker-example-2019-10-25-06-11-22-366: Failed. >Reason: The primary container for production variant AllTraffic did not pass >the ping health check. Please check CloudWatch logs for this endpoint...\"<\/p>\n<\/blockquote>\n\n<p>When I check the logs, I find the following:<\/p>\n\n<blockquote>\n  <p>2019\/11\/11 11:53:32 [crit] 19#19: *3 connect() to unix:\/tmp\/gunicorn.sock >failed (2: No such file or directory) while connecting to upstream, client: >10.32.0.4, server: , request: \"GET \/ping HTTP\/1.1\", upstream: >\"<a href=\"http:\/\/unix:\/tmp\/gunicorn.sock:\/ping\" rel=\"nofollow noreferrer\">http:\/\/unix:\/tmp\/gunicorn.sock:\/ping<\/a>\", host: \"model.aws.local:8080\"<\/p>\n<\/blockquote>\n\n<p>and <\/p>\n\n<blockquote>\n  <p>Traceback (most recent call last):\n   File \"\/usr\/local\/bin\/serve\", line 8, in \n     sys.exit(main())\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/cli\/serve.py\", line 19, in main\n     server.start(env.ServingEnv().framework_module)\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/_server.py\", line 107, in start\n     module_app,\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 711, in <strong>init<\/strong>\n     errread, errwrite)\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 1343, in _execute_child\n     raise child_exception<\/p>\n<\/blockquote>\n\n<p>I tried to deploy the same model in AWS Sagemaker with these files in my local computer and the model was deployed successfully but inside AWS, I am facing this problem.<\/p>\n\n<p>Here is my serve file code:<\/p>\n\n<pre><code>from __future__ import print_function\nimport multiprocessing\nimport os\nimport signal\nimport subprocess\nimport sys\n\ncpu_count = multiprocessing.cpu_count()\n\nmodel_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)\nmodel_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))\n\n\ndef sigterm_handler(nginx_pid, gunicorn_pid):\n    try:\n        os.kill(nginx_pid, signal.SIGQUIT)\n    except OSError:\n        pass\n    try:\n        os.kill(gunicorn_pid, signal.SIGTERM)\n    except OSError:\n        pass\n\n    sys.exit(0)\n\n\ndef start_server():\n    print('Starting the inference server with {} workers.'.format(model_server_workers))\n\n\n    # link the log streams to stdout\/err so they will be logged to the container logs\n    subprocess.check_call(['ln', '-sf', '\/dev\/stdout', '\/var\/log\/nginx\/access.log'])\n    subprocess.check_call(['ln', '-sf', '\/dev\/stderr', '\/var\/log\/nginx\/error.log'])\n\n    nginx = subprocess.Popen(['nginx', '-c', '\/opt\/ml\/code\/nginx.conf'])\n    gunicorn = subprocess.Popen(['gunicorn',\n                                 '--timeout', str(model_server_timeout),\n                                 '-b', 'unix:\/tmp\/gunicorn.sock',\n                                 '-w', str(model_server_workers),\n                                 'wsgi:app'])\n\n    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\n\n    # If either subprocess exits, so do we.\n    pids = set([nginx.pid, gunicorn.pid])\n    while True:\n        pid, _ = os.wait()\n        if pid in pids:\n            break\n\n    sigterm_handler(nginx.pid, gunicorn.pid)\n    print('Inference server exiting')\n\n\n# The main routine just invokes the start function.\nif __name__ == '__main__':\n    start_server()\n<\/code><\/pre>\n\n<p>I deploy the model using the following:<\/p>\n\n<blockquote>\n  <p>predictor = classifier.deploy(1, 'ml.t2.medium', serializer=csv_serializer)<\/p>\n<\/blockquote>\n\n<p>Kindly let me know the mistake I am doing while deploying.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1573549904240,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1573550204732,
        "Question_score":1.0,
        "Question_view_count":4605.0,
        "Answer_body":"<p>Using Sagemaker script mode can be much simpler than dealing with container and nginx low-level stuff like you're trying to do, have you considered that?<br>\nYou only need to provide the keras script:   <\/p>\n\n<blockquote>\n  <p>With Script Mode, you can use training scripts similar to those you would use outside SageMaker with SageMaker's prebuilt containers for various deep learning frameworks such TensorFlow, PyTorch, and Apache MXNet.<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58815367",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1573630807523,
        "Question_original_content":"solv error deploi model deploi custom kera model creat notebook instanc follow file amazon codeset ann nginx conf predictor serv train wsgi dockerfil open aw termin build docker imag push imag ecr repositori open new jupyt python notebook try fit model deploi train correctli deploi follow error error host endpoint exampl fail reason primari contain product variant alltraff pass ping health check check cloudwatch log endpoint check log follow crit connect unix tmp gunicorn sock fail file directori connect upstream client server request ping http upstream http unix tmp gunicorn sock ping host model aw local traceback recent file usr local bin serv line sy exit main file usr local lib python dist packag contain cli serv line main server start env servingenv framework modul file usr local lib python dist packag contain server line start modul app file usr lib python subprocess line init eead errwrit file usr lib python subprocess line execut child rais child except tri deploi model file local model deploi successfulli insid aw face problem serv file code futur import print function import multiprocess import import signal import subprocess import sy cpu count multiprocess cpu count model server timeout environ model server timeout model server worker int environ model server worker cpu count def sigterm handler nginx pid gunicorn pid try kill nginx pid signal sigquit oserror pass try kill gunicorn pid signal sigterm oserror pass sy exit def start server print start infer server worker format model server worker link log stream stdout err log contain log subprocess check dev stdout var log nginx access log subprocess check dev stderr var log nginx error log nginx subprocess popen nginx opt code nginx conf gunicorn subprocess popen gunicorn timeout str model server timeout unix tmp gunicorn sock str model server worker wsgi app signal signal signal sigterm lambda sigterm handler nginx pid gunicorn pid subprocess exit pid set nginx pid gunicorn pid true pid wait pid pid break sigterm handler nginx pid gunicorn pid print infer server exit main routin invok start function main start server deploi model follow predictor classifi deploi medium serial csv serial kindli let know mistak deploi",
        "Question_preprocessed_content":"solv error deploi model deploi custom kera model creat notebook instanc follow file open aw termin build docker imag push imag ecr repositori open new jupyt python notebook try fit model deploi train correctli deploi follow error error host endpoint fail reason primari contain product variant alltraff pass ping health check check cloudwatch log check log follow connect fail connect upstream client server request upstream host traceback file line file line main file line start file line init eead errwrit file line rais tri deploi model file local model deploi successfulli insid aw face problem serv file code deploi model follow predictor kindli let know mistak deploi",
        "Question_gpt_summary_original":"The user is facing an error while deploying a custom Keras model in AWS Sagemaker. The training is done correctly, but while deploying, the primary container for production variant AllTraffic did not pass the ping health check. The user checked the logs and found that the connection to unix:\/tmp\/gunicorn.sock failed, and the subprocess failed to execute child. The user tried to deploy the same model in AWS Sagemaker with the same files on their local computer, and the model was deployed successfully, but inside AWS, they are facing this problem. The user has provided the serve file code and the deployment code used.",
        "Question_gpt_summary":"user face error deploi custom kera model train correctli deploi primari contain product variant alltraff pass ping health check user check log connect unix tmp gunicorn sock fail subprocess fail execut child user tri deploi model file local model deploi successfulli insid aw face problem user provid serv file code deploy code",
        "Answer_original_content":"script mode simpler deal contain nginx low level stuff like try consid need provid kera script script mode us train script similar us outsid prebuilt contain deep learn framework tensorflow pytorch apach mxnet http github com aw sampl amazon script mode blob master sentiment script mode sentiment analysi ipynb",
        "Answer_preprocessed_content":"script mode simpler deal contain nginx stuff like try consid need provid kera script script mode us train script similar us outsid prebuilt contain deep learn framework tensorflow pytorch apach mxnet",
        "Answer_gpt_summary_original":"the possible solution suggested in the answer is to use script mode instead of dealing with container and nginx low-level stuff. the user can use training scripts similar to those used outside with prebuilt containers for various deep learning frameworks such as tensorflow, pytorch, and apache mxnet. the link to the github repository for an example of using script mode with a custom keras model is also provided.",
        "Answer_gpt_summary":"possibl solut suggest answer us script mode instead deal contain nginx low level stuff user us train script similar outsid prebuilt contain deep learn framework tensorflow pytorch apach mxnet link github repositori exampl script mode custom kera model provid"
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker Ground Truth notification after annotation job considered \"done\"?",
        "Question_body":"Hi all,\n\nIs it possible to generate a notification after an annotation job is done? Couldn't find this info anywhere in the API docs, or by trawling the web. Being able to receive such notifications, be it SNS or something else, would be lot better than having to periodically poll the job's status.\n\nJoni",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1558877970000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":29.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuT3kd2CJRcanme8YzDa73Q\/sage-maker-ground-truth-notification-after-annotation-job-considered-done",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-07-22T01:38:35.000Z",
                "Answer_score":0,
                "Answer_body":"We apologize for the delay in reply. Unfortunately, today we do not have such mechanisms to get notification for the completion of annotation. However, we recognize the importance of this feature and its usability and are working towards it. Watch out this space for updates. Thank you being our valued customer.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"ground truth notif annot job consid possibl gener notif annot job couldn info api doc trawl web abl receiv notif sn lot better have period poll job statu joni",
        "Question_preprocessed_content":"ground truth notif annot job consid possibl gener notif annot job couldn info api doc trawl web abl receiv notif sn lot better have period poll job statu joni",
        "Question_gpt_summary_original":"The user is seeking information on how to generate a notification after an annotation job is completed in SageMaker Ground Truth. They are unable to find this information in the API docs or online and believe that receiving notifications would be more efficient than periodically checking the job's status.",
        "Question_gpt_summary":"user seek inform gener notif annot job complet ground truth unabl inform api doc onlin believ receiv notif effici period check job statu",
        "Answer_original_content":"apolog delai repli unfortun todai mechan notif complet annot recogn import featur usabl work watch space updat thank valu custom",
        "Answer_preprocessed_content":"apolog delai repli unfortun todai mechan notif complet annot recogn import featur usabl work watch space updat thank valu custom",
        "Answer_gpt_summary_original":"there are currently no mechanisms in place to receive notifications for the completion of an annotation job. however, the company recognizes the importance of this feature and is working towards implementing it in the future. no immediate solutions are available at this time.",
        "Answer_gpt_summary":"current mechan place receiv notif complet annot job compani recogn import featur work implement futur immedi solut avail time"
    },
    {
        "Question_id":null,
        "Question_title":"Closing up a Sagemaker user profile - intended behavior?",
        "Question_body":"I had a Sagemaker user I wasn't using, so I tried to delete it and initially came across this tutorial: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-delete-domain.html . As part of the larger process for how to delete a domain, it shows how to delete any user profiles within that domain. The steps are:\n\nChoose the user.\nOn the User Details page, for each non-failed app in the Apps list, choose Delete app.\nOn the Delete app dialog, choose Yes, delete app, type delete in the confirmation field, and then choose Delete.\nWhen the Status for all apps show as Deleted, choose Delete user.\n\nThe problem comes on the final step: I wasn't able to find a \"Delete user\" button. This feels like a bug, because without such a button the only way to stop charges on a Sagemaker user is to use the CLI, which I eventually did. You can only delete the domain if you have deleted all users, meaning it only works using the CLI for that as well. For every other AWS service I've used, there is an easy way to delete everything from the GUI.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640092026826,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":529.0,
        "Answer_body":"Try clicking on the user, then Edit, and then Delete? I don't remember if that is the exact flow, but I do know that you can do it in the GUI. I've done it a few times.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmSJa7T1nRm6PQkiXDxZJqA\/closing-up-a-sagemaker-user-profile-intended-behavior",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-21T13:51:27.751Z",
                "Answer_score":0,
                "Answer_body":"Try clicking on the user, then Edit, and then Delete? I don't remember if that is the exact flow, but I do know that you can do it in the GUI. I've done it a few times.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1640094687751,
        "Question_original_content":"close user profil intend behavior user wasn tri delet initi came tutori http doc aw amazon com latest studio delet domain html larger process delet domain show delet user profil domain step choos user user detail page non fail app app list choos delet app delet app dialog choos ye delet app type delet confirm field choos delet statu app delet choos delet user problem come final step wasn abl delet user button feel like bug button wai stop charg user us cli eventu delet domain delet user mean work cli aw servic easi wai delet gui",
        "Question_preprocessed_content":"close user profil intend behavior user wasn tri delet initi came tutori larger process delet domain show delet user profil domain step choos user user detail page app app list choos delet app delet app dialog choos ye delet app type delet confirm field choos delet statu app delet choos delet user problem come final step wasn abl delet user button feel like bug button wai stop charg user us cli eventu delet domain delet user mean work cli aw servic easi wai delet gui",
        "Question_gpt_summary_original":"The user encountered challenges while trying to delete a Sagemaker user profile. The tutorial provided by AWS showed how to delete any user profiles within the domain, but the final step of deleting the user was missing. This made it difficult to stop charges on the Sagemaker user, and the only way to delete the user was through the CLI. The user found this to be a bug and noted that other AWS services have an easy way to delete everything from the GUI.",
        "Question_gpt_summary":"user encount challeng try delet user profil tutori provid aw show delet user profil domain final step delet user miss difficult stop charg user wai delet user cli user bug note aw servic easi wai delet gui",
        "Answer_original_content":"try click user edit delet rememb exact flow know gui time",
        "Answer_preprocessed_content":"try click user edit delet rememb exact flow know gui time",
        "Answer_gpt_summary_original":"possible solutions to closing a user profile include clicking on the user, then editing, and then deleting in the gui. it is unclear if this is the exact flow, but the answerer has done it a few times.",
        "Answer_gpt_summary":"possibl solut close user profil includ click user edit delet gui unclear exact flow answer time"
    },
    {
        "Question_id":65362133.0,
        "Question_title":"What happens when I add\/remove parameters dynamically during an Optuna study?",
        "Question_body":"<p>Optuna's FAQ has a <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#id10\" rel=\"nofollow noreferrer\">clear answer<\/a> when it comes to dynamically adjusting the range of parameter during a study: it poses no problem since each sampler is defined individually.<\/p>\n<p>But what about adding and\/or removing parameters? Is Optuna able to handle such adjustments?<\/p>\n<p>One thing I noticed when doing this is that in the results dataframe these parameters get <code>nan<\/code> entries for other trials. Would there be any benefit to being able to set these <code>nan<\/code>s to their (default) value that they had when not being sampled? Is the study still sound with all these unknown values?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608315900387,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":227.0,
        "Answer_body":"<p>Question was answered <a href=\"https:\/\/github.com\/optuna\/optuna\/issues\/2141\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<blockquote>\n<p>Thanks for the question. Optuna internally supports two types of sampling: <code>optuna.samplers.BaseSampler.sample_independent<\/code> and <code>optuna.samplers.BaseSampler.sample_relative<\/code>.<\/p>\n<p>The former <code>optuna.samplers.BaseSampler.sample_independent<\/code> is a method that samples independently on each parameter, and is not affected by the addition or removal of parameters. The added parameters are taken into account from the timing when they are added.<\/p>\n<p>The latter <code>optuna.samplers.BaseSampler.sample_relative<\/code> is a method that samples by considering the correlation of parameters and is affected by the addition or removal of parameters. Optuna's default search space for correlation is the product set of the domains of the parameters that exist from the beginning of the hyperparameter tuning to the present. Developers who implement samplers can implement their own search space calculation method <code>optuna.samplers.BaseSampler.infer_relative_search_space<\/code>. This may allow correlations to be considered for hyperparameters that have been added or removed, but this depends on the sampling algorithm, so there is no API for normal users to modify.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65362133",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1609649865303,
        "Question_original_content":"happen add remov paramet dynam studi faq clear answer come dynam adjust rang paramet studi pose problem sampler defin individu ad remov paramet abl handl adjust thing notic result datafram paramet nan entri trial benefit abl set nan default valu sampl studi sound unknown valu",
        "Question_preprocessed_content":"happen paramet dynam studi faq clear answer come dynam adjust rang paramet studi pose problem sampler defin individu ad remov paramet abl handl adjust thing notic result datafram paramet entri trial benefit abl set valu sampl studi sound unknown valu",
        "Question_gpt_summary_original":"The user is facing challenges with adding and removing parameters dynamically during an Optuna study. While adjusting the range of parameters poses no problem, adding and removing parameters results in \"nan\" entries for other trials in the results dataframe. The user is questioning whether setting these \"nan\" values to their default value would be beneficial and if the study is still valid with these unknown values.",
        "Question_gpt_summary":"user face challeng ad remov paramet dynam studi adjust rang paramet pose problem ad remov paramet result nan entri trial result datafram user question set nan valu default valu benefici studi valid unknown valu",
        "Answer_original_content":"question answer thank question intern support type sampl sampler basesampl sampl independ sampler basesampl sampl rel sampler basesampl sampl independ method sampl independ paramet affect addit remov paramet ad paramet taken account time ad sampler basesampl sampl rel method sampl consid correl paramet affect addit remov paramet default search space correl product set domain paramet exist begin hyperparamet tune present develop implement sampler implement search space calcul method sampler basesampl infer rel search space allow correl consid hyperparamet ad remov depend sampl algorithm api normal user modifi",
        "Answer_preprocessed_content":"question answer thank question intern support type sampl method sampl independ paramet affect addit remov paramet ad paramet taken account time ad method sampl consid correl paramet affect addit remov paramet default search space correl product set domain paramet exist begin hyperparamet tune present develop implement sampler implement search space calcul method allow correl consid hyperparamet ad remov depend sampl algorithm api normal user modifi",
        "Answer_gpt_summary_original":"The answer suggests that Optuna internally supports two types of sampling: independent and relative. Independent sampling is not affected by the addition or removal of parameters, while relative sampling considers the correlation of parameters and is affected by the addition or removal of parameters. Optuna's default search space for correlation is the product set of the domains of the parameters that exist from the beginning of the hyperparameter tuning to the present. Developers who implement samplers can implement their own search space calculation method.",
        "Answer_gpt_summary":"answer suggest intern support type sampl independ rel independ sampl affect addit remov paramet rel sampl consid correl paramet affect addit remov paramet default search space correl product set domain paramet exist begin hyperparamet tune present develop implement sampler implement search space calcul method"
    },
    {
        "Question_id":null,
        "Question_title":"Trying to do multiple voice files with speech-to-text",
        "Question_body":"Hello.I'm someone who's trying to make speech-to-text work without being a coder in any way whatsoever. I have let's say hundreds of individual audio files and they go from 30 seconds to a minute and a half. The problem is that uploading them to the bucket makes it so there's hundreds of individual ones. And I need to create a transcriptions individually. what do I do? can I not just transcribe everything in one folder?",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1652292780000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":57.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Trying-to-do-multiple-voice-files-with-speech-to-text\/td-p\/422295\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-13T22:37:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"It requires coding - you load files in a bucket (say in 'input' folder) and run a background job to produce a \"txt\" file for each using speech-to-text API (say in 'output' folder).\u00a0 If you have files formats such as mp4 then use transcoding. This is the step roughly."
            },
            {
                "Answer_creation_time":"2022-05-15T21:36:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Understood. But do you have any idea how I'd code that? Of course I am\u00a0 unfamiliar with how to code... but anyway no problem at all if you cannot help there."
            },
            {
                "Answer_creation_time":"2022-05-15T22:10:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Coding it will not be easy if you are not a programmer. It requires a bunch of technology and tools. Roughly steps would be:\n\n1. Using gsutil tool of the GCP, upload files to a bucket.\n\n2. Write a program to read file from the bucket and invoke\u00a0 Speech-to-Text API. It will require you to acquire an access-token (OAuth2).\u00a0\n\n3. If files are small then you could do 2 without uploading files to the bucket.\u00a0\n\nYou can find the example programs here:\u00a0https:\/\/cloud.google.com\/speech-to-text\/docs\/samples"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"try multipl voic file speech text hello try speech text work coder wai whatsoev let hundr individu audio file second minut half problem upload bucket make hundr individu on need creat transcript individu transcrib folder",
        "Question_preprocessed_content":"try multipl voic file try work coder wai whatsoev let hundr individu audio file second minut half problem upload bucket make hundr individu on need creat transcript individu transcrib folder",
        "Question_gpt_summary_original":"The user has encountered a challenge in transcribing multiple voice files using speech-to-text technology. They have hundreds of individual audio files ranging from 30 seconds to a minute and a half, which they need to transcribe individually after uploading them to the bucket. The user is looking for a solution to transcribe everything in one folder instead of creating transcriptions individually.",
        "Question_gpt_summary":"user encount challeng transcrib multipl voic file speech text technolog hundr individu audio file rang second minut half need transcrib individu upload bucket user look solut transcrib folder instead creat transcript individu",
        "Answer_original_content":"requir code load file bucket input folder run background job produc txt file speech text api output folder file format us transcod step roughli understood idea code cours unfamiliar code problem help code easi programm requir bunch technolog tool roughli step gsutil tool gcp upload file bucket write program read file bucket invok speech text api requir acquir access token oauth file small upload file bucket exampl program http cloud googl com speech text doc sampl",
        "Answer_preprocessed_content":"requir code load file bucket run background job produc txt file api file format us transcod step roughli understood idea code cours unfamiliar problem help code easi programm requir bunch technolog tool roughli step gsutil tool gcp upload file bucket write program read file bucket invok api requir acquir file small upload file bucket exampl program",
        "Answer_gpt_summary_original":"possible solutions to transcribe multiple audio files without coding knowledge are not provided in the answer. however, the answer suggests that the task requires coding and provides a rough outline of the steps involved in coding the solution. the steps include uploading files to a bucket using gsutil tool, writing a program to read files from the bucket and invoke speech-to-text api, and acquiring an access-token (oauth2). the answer also provides a link to example programs that can be used as a reference.",
        "Answer_gpt_summary":"possibl solut transcrib multipl audio file code knowledg provid answer answer suggest task requir code provid rough outlin step involv code solut step includ upload file bucket gsutil tool write program read file bucket invok speech text api acquir access token oauth answer provid link exampl program refer"
    },
    {
        "Question_id":null,
        "Question_title":"Preparing ML object detction dataset for deep learning in PyTorch or similar",
        "Question_body":"The intent of what I'm trying to achieve is:\n\nExport data labelling project as a Dataset\n\n\nConsume the Dataset in a notebook (converting to a Pandas dataframe)\n\n\nPerform a custom train \/ test split that maintains particular file groupings\n\n\nRegister the resulting training and testing dataframes as Datasets\n\n\nUse these Datasets to train and test a custom object detection model\n\n\n\n\nI need help in preparing the data for that final step. I'm familiar with different deep learning libraries, but have never implemented them in the Azure environment before. I've managed to complete 1 to 4. For step 4, I ended up writing the data to csv files and uploading these to the datastore.\n\n # define path for training data file and create new delimited file\n train_path = '.\/data\/train.csv'\n train_dataframe.to_csv(train_path, sep = ';', index = False)\n    \n # repeat for testing\n test_path = '.\/data\/test.csv'\n test_dataframe.to_csv(test_path, sep = ';', index = False)\n    \n # get the datastore to upload prepared data\n datastore = Datastore.get(ws, datastore_name='learningdata')\n    \n # upload the local files from src_dir to the target_path in datastore\n datastore.upload(src_dir='data', target_path='train-test', overwrite=True)\n    \n # create and register training dataset from datastore files\n training_ds = Dataset.Tabular.from_delimited_files(path = [(datastore, 'train-test\/train.csv')], separator=';')\n training_ds = training_ds.register(workspace=ws, name = 'train', description = 'training dataset sampled from labelled data', create_new_version=True)\n    \n # create and register testing dataset from datastore files\n testing_ds = Dataset.Tabular.from_delimited_files(path = [(datastore, 'train-test\/test.csv')], separator=';')\n testing_ds = testing_ds.register(workspace=ws, name = 'test', description = 'testing dataset sampled from labelled data', create_new_version=True)\n\n\n\nThe approach I was intending to use for step 5 was to use to_torchvision() to convert it into a Torchvision dataset. This doesn't work, I receive the following error:\n\n UserErrorException: UserErrorException:\n  Message: Cannot perform torchvision conversion on dataset without labeled columns defined\n  InnerException None\n  ErrorResponse \n {\n     \"error\": {\n         \"code\": \"UserError\",\n         \"message\": \"Cannot perform torchvision conversion on dataset without labeled columns defined\"\n     }\n }\n\n\n\nI suspect that the issue has to do with DataTypes. The original Dataset (exported from the data labelling project) has the DataTypes displayed below. By comparison, all column types in the train and test Datasets are parsed as strings. From my understanding, there's no way to convert to these data types.\n\nimage_url = Stream\n\n\nlabel = List\n\n\nlabel_confidence = List\n\nAny advice on how to prepare this dataset for use in PyTorch or recommendation for an alternative approach would be greatly appreciated.\n\n\n\n\n\n\nUpdate as per comment below:\n\nI'm currently mounting the dataframe rather than downloading it due to data size.\n\n\nI can view images from the originally mounted Dataset, but when loading the newly registered training Dataset I can't access images as '\/tmp\/tmpog809x4v\/[...].jpg' is no longer relevant.\n\n\nI can't perform random split because I'm using clustered sampling.\n\n\nI'm working on creating a class object to define the dataset, but I cannot currently create the PIL Image object as required by PyTorch (https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html#defining-the-dataset)",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1607567444300,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Answer_body":"I modified the methodology and was able to successfully resolve this issue as follows:\n\nExport data labelling project as Dataset\n\n\nConsume the Dataset in the notebook by creating both a PyTorch dataset and a Pandas dataframe\n\n\nUse the Pandas dataframe to determine indices for the train \/ test split based on required sampling\n\n\nUse the indices as an input to torch.utils.data.Subset() to split the PyTorch dataset into train and test",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/192973\/preparing-ml-object-detction-dataset-for-deep-lear.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-15T22:30:13.793Z",
                "Answer_score":0,
                "Answer_body":"I modified the methodology and was able to successfully resolve this issue as follows:\n\nExport data labelling project as Dataset\n\n\nConsume the Dataset in the notebook by creating both a PyTorch dataset and a Pandas dataframe\n\n\nUse the Pandas dataframe to determine indices for the train \/ test split based on required sampling\n\n\nUse the indices as an input to torch.utils.data.Subset() to split the PyTorch dataset into train and test",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2020-12-10T13:06:31.287Z",
                "Answer_score":0,
                "Answer_body":"@JoeDuncan-2610 Thanks for the great question. End-to-end image detection that leverages training\/test datasets created from a Data Labeling project. you are well aware that you can also \u2018solve\u2019 this problem with CustomVision, but I\u2019d like to showcase how a custom vision problem which may not be handle well enough by Custom Vision could be handled easily with Azure ML with full control of the underlying ML algorithms and the power of Data Labeling.\n\nThe best practices to get back to the images referenced by the dataset, i.e. leverage the DataStore \/ StreamInfo from the TabularDataset extracted DataFrame, to prepare the data for a model training.\n\nThis code here that I put together is probably the way to proceed to retrieve the original image assets from a labeled TabularDataset.\n\n # azureml-core of version 1.0.72 or higher is required\n # azureml-contrib-dataset of version 1.0.72 or higher is required\n    \n    \n from azureml.core import Workspace, Dataset, Datastore\n import azureml.contrib.dataset\n import azureml.dataprep.native\n     \n subscription_id = '_set_it_to_yours_'\n resource_group = '_set_it_to_yours_'\n workspace_name = '_set_it_to_yours_'\n     \n workspace = Workspace(subscription_id, resource_group, workspace_name)\n     \n # get dataset and extract as a DataFrame\n ds = Dataset.get_by_name(workspace, name=_set_it_to_yours_')\n df = ds.to_pandas_dataframe()\n     \n # download images\n index = 0\n datastore = None\n while index < len(df):\n     # image_url is a azureml.dataprep.native.StreamInfo object, convert to dict with to_pod()\n     si = df.loc[index].image_url.to_pod()\n     if index == 0:\n         # retrieve datastore based on metadata from first row\n         # assuming all images come from the same store\n         # since they come from a single dataset\n         datastore = Datastore.get(workspace, si['arguments']['datastoreName'])\n     # download image locally\n     datastore.download(target_path='.',prefix=si['resourceIdentifier'],overwrite=True,show_progress=True)\n     index += 1\n     \n # create training, test sets\n [training, test] = ds.random_split(0.8)\n\n\n\nbuild model based on image assets and labels...\nFrom there, build your train_x,y and test_x,y datasets\u2026\n\n\n\n\nWe have checked in a sample notebook about labeled dataset to public github repo. You can find it here:\n\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/datasets-tutorial\/labeled-datasets\/labeled-datasets.ipynb",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1608071413792,
        "Question_original_content":"prepar object detction dataset deep learn pytorch similar intent try achiev export data label project dataset consum dataset notebook convert panda datafram perform custom train test split maintain particular file group regist result train test datafram dataset us dataset train test custom object detect model need help prepar data final step familiar differ deep learn librari implement azur environ manag complet step end write data csv file upload datastor defin path train data file creat new delimit file train path data train csv train datafram csv train path sep index fals repeat test test path data test csv test datafram csv test path sep index fals datastor upload prepar data datastor datastor datastor learningdata upload local file src dir target path datastor datastor upload src dir data target path train test overwrit true creat regist train dataset datastor file train dataset tabular delimit file path datastor train test train csv separ train train regist workspac train descript train dataset sampl label data creat new version true creat regist test dataset datastor file test dataset tabular delimit file path datastor train test test csv separ test test regist workspac test descript test dataset sampl label data creat new version true approach intend us step us torchvis convert torchvis dataset work receiv follow error usererrorexcept usererrorexcept messag perform torchvis convers dataset label column defin innerexcept errorrespons error code usererror messag perform torchvis convers dataset label column defin suspect issu datatyp origin dataset export data label project datatyp displai comparison column type train test dataset pars string understand wai convert data type imag url stream label list label confid list advic prepar dataset us pytorch recommend altern approach greatli appreci updat comment current mount datafram download data size view imag origin mount dataset load newli regist train dataset access imag tmp tmpogxv jpg longer relev perform random split cluster sampl work creat class object defin dataset current creat pil imag object requir pytorch http pytorch org tutori intermedi torchvis tutori html defin dataset",
        "Question_preprocessed_content":"prepar object detction dataset deep learn pytorch similar intent try achiev export data label project dataset consum dataset notebook perform custom train test split maintain particular file group regist result train test datafram dataset us dataset train test custom object detect model need help prepar data final step familiar differ deep learn librari implement azur environ manag complet step end write data csv file upload datastor defin path train data file creat new delimit file sep index fals repeat test sep index fals datastor upload prepar data datastor upload local file datastor overwrit true creat regist train dataset datastor file separ train descript train dataset sampl label data creat regist test dataset datastor file separ test descript test dataset sampl label data approach intend us step us convert torchvis dataset work receiv follow error usererrorexcept usererrorexcept messag perform torchvis convers dataset label column defin innerexcept errorrespons suspect issu datatyp origin dataset datatyp displai comparison column type train test dataset pars string understand wai convert data type stream label list list advic prepar dataset us pytorch recommend altern approach greatli appreci updat comment current mount datafram download data size view imag origin mount dataset load newli regist train dataset access imag longer relev perform random split cluster sampl work creat class object defin dataset current creat pil imag object requir pytorch",
        "Question_gpt_summary_original":"The user is facing challenges in preparing a machine learning object detection dataset for deep learning in PyTorch or similar. They have completed steps 1 to 4, but are having difficulty with step 5, which involves converting the dataset to a Torchvision dataset. They are receiving an error message stating that the dataset does not have labeled columns defined, and suspect that the issue may be related to data types. The user is seeking advice on how to prepare the dataset for use in PyTorch or recommendations for an alternative approach. They are also facing challenges with accessing images and creating a PIL Image object as required by PyTorch.",
        "Question_gpt_summary":"user face challeng prepar machin learn object detect dataset deep learn pytorch similar complet step have difficulti step involv convert dataset torchvis dataset receiv error messag state dataset label column defin suspect issu relat data type user seek advic prepar dataset us pytorch recommend altern approach face challeng access imag creat pil imag object requir pytorch",
        "Answer_original_content":"modifi methodolog abl successfulli resolv issu follow export data label project dataset consum dataset notebook creat pytorch dataset panda datafram us panda datafram determin indic train test split base requir sampl us indic input torch util data subset split pytorch dataset train test",
        "Answer_preprocessed_content":"modifi methodolog abl successfulli resolv issu follow export data label project dataset consum dataset notebook creat pytorch dataset panda datafram us panda datafram determin indic train test split base requir sampl us indic input split pytorch dataset train test",
        "Answer_gpt_summary_original":"the answer suggests the following solutions to the challenges faced by the user in preparing a machine learning object detection dataset for deep learning in pytorch or a similar library: \n1. export data labelling project as dataset \n2. consume the dataset in the notebook by creating both a pytorch dataset and a pandas dataframe \n3. use the pandas dataframe to determine indices for the train\/test split based on required sampling \n4. use the indices as an input to torch.utils.data.subset() to split the pytorch dataset into train and test.",
        "Answer_gpt_summary":"answer suggest follow solut challeng face user prepar machin learn object detect dataset deep learn pytorch similar librari export data label project dataset consum dataset notebook creat pytorch dataset panda datafram us panda datafram determin indic train test split base requir sampl us indic input torch util data subset split pytorch dataset train test"
    },
    {
        "Question_id":61240809.0,
        "Question_title":"How to use fitted_model.forecast() for AutoML forecasting model?",
        "Question_body":"<p>Say I have a dataset with a monthly granularity with the following columns:<\/p>\n\n<ul>\n<li>Timestamp<\/li>\n<li>Issues (i.e. number of GitHub issues)<\/li>\n<\/ul>\n\n<p>There is data for each month for all of 2016-2019, so I divide the data accordingly.<\/p>\n\n<ul>\n<li><code>training_data<\/code>: 2016-2017<\/li>\n<li><code>validation_data<\/code>: 2018<\/li>\n<li><code>holdout_data<\/code>: 2019<\/li>\n<\/ul>\n\n<p>If I have a <code>fitted_model<\/code> that is a <code>ForecastingPipelineWrapper<\/code> which is the best run from AutoML where I passed gave it <code>training_data<\/code> and <code>validation_data<\/code>.<\/p>\n\n<p>Looking at the <a href=\"https:\/\/gist.github.com\/swanderz\/c68514d955bebf20f48ffd91aabdc55d\" rel=\"nofollow noreferrer\">ForecastingPipelineWrapper class docstring<\/a> documentation only serves to confuse me more. What is <code>X_past<\/code>, <code>X_future<\/code>, and <code>Y_future<\/code>?<\/p>\n\n<p>How do I use the above dataframes with <code>fitted_model.forecast()<\/code> to manually validate model fit on the <code>holdout_data<\/code> dataframe?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586997104997,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1587136317380,
        "Question_score":2.0,
        "Question_view_count":284.0,
        "Answer_body":"<p>The following notebook illustrates how to leverage y_past, x_past, y_future, x_future, and fitted_model.forecast in the bottom half, 'Forecasting away from training data'. <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-high-frequency\/auto-ml-forecasting-function.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-high-frequency\/auto-ml-forecasting-function.ipynb<\/a><\/p>\n\n<p>The notebook will be a much better guide to grasping these concepts than perhaps a docstring doc. Should you have any more questions or need clarity, let us know!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61240809",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1587062122060,
        "Question_original_content":"us fit model forecast automl forecast model dataset monthli granular follow column timestamp issu number github issu data month divid data accordingli train data valid data holdout data fit model forecastingpipelinewrapp best run automl pass gave train data valid data look forecastingpipelinewrapp class docstr document serv confus past futur futur us datafram fit model forecast manual valid model fit holdout data datafram",
        "Question_preprocessed_content":"us automl forecast model dataset monthli granular follow column timestamp issu data month divid data accordingli best run automl pass gave look forecastingpipelinewrapp class docstr document serv confus us datafram manual valid model fit datafram",
        "Question_gpt_summary_original":"The user is facing challenges in understanding how to use the fitted_model.forecast() function for an AutoML forecasting model. They have a dataset with monthly granularity and are dividing the data into training_data, validation_data, and holdout_data. The confusion arises from the ForecastingPipelineWrapper class docstring documentation, specifically regarding the X_past, X_future, and Y_future dataframes, and how to use them with the fitted_model.forecast() function to validate the model fit on the holdout_data dataframe.",
        "Question_gpt_summary":"user face challeng understand us fit model forecast function automl forecast model dataset monthli granular divid data train data valid data holdout data confus aris forecastingpipelinewrapp class docstr document specif past futur futur datafram us fit model forecast function valid model fit holdout data datafram",
        "Answer_original_content":"follow notebook illustr leverag past past futur futur fit model forecast half forecast awai train data http github com azur machinelearningnotebook blob master us autom machin learn forecast high frequenc auto forecast function ipynb notebook better guid grasp concept docstr doc question need clariti let know",
        "Answer_preprocessed_content":"follow notebook illustr leverag half forecast awai train data notebook better guid grasp concept docstr doc question need clariti let know",
        "Answer_gpt_summary_original":"the answer suggests that the user can refer to a specific notebook on github that demonstrates how to use y_past, x_past, y_future, x_future, and fitted_model.forecast to manually validate the model fit on the holdout_data dataframe. the notebook is recommended as a better guide than a docstring doc. the user is also encouraged to ask for further clarification if needed.",
        "Answer_gpt_summary":"answer suggest user refer specif notebook github demonstr us past past futur futur fit model forecast manual valid model fit holdout data datafram notebook recommend better guid docstr doc user encourag ask clarif need"
    },
    {
        "Question_id":null,
        "Question_title":"Failed transfer to remote S3 storage",
        "Question_body":"<p>Hey, I am an intern and trying to learn DVC for S3 remote storage. I got this error . Can you help me?<br>\nThank you,<\/p>\n<p>dvc push<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 70257f9e4b0b3ec72deeed5e1b880c3b.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 8a31b1338b609cf1133d1df639de606c.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 0ebacd64e04c0e4a97d8fa48a4d707e6.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: b6c29e96813679dd8c451c9b5ca86e2c.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: e42412b82dcab425ce9c7e2d0abdfb78.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 84453a3661e17405f48087e6cf409c51.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 22e3f61e52c0ba45334d973244efc155.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: tutorials\/versioning\/new-labels.zip, md5: 2eaa473159443e75e6fb7b29e56c0787<br>\nname: tutorials\/nlp\/pipeline.zip, md5: 1d2070ee188fc5e4d94ad920e6cc82aa<br>\nname: images\/owl_sticker.png, md5: fa8b9e82c893eb401f30c353bd550ada<br>\nname: tutorials\/versioning\/data.zip, md5: fa9c0eb4173d86695b4e800219651360<br>\nname: mnist\/images.tar.gz, md5: acb39268fb9dd785aac887f69b89282e<br>\nname: tutorials\/nlp\/Posts.xml.zip, md5: ce68b98d82545628782c66192c96f2d2<br>\nname: fashion-mnist\/images.tar.gz, md5: 3c8f026df7c3973bcec8e9f945026eb2<br>\nname: images\/dvc-logo-outlines.png, md5: 5846bf188572bbc78a10124f36e92631<br>\nname: images\/owl_sticker.svg, md5: c76a031091a240c014baea8eff8619c4<br>\nERROR: failed to transfer \u2018md5: a304afb96060aad90176268345e10355\u2019 - [Errno 13] Permission denied: \u2018\/path\u2019<br>\nERROR: failed to push data to the cloud - 1 files failed to upload<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1650448395548,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":243.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/failed-transfer-to-remote-s3-storage\/1166",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-20T12:59:30.013Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"zey\" data-post=\"1\" data-topic=\"1166\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/z\/c57346\/40.png\" class=\"avatar\"> zey:<\/div>\n<blockquote>\n<p>md5<\/p>\n<\/blockquote>\n<\/aside>\n<p>Hi, zey:<br>\nDo you have the correct permission for the remote S3 storage? You can check it through aws s3 cli command.<\/p>",
                "Answer_score":6.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-25T08:23:46.931Z",
                "Answer_body":"<p>I fixed it out ,thanks;<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"fail transfer remot storag hei intern try learn remot storag got error help thank push warn cach file exist local remot miss cach file febbecddebcb dir warn cach file exist local remot miss cach file abbcfddfdec dir warn cach file exist local remot miss cach file ebacdeceadfaad dir warn cach file exist local remot miss cach file bceddccbcaec dir warn cach file exist local remot miss cach file ebdcabcecedabdfb dir warn cach file exist local remot miss cach file aefecfc dir warn cach file exist local remot miss cach file efecbadefc dir warn cach file exist local remot miss cach file tutori version new label zip eaaeefbbec tutori nlp pipelin zip deefcedadeccaa imag owl sticker png fabecebfcbdada tutori version data zip facebdb mnist imag tar acbfbddaacfb tutori nlp post xml zip cebdccfd fashion mnist imag tar cfdfcbcecefeb imag logo outlin png bfbbcafe imag owl sticker svg caacbaeaeffc error fail transfer aafbaad errno permiss deni path error fail push data cloud file fail upload",
        "Question_preprocessed_content":"fail transfer remot storag hei intern try learn remot storag got error help thank push warn cach file exist local remot miss cach file warn cach file exist local remot miss cach file warn cach file exist local remot miss cach file warn cach file exist local remot miss cach file warn cach file exist local remot miss cach file warn cach file exist local remot miss cach file warn cach file exist local remot miss cach file warn cach file exist local remot miss cach file eaa ada acb aac bcec bbc baea eff error fail transfer afb aad permiss deni error fail push data cloud file fail upload",
        "Question_gpt_summary_original":"The user encountered challenges while trying to learn DVC for S3 remote storage. They received a warning that some cache files were missing, and several files failed to upload due to a permission denied error.",
        "Question_gpt_summary":"user encount challeng try learn remot storag receiv warn cach file miss file fail upload permiss deni error",
        "Answer_original_content":"zei zei correct permiss remot storag check aw cli command fix thank",
        "Answer_preprocessed_content":"zei zei correct permiss remot storag check aw cli command fix thank",
        "Answer_gpt_summary_original":"the possible solution to the challenge of missing cache files and permission denied errors while transferring files to a remote s3 storage is to check the correct permission for the remote s3 storage using the aws s3 cli command.",
        "Answer_gpt_summary":"possibl solut challeng miss cach file permiss deni error transfer file remot storag check correct permiss remot storag aw cli command"
    },
    {
        "Question_id":null,
        "Question_title":"security setup - username\/password experiment level",
        "Question_body":"Hi\n\n\nCurrently the way mlflow works is that we can set an experiment ID for a MLFLOW tracker server URL and call it using our training model code. We would like to see if we can add security to this - say setup username\/password and also assign users with permissions for certain experiments\/projects. Is it possible to do that?",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1542243319000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":4154.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/E9QW4HdS8a8",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-11-20T21:46:46",
                "Answer_body":"Hi Maheshwar,\n\nCurrently, we recommend adding authentication in front of your MLflow server using a HTTP proxy such as nginx. You can configure this to use some other internal authentication mechanism, such as verifying whether the user is part of a group. This won\u2019t isolate things per user, but you could run a separate tracking server for each team using ML for example.\n\nMatei\n\n\n\n> On Nov 14, 2018, at 9:55 PM, Maheshwar Dattatri <mahes...@gmail.com> wrote:\n>\n>\n> Hi\n>\n> Currently the way mlflow works is that we can set an experiment ID for a MLFLOW tracker server URL and call it using our training model code. We would like to see if we can add security to this - say setup username\/password and also assign users with permissions for certain experiments\/projects. Is it possible to do that?\n>\n\n> --\n> You received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n> To unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n> To post to this group, send email to mlflow...@googlegroups.com.\n> To view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/de8d707f-fee9-4450-b906-89142f5d9827%40googlegroups.com.\n> For more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2018-11-22T03:28:48",
                "Answer_body":"Hi Matei\n\n\nIn \"rest_utils.py\" , there is \"http_request\" function, which receives host_creds (include username, password) to make http request for authentication\nhttps:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/utils\/rest_utils.py#L20\u00a0\u00a0\n\nThe \"http_request\" fucntion is also used in \"databricks.py\"\nhttps:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/projects\/databricks.py#L88\n\n\n\nCould we use this function to make authentication for MLflow?\nDoes MLflow support authentication for next versions ?\n\n\nTriet Nguyen\n\ue5d3"
            },
            {
                "Answer_creation_time":"2018-11-22T18:50:09",
                "Answer_body":"Yes, you can actually pass these already using the environment variables MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD or MLFLOW_TRACKING_TOKEN (see https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/tracking\/utils.py). These will be used with all HTTP requests to the tracking server. You can run the server behind a proxy such as nginx to perform authentication before passing requests through (the goal here is to make it possible for people to integrate it with their own company\u2019s auth systems instead of baking one into our server).\n\nMatei\n\n\ue5d3\n> To view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/a25a55cb-9d64-438d-9e84-4f5061b59aed%40googlegroups.com.\n\n\ue5d3"
            },
            {
                "Answer_creation_time":"2021-11-29T10:21:54",
                "Answer_body":"Hello, this is an old question but we tried the solution suggested by @matei and we hit a dead end because of the following reasons\n\n\n\n- Nginx `auth_request` erases the data before sending `\/authorize` requests and MLFlow sends the `experiment_id`\/`run_id` in the data of `POST` and `UPDATE` requests instead of the URL (`POST \/tracking\/experiments\/1`), this makes it impossible to authorize such requests, we are denying all them right now.\n- We can't filter out the list of the experiments a user can see, we can only allow them to do a request or not. If we don't allow them to see the list of experiments, the front page of MLFlow will broken.\n\n\nI think access control mechanics over MLFlow resources should implemented in MLFlow itself or in an MLFlow plugin or at least think about how third party application can do that and makes it possible for them, that way we'll have access to the database, the UI and everything we need to implement such features.\n\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"secur setup usernam password experi level current wai work set experi tracker server url train model code like add secur setup usernam password assign user permiss certain experi project possibl",
        "Question_preprocessed_content":"secur setup experi level current wai work set experi tracker server url train model code like add secur setup assign user permiss certain possibl",
        "Question_gpt_summary_original":"The user is facing a challenge with the security setup of their MLFLOW tracker server URL. They want to add a username\/password and assign permissions to users for certain experiments\/projects. They are unsure if this is possible.",
        "Question_gpt_summary":"user face challeng secur setup tracker server url want add usernam password assign permiss user certain experi project unsur possibl",
        "Answer_original_content":"maheshwar current recommend ad authent server http proxi nginx configur us intern authent mechan verifi user group wont isol thing user run separ track server team exampl matei nov maheshwar dattatri wrote current wai work set experi tracker server url train model code like add secur setup usernam password assign user permiss certain experi project possibl receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user dedf fee googlegroup com option visit http group googl com optout matei rest util http request function receiv host cred includ usernam password http request authent http github com blob master util rest util http request fucntion databrick http github com blob master project databrick us function authent support authent version triet nguyen ye actual pass environ variabl track usernam track password track token http github com blob master track util http request track server run server proxi nginx perform authent pass request goal possibl peopl integr compani auth system instead bake server matei view discuss web visit http group googl com msgid user aacb fba googlegroup com hello old question tri solut suggest matei hit dead end follow reason nginx auth request eras data send author request send experi run data post updat request instead url post track experi make imposs author request deni right filter list experi user allow request allow list experi page broken think access control mechan resourc implement plugin think parti applic make possibl wai access databas need implement featur",
        "Answer_preprocessed_content":"maheshwar current recommend ad authent server http proxi nginx configur us intern authent mechan verifi user group wont isol thing user run separ track server team exampl matei nov maheshwar dattatri wrote current wai work set experi tracker server url train model code like add secur setup assign user permiss certain possibl receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit matei function receiv http request authent fucntion us function authent support authent version triet nguyen ye actual pass environ variabl http request track server run server proxi nginx perform authent pass request matei view discuss web visit hello old question tri solut suggest hit dead end follow reason nginx eras data send request send data request instead url make imposs author request deni right filter list experi user allow request allow list experi page broken think access control mechan resourc implement plugin think parti applic make possibl wai access databas need implement featur",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Add authentication in front of the MLflow server using a HTTP proxy such as nginx. This can be configured to use some other internal authentication mechanism, such as verifying whether the user is part of a group. However, this won\u2019t isolate things per user, but a separate tracking server can be run for each team using ML for example.\n- Pass the environment variables MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD or MLFLOW_TRACKING_TOKEN to authenticate HTTP requests to the tracking server. The server can be run behind a proxy such as nginx to perform authentication before passing requests through.\n- Implement access control mechanics over MLFlow resources in MLFlow itself or in an MLFlow plugin to make it possible for third-party applications to do that.",
        "Answer_gpt_summary":"possibl solut mention discuss add authent server http proxi nginx configur us intern authent mechan verifi user group wont isol thing user separ track server run team exampl pass environ variabl track usernam track password track token authent http request track server server run proxi nginx perform authent pass request implement access control mechan resourc plugin possibl parti applic"
    },
    {
        "Question_id":null,
        "Question_title":"Getting Error Deadline Exceeded when deploying model from Cloud Firestoer functions",
        "Question_body":"Hello,I currently have a firebase function that is set to deploy my AutoML tables model everyday at 5am. This has been working fine for the past month, up until the last week. I have been getting the following error below when the function attempts to deploy the model.I watched a google tutorial and it recommend to return a promise from my cloud function. That seemed to work for 1 day, but I received the error again this morning.I am going to try to implement a retry function, but I figured I would ask on here as well. Also, I am thinking that moving from autoML to VertexAI might help alleviate my issues. Any guidance here is helpful.See below for my deploy model code:  ",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1638530220000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":860.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Getting-Error-Deadline-Exceeded-when-deploying-model-from-Cloud\/td-p\/177128\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-03T12:46:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Is deploying the model fails via Cloud Functions only, or does it also fail when the model is deployed manually, or by any other means?"
            },
            {
                "Answer_creation_time":"2021-12-03T15:00:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Manually (through the cloud console) it usually works. I have been testing using an http call in Firebase functions. Sometimes (not consistantly) that fails if I have recently undeployed the model. Although, I do not get any type of error notification, I just know it fails by checking the cloud console.\n\nThe deploy model function only runs once a day though, and the model has typically been undeployed for at least 20 hours before that, so I don't think I am getting that error because I am calling it too often."
            },
            {
                "Answer_creation_time":"2021-12-09T10:39:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"So after further investigation it looks like the model is failing when I am requesting predictions through firebase functions. I get the same error \"DEADLINE_EXCEEDED\". This is not consistent though, it worked for the previous 2 days before this and today failed again. I haven't changed anything.\n\nI have 2 questions:\n\n1) is it possible that congestion on the network is causing these to fail? Would it help if I moved the prediction to a different time? Currently I have it set at 6am PST.\n\n2) Since autoML is beta, would it help if I moved the model to VertexAI? I can make that move if it helps"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"get error deadlin exceed deploi model cloud firesto function hello current firebas function set deploi automl tabl model everydai work fine past month week get follow error function attempt deploi model watch googl tutori recommend return promis cloud function work dai receiv error morn go try implement retri function figur ask think move automl vertexai help allevi issu guidanc help deploi model code",
        "Question_preprocessed_content":"get error deadlin exceed deploi model cloud firesto function hello current firebas function set deploi automl tabl model everydai work fine past month week get follow error function attempt deploi watch googl tutori recommend return promis cloud function work dai receiv error go try implement retri function figur ask think move automl vertexai help allevi issu guidanc deploi model code",
        "Question_gpt_summary_original":"The user is encountering an error of \"Deadline Exceeded\" when deploying their AutoML tables model from Cloud Firestore functions. This issue has been occurring for the past week, despite implementing a solution recommended in a Google tutorial. The user plans to implement a retry function and is considering switching to VertexAI to resolve the issue.",
        "Question_gpt_summary":"user encount error deadlin exceed deploi automl tabl model cloud firestor function issu occur past week despit implement solut recommend googl tutori user plan implement retri function consid switch vertexai resolv issu",
        "Answer_original_content":"deploi model fail cloud function fail model deploi manual mean manual cloud consol usual work test http firebas function consistantli fail recent undeploi model type error notif know fail check cloud consol deploi model function run dai model typic undeploi hour think get error call investig look like model fail request predict firebas function error deadlin exceed consist work previou dai todai fail haven chang question possibl congest network caus fail help move predict differ time current set pst automl beta help move model vertexai help",
        "Answer_preprocessed_content":"deploi model fail cloud function fail model deploi manual mean manual usual work test http firebas function fail recent undeploi model type error notif know fail check cloud consol deploi model function run dai model typic undeploi hour think get error call investig look like model fail request predict firebas function error consist work previou dai todai fail haven chang question possibl congest network caus fail help move predict differ time current set pst automl beta help move model vertexai help",
        "Answer_gpt_summary_original":"possible solutions from the answer are: \n1. the user can try deploying the model manually through the cloud console instead of using an http call in firebase functions. \n2. the user can check if network congestion is causing the issue and try moving the prediction to a different time. \n3. the user can consider moving the model to vertexai as it may help since automl is still in beta.",
        "Answer_gpt_summary":"possibl solut answer user try deploi model manual cloud consol instead http firebas function user check network congest caus issu try move predict differ time user consid move model vertexai help automl beta"
    },
    {
        "Question_id":null,
        "Question_title":"Using DVC in github codespaces",
        "Question_body":"<p>I\u2019m trying to use dvc in a github codespace and gdrive remote storage. However once I try to \u201cdvc push\u201d<br>\nI get this error<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/0ba0f9f145ef2814e08ffd3f4e84be037bd79a34.png\" data-download-href=\"\/uploads\/short-url\/1ES8Pl3nOEFzmQGXs0qEaUQLYRC.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/0ba0f9f145ef2814e08ffd3f4e84be037bd79a34.png\" alt=\"image\" data-base62-sha1=\"1ES8Pl3nOEFzmQGXs0qEaUQLYRC\" width=\"690\" height=\"169\" data-dominant-color=\"282828\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1015\u00d7249 15.3 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1673510308004,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":48.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/using-dvc-in-github-codespaces\/1461",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-12T16:22:28.776Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mouadaqsone\">@mouadAqsone<\/a> ! I haven\u2019t looked in depth, but looks like the authorization method we use for GDrive remotes (<a href=\"https:\/\/github.com\/iterative\/PyDrive2\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - iterative\/PyDrive2: Google Drive API Python wrapper library. Maintained fork of PyDrive.<\/a>) doesn\u2019t work inside a GitHub codespace.<\/p>\n<p>As a workaround, I would suggest that you run a first-time <code>dvc push<\/code> on your local machine. This would  create a credentials files in that local machine (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/setup-google-drive-remote#authorization\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">How to Setup a Google Drive DVC Remote<\/a>) and you could use Codespaces secrets (<a href=\"https:\/\/docs.github.com\/en\/codespaces\/managing-your-codespaces\/managing-encrypted-secrets-for-your-codespaces#about-encrypted-secrets-for-github-codespaces\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Managing encrypted secrets for your codespaces - GitHub Docs<\/a>) to expose the contents of that file as an environment variable (<code>GDRIVE_CREDENTIALS_DATA<\/code>) .<\/p>\n<p>I think that would make <code>dvc push<\/code> work inside the codespace<\/p>",
                "Answer_score":66.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-10T11:08:25.145Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/daavoo\">@daavoo<\/a>, I\u2019ve finally came back to this as it\u2019s important for our company to work on codespaces. I have made the codespaces secrets with the full content of the credentials files (default.json).<\/p>\n<p>But a dvc pull still just creates a new port and tries to open a new window from which I have to agree to access for the gdrive ( and when I do, i get nothing )<\/p>\n<p>this is my error message<\/p>\n<p>\/usr\/local\/python\/3.10.4\/lib\/python3.10\/site-packages\/oauth2client\/_helpers.py:255: UserWarning: Cannot access \/home\/codespace\/.cache\/pydrive2fs\/710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com\/default.json: No such file or directory<br>\nwarnings.warn(_MISSING_FILE_MESSAGE.format(filename))<br>\nYour browser has been opened to visit:<br>\n\u2026<\/p>\n<p>I\u2019m not very familiar with these types of things, but the logical thing that came to my mind is just copy the default.json file into this folder. Is there a better way than that with codespace Secrets ?<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-11T02:08:41.738Z",
                "Answer_body":"<p>I know it\u2019s super confusing with GDrive, but there are two different credentials files:<\/p>\n<ol>\n<li>\n<p>The one you downloaded from the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/setup-google-drive-remote#using-a-custom-google-cloud-project-recommended\">Google Cloud console<\/a> (this step is optional though, and I\u2019m not sure if you did it or not).<\/p>\n<\/li>\n<li>\n<p>The one that is generated during the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/setup-google-drive-remote#authorization\">Authorization<\/a> process.<\/p>\n<\/li>\n<\/ol>\n<p>You need to get the second one and put it either as an env variable (should be possible to set an env variable in codespaces secrets?) or copy in some location and set up the <code>gdrive_user_credentials_file<\/code> path. Could you please make sure that you are indeed using the second file? What is the structure of the file (field names, etc) that you are copying?<\/p>\n<hr>\n<p>Btw, I definitely recommend taking a look into Dev Containers, and specifically DVC feature <a href=\"https:\/\/github.com\/iterative\/features\" class=\"inline-onebox\">GitHub - iterative\/features: A collection of development container 'features'<\/a> . It simplifies the Codespaces setup with DVC. You can find a bit more sophisticated example here- <a href=\"https:\/\/github.com\/shcheklein\/hackathon\/blob\/main\/.devcontainer.json\" class=\"inline-onebox\">hackathon\/.devcontainer.json at main \u00b7 shcheklein\/hackathon \u00b7 GitHub<\/a><\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"github codespac try us github codespac gdrive remot storag try push error imag",
        "Question_preprocessed_content":"github codespac try us github codespac gdrive remot storag try push error imag",
        "Question_gpt_summary_original":"The user is facing challenges while trying to use DVC in a Github codespace and Gdrive remote storage. They encounter an error when attempting to \"dvc push\".",
        "Question_gpt_summary":"user face challeng try us github codespac gdrive remot storag encount error attempt push",
        "Answer_original_content":"mouadaqson havent look depth look like author method us gdrive remot github iter pydriv googl drive api python wrapper librari maintain fork pydriv doesnt work insid github codespac workaround suggest run time push local machin creat credenti file local machin setup googl drive remot us codespac secret manag encrypt secret codespac github doc expos content file environ variabl gdrive credenti data think push work insid codespac hello daavoo iv final came import compani work codespac codespac secret content credenti file default json pull creat new port tri open new window agre access gdrive error messag usr local python lib python site packag oauthclient helper userwarn access home codespac cach pydrivef iivsgbgsbuvfapdhvueioc app googleusercont com default json file directori warn warn miss file messag format filenam browser open visit familiar type thing logic thing came mind copi default json file folder better wai codespac secret know super confus gdrive differ credenti file download googl cloud consol step option sure gener author process need second env variabl possibl set env variabl codespac secret copi locat set gdrive user credenti file path sure second file structur file field name copi btw definit recommend take look dev contain specif featur github iter featur collect develop contain featur simplifi codespac setup bit sophist exampl hackathon devcontain json main shcheklein hackathon github",
        "Answer_preprocessed_content":"havent look depth look like author method us gdrive remot doesnt work insid github codespac workaround suggest run local machin creat credenti file local machin us codespac secret expos content file environ variabl think work insid codespac hello iv final came import compani work codespac codespac secret content credenti file pull creat new port tri open new window agre access gdrive error messag userwarn access file directori browser open visit familiar type thing logic thing came mind copi file folder better wai codespac secret know super confus gdrive differ credenti file download googl cloud consol gener author process need second env variabl copi locat set path sure second file structur file copi btw definit recommend take look dev contain specif featur github collect develop contain featur simplifi codespac setup bit sophist exampl main github",
        "Answer_gpt_summary_original":"possible solutions to the error encountered when attempting to push to github codespace with gdrive remote storage are:\n\n- run a first-time push on your local machine to create a credentials file and use codespaces secrets to expose the contents of that file as an environment variable.\n- copy the second credentials file into a location and set up the gdrive_user_credentials_file path.\n- use dev containers to simplify the codespaces setup.",
        "Answer_gpt_summary":"possibl solut error encount attempt push github codespac gdrive remot storag run time push local machin creat credenti file us codespac secret expos content file environ variabl copi second credenti file locat set gdrive user credenti file path us dev contain simplifi codespac setup"
    },
    {
        "Question_id":null,
        "Question_title":"Endpoint in GCP",
        "Question_body":"In GCP I was deploying a model which obtain from training a dataset and after success full Vertex AI Model Registry. It takes too much time around 10 min to create endpoint for model. How can I reduce creation time when creating endpoints on GCP? What factors affected endpoint creation ?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1665751020000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":77.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Endpoint-in-GCP\/td-p\/478360\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-20T16:06:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"This video would be a good starting point:\n\nDeploying quick, cost-effective ML models with Vertex AI\n\nManaged notebook environments make it easier, faster, and more cost-effective to get high-quality models into production without having to set up infrastructure or install libraries. In this session, we\u2019ll demo how to use Vertex AI to get batch and online predictions and use the Vertex AI Python SDK to upload models to Vertex AI Model Registry and deploy to an endpoint with little code."
            },
            {
                "Answer_creation_time":"2022-11-04T04:47:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"thanks for\u00a0 reply but given resource does not work for me !!!"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"endpoint gcp gcp deploi model obtain train dataset success model registri take time min creat endpoint model reduc creation time creat endpoint gcp factor affect endpoint creation",
        "Question_preprocessed_content":"endpoint gcp gcp deploi model obtain train dataset success model registri take time min creat endpoint model reduc creation time creat endpoint gcp factor affect endpoint creation",
        "Question_gpt_summary_original":"The user encountered a challenge in GCP while deploying a model and creating an endpoint, which took around 10 minutes to complete. The user is seeking ways to reduce the creation time and is curious about the factors that affect endpoint creation.",
        "Question_gpt_summary":"user encount challeng gcp deploi model creat endpoint took minut complet user seek wai reduc creation time curiou factor affect endpoint creation",
        "Answer_original_content":"video good start point deploi quick cost effect model manag notebook environ easier faster cost effect high qualiti model product have set infrastructur instal librari session demo us batch onlin predict us python sdk upload model model registri deploi endpoint littl code thank repli given resourc work",
        "Answer_preprocessed_content":"video good start point deploi quick model manag notebook environ easier faster model product have set infrastructur instal librari session demo us batch onlin predict us python sdk upload model model registri deploi endpoint littl code thank repli given resourc work",
        "Answer_gpt_summary_original":"there are no possible solutions provided in the answer. the answer suggests a video that may be helpful for deploying ml models quickly and cost-effectively, but it does not address the specific challenge of creating endpoints on gcp. the user also mentions that the suggested resource does not work for them.",
        "Answer_gpt_summary":"possibl solut provid answer answer suggest video help deploi model quickli cost effect address specif challeng creat endpoint gcp user mention suggest resourc work"
    },
    {
        "Question_id":62180798.0,
        "Question_title":"How to increase number of tested images in MS Azure Custom Vision?",
        "Question_body":"<p>I've created a project in Azure Custom Vision (Object Detection, General Compact, Tier S0). I uploaded about 70 images, 35 images per tag then started training my model.<\/p>\n\n<p>Checked tags in the Iterations screen after training (Quick Training) was done. For my surprise, only 7 images were tested per tag.<\/p>\n\n<p>Tried to run Advanced Training for 1 hour. Nothing has changed. Only 7 images per tag were tested.<\/p>\n\n<p>Am I doing something wrong?<\/p>\n\n<p>Is there a way to use all images for object detection training so it can give me a better accuracy?<\/p>\n\n<p>Thanks,\n+ftex<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591211290130,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":159.0,
        "Answer_body":"<p>What you are seeing in the test interface after the training is only a part of the total images because these metrics are calculated using k-fold cross validation.<\/p>\n<p>You are not doing something wrong. It would not be logic to test all the images because it would mean testing with your training images.<\/p>\n<p>To have a better accuracy, there's no magic: add more images, relevant to your use-case<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/custom-vision-service\/getting-started-build-a-classifier#evaluate-the-classifier?WT.mc_id=AI-MVP-5003365\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/custom-vision-service\/getting-started-build-a-classifier#evaluate-the-classifier<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1600465129972,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62180798",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1591541807696,
        "Question_original_content":"increas number test imag azur custom vision creat project azur custom vision object detect gener compact tier upload imag imag tag start train model check tag iter screen train quick train surpris imag test tag tri run advanc train hour chang imag tag test wrong wai us imag object detect train better accuraci thank ftex",
        "Question_preprocessed_content":"increas number test imag azur custom vision creat project azur custom vision upload imag imag tag start train model check tag iter screen train surpris imag test tag tri run advanc train hour chang imag tag test wrong wai us imag object detect train better accuraci thank ftex",
        "Question_gpt_summary_original":"The user is facing a challenge with MS Azure Custom Vision as only 7 images per tag are being tested despite uploading 70 images and running advanced training for an hour. The user is seeking advice on how to use all images for object detection training to improve accuracy.",
        "Question_gpt_summary":"user face challeng azur custom vision imag tag test despit upload imag run advanc train hour user seek advic us imag object detect train improv accuraci",
        "Answer_original_content":"see test interfac train total imag metric calcul fold cross valid wrong logic test imag mean test train imag better accuraci magic add imag relev us case http doc microsoft com azur cognit servic custom vision servic get start build classifi evalu classifi",
        "Answer_preprocessed_content":"see test interfac train total imag metric calcul cross valid wrong logic test imag mean test train imag better accuraci magic add imag relev",
        "Answer_gpt_summary_original":"the answer suggests that the user is not doing anything wrong and that the metrics are calculated using k-fold cross-validation. to improve accuracy, the user should add more relevant images to their use-case.",
        "Answer_gpt_summary":"answer suggest user wrong metric calcul fold cross valid improv accuraci user add relev imag us case"
    },
    {
        "Question_id":null,
        "Question_title":"Access to NCasT4_v3-series and ND A100 v4-series VMs",
        "Question_body":"How could I request quota for the NCasT4_v3-series and ND A100 v4-series VMs for Machine Learning services and as regular VMs\n\nThey both do not appear as an option on the usual form to request quota increase in any of the 4 US regions I looked\n\nThanks\n\nManuel",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629921673763,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Hi @ManuelReyesGomez-1028 ,\n\nthe VM series NCasT4_v3 and ND A100 v4 are only available in 3 US regions (both series together)\n\nSource: https:\/\/azure.microsoft.com\/en-us\/global-infrastructure\/services\/?products=virtual-machines&regions=us-central,us-east,us-east-2,us-north-central,us-south-central,us-west-central,us-west,us-west-2,us-west-3\n\n(If the reply was helpful please don't forget to upvote and\/or accept as answer, thank you)\n\nRegards\nAndreas Baumgarten",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/528034\/access-to-ncast4-v3-series-and-nd-a100-v4-series-v.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-25T20:15:51.63Z",
                "Answer_score":1,
                "Answer_body":"Hi @ManuelReyesGomez-1028 ,\n\nthe VM series NCasT4_v3 and ND A100 v4 are only available in 3 US regions (both series together)\n\nSource: https:\/\/azure.microsoft.com\/en-us\/global-infrastructure\/services\/?products=virtual-machines&regions=us-central,us-east,us-east-2,us-north-central,us-south-central,us-west-central,us-west,us-west-2,us-west-3\n\n(If the reply was helpful please don't forget to upvote and\/or accept as answer, thank you)\n\nRegards\nAndreas Baumgarten",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":22.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1629922551630,
        "Question_original_content":"access ncast seri seri vm request quota ncast seri seri vm machin learn servic regular vm appear option usual form request quota increas region look thank manuel",
        "Question_preprocessed_content":"access vm request quota vm machin learn servic regular vm appear option usual form request quota increas region look thank manuel",
        "Question_gpt_summary_original":"The user is facing challenges in requesting quota for NCasT4_v3-series and ND A100 v4-series VMs for machine learning services and regular VMs as they do not appear as an option on the usual form to request quota increase in any of the 4 US regions.",
        "Question_gpt_summary":"user face challeng request quota ncast seri seri vm machin learn servic regular vm appear option usual form request quota increas region",
        "Answer_original_content":"manuelreyesgomez seri ncast avail region seri sourc http azur microsoft com global infrastructur servic product virtual machin region central east east north central south central west central west west west repli help forget upvot accept answer thank regard andrea baumgarten",
        "Answer_preprocessed_content":"seri avail region sourc repli help forget upvot accept answer thank regard andrea baumgarten",
        "Answer_gpt_summary_original":"the possible solution to the challenge faced by manuel in requesting quota for ncast4_v3-series and nd a100 v4-series vms is that these vm series are only available in 3 us regions. the answer provides a source link to check the availability of these vms in different regions.",
        "Answer_gpt_summary":"possibl solut challeng face manuel request quota ncast seri seri vm seri avail region answer provid sourc link check avail vm differ region"
    },
    {
        "Question_id":null,
        "Question_title":"Real-time endpoint response is empty",
        "Question_body":"Hi, I've created and deployed NLP pipeline as a real-time endpoint, but the response that I get is an empty result ( {\"Results\":{}} ) with Status 200 OK.\n\n\n\n\nI have the Owner role, so i think that the permission is not a problem. I've also tried to deploy the preset experiment \"Sample 1: Regression - Automobile Price Prediction (Basic)\" using the same resources and it returned the results without a problem.\n\nIn the training pipeline, I've used the train and validation datasets. After creating the real-time inference pipeline, it automatically created two Web Service inputs the one linked to the train dataset input is irrelevant in the real-time inference pipeline as it's used only to create the Vocabulary for the Extract N-Gram Features from Text module linked to the validation dataset.\nAfter that, I've deleted the Web Service input connected to the training dataset and tried to deploy the real-time endpoint.\n\n\n\n\n\n\nAfter submitting the training pipeline, I've saved the Result Vocabulary from the Extract N-Gram Features from Text module that is connected to the training dataset and used it as an input vocabulary for the N-Gram module (ReadOnly vocabulary mode) in the real-time inference pipeline and deployed that as a real-time endpoint.\n\n\n\n\nIn both cases above, I didn't get any errors, scoring and evaluation of a model are correct but the response from the endpoint is empty.\nIt's worth mentioning, that if I leave the real-time pipeline as is (with two inputs) it doesn't work because of the parameter settings in the Extract N-Gram Features from Text module where training dataset is connected.",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1592313268633,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/36610\/real-time-endpoint-response-is-empty.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-01T07:58:21.297Z",
                "Answer_score":0,
                "Answer_body":"@AntonioAli-4907 Is it possible to share details of your experiment and issue from the ml.azure.com portal for a service engineer to lookup the issue? This option is available from the top right hand corner of the portal by clicking the smiley face, Please select the option Microsoft can email you about the feedback along with a screen shot so our service team can lookup and advise through email.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"real time endpoint respons creat deploi nlp pipelin real time endpoint respons result result statu owner role think permiss problem tri deploi preset experi sampl regress automobil price predict basic resourc return result problem train pipelin train valid dataset creat real time infer pipelin automat creat web servic input link train dataset input irrelev real time infer pipelin creat vocabulari extract gram featur text modul link valid dataset delet web servic input connect train dataset tri deploi real time endpoint submit train pipelin save result vocabulari extract gram featur text modul connect train dataset input vocabulari gram modul readonli vocabulari mode real time infer pipelin deploi real time endpoint case error score evalu model correct respons endpoint worth mention leav real time pipelin input work paramet set extract gram featur text modul train dataset connect",
        "Question_preprocessed_content":"endpoint respons creat deploi nlp pipelin endpoint respons result statu owner role think permiss problem tri deploi preset experi sampl regress automobil price predict resourc return result problem train pipelin train valid dataset creat infer pipelin automat creat web servic input link train dataset input irrelev infer pipelin creat vocabulari extract featur text modul link valid dataset delet web servic input connect train dataset tri deploi endpoint submit train pipelin save result vocabulari extract featur text modul connect train dataset input vocabulari modul infer pipelin deploi endpoint case error score evalu model correct respons endpoint worth mention leav pipelin work paramet set extract featur text modul train dataset connect",
        "Question_gpt_summary_original":"The user has encountered a challenge with their NLP pipeline real-time endpoint, as the response they receive is empty despite having the Owner role and using the same resources as a successful experiment. They have tried deleting the Web Service input connected to the training dataset and using the Result Vocabulary from the Extract N-Gram Features from Text module as an input vocabulary for the N-Gram module, but still receive an empty response. The Extract N-Gram Features from Text module's parameter settings may be causing issues with the real-time pipeline.",
        "Question_gpt_summary":"user encount challeng nlp pipelin real time endpoint respons receiv despit have owner role resourc success experi tri delet web servic input connect train dataset result vocabulari extract gram featur text modul input vocabulari gram modul receiv respons extract gram featur text modul paramet set caus issu real time pipelin",
        "Answer_original_content":"antonioali possibl share detail experi issu azur com portal servic engin lookup issu option avail right hand corner portal click smilei face select option microsoft email feedback screen shot servic team lookup advis email",
        "Answer_preprocessed_content":"possibl share detail experi issu portal servic engin lookup issu option avail right hand corner portal click smilei face select option microsoft email feedback screen shot servic team lookup advis email",
        "Answer_gpt_summary_original":"the possible solution to the challenge of an empty response from a real-time endpoint is to share the details of the experiment and issue from the ml.azure.com portal for a service engineer to lookup the issue. this can be done by clicking the smiley face on the top right-hand corner of the portal and selecting the option for microsoft to email the user about the feedback along with a screenshot. the service team can then advise through email.",
        "Answer_gpt_summary":"possibl solut challeng respons real time endpoint share detail experi issu azur com portal servic engin lookup issu click smilei face right hand corner portal select option microsoft email user feedback screenshot servic team advis email"
    },
    {
        "Question_id":null,
        "Question_title":"Tensorboard logging twice + is slow",
        "Question_body":"<p>I  have an operation, let\u2019s call it <code>a<\/code> that is kinda the \u201cbase operation\u201d that I could run a number ways with different flags.  Then I have another operation <code>b<\/code> that has one step  that runs operation <code>a<\/code> with the proper flags.<\/p>\n<p>What\u2019s strange to me is that I seem to get multiple of everything I logged with tensorboard. Additionally, <code>guild view<\/code>  is pretty  slow to open  my runs,  and viewing in tensorboard from  there is much slower (takes 10+s) . I saw someone had an issue with symlinks but I don\u2019t think that\u2019s the issue here since I don\u2019t have any set up. I feel like these issues are probably linked, considering I wasn\u2019t having this problem before. If I run just 1 configuration I ended up with:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985.png\" data-download-href=\"\/uploads\/short-url\/ppPT2noEZFMHbXQsJsQEYjoFJoF.png?dl=1\" title=\"Screen Shot 2020-10-19 at 10.35.07 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985_2_690x162.png\" alt=\"Screen Shot 2020-10-19 at 10.35.07 PM\" data-base62-sha1=\"ppPT2noEZFMHbXQsJsQEYjoFJoF\" width=\"690\" height=\"162\" srcset=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985_2_690x162.png, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985.png 1.5x, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2020-10-19 at 10.35.07 PM<\/span><span class=\"informations\">779\u00d7184 45.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg><\/div><\/a><\/div><\/p>\n<p>Not sure how to go about fixing this, I probably  did something wrong.<\/p>",
        "Question_answer_count":10,
        "Question_comment_count":0,
        "Question_creation_time":1603173353724,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1345.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/tensorboard-logging-twice-is-slow\/423",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-20T14:41:13.834Z",
                "Answer_body":"<p>The duplication in results in TensorBoard is by design. Steps (pipeline) ops inherit the scalar results of their child runs. To avoid this, use a filter to limit the runs that you\u2019re viewing in TensorBoard. E.g. to view just the <code>ap<\/code> results (which include <code>imputer<\/code>) run:<\/p>\n<pre><code class=\"lang-command\">guild tensorboard --operation ap\n<\/code><\/pre>\n<p>Re slow performance in View, you\u2019re right this is not likely an issue with symlinks (you\u2019d have seen poor performance with <code>guild tensorboard<\/code> as well in that case \u2013 though this has been fixed).<\/p>\n<p>Guild View is missing some optimizations, which cause it to slow down when viewing more and more runs. When running View in this case, you can work around this by limiting the runs you show. Use filters, e.g.<\/p>\n<p>But to address the issue with TensorBoard performance, I recommend just running <code>guild tensorboard<\/code> and not launching via View.<\/p>\n<p>View is due for a series of enhancements including performance optimization \u2014 but until then these are the workaround I recommend.<\/p>",
                "Answer_score":28.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-25T12:06:12.707Z",
                "Answer_body":"<p>I face the same issue but I am unsure how to resolve it. I have an operation called <code>start<\/code> which I run when I start an experiment. For every run, there are two entries in Tensorboard. How can I filter out one of them?<\/p>\n<p><code>guild tensorboard --operation start<\/code> doesn\u2019t work.<\/p>\n<p>Do you mind sharing some details where the second entry in Tensorboard is coming from?<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/50463d322584fcb8434aaade01eeb94055edcab7.png\" alt=\"Screenshot 2020-11-25 at 13.03.53\" data-base62-sha1=\"bs8GpFUxJikxKUZNXl2BrQpyPjh\" width=\"300\" height=\"253\"><\/p>",
                "Answer_score":22.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-26T10:51:09.635Z",
                "Answer_body":"<p>Is it possible that your code is also writing to the tensorboard in a directory under the working directory or a directory you requested in the resources?<\/p>",
                "Answer_score":16.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-26T11:05:37.691Z",
                "Answer_body":"<p>This is the way I log results right now:<\/p>\n<pre><code>torch.utils.tensorboard import SummaryWriter\nself.writer = SummaryWriter()\nself.writer.add_scalar('Loss\/train', loss_train\/(batch_idx+1), global_step\n<\/code><\/pre>\n<p>Does this create the duplicate entry in tensorboard?<\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-26T11:32:39.473Z",
                "Answer_body":"<p>From <a>here<\/a> it says that by default it writes to <code>.\/runs\/<\/code> so I guess this is most likely whats happening.<\/p>\n<p>What I recommend is to create a static directory on your machine and write all runs to that directory (of course you need to clean it yourself from time to time otherwise it could bleed your hardisc dry in no time).<\/p>\n<p>And then you change your line to<\/p>\n<pre><code class=\"lang-python\">self.writer = SummaryWriter('\/some\/static\/path\/')\n<\/code><\/pre>\n<p>Or better still, use env variable to specify where to write it so you track your experiment better:<\/p>\n<pre><code class=\"lang-bash\">TENSORBOARD_LOGDIR=\/some\/dir\/;guild run XXX\n<\/code><\/pre>\n<pre><code class=\"lang-python\">import os\nTENSORBOARD_LOGDIR=os.environ('TENSORBOARD_LOGDIR')\nself.writer = SummaryWriter(TENSORBOARD_LOGDIR)\n<\/code><\/pre>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-26T11:48:33.686Z",
                "Answer_body":"<p>Ok it since this static logging directory is not inside the run folder it won\u2019t be shown on tensorboard, right?<\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-26T11:53:34.166Z",
                "Answer_body":"<p>Correct, as long as it not under guild\u2019s home directory (usually <code>~\/.guild<\/code>).<\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-26T13:48:00.980Z",
                "Answer_body":"<p>Ok the only purpose of logging to the static dir is to trigger guildai?<br>\nThese seems a bit hacky.<\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-26T14:19:15.148Z",
                "Answer_body":"<p>Actually the purpose is to NOT trigger guild\u2019s tensorboard so you don\u2019t show duplicate items in it.<\/p>\n<p>Generally, you want something to be logged by guild, and you write those into the <code>flags<\/code> and <code>scalars<\/code> in your .yml file so that they will show up on the tensorboard and there\u2019s something that\u2019s more complicated and guild doesn\u2019t support yet like network parameters histograms so that you write them into another tensorboard logdir that you manage yourself.<\/p>\n<p>If you want to write your own data output to the same tensorboard <code>event<\/code> that guild manage, you can do so by specifying the logdir to be the <code>.guild<\/code> folder in a run sub-directory. You can do that by:<\/p>\n<pre><code class=\"lang-python\">self.writer = SummaryWriter('.\/.guild\/')\n<\/code><\/pre>\n<p>You can also understand more about guild run file hierarchy by browsing the run sub-directory, try <code>guild ls<\/code> and explore the it. Note that under the run sub-directory, there\u2019s a hidden directory <code>.guild<\/code> that hold the source code, the tensorboard event and the environmental variables\u2026etc.<\/p>",
                "Answer_score":7.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-27T16:30:10.184Z",
                "Answer_body":"<p>Just to add a bit to the excellent info <a class=\"mention\" href=\"\/u\/teracamo\">@teracamo<\/a> provides\u2026<\/p>\n<p>TensorBoard doesn\u2019t really know about \u201cruns\u201d in the Scalars plugin. It enumerates unique <em>directories<\/em> that contain TF event files under it\u2019s log directory. It calls them runs in the UI, but it has not idea what a run is. In fact, it\u2019s quite common to log events under separate subdirectories for a run to help organize the layout in TB. E.g. you\u2019ll see \u201ctrain\u201d and \u201cvalidate\u201d or \u201ceval\u201d subdirs used to separate scalars.<\/p>\n<p>The reason you\u2019re seeing two separate \u201cruns\u201d there is that there are TF event files landing in separate subdirectories. That\u2019s confusing. It\u2019d be better if TB used a term other than \u201cruns\u201d. Alas, that\u2019s the way they present the info.<\/p>\n<p>The somewhat odd appearance of <code>&lt;run dir&gt;\/.guild<\/code> in this list is because Guild writes its TF event logs in a subdirectory <code>.guild<\/code>. This is to avoid possible collisions with any files that your script runs. As <a class=\"mention\" href=\"\/u\/teracamo\">@teracamo<\/a> says, it\u2019s sometimes helpful to poke around this directory to see what Guild saves with your run. You don\u2019t need to worry too much about it, but it\u2019s there in plain view if you ever need to understand something in more detail.<\/p>\n<p>Okay, to the problem at hand! As I see it there are three options to address the point of confusion:<\/p>\n<ol>\n<li>\n<p>Don\u2019t worry about it. It\u2019s okay to have multiple subdirs in TB associated with a run. Look at the two runs and in your head say, \u201cone run, one run\u201d until the problem resolves itself <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"><\/p>\n<\/li>\n<li>\n<p>If you\u2019re already logging scalars, you don\u2019t really need Guild\u2019s <a href=\"https:\/\/my.guild.ai\/t\/scalars\/160#output-scalars\">output scalar<\/a> support. You can disable it for a single operation this way:<\/p>\n<\/li>\n<\/ol>\n<pre><code class=\"lang-yaml\">op:\n  output-scalars: no\n<\/code><\/pre>\n<p>Alternatively, use the <a href=\"https:\/\/my.guild.ai\/t\/guild-file-reference\/197#operation-defaults\"><code>operation-defaults<\/code><\/a> model attr in <a href=\"https:\/\/my.guild.ai\/t\/guild-file-reference\/197#full-format\">\u201cfull format\u201d<\/a> Guild file.<\/p>\n<pre><code class=\"lang-yaml\">- operation-defaults:\n    output-scalars: no\n  operations:\n    op: ...\n\n<\/code><\/pre>\n<p>This eliminates the <code>.guild<\/code> entry in the runs list in TB. That\u2019s simple enough but you\u2019ll be responsible for logging scalars. Since you\u2019re doing that already, I think this is a pretty good option.<\/p>\n<ol start=\"3\">\n<li>Write your summaries to <code>&lt;run dir&gt;\/.guild<\/code>. This will consolidate the summaries you write with the summaries that Guild writes. I personally don\u2019t like this option and would discourage it. I think your TF event files should land wherever you want them \u2014 root of the run dir or a subdirectory. That\u2019s a pretty standard convention in TensorFlow land and writing to <code>.guild<\/code> is a bit unconventional.<\/li>\n<\/ol>\n<p>I was hoping for an <code>output-scalars<\/code> attribute that let you write to a different directory but 0.7.0 doesn\u2019t support this. I think that\u2019d be a good option 4. Something like this:<\/p>\n<pre><code class=\"lang-yaml\">- op:\n  output-scalars:\n    summary-path: .  # This is hypothetical - Guild 0.7.0 does not support this\n<\/code><\/pre>\n<p>Long winded responses but hopefully it gives you some useful background.<\/p>",
                "Answer_score":42.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"tensorboard log twice slow oper let kinda base oper run number wai differ flag oper step run oper proper flag what strang multipl log tensorboard addition view pretti slow open run view tensorboard slower take saw issu symlink dont think that issu dont set feel like issu probabl link consid wasnt have problem run configur end screen shot sure fix probabl wrong",
        "Question_preprocessed_content":"tensorboard log twice slow oper let kinda base oper run number wai differ flag oper step run oper proper flag what strang multipl log tensorboard addition pretti slow open run view tensorboard slower saw issu symlink dont think that issu dont set feel like issu probabl link consid wasnt have problem run configur end screen shot sure fix probabl wrong",
        "Question_gpt_summary_original":"The user is facing two challenges with Tensorboard. Firstly, they are getting multiple logs for everything they logged with Tensorboard. Secondly, the user is experiencing slow loading times when opening their runs in guild view and viewing them in Tensorboard, which takes around 10 seconds. The user suspects that these issues are linked and may have been caused by something they did wrong.",
        "Question_gpt_summary":"user face challeng tensorboard firstli get multipl log log tensorboard secondli user experienc slow load time open run view view tensorboard take second user suspect issu link caus wrong",
        "Answer_original_content":"duplic result tensorboard design step pipelin op inherit scalar result child run avoid us filter limit run your view tensorboard view result includ imput run tensorboard oper slow perform view your right like issu symlink youd seen poor perform tensorboard case fix view miss optim caus slow view run run view case work limit run us filter address issu tensorboard perform recommend run tensorboard launch view view seri enhanc includ perform optim workaround recommend face issu unsur resolv oper call start run start experi run entri tensorboard filter tensorboard oper start doesnt work mind share detail second entri tensorboard come possibl code write tensorboard directori work directori directori request resourc wai log result right torch util tensorboard import summarywrit self writer summarywrit self writer add scalar loss train loss train batch idx global step creat duplic entri tensorboard sai default write run guess like what happen recommend creat static directori machin write run directori cours need clean time time bleed hardisc dry time chang line self writer summarywrit static path better us env variabl specifi write track experi better tensorboard logdir dir run import tensorboard logdir environ tensorboard logdir self writer summarywrit tensorboard logdir static log directori insid run folder wont shown tensorboard right correct long home directori usual purpos log static dir trigger bit hacki actual purpos trigger tensorboard dont duplic item gener want log write flag scalar yml file tensorboard there that complic doesnt support like network paramet histogram write tensorboard logdir manag want write data output tensorboard event manag specifi logdir folder run sub directori self writer summarywrit understand run file hierarchi brows run sub directori try explor note run sub directori there hidden directori hold sourc code tensorboard event environment variablesetc add bit excel info teracamo provid tensorboard doesnt know run scalar plugin enumer uniqu directori contain event file log directori call run idea run fact common log event separ subdirectori run help organ layout youll train valid eval subdir separ scalar reason your see separ run event file land separ subdirectori that confus itd better term run ala that wai present info somewhat odd appear list write event log subdirectori avoid possibl collis file script run teracamo sai help poke directori save run dont need worri plain view need understand okai problem hand option address point confus dont worri okai multipl subdir associ run look run head run run problem resolv your log scalar dont need output scalar support disabl singl oper wai output scalar altern us oper default model attr format file oper default output scalar oper elimin entri run list that simpl youll respons log scalar your think pretti good option write summari consolid summari write summari write person dont like option discourag think event file land want root run dir subdirectori that pretti standard convent tensorflow land write bit unconvent hope output scalar attribut let write differ directori doesnt support think thatd good option like output scalar summari path hypothet support long wind respons hopefulli give us background",
        "Answer_preprocessed_content":"duplic result tensorboard design step op inherit scalar result child run avoid us filter limit run your view tensorboard view result run slow perform view your right like issu symlink view miss optim caus slow view run run view case work limit run us filter address issu tensorboard perform recommend run launch view view seri enhanc includ perform optim workaround recommend face issu unsur resolv oper call run start experi run entri tensorboard filter doesnt work mind share detail second entri tensorboard come possibl code write tensorboard directori work directori directori request resourc wai log result right creat duplic entri tensorboard sai default write guess like what happen recommend creat static directori machin write run directori chang line better us env variabl specifi write track experi better static log directori insid run folder wont shown tensorboard right correct long home directori purpos log static dir trigger bit hacki actual purpos trigger tensorboard dont duplic item gener want log write yml file tensorboard there that complic doesnt support like network paramet histogram write tensorboard logdir manag want write data output tensorboard manag specifi logdir folder run understand run file hierarchi brows run try explor note run there hidden directori hold sourc code tensorboard event environment variablesetc add bit excel info provid tensorboard doesnt know run scalar plugin enumer uniqu directori contain event file log directori call run idea run fact common log event separ subdirectori run help organ layout youll train valid eval subdir separ scalar reason your see separ run event file land separ subdirectori that confus itd better term run ala that wai present info somewhat odd appear list write event log subdirectori avoid possibl collis file script run sai help poke directori save run dont need worri plain view need understand okai problem hand option address point confus dont worri okai multipl subdir associ run look run head run run problem resolv your log scalar dont need output scalar support disabl singl oper wai altern us model attr format file elimin entri run list that simpl youll respons log scalar your think pretti good option write summari consolid summari write summari write person dont like option discourag think event file land want root run dir subdirectori that pretti standard convent tensorflow land write bit unconvent hope attribut let write differ directori doesnt support think thatd good option like long wind respons hopefulli give us background",
        "Answer_gpt_summary_original":"possible solutions to the issues with tensorboard logging twice and slow loading times when viewing runs in guild view and tensorboard are:\n\n- use a filter to limit the runs that you're viewing in tensorboard to avoid duplication of results.\n- limit the runs you show in guild view by using filters.\n- run guild tensorboard separately instead of launching it via guild view to address the issue with tensorboard performance.\n- create a static directory on your machine and write all runs to that directory to avoid triggering guild's tensorboard and showing duplicate items in it.\n- disable guild's output scalar support for a single operation or use the operation-defaults model attribute to eliminate the .guild entry in the runs list in tensorboard.\n- write your summaries to <run dir>\/.guild to consolidate the summaries you write with the summaries that guild writes.",
        "Answer_gpt_summary":"possibl solut issu tensorboard log twice slow load time view run view tensorboard us filter limit run view tensorboard avoid duplic result limit run view filter run tensorboard separ instead launch view address issu tensorboard perform creat static directori machin write run directori avoid trigger tensorboard show duplic item disabl output scalar support singl oper us oper default model attribut elimin entri run list tensorboard write summari consolid summari write summari write"
    },
    {
        "Question_id":null,
        "Question_title":"Run best model off sweep?",
        "Question_body":"<p>Hi,<br>\nI am using Sweeps to run through different configuration models and I was told by the wandb chat support that to run the best model configuration off sweeps is to create a new sweep with the best performing parameter set and running off it.<\/p>\n<p>But this is lot of tedious work, is there any other elegant way of quering wandb project for the best model configuration and running off it?<\/p>\n<p>tldr: I run a sweep with different configuration, would like to run predictions off a specific set of parameters (or best performing set of parameters). How  to do it with the sweep API?<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1652672302869,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":384.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cyrilw\">@cyrilw<\/a><\/p>\n<p>Thanks for persisting with this and posting it here, here is how you do it with the Api.<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\napi = wandb.Api()\nsweep = api.sweep(f\"_scott\/project-name\/sweeps\/qwbwbwbz\")\n\n# Get best run parameters\nbest_run = sweep.best_run(order='validation\/accuracy')\nbest_parameters = best_run.config\nprint(best_parameters)\n<\/code><\/pre>\n<p>Hope this helps <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/magic_wand.png?v=12\" title=\":magic_wand:\" class=\"emoji\" alt=\":magic_wand:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/run-best-model-off-sweep\/2423",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-16T10:11:09.278Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cyrilw\">@cyrilw<\/a><\/p>\n<p>Thanks for persisting with this and posting it here, here is how you do it with the Api.<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\napi = wandb.Api()\nsweep = api.sweep(f\"_scott\/project-name\/sweeps\/qwbwbwbz\")\n\n# Get best run parameters\nbest_run = sweep.best_run(order='validation\/accuracy')\nbest_parameters = best_run.config\nprint(best_parameters)\n<\/code><\/pre>\n<p>Hope this helps <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/magic_wand.png?v=12\" title=\":magic_wand:\" class=\"emoji\" alt=\":magic_wand:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
                "Answer_score":27.2,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-05-16T14:52:40.371Z",
                "Answer_body":"<p>Thank you for the reply.<\/p>\n<p>in this,<br>\n<code>best_run = sweep.best_run(order='validation\/accuracy')<\/code><\/p>\n<p>do I need to change the validation\/accuracy based on my sweep columns?<\/p>",
                "Answer_score":57.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-16T15:02:05.630Z",
                "Answer_body":"<p>Yes, sorry I should have said that. <code>order<\/code> is the metric you want it to order by.<\/p>\n<p>If you have set a goal in your Sweep config, it\u2019ll use that if <code>order<\/code> is not given.<\/p>\n<p>Here\u2019s the source if you\u2019re curious: <a href=\"https:\/\/wandb\/client\">https:\/\/github.com\/wandb\/client\/blob\/a339333b3ee93864daf416f04c1501186dffac5c\/wandb\/apis\/public.py#L2137<\/a><\/p>",
                "Answer_score":11.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-16T15:36:04.777Z",
                "Answer_body":"<p>I see. Just to confirm, my current sweep configuration is;<\/p>\n<pre><code class=\"lang-auto\">method: random\nmetric:\n  goal: minimize\n  name: KL\nparameters:\n  K:\n    distribution: int_uniform\n    max: 15\n    min: 3\n<\/code><\/pre>\n<p>since I do have the goal\/metric set I don\u2019t need to worry about order, right? but if not I do collect KL (column name is <code>KL<\/code>) score (kl-divergence) so I\u2019d modify the code snipet as;<\/p>\n<p><code>best_run = sweep.best_run(order='KL')<\/code><\/p>\n<p>But in case of KL, the lower is better any recommendation on how to select best_run based on it?<\/p>",
                "Answer_score":51.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-16T16:51:05.087Z",
                "Answer_body":"<p>Because your configuration has a goal, you can leave <code>order<\/code> out.<br>\nSo you can just do:<\/p>\n<pre><code class=\"lang-auto\">best_run = sweep.best_run() \n<\/code><\/pre>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-15T16:51:46.560Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1652695869278,
        "Question_original_content":"run best model sweep sweep run differ configur model told chat support run best model configur sweep creat new sweep best perform paramet set run lot tediou work eleg wai quer project best model configur run tldr run sweep differ configur like run predict specif set paramet best perform set paramet sweep api",
        "Question_preprocessed_content":"run best model sweep sweep run differ configur model told chat support run best model configur sweep creat new sweep best perform paramet set run lot tediou work eleg wai quer project best model configur run tldr run sweep differ configur like run predict specif set paramet sweep api",
        "Question_gpt_summary_original":"The user is facing a challenge in running the best model configuration off sweeps. They were advised to create a new sweep with the best performing parameter set, but find it to be a tedious task. The user is seeking an alternative and more efficient way to query the wandb project for the best model configuration and run predictions off a specific set of parameters using the sweep API.",
        "Question_gpt_summary":"user face challeng run best model configur sweep advis creat new sweep best perform paramet set tediou task user seek altern effici wai queri project best model configur run predict specif set paramet sweep api",
        "Answer_original_content":"cyrilw thank persist post api import api api sweep api sweep scott project sweep qwbwbwbz best run paramet best run sweep best run order valid accuraci best paramet best run config print best paramet hope help",
        "Answer_preprocessed_content":"thank persist post api hope help",
        "Answer_gpt_summary_original":"the answer suggests using an api to query a project for the best model configuration after running a sweep with different configurations. the code provided shows how to import the api and retrieve the best run parameters and configuration.",
        "Answer_gpt_summary":"answer suggest api queri project best model configur run sweep differ configur code provid show import api retriev best run paramet configur"
    },
    {
        "Question_id":73833320.0,
        "Question_title":"parameters error in azure ML designer in evaluation metrics in regression model",
        "Question_body":"<p>I developed a designer to implement regression models in azure machine learning studio. I have taken the data set pill and then split the data set into train and test in prescribed manner. When I am trying to implement the evaluation metrics and run the pipeline, it was showing a warning and error in the moment I called the dataset for the operation. I am bit confused, with the same implementation, when i tried to run with linear regression and it worked as shown in the image. If the same approach is used to implement logistic regression it was showing some warning and error in building the evaluation metrics.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DbEeq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DbEeq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>the above success is in linear regression. When it comes to logistic regression it was showing the warning and error in pipeline.<\/p>\n<p>Any help is appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663970010913,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":29.0,
        "Answer_body":"<p>Creating a sample pipeline with designer with mathematical format.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kCx3A.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kCx3A.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>We need to create a compute instance.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/7bQA5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7bQA5.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NQWZV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NQWZV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/MpxPY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/MpxPY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bAfEv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bAfEv.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Assign the compute instance and click on create<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wIS0d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wIS0d.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Now the import data warning will be removed. In the same manner, we will be getting similar error in other pills too.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/zFK74.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zFK74.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yk5gY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yk5gY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Create a mathematical format. If not needed for your case, try to remove that math operation and give the remaining.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/uhKlv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uhKlv.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Assign the column set. Select any option according to the requirement.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/mcNZe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mcNZe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Finally, we can find the pills which have no warning or error.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73833320",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1664021303563,
        "Question_original_content":"paramet error design evalu metric regress model develop design implement regress model studio taken data set pill split data set train test prescrib manner try implement evalu metric run pipelin show warn error moment call dataset oper bit confus implement tri run linear regress work shown imag approach implement logist regress show warn error build evalu metric success linear regress come logist regress show warn error pipelin help appreci",
        "Question_preprocessed_content":"paramet error design evalu metric regress model develop design implement regress model studio taken data set pill split data set train test prescrib manner try implement evalu metric run pipelin show warn error moment call dataset oper bit confus implement tri run linear regress work shown imag approach implement logist regress show warn error build evalu metric success linear regress come logist regress show warn error pipelin help appreci",
        "Question_gpt_summary_original":"The user encountered an error in implementing evaluation metrics in a regression model using Azure ML Designer. The error occurred when calling the dataset for the operation, and the user is confused as to why the same approach worked for linear regression but not for logistic regression. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount error implement evalu metric regress model design error occur call dataset oper user confus approach work linear regress logist regress user seek help resolv issu",
        "Answer_original_content":"creat sampl pipelin design mathemat format need creat comput instanc assign comput instanc click creat import data warn remov manner get similar error pill creat mathemat format need case try remov math oper remain assign column set select option accord requir final pill warn error",
        "Answer_preprocessed_content":"creat sampl pipelin design mathemat format need creat comput instanc assign comput instanc click creat import data warn remov manner get similar error pill creat mathemat format need case try remov math oper remain assign column set select option accord requir final pill warn error",
        "Answer_gpt_summary_original":"the possible solutions to the challenge of encountering parameter errors in the designer when implementing evaluation metrics in a regression model are: creating a sample pipeline with a mathematical format, creating a compute instance, assigning the compute instance, removing the import data warning, creating a mathematical format for other pills, removing unnecessary math operations, assigning the column set, selecting an option according to the requirement, and finding pills with no warning or error.",
        "Answer_gpt_summary":"possibl solut challeng encount paramet error design implement evalu metric regress model creat sampl pipelin mathemat format creat comput instanc assign comput instanc remov import data warn creat mathemat format pill remov unnecessari math oper assign column set select option accord requir find pill warn error"
    },
    {
        "Question_id":71505415.0,
        "Question_title":"\"The timestamp column must have valid timestamp entries.\" error when using `timestamp_split_column_name` arg in `AutoMLTabularTrainingJob.run`",
        "Question_body":"<p>From <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html#google.cloud.aiplatform.AutoMLTabularTrainingJob.run\" rel=\"nofollow noreferrer\">the docs<\/a> it says that<\/p>\n<blockquote>\n<p>The value of the key values of the key (the values in the column) must be in RFC 3339 date-time format, where time-offset = \u201cZ\u201d (e.g. 1985-04-12T23:20:50.52Z)<\/p>\n<\/blockquote>\n<p>The dataset that I'm pointing to is a CSV in cloud storage, where the data is in the format suggested by the docs:<\/p>\n<pre><code>$ gsutil cat gs:\/\/my-data.csv | head | xsv select TS_SPLIT_COL\nTS_SPLIT_COL\n2021-01-18T00:00:00.00Z\n2021-01-18T00:00:00.00Z\n2021-01-04T00:00:00.00Z\n2021-03-06T00:00:00.00Z\n2021-01-15T00:00:00.00Z\n2021-02-11T00:00:00.00Z\n2021-02-05T00:00:00.00Z\n2021-05-20T00:00:00.00Z\n2021-01-05T00:00:00.00Z\n<\/code><\/pre>\n<p>But I receive a <code>Training pipeline failed with error message: The timestamp column must have valid timestamp entries.<\/code> error when I try to run a training job<\/p>\n<p>EDIT: this should hopefully make it more reproducible<\/p>\n<p>data: <a href=\"https:\/\/pastebin.com\/qEDqvzX6\" rel=\"nofollow noreferrer\">https:\/\/pastebin.com\/qEDqvzX6<\/a><\/p>\n<p>Code I'm running:<\/p>\n<pre><code>from google.cloud import aiplatform\n\nPROJECT = &quot;my-project&quot;\nDATASET_ID = &quot;dataset-id&quot;  # points to CSV \n\naiplatform.init(project=PROJECT)\n\ndataset = aiplatform.TabularDataset(DATASET_ID)\n\njob = aiplatform.AutoMLTabularTrainingJob(\n    display_name=&quot;so-58454722&quot;,\n    optimization_prediction_type=&quot;classification&quot;,\n    optimization_objective=&quot;maximize-au-roc&quot;,\n)\n\nmodel = job.run(\n    dataset=dataset,\n    model_display_name=&quot;so-58454722&quot;,\n    target_column=&quot;Y&quot;,\n    training_fraction_split=0.8,\n    validation_fraction_split=0.1,\n    test_fraction_split=0.1,\n    timestamp_split_column_name=&quot;TS_SPLIT_COL&quot;,\n)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1647473614997,
        "Question_favorite_count":null,
        "Question_last_edit_time":1647534964220,
        "Question_score":0.0,
        "Question_view_count":157.0,
        "Answer_body":"<p>Try this timestamp format instead:<\/p>\n<p><code>2022-03-18T01:23:45.123456+00:00<\/code><\/p>\n<p>It uses <code>+00:00<\/code> instead of <code>Z<\/code> to specify timezone.<\/p>\n<p>This change will eliminate the &quot;The timestamp column must have valid timestamp entries.&quot; error<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71505415",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1647602028112,
        "Question_original_content":"timestamp column valid timestamp entri error timestamp split column arg automltabulartrainingjob run doc sai valu kei valu kei valu column rfc date time format time offset dataset point csv cloud storag data format suggest doc gsutil cat data csv head xsv select split col split col receiv train pipelin fail error messag timestamp column valid timestamp entri error try run train job edit hopefulli reproduc data http pastebin com qedqvzx code run googl cloud import aiplatform project project dataset dataset point csv aiplatform init project project dataset aiplatform tabulardataset dataset job aiplatform automltabulartrainingjob displai optim predict type classif optim object maxim roc model job run dataset dataset model displai target column train fraction split valid fraction split test fraction split timestamp split column split col",
        "Question_preprocessed_content":"timestamp column valid timestamp error arg doc sai valu kei valu kei rfc format dataset point csv cloud storag data format suggest doc receiv error try run train job edit hopefulli reproduc data code run",
        "Question_gpt_summary_original":"The user is encountering an error when running a training job using `AutoMLTabularTrainingJob.run` with the `timestamp_split_column_name` argument. The error message states that the timestamp column must have valid timestamp entries in RFC 3339 date-time format with time-offset = \u201cZ\u201d. The user's dataset is in the correct format, but they are still receiving the error.",
        "Question_gpt_summary":"user encount error run train job automltabulartrainingjob run timestamp split column argument error messag state timestamp column valid timestamp entri rfc date time format time offset user dataset correct format receiv error",
        "Answer_original_content":"try timestamp format instead us instead specifi timezon chang elimin timestamp column valid timestamp entri error",
        "Answer_preprocessed_content":"try timestamp format instead us instead specifi timezon chang elimin timestamp column valid timestamp error",
        "Answer_gpt_summary_original":"the solution to the error encountered when using the `timestamp_split_column_name` arg in `automltabulartrainingjob.run` is to use a different timestamp format that specifies the timezone using `+00:00` instead of `z`. this change will eliminate the error message \"the timestamp column must have valid timestamp entries.\"",
        "Answer_gpt_summary":"solut error encount timestamp split column arg automltabulartrainingjob run us differ timestamp format specifi timezon instead chang elimin error messag timestamp column valid timestamp entri"
    },
    {
        "Question_id":49006174.0,
        "Question_title":"How is the entry point to the code specified in AWS sagemaker bring your own code?",
        "Question_body":"<p>I'm trying out the sample notebooks in AWS Sagemaker, currently in the mxnet mnist example which demonstrates bringing your own code. The entry point parameter passed in when instantiating an estimator instance, only mentions the source file (mnist.py) and not a method name or any other point inside the source file.<\/p>\n\n<p>So how does aws sagemaker figure out which method to send the training data to? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1519726485703,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1519802441487,
        "Question_score":1.0,
        "Question_view_count":3327.0,
        "Answer_body":"<p>Your python script should implement a few methods like train, model_fn, transform_fn, input_fn etc. SagaMaker would call appropriate method when needed. <\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet-training-inference-code-template.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet-training-inference-code-template.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49006174",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1520316895088,
        "Question_original_content":"entri point code specifi bring code try sampl notebook current mxnet mnist exampl demonstr bring code entri point paramet pass instanti estim instanc mention sourc file mnist method point insid sourc file figur method send train data",
        "Question_preprocessed_content":"entri point code specifi bring code try sampl notebook current mxnet mnist exampl demonstr bring code entri point paramet pass instanti estim instanc mention sourc file method point insid sourc file figur method send train data",
        "Question_gpt_summary_original":"The user is facing a challenge in AWS Sagemaker while trying to specify the entry point to the code for the mxnet mnist example. The entry point parameter only mentions the source file and not a method name or any other point inside the source file, which makes it difficult for the user to understand how AWS Sagemaker figures out which method to send the training data to.",
        "Question_gpt_summary":"user face challeng try specifi entri point code mxnet mnist exampl entri point paramet mention sourc file method point insid sourc file make difficult user understand figur method send train data",
        "Answer_original_content":"python script implement method like train model transform input sagamak appropri method need http doc aw amazon com latest mxnet train infer code templat html",
        "Answer_preprocessed_content":"python script implement method like train sagamak appropri method need",
        "Answer_gpt_summary_original":"to specify the entry point to the code when using bring your own code in the mxnet mnist example, the user needs to implement methods like train, model_fn, transform_fn, input_fn, etc. in their python script.  will then call the appropriate method when needed. the documentation provides a code template for reference.",
        "Answer_gpt_summary":"specifi entri point code bring code mxnet mnist exampl user need implement method like train model transform input python script appropri method need document provid code templat refer"
    },
    {
        "Question_id":null,
        "Question_title":"How to export data from compute instance to Datastore",
        "Question_body":"I pull in data from a Datastore to the compute instance using the code below. I then do stuff with that data. What I'd like to do is to push the modified data back to the Datastore but I'm not finding the documentation that can show me how this is done.\n\n# Azure management\nfrom azureml.core import Workspace, Dataset\n# MetaData\nsubscription_id = '09b5fdb3-165d-4e2b-8ca0-34f998d176d5'\nresource_group = 'xCloudData'\nworkspace_name = 'xCloudML'\n# Create workspace \nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n# Retention_Engagement_CombinedData from cosmos ss\ndataset = Dataset.get_by_name(workspace, name='retention-engagement-combineddata')\n# Save data to file\ndf = dataset.to_pandas_dataframe()\ndf.to_csv('\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/v-aantico1\/code\/RetentionEngagement_CombinedData.csv')\n# Do stuff with data\n...\n# Push data back to Datastore\n...",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1631557345167,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/550288\/how-to-export-data-from-compute-instance-to-datast.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-04T15:25:09.183Z",
                "Answer_score":0,
                "Answer_body":"@AdrianAnticoTEKsystemsInc-1526\n\nThanks for the feedback again, I saw the deprecated method as well. I have forwarded this topic to product team, hope we can have some official guidance about this or workaround to help.\n\nRegards,\nYutong",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"export data comput instanc datastor pull data datastor comput instanc code stuff data like push modifi data datastor find document azur manag core import workspac dataset metadata subscript bfdb fdd resourc group xclouddata workspac xcloudml creat workspac workspac workspac subscript resourc group workspac retent engag combineddata cosmo dataset dataset workspac retent engag combineddata save data file dataset panda datafram csv mnt batch task share root mount cluster aantico code retentionengag combineddata csv stuff data push data datastor",
        "Question_preprocessed_content":"export data comput instanc datastor pull data datastor comput instanc code stuff data like push modifi data datastor find document azur manag core import workspac dataset metadata xclouddata xcloudml creat workspac workspac cosmo dataset save data file stuff data push data datastor",
        "Question_gpt_summary_original":"The user is facing a challenge in finding documentation on how to push modified data back to the Datastore after pulling in data from a Datastore to the compute instance and doing some modifications.",
        "Question_gpt_summary":"user face challeng find document push modifi data datastor pull data datastor comput instanc modif",
        "Answer_original_content":"adriananticoteksystemsinc thank feedback saw deprec method forward topic product team hope offici guidanc workaround help regard yutong",
        "Answer_preprocessed_content":"thank feedback saw deprec method forward topic product team hope offici guidanc workaround help regard yutong",
        "Answer_gpt_summary_original":"there are no specific solutions mentioned in the answer. the user has reported an issue with exporting data from a compute instance to a datastore after performing operations on the data. the person who responded has acknowledged the feedback and forwarded the issue to the product team for official guidance or a workaround.",
        "Answer_gpt_summary":"specif solut mention answer user report issu export data comput instanc datastor perform oper data person respond acknowledg feedback forward issu product team offici guidanc workaround"
    },
    {
        "Question_id":65465114.0,
        "Question_title":"AWS Sagemaker AttributeError: can't set attribute error",
        "Question_body":"<p>I am new to python programming. Following the AWS learning path:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult<\/a><\/p>\n<p>I am getting an error when excuting the following block (in conda_python3):<\/p>\n<pre><code>test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\nxgb_predictor.content_type = 'text\/csv' # set the data type for an inference\nxgb_predictor.serializer = csv_serializer # set the serializer type\npredictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\npredictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an \narray\nprint(predictions_array.shape)\n<\/code><\/pre>\n<blockquote>\n<p>AttributeError                            Traceback (most recent call last)\n in \n1 test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\n----&gt; 2 xgb_predictor.content_type = 'text\/csv' # set the data type for an inference\n3 xgb_predictor.serializer = csv_serializer # set the serializer type\n4 predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n5 predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array<\/p>\n<\/blockquote>\n<blockquote>\n<p>AttributeError: can't set attribute<\/p>\n<\/blockquote>\n<p>I have looked at several prior questions but couldn't find much information related to this error when it comes to creating data types.<\/p>\n<p>Thanks in advance for any help.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1609067497070,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":2479.0,
        "Answer_body":"<p>If you just remove it then the prediction will work. Therefore, recommend removing this code line.\nxgb_predictor.content_type = 'text\/csv'<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":8.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65465114",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1609399068572,
        "Question_original_content":"attributeerror set attribut error new python program follow aw learn path http aw amazon com get start hand build train deploi machin learn model trk apywilaa trkcampaign tutori channel campaign data scientist hand tutori outcom product market geo mult get error excut follow block conda python test data arrai test data drop ye axi valu load data arrai xgb predictor content type text csv set data type infer xgb predictor serial csv serial set serial type predict xgb predictor predict test data arrai decod utf predict predict arrai fromstr predict sep turn predict arrai print predict arrai shape attributeerror traceback recent test data arrai test data drop ye axi valu load data arrai xgb predictor content type text csv set data type infer xgb predictor serial csv serial set serial type predict xgb predictor predict test data arrai decod utf predict predict arrai fromstr predict sep turn predict arrai attributeerror set attribut look prior question couldn inform relat error come creat data type thank advanc help",
        "Question_preprocessed_content":"attributeerror set attribut error new python program follow aw learn path get error excut follow block attributeerror traceback load data arrai set data type infer set serial type predict predict sep turn predict arrai attributeerror set attribut look prior question couldn inform relat error come creat data type thank advanc help",
        "Question_gpt_summary_original":"The user encountered an AttributeError while executing a block of code in AWS Sagemaker. The error occurred when trying to set the data type for an inference, and the user has not been able to find much information related to this error when creating data types.",
        "Question_gpt_summary":"user encount attributeerror execut block code error occur try set data type infer user abl inform relat error creat data type",
        "Answer_original_content":"remov predict work recommend remov code line xgb predictor content type text csv",
        "Answer_preprocessed_content":"remov predict work recommend remov code line",
        "Answer_gpt_summary_original":"solution: the solution to the \"attributeerror: can't set attribute\" error is to remove the code line that sets the data type for an inference. the recommendation is to remove the line \"xgb_predictor.content_type = 'text\/csv'\" and the prediction will work.",
        "Answer_gpt_summary":"solut solut attributeerror set attribut error remov code line set data type infer recommend remov line xgb predictor content type text csv predict work"
    },
    {
        "Question_id":null,
        "Question_title":"Unable to log each run when using pytorch lightning integration",
        "Question_body":"<p>I\u2019m able to log a training run with pytorch lightning + wandb based on <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/lightning\">these instructions<\/a> in google colab.  Here\u2019s a snippet of code I\u2019m running:<\/p>\n<pre><code class=\"lang-auto\">wandb_logger = WandbLogger(project=\"p\", entity=\"e\")\ntrainer = pl.Trainer(\n    logger=wandb_logger,    # W&amp;B integration\n    ..\n)\ntrainer.fit(model)\n<\/code><\/pre>\n<p>it outputs the link to the run and I can see all of the stats etc.<\/p>\n<p>However, how can I retrain?  If I try re-training with:<\/p>\n<pre><code class=\"lang-auto\">trainer = pl.Trainer(\n    logger=wandb_logger,    # W&amp;B integration\n    ..\n)\ntrainer.fit(model)\n<\/code><\/pre>\n<p>It doesn\u2019t seem to log a new run.  It looks like it doesn\u2019t even log the data to the existing run, it is just completely lost.<\/p>\n<p>If I try to create a new wandb logger before the re-training:<\/p>\n<pre><code class=\"lang-auto\">wandb_logger = WandbLogger(project=\"p\", entity=\"e\")\ntrainer = pl.Trainer(\n    logger=wandb_logger,    # W&amp;B integration\n    ..\n)\ntrainer.fit(model)\n<\/code><\/pre>\n<p>it times out after 1 minute with this error:<\/p>\n<pre><code class=\"lang-auto\">andb: ERROR Error communicating with wandb process\nwandb: ERROR For more info see: https:\/\/docs.wandb.ai\/library\/init#init-start-error\nProblem at: \/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/wandb.py 406 experiment\n---------------------------------------------------------------------------\nUsageError                                Traceback (most recent call last)\n&lt;ipython-input-44-6016437e3426&gt; in &lt;module&gt;\n----&gt; 1 wandb_logger = WandbLogger(project=\"p\", entity=\"e\")\n\n6 frames\n\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/wandb_init.py in init(self)\n    717                     backend.cleanup()\n    718                     self.teardown()\n--&gt; 719                 raise UsageError(error_message)\n    720             assert run_result and run_result.run\n    721             if run_result.run.resumed:\n\nUsageError: Error communicating with wandb process\nFor more info see: https:\/\/docs.wandb.ai\/library\/init#init-start-error```\n\nAm I using wandb + pytorch lightning the correct way?  What is the expected lifecycle of the wandb logger in relation to the pl training object?<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1668811355261,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":193.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/unable-to-log-each-run-when-using-pytorch-lightning-integration\/3443",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-19T00:32:56.410Z",
                "Answer_body":"<p>Actually nevermind, it\u2019s working now!  I don\u2019t know what I changed that fixed it while I was trying to debug, maybe I had forgotten to call .finish() on the first run.<\/p>\n<p>Here\u2019s the gist of the code I\u2019m running:<\/p>\n<pre><code class=\"lang-auto\">wandb_logger = WandbLogger(project=\"p\", entity=\"e\", log_model=True)\ntrainer = pl.Trainer(\n    logger=wandb_logger,    # W&amp;B integration\n    ..\n)\ntrainer.fit(model)\nwandb.finish()\n<\/code><\/pre>\n<p>I\u2019m able to run the above snippet repeatedly and it creates a new run each time.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-18T00:33:02.616Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"unabl log run pytorch lightn integr abl log train run pytorch lightn base instruct googl colab here snippet code run logger logger project entiti trainer trainer logger logger integr trainer fit model output link run stat retrain try train trainer trainer logger logger integr trainer fit model doesnt log new run look like doesnt log data exist run complet lost try creat new logger train logger logger project entiti trainer trainer logger logger integr trainer fit model time minut error andb error error commun process error info http doc librari init init start error problem usr local lib python dist packag pytorch lightn logger experi usageerror traceback recent logger logger project entiti frame usr local lib python dist packag sdk init init self backend cleanup self teardown rais usageerror error messag assert run result run result run run result run resum usageerror error commun process info http doc librari init init start error pytorch lightn correct wai expect lifecycl logger relat train object",
        "Question_preprocessed_content":"unabl log run pytorch lightn integr abl log train run pytorch lightn base instruct googl colab here snippet code run output link run stat retrain try doesnt log new run look like doesnt log data exist run complet lost try creat new logger time minut error",
        "Question_gpt_summary_original":"The user is facing challenges while trying to retrain a model using PyTorch Lightning and Wandb integration. The user is unable to log a new run and it seems that the data is completely lost. When the user tries to create a new Wandb logger before retraining, it times out after 1 minute with an error. The user is seeking guidance on the correct way to use Wandb + PyTorch Lightning and the expected lifecycle of the Wandb logger in relation to the PL training object.",
        "Question_gpt_summary":"user face challeng try retrain model pytorch lightn integr user unabl log new run data complet lost user tri creat new logger retrain time minut error user seek guidanc correct wai us pytorch lightn expect lifecycl logger relat train object",
        "Answer_original_content":"actual nevermind work dont know chang fix try debug mayb forgotten finish run here gist code run logger logger project entiti log model true trainer trainer logger logger integr trainer fit model finish abl run snippet repeatedli creat new run time topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"actual nevermind work dont know chang fix try debug mayb forgotten finish run here gist code run abl run snippet repeatedli creat new run time topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no possible solutions mentioned in the answer as the user states that the issue has been resolved and they are now able to log each run using pytorch lightning integration. the user suggests that they may have forgotten to call .finish() on the first run, but they provide the code snippet they are currently using which creates a new run each time it is run.",
        "Answer_gpt_summary":"possibl solut mention answer user state issu resolv abl log run pytorch lightn integr user suggest forgotten finish run provid code snippet current creat new run time run"
    },
    {
        "Question_id":null,
        "Question_title":"AzureML endpoint - gunicorn worker timeout",
        "Question_body":"Hello everyone,\nI am trying to deploy a large model using AzureML endpoint.\nThe model is made up of many sub-models which get loaded by the init() method as described in the documentation here.\nThe model is trained and then registered in AzureML.\n\nWhen I deploy the model I can see in the logs that the gunicorn worker resets themselves after 300 seconds, so the whole ensemble of sub-models never have time to completely be loaded.\n\nIs there a way to manually set the timeout of the gunicorn workers?",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1630071044527,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/530811\/azureml-endpoint-gunicorn-worker-timeout.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-31T08:14:45.617Z",
                "Answer_score":2,
                "Answer_body":"Hi @ramr-msft thanks for the time.\nSo I have a notebook that I use for deploying. The steps are the following:\nFirst I connect to the Workspace and select a registered model and a registered env.\n\n ws = Workspace.from_config(path=\".\/config.json\")\n model = Model(ws, 'Model_Name')\n env = Environment.get(workspace=ws, name=\"condaenv\")\n\n\n\nThen I create a inference config and deploy config with this cell:\n\n inference_config = InferenceConfig(\n     environment=env,\n     source_directory=\"..\/lib\",\n     entry_script=\"azureml\/score.py\",\n )\n deploy_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=4)\n\n\n\nMy score.py has the required two methods init() and run() plus a couple of methods to postprocess the inferred data:\n\n def init():\n global model\n model = MyModel.load(os.getenv('AZUREML_MODEL_DIR'))\n    \n def run(request):\n predictions = model.predict(request)\n return postprocess(predictons)\n\n\n\n\nThe actual deployment is then done by running the next cell:\n\n service = Model.deploy(\n     ws,\n     \"model-service\",\n     [model],\n     inference_config,\n     deploy_config, \n     overwrite=True,\n     show_output=True,\n )\n    \n service.wait_for_deployment(show_output=True)\n\n\n\n\n\nSo then AzureML work his magic stuff until then it runs the init method in score.py.\nProblem starts here, since the loading of all the sub-models takes more than 300s and the workers timeout and are killed, then a new one is spawned but then the cycle starts again.\n\nIn other project (which did not involve using AzureML endpoints) I had a more direct access to gunicorn configuration and I could change the timeout or set the loading of the module to complete before workers were spawn.\n\nIs something similar possible with AzureML endpoints?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-31T20:35:13.437Z",
                "Answer_score":0,
                "Answer_body":"@Matteo-0936 @ramr-msft Was there ever a solution found to this problem?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"endpoint gunicorn worker timeout hello try deploi larg model endpoint model sub model load init method describ document model train regist deploi model log gunicorn worker reset second ensembl sub model time complet load wai manual set timeout gunicorn worker",
        "Question_preprocessed_content":"endpoint gunicorn worker timeout hello try deploi larg model endpoint model load init method describ document model train regist deploi model log gunicorn worker reset second ensembl time complet load wai manual set timeout gunicorn worker",
        "Question_gpt_summary_original":"The user is facing challenges with deploying a large model using AzureML endpoint. The gunicorn worker resets after 300 seconds, which does not allow the sub-models to be completely loaded. The user is seeking a way to manually set the timeout of the gunicorn workers.",
        "Question_gpt_summary":"user face challeng deploi larg model endpoint gunicorn worker reset second allow sub model complet load user seek wai manual set timeout gunicorn worker",
        "Answer_original_content":"ramr msft thank time notebook us deploi step follow connect workspac select regist model regist env workspac config path config json model model model env environ workspac condaenv creat infer config deploi config cell infer config inferenceconfig environ env sourc directori lib entri script score deploi config aciwebservic deploi configur cpu core memori score requir method init run plu coupl method postprocess infer data def init global model model mymodel load getenv model dir def run request predict model predict request return postprocess predicton actual deploy run cell servic model deploi model servic model infer config deploi config overwrit true output true servic wait deploy output true work magic stuff run init method score problem start load sub model take worker timeout kill new spawn cycl start project involv endpoint direct access gunicorn configur chang timeout set load modul complet worker spawn similar possibl endpoint matteo ramr msft solut problem",
        "Answer_preprocessed_content":"thank time notebook us deploi step follow connect workspac select regist model regist env model model env condaenv creat infer config deploi config cell inferenceconfig requir method init run plu coupl method postprocess infer data def init global model model def run predict return postprocess actual deploy run cell servic overwrit true work magic stuff run init method problem start load take worker timeout kill new spawn cycl start project direct access gunicorn configur chang timeout set load modul complet worker spawn similar possibl endpoint solut problem",
        "Answer_gpt_summary_original":"the user is encountering an issue with gunicorn workers timing out before their large model can be fully loaded when deploying with endpoint. the answer suggests that the problem starts when the loading of all the sub-models takes more than 300s and the workers timeout and are killed. the user had a more direct access to gunicorn configuration in another project and could change the timeout or set the loading of the module to complete before workers were spawned. the answer does not provide any specific solutions for this problem.",
        "Answer_gpt_summary":"user encount issu gunicorn worker time larg model fulli load deploi endpoint answer suggest problem start load sub model take worker timeout kill user direct access gunicorn configur project chang timeout set load modul complet worker spawn answer provid specif solut problem"
    },
    {
        "Question_id":70560288.0,
        "Question_title":"DVC Shared Windows Directory Setup",
        "Question_body":"<p>I have one Linux machine and one Windows machine for developments. For data sharing, we have set up a shared Windows directory in another Windows machine, which both my Linux and Windows can access.<\/p>\n<p>I am now using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> for version control of the shared data. To make it easy, I mount the shared Windows folder both in Windows and in Linux development machine. In Windows, it looks like<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>In Linux, it looks like:<\/p>\n<pre><code>[core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>As you can see, Windows and Linux have different mounting points. So my question is: is there a way to make that both Windows and Linux have the same <code>\u00f9rl<\/code> in the DVC configuration file?<\/p>\n<p>If this is impossible, is there another alternative solution for DVC keeps data in remote shared Windows folder? Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641163263100,
        "Question_favorite_count":null,
        "Question_last_edit_time":1641199464667,
        "Question_score":1.0,
        "Question_view_count":128.0,
        "Answer_body":"<p>If you are using a local remote this way, you won't be able to have to the same <code>url<\/code> on both platforms since the mount points are different (as you already realized).<\/p>\n<p>The simplest way to configure this would be to pick one (Linux or Windows) <code>url<\/code> to use as your default case that gets git-committed into <code>.dvc\/config<\/code>. On the other platform you (or your users) can override that <code>url<\/code> in the local configuration file: <code>.dvc\/config.local<\/code>.<\/p>\n<p>(Note that <code>.dvc\/config.local<\/code> is a git-ignored file and will not be included in any commits)<\/p>\n<p>So if you wanted Windows to be the default case, in <code>.dvc\/config<\/code> you would have:<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>and on your Linux machine you would add the file <code>.dvc\/config.local<\/code> containing:<\/p>\n<pre><code>['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>See the DVC docs for <code>dvc config --local<\/code> and <code>dvc remote modify --local<\/code> for more details:<\/p>\n<ul>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/config#description<\/a><\/li>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70560288",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1641179335767,
        "Question_original_content":"share window directori setup linux machin window machin develop data share set share window directori window machin linux window access version control share data easi mount share window folder window linux develop machin window look like core analyt fals remot remot storag remot remot storag url share storag project dir linux look like core analyt fals remot remot storag remot remot storag url mnt mount point project dir window linux differ mount point question wai window linux configur file imposs altern solut keep data remot share window folder thank",
        "Question_preprocessed_content":"share window directori setup linux machin window machin develop data share set share window directori window machin linux window access version control share data easi mount share window folder window linux develop machin window look like linux look like window linux differ mount point question wai window linux configur file imposs altern solut keep data remot share window folder thank",
        "Question_gpt_summary_original":"The user has set up a shared Windows directory between a Linux and a Windows machine for data sharing. They are using DVC for version control of the shared data and have mounted the shared Windows folder in both Windows and Linux development machines. However, Windows and Linux have different mounting points, and the user is looking for a way to have the same URL in the DVC configuration file for both operating systems. The user is seeking an alternative solution for DVC to keep data in a remote shared Windows folder if this is not possible.",
        "Question_gpt_summary":"user set share window directori linux window machin data share version control share data mount share window folder window linux develop machin window linux differ mount point user look wai url configur file oper system user seek altern solut data remot share window folder possibl",
        "Answer_original_content":"local remot wai won abl url platform mount point differ realiz simplest wai configur pick linux window url us default case get git commit config platform user overrid url local configur file config local note config local git ignor file includ commit want window default case config core analyt fals remot remot storag remot remot storag url share storag project dir linux machin add file config local contain remot remot storag url mnt mount point project dir doc config local remot modifi local detail http org doc command refer config descript http org doc command refer remot modifi command option flag",
        "Answer_preprocessed_content":"local remot wai won abl platform mount point differ simplest wai configur pick us default case get platform overrid local configur file note file includ commit want window default case linux machin add file contain doc detail",
        "Answer_gpt_summary_original":"the solution suggested is to pick one url (either linux or windows) as the default case and commit it into the .\/config file. on the other platform, users can override the default url in the local configuration file .\/config.local. the .\/config.local file is git-ignored and will not be included in any commits. the answer also provides links to the documentation for more details on config --local and remote modify --local.",
        "Answer_gpt_summary":"solut suggest pick url linux window default case commit config file platform user overrid default url local configur file config local config local file git ignor includ commit answer provid link document detail config local remot modifi local"
    },
    {
        "Question_id":72073763.0,
        "Question_title":"GCP Vertex Pipeline - Why kfp.v2.dsl.Output as function arguments work without being provided?",
        "Question_body":"<p>Why <code> kfp.v2.dsl.Output<\/code> as function argument works without being provided?<\/p>\n<p>I am following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/training-data-analyst\/blob\/master\/courses\/machine_learning\/deepdive2\/machine_learning_in_the_enterprise\/solutions\/create_run_vertex_pipeline.ipynb\" rel=\"nofollow noreferrer\">Create and run ML pipelines with Vertex Pipelines!<\/a> Jupyter notebook example from GCP.<\/p>\n<p>The function <code>classif_model_eval_metrics<\/code> takes <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> which have no default values.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],                # No default value set, hence must be mandatory\n    metricsc: Output[ClassificationMetrics], # No default value set, hence must be mandatory\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]): \n    # Full code at the bottom.\n<\/code><\/pre>\n<p>Hence those arguments should be mandatory, but the function is called without those arguments.<\/p>\n<pre><code>    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p>The entire pipeline code is below.<\/p>\n<pre><code>@kfp.dsl.pipeline(name=&quot;automl-tab-beans-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = &quot;bq:\/\/aju-dev-demos.beans.beans1&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auRoc&quot;: 0.95}',\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=display_name, bq_source=bq_source\n    )\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=COLUMNS,\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;Class&quot;,\n    )\n    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Why does it work and what are <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> of type <code>kfp.v2.dsl.Output<\/code>?<\/p>\n<hr \/>\n<h2>classif_model_eval_metrics function code<\/h2>\n<pre><code>from kfp.v2.dsl import (\n    Dataset, Model, Output, Input, \n    OutputPath, ClassificationMetrics, Metrics, component\n)\n\n@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]):  # Return parameter.\n    &quot;&quot;&quot;Renders evaluation metrics for an AutoML Tabular classification model.\n    Retrieves the classification model evaluation and render the ROC and confusion matrix\n    for the model. Determine whether the model is sufficiently accurate to deploy.\n    &quot;&quot;&quot;\n    import json\n    import logging\n    from google.cloud import aiplatform\n\n    # Fetch model eval info\n    def get_eval_info(client, model_name):\n        from google.protobuf.json_format import MessageToDict\n        response = client.list_model_evaluations(parent=model_name)\n        metrics_list = []\n        metrics_string_list = []\n        for evaluation in response:\n            metrics = MessageToDict(evaluation._pb.metrics)\n            metrics_str = json.dumps(metrics)\n            metrics_list.append(metrics)\n            metrics_string_list.append(metrics_str)\n        return (\n            evaluation.name,\n            metrics_list,\n            metrics_string_list,\n        )\n\n    def classification_thresholds_check(metrics_dict, thresholds_dict):\n        for k, v in thresholds_dict.items():\n            if k in [&quot;auRoc&quot;, &quot;auPrc&quot;]:  # higher is better\n                if metrics_dict[k] &lt; v:  # if under threshold, don't deploy\n                    return False\n        return True\n\n    def log_metrics(metrics_list, metricsc):\n        test_confusion_matrix = metrics_list[0][&quot;confusionMatrix&quot;]\n        logging.info(&quot;rows: %s&quot;, test_confusion_matrix[&quot;rows&quot;])\n        # log the ROC curve\n        fpr = [], tpr = [], thresholds = []\n        for item in metrics_list[0][&quot;confidenceMetrics&quot;]:\n            fpr.append(item.get(&quot;falsePositiveRate&quot;, 0.0))\n            tpr.append(item.get(&quot;recall&quot;, 0.0))\n            thresholds.append(item.get(&quot;confidenceThreshold&quot;, 0.0))\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        # log the confusion matrix\n        annotations = []\n        for item in test_confusion_matrix[&quot;annotationSpecs&quot;]:\n            annotations.append(item[&quot;displayName&quot;])\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n        # log textual metrics info as well\n        for metric in metrics_list[0].keys():\n            if metric != &quot;confidenceMetrics&quot;:\n                val_string = json.dumps(metrics_list[0][metric])\n                metrics.log_metric(metric, val_string)\n        # metrics.metadata[&quot;model_type&quot;] = &quot;AutoML Tabular classification&quot;\n\n    aiplatform.init(project=project)\n\n    client = aiplatform.gapic.ModelServiceClient(client_options={&quot;api_endpoint&quot;: api_endpoint})\n    eval_name, metrics_list, metrics_str_list = get_eval_info(\n        client, model.uri.replace(&quot;aiplatform:\/\/v1\/&quot;, &quot;&quot;)\n    )\n    log_metrics(metrics_list, metricsc)\n    thresholds_dict = json.loads(thresholds_dict_str)\n\n    return (&quot;true&quot;,) if classification_thresholds_check(metrics_list[0], thresholds_dict) else (&quot;false&quot;, )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1651375335713,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":358.0,
        "Answer_body":"<p>The custom component is defined as a Python function with a <code>@kfp.v2.dsl.component<\/code> decorator.<\/p>\n<p>The <code>@component<\/code> decorator specifies three optional arguments: the base container image to use; any packages to install; and the yaml file to which to write the component specification.<\/p>\n<p>The component function, <code>classif_model_eval_metrics<\/code>, has some input parameters.  The model parameter is an input <code>kfp.v2.dsl.Model artifact<\/code>.<\/p>\n<p>The two function args, <code>metrics<\/code> and <code>metricsc<\/code>, are component Outputs, in this case of types Metrics and ClassificationMetrics. They\u2019re not explicitly passed as inputs to the component step, but rather are automatically instantiated and can be used in the component.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;,\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,  # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n)\n<\/code><\/pre>\n<p>For example, in the function below, we\u2019re calling <code>metricsc.log_roc_curve()<\/code> and <code>metricsc.log_confusion_matrix()<\/code> to render these visualizations in the Pipelines UI. These Output params become component outputs when the component is compiled, and can be consumed by other pipeline steps.<\/p>\n<pre><code>def log_metrics(metrics_list, metricsc):\n        ...\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        ...\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n<\/code><\/pre>\n<p>For more information you can refer to this <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/use-vertex-pipelines-build-automl-classification-end-end-workflow\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72073763",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1652802212843,
        "Question_original_content":"gcp vertex pipelin kfp dsl output function argument work provid kfp dsl output function argument work provid follow creat run pipelin vertex pipelin jupyt notebook exampl gcp function classif model eval metric take metric output metric metricsc output classificationmetr default valu compon base imag gcr deeplearn platform releas cpu latest output compon file tabl eval compon yaml option us load compon later packag instal googl cloud aiplatform def classif model eval metric project str locat str central api endpoint str central aiplatform googleapi com threshold dict str str model input model metric output metric default valu set mandatori metricsc output classificationmetr default valu set mandatori namedtupl output dep decis str code argument mandatori function call argument model eval task classif model eval metric project gcp region api endpoint threshold dict str train output model namedtupl output dep decis str return paramet render evalu metric automl tabular classif model retriev classif model evalu render roc confus matrix model determin model suffici accur deploi import json import log googl cloud import aiplatform fetch model eval info def eval info client model googl protobuf json format import messagetodict respons client list model evalu parent model metric list metric string list evalu respons metric messagetodict evalu metric metric str json dump metric metric list append metric metric string list append metric str return evalu metric list metric string list def classif threshold check metric dict threshold dict threshold dict item auroc auprc higher better metric dict threshold deploi return fals return true def log metric metric list metricsc test confus matrix metric list confusionmatrix log info row test confus matrix row log roc curv fpr tpr threshold item metric list confidencemetr fpr append item falsepositiver tpr append item recal threshold append item confidencethreshold metricsc log roc curv fpr tpr threshold log confus matrix annot item test confus matrix annotationspec annot append item displaynam metricsc log confus matrix annot test confus matrix row log textual metric info metric metric list kei metric confidencemetr val string json dump metric list metric metric log metric metric val string metric metadata model type automl tabular classif aiplatform init project project client aiplatform gapic modelservicecli client option api endpoint api endpoint eval metric list metric str list eval info client model uri replac aiplatform log metric metric list metricsc threshold dict json load threshold dict str return true classif threshold check metric list threshold dict fals",
        "Question_preprocessed_content":"gcp vertex pipelin function argument work provid function argument work provid follow creat run pipelin vertex pipelin jupyt notebook exampl gcp function take default valu argument mandatori function call argument entir pipelin code work type function code",
        "Question_gpt_summary_original":"The user is encountering a challenge with the function `classif_model_eval_metrics` in the GCP Vertex Pipeline. The function takes `metrics: Output[Metrics]` and `metricsc: Output[ClassificationMetrics]` as mandatory arguments, but the function is being called without these arguments. The user is confused about why the function works without these arguments and what `metrics: Output[Metrics]` and `metricsc: Output[ClassificationMetrics]` of type `kfp.v2.dsl.Output` are.",
        "Question_gpt_summary":"user encount challeng function classif model eval metric gcp vertex pipelin function take metric output metric metricsc output classificationmetr mandatori argument function call argument user confus function work argument metric output metric metricsc output classificationmetr type kfp dsl output",
        "Answer_original_content":"custom compon defin python function kfp dsl compon decor compon decor specifi option argument base contain imag us packag instal yaml file write compon specif compon function classif model eval metric input paramet model paramet input kfp dsl model artifact function arg metric metricsc compon output case type metric classificationmetr theyr explicitli pass input compon step automat instanti compon compon base imag gcr deeplearn platform releas cpu latest output compon file tabl eval compon yaml packag instal googl cloud aiplatform def classif model eval metric project str locat str central api endpoint str central aiplatform googleapi com threshold dict str str model input model metric output metric metricsc output classificationmetr exampl function call metricsc log roc curv metricsc log confus matrix render visual pipelin output param compon output compon compil consum pipelin step def log metric metric list metricsc metricsc log roc curv fpr tpr threshold metricsc log confus matrix annot test confus matrix row inform refer document",
        "Answer_preprocessed_content":"custom compon defin python function decor decor specifi option argument base contain imag us packag instal yaml file write compon specif compon function input paramet model paramet input function arg compon output case type metric classificationmetr theyr explicitli pass input compon step automat instanti compon exampl function call render visual pipelin output param compon output compon compil consum pipelin step inform refer document",
        "Answer_gpt_summary_original":"the solution to the challenge with the gcp vertex pipeline is to ensure that the function classif_model_eval_metrics takes arguments of type kfp.v2.dsl.output as function arguments. the custom component is defined as a python function with a @kfp.v2.dsl.component decorator, which specifies three optional arguments: the base container image to use, any packages to install, and the yaml file to which to write the component specification. the component function, classif_model_eval_metrics, has some input parameters, and the two function args, metrics and metricsc, are component outputs, which can be used in the component. these output params become component outputs when the component is compiled and can be consumed by other pipeline steps.",
        "Answer_gpt_summary":"solut challeng gcp vertex pipelin ensur function classif model eval metric take argument type kfp dsl output function argument custom compon defin python function kfp dsl compon decor specifi option argument base contain imag us packag instal yaml file write compon specif compon function classif model eval metric input paramet function arg metric metricsc compon output compon output param compon output compon compil consum pipelin step"
    },
    {
        "Question_id":null,
        "Question_title":"How to log configs except for configs that need to be tuned in w&b when I use ray tune for tuning",
        "Question_body":"<p>Hallo! I want to log some configs that are not for tuning, for example, the name for datasets, and the name for different losses. When I call WandbLoggerCallback in tune.Tuner,  it turns out that only the hyperparameters that it tuned are logged, other configs are not there, even though I have called wandb.config[\u201clossfn\u201d] = \"cross_entropy \" at the beginning.<\/p>\n<p>for example, as you see that there is nothing in the columns dataset and lossfn.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/1351feeae05de6ac2eac8a7aa62014b8d737b5b1.jpeg\" data-download-href=\"\/uploads\/short-url\/2KUKjJmSyAImPaso4ffQ0ZnBcsh.jpeg?dl=1\" title=\"Screenshot 2022-09-06 at 21.14.38\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1351feeae05de6ac2eac8a7aa62014b8d737b5b1_2_690x336.jpeg\" alt=\"Screenshot 2022-09-06 at 21.14.38\" data-base62-sha1=\"2KUKjJmSyAImPaso4ffQ0ZnBcsh\" width=\"690\" height=\"336\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1351feeae05de6ac2eac8a7aa62014b8d737b5b1_2_690x336.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1351feeae05de6ac2eac8a7aa62014b8d737b5b1_2_1035x504.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1351feeae05de6ac2eac8a7aa62014b8d737b5b1_2_1380x672.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1351feeae05de6ac2eac8a7aa62014b8d737b5b1_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2022-09-06 at 21.14.38<\/span><span class=\"informations\">1920\u00d7936 85.2 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":9,
        "Question_comment_count":0,
        "Question_creation_time":1662491691603,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":145.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-log-configs-except-for-configs-that-need-to-be-tuned-in-w-b-when-i-use-ray-tune-for-tuning\/3076",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-12T16:30:43.219Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ziwencheng\">@ziwencheng<\/a>, you may just need to call <code>wandb.run.update()<\/code>  after you update the config to set the config for the run. If that doesn\u2019t work could you should me the order in which you are setting the config and initializing the WandbLoggerCallback object?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-13T16:58:24.399Z",
                "Answer_body":"<p>Hi Nate, thanks for your reply. wanb.run.update() doesn\u2019t work. I only initialize the WandbLoggerCallback object in the tune.Tuner. The setting config is before the WandbLoggerCallback. I can show you the whole function.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3ffad77b104a7d39f0bc8f86e6fb4c8a66dbf2c2.png\" data-download-href=\"\/uploads\/short-url\/97ZvpaBquUOOytSM5apu62oXsH0.png?dl=1\" title=\"Screenshot 2022-09-13 at 18.57.19\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ffad77b104a7d39f0bc8f86e6fb4c8a66dbf2c2_2_548x500.png\" alt=\"Screenshot 2022-09-13 at 18.57.19\" data-base62-sha1=\"97ZvpaBquUOOytSM5apu62oXsH0\" width=\"548\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ffad77b104a7d39f0bc8f86e6fb4c8a66dbf2c2_2_548x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ffad77b104a7d39f0bc8f86e6fb4c8a66dbf2c2_2_822x750.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ffad77b104a7d39f0bc8f86e6fb4c8a66dbf2c2_2_1096x1000.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ffad77b104a7d39f0bc8f86e6fb4c8a66dbf2c2_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2022-09-13 at 18.57.19<\/span><span class=\"informations\">1914\u00d71744 318 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-13T16:59:19.850Z",
                "Answer_body":"<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/6b137998aa3161e7e777cd8252e85a02b0f38c52.png\" data-download-href=\"\/uploads\/short-url\/fheOub5Y58zSSzKSoC92fLLzHVw.png?dl=1\" title=\"Screenshot 2022-09-13 at 18.59.05\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/6b137998aa3161e7e777cd8252e85a02b0f38c52_2_575x500.png\" alt=\"Screenshot 2022-09-13 at 18.59.05\" data-base62-sha1=\"fheOub5Y58zSSzKSoC92fLLzHVw\" width=\"575\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/6b137998aa3161e7e777cd8252e85a02b0f38c52_2_575x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/6b137998aa3161e7e777cd8252e85a02b0f38c52_2_862x750.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/6b137998aa3161e7e777cd8252e85a02b0f38c52_2_1150x1000.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/6b137998aa3161e7e777cd8252e85a02b0f38c52_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2022-09-13 at 18.59.05<\/span><span class=\"informations\">1760\u00d71528 272 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-03T15:29:52.377Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/ziwencheng\">@ziwencheng<\/a>, it looks like we don\u2019t have a way to do this currently with our Ray integration. Could you possibly add the parameters to your search space but only as single values? This wouldn\u2019t affect your search space since they are single values but would log the config values to W&amp;B.<\/p>\n<p>If you\u2019d like I can put in a feature request for a more official way of doing this?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-07T14:09:39.979Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/ziwencheng\">@ziwencheng<\/a> I wanted to follow up and see if this worked for you and if you\u2019d like us to make a feature request around this?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-11T19:10:55.641Z",
                "Answer_body":"<p>hi\uff01Sorry for the late response! It still doesn\u2019t work. It would be great if you make a feature request of course! Thanks!<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-27T13:53:48.069Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ziwencheng\">@ziwencheng<\/a>, sorry for the delay! I was able to get this to work by using <code>wandb.config.update(&lt;dictionary of config I'd like to add&gt;)<\/code>. I know we tried <code>run.update<\/code> but could you possibly try using <code>wandb.config.update()<\/code> instead of simply assigning the values to the config object?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-01T16:19:06.140Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/ziwencheng\">@ziwencheng<\/a>, I just wanted to bump this and see if you got a chance to try the above method of updating the config?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-31T16:19:57.576Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"log config config need tune us rai tune tune hallo want log config tune exampl dataset differ loss loggercallback tune tuner turn hyperparamet tune log config call config lossfn cross entropi begin exampl column dataset lossfn screenshot",
        "Question_preprocessed_content":"log config config need tune us rai tune tune hallo want log config tune exampl dataset differ loss loggercallback turn hyperparamet tune log config call config begin exampl column dataset lossfn screenshot",
        "Question_gpt_summary_original":"The user is facing a challenge in logging certain configurations that are not for tuning, such as dataset names and loss names, when using WandbLoggerCallback in tune.Tuner. Only the hyperparameters that were tuned are being logged, and other configurations are not being recorded, despite calling wandb.config[\"lossfn\"] = \"cross_entropy\" at the beginning.",
        "Question_gpt_summary":"user face challeng log certain configur tune dataset name loss name loggercallback tune tuner hyperparamet tune log configur record despit call config lossfn cross entropi begin",
        "Answer_original_content":"ziwencheng need run updat updat config set config run doesnt work order set config initi loggercallback object thank nate nate thank repli wanb run updat doesnt work initi loggercallback object tune tuner set config loggercallback function screenshot screenshot ziwencheng look like dont wai current rai integr possibl add paramet search space singl valu wouldnt affect search space singl valu log config valu youd like featur request offici wai thank nate ziwencheng want follow work youd like featur request thank nate hisorri late respons doesnt work great featur request cours thank ziwencheng sorri delai abl work config updat know tri run updat possibl try config updat instead simpli assign valu config object thank nate ziwencheng want bump got chanc try method updat config thank nate topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"need updat config set config run doesnt work order set config initi loggercallback object thank nate nate thank repli doesnt work initi loggercallback object set config loggercallback function screenshot screenshot look like dont wai current rai integr possibl add paramet search space singl valu wouldnt affect search space singl valu log config valu youd like featur request offici wai thank nate want follow work youd like featur request thank nate hisorri late respons doesnt work great featur request cours thank sorri delai abl work know tri possibl try instead simpli assign valu config object thank nate want bump got chanc try method updat config thank nate topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the user is facing a challenge of logging configs that are not meant to be tuned in  when using ray tune for tuning. one possible solution is to add the parameters to the search space but only as single values, which would log the config values to w&b. another solution is to use .config.update() instead of simply assigning the values to the config object.",
        "Answer_gpt_summary":"user face challeng log config meant tune rai tune tune possibl solut add paramet search space singl valu log config valu solut us config updat instead simpli assign valu config object"
    },
    {
        "Question_id":null,
        "Question_title":"A 10+2 Students thrust for Programming can really enable his abilities to capture the AI?",
        "Question_body":"Students passed out from 10+2 are desperate enough to pursue AI through certifications, bachelor degree and in many other ways.... I am wondering is it really possible for them to capture AI and navigate in an easy manner these days. ",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1669190700000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":18.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/A-10-2-Students-thrust-for-Programming-can-really-enable-his\/td-p\/492261\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-23T08:05:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Students passed out from 10+2 are desperate enough to pursue AI through certifications, bachelor degree and in many other ways....\n\n\u00a0\n\nI am wondering is it really possible for them to capture AI and navigate in an easy manner these days."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"student thrust program enabl abil captur student pass desper pursu certif bachelor degre wai wonder possibl captur navig easi manner dai",
        "Question_preprocessed_content":"student thrust program enabl abil captur student pass desper pursu certif bachelor degre wonder possibl captur navig easi manner dai",
        "Question_gpt_summary_original":"The challenge faced by 10+2 students is whether they can successfully pursue AI through certifications and bachelor degrees, and navigate the field easily.",
        "Question_gpt_summary":"challeng face student successfulli pursu certif bachelor degre navig field easili",
        "Answer_original_content":"student pass desper pursu certif bachelor degre wai wonder possibl captur navig easi manner dai",
        "Answer_preprocessed_content":"student pass desper pursu certif bachelor degre wonder possibl captur navig easi manner dai",
        "Answer_gpt_summary_original":"possible solutions: pursuing ai through certifications and bachelor's degrees.",
        "Answer_gpt_summary":"possibl solut pursu certif bachelor degre"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning resync keys not working - no longer able to access files\/submit experiments to ML service",
        "Question_body":"I followed this tutorial: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-change-storage-access-key - to update the access keys of my default blob datastore linked to my Machine Learning service. This worked a month ago. After doing it today, every time when submitting an experiment I get the error:\n\"Job preparation failed: HTTP Error 403: Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature\"\n\nWhen opening log files for an experiment I get:\n\"403: You are not authorized to access this resource.\"\n\nThe resync keys operation from the command line works, as I can see in the Machine Learning service Activity log. The Python SDK code also works, as I tried updating the resource group name or default datastore and this gets through.\n\nAny ideas on how to fix this? I am at a loss.",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1591893115170,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/35043\/azure-machine-learning-resync-keys-not-working-no.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-16T10:35:20.733Z",
                "Answer_score":0,
                "Answer_body":"Updating this thread with the resolution steps performed by our team in this scenario.\n\n\n\n\nIssue:\n\n\n\n\nAfter updating the storage access keys, the job submission was failing with 403\n\n\n\n\nCause:\n\n\n\n\nThe values for the storage keys was cached in the storage connection strings at the backend\n\n\n\n\nResolution:\n\n\n\n\nThe engineering team manually deleted the cached storage keys from the backend\n\n\n\n\n@Jim-9799 Please feel free to accept this as answer so any other user can follow the steps.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"resync kei work longer abl access file submit experi servic follow tutori http doc microsoft com azur machin learn chang storag access kei updat access kei default blob datastor link machin learn servic work month ago todai time submit experi error job prepar fail http error server fail authent request sure valu author header form correctli includ signatur open log file experi author access resourc resync kei oper command line work machin learn servic activ log python sdk code work tri updat resourc group default datastor get idea fix loss",
        "Question_preprocessed_content":"resync kei work longer abl access experi servic follow tutori updat access kei default blob datastor link machin learn servic work month ago todai time submit experi error job prepar fail http error server fail authent request sure valu author header form correctli includ signatur open log file experi author access resync kei oper command line work machin learn servic activ log python sdk code work tri updat resourc group default datastor get idea fix loss",
        "Question_gpt_summary_original":"The user is facing challenges with accessing files and submitting experiments to the Azure Machine Learning service after updating the access keys of their default blob datastore. They are receiving a \"Job preparation failed\" error and a \"403: You are not authorized to access this resource\" message when opening log files for an experiment. The resync keys operation from the command line and Python SDK code works, but the user is still unable to access the service.",
        "Question_gpt_summary":"user face challeng access file submit experi servic updat access kei default blob datastor receiv job prepar fail error author access resourc messag open log file experi resync kei oper command line python sdk code work user unabl access servic",
        "Answer_original_content":"updat thread resolut step perform team scenario issu updat storag access kei job submiss fail caus valu storag kei cach storag connect string backend resolut engin team manual delet cach storag kei backend jim feel free accept answer user follow step",
        "Answer_preprocessed_content":"updat thread resolut step perform team scenario issu updat storag access kei job submiss fail caus valu storag kei cach storag connect string backend resolut engin team manual delet cach storag kei backend feel free accept answer user follow step",
        "Answer_gpt_summary_original":"the solution to the authentication error when accessing files and submitting experiments to the machine learning service after updating access keys of the default blob datastore is to manually delete the cached storage keys from the backend.",
        "Answer_gpt_summary":"solut authent error access file submit experi machin learn servic updat access kei default blob datastor manual delet cach storag kei backend"
    },
    {
        "Question_id":72096297.0,
        "Question_title":"Hyperparameter tuning job In Sagemaker with cross valdiation",
        "Question_body":"<p>I managed to get something <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-tuning-job.html\" rel=\"nofollow noreferrer\">along those lines<\/a> to work. This is great but to be more on the save side (i.e. not rely too much on the train validation split) one should really use cross validation. I am curious, if this can also be achieved via Sagemaker hyperparameter tuning jobs? I googled extensively ...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1651564321897,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":70.0,
        "Answer_body":"<p>It is not possible through HPO.<\/p>\n<p>You need to add additional step in your workflow to achieve cross-validation.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72096297",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1651629955790,
        "Question_original_content":"hyperparamet tune job cross valdiat manag line work great save reli train valid split us cross valid curiou achiev hyperparamet tune job googl extens",
        "Question_preprocessed_content":"hyperparamet tune job cross valdiat manag line work great save us cross valid curiou achiev hyperparamet tune job googl extens",
        "Question_gpt_summary_original":"The user has successfully implemented a hyperparameter tuning job in Sagemaker but is now facing the challenge of incorporating cross validation to ensure more reliable results. They are unsure if Sagemaker hyperparameter tuning jobs can achieve this and have searched extensively for a solution.",
        "Question_gpt_summary":"user successfulli implement hyperparamet tune job face challeng incorpor cross valid ensur reliabl result unsur hyperparamet tune job achiev search extens solut",
        "Answer_original_content":"possibl hpo need add addit step workflow achiev cross valid",
        "Answer_preprocessed_content":"possibl hpo need add addit step workflow achiev",
        "Answer_gpt_summary_original":"possible solutions: \n- add an additional step in the workflow to achieve cross-validation. \n- hyperparameter tuning jobs alone cannot achieve cross-validation.",
        "Answer_gpt_summary":"possibl solut add addit step workflow achiev cross valid hyperparamet tune job achiev cross valid"
    },
    {
        "Question_id":null,
        "Question_title":"Ground Truth Text Format",
        "Question_body":"Hello\nWe are trying to setup labeling for text with ground truth. Is it possible to format the source text to be labeled in some way e.g. html, markdown? A new line would already help a lot. I could not find any documentation on this here https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html\n\nThanks\nNicolas",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1556263122000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":125.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzhqhRbWOReCIYtX925_VPw\/ground-truth-text-format",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-04-29T06:56:34.000Z",
                "Answer_score":0,
                "Answer_body":"When you post tasks to Ground Truth they are displayed to workers using a web interface so HTML is the best bet for formatting the text you want annotated. For example, you could simply use the following to create a multi-line input:\n\n{\"source\": \"Lorem ipsum <br\/>dolor sit amet\"}\n\n\nUnfortunately, by default, inputs are HTML escaped to prevent confusion between your variable text and HTML. As a result if you use the Text Classification widget with the text above will just be displayed to workers as Lorem ipsum <br\/>dolor sit amet. To pass those values without escaping them you'll need to create a custom template that includes a filter on the variable to prevent it from being escaped.\n\nTo setup a custom task, start by creating Lambdas to handle for the pre and post processing required. Information on setting these up can be found at https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step3.html\n\nI've included simple pre and post Lambda examples below that I find are a good place to start for text annotation. Note that the post Lambda doesn't do any answer consolidation, just simply passes back all of the answers provided by Workers.\n\nCreate a custom template in Ground Truth and use the Sentiment Analysis template to create a starter task for text annotation. To prevent it from escaping HTML values, update the Liquid variables to include the skip_autoescape filter.\n\n{{ task.input.text | skip_autoescape }}\n\n\nYou can find more info on using Liquid template values here:\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step2.html#sms-custom-templates-step2-automate\n\nPre\n\ndef lambda_handler(event, context):\r\n    print(event)\r\n    source = event['dataObject'].get('source')\r\n\r\n    if source:\r\n        print(\"text is {}\".format(source))\r\n    else:\r\n        print(\"Missing text in dataObject\")\r\n        return {}\r\n    \r\n    response = {\r\n        \"taskInput\": {\r\n            \"text\": source\r\n        }\r\n    }\r\n    print(response)\r\n    return response\n\n\nPost\n\nimport json\r\nimport boto3\r\nfrom urllib.parse import urlparse\r\n\r\n\r\ndef lambda_handler(event, context):\r\n    print(json.dumps(event))\r\n\r\n    payload = get_payload(event)\r\n    print(json.dumps(payload))\r\n\r\n    consolidated_response = []\r\n    for dataset in payload:\r\n        annotations = dataset['annotations']\r\n        responses = []\r\n        for annotation in annotations:\r\n            response = json.loads(annotation['annotationData']['content'])\r\n            if 'annotatedResult' in response:\r\n                response = response['annotatedResult']\r\n\r\n            responses.append({\r\n                'workerId': annotation['workerId'],\r\n                'annotation': response\r\n            })\r\n\r\n        consolidated_response.append({\r\n            'datasetObjectId': dataset['datasetObjectId'],\r\n            'consolidatedAnnotation' : {\r\n                'content': {\r\n                    event['labelAttributeName']: {\r\n                        'responses': responses\r\n                    }\r\n                }\r\n            }\r\n        })\r\n\r\n    print(json.dumps(consolidated_response))\r\n    return consolidated_response\r\n\r\n\r\ndef get_payload(event):\r\n    if 'payload' in event:\r\n        parsed_url = urlparse(event['payload']['s3Uri'])\r\n        s3 = boto3.client('s3')\r\n        text_file = s3.get_object(Bucket=parsed_url.netloc, Key=parsed_url.path[1:])\r\n        return json.loads(text_file['Body'].read())\r\n    else:\r\n        return event.get('test_payload',[])",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-06-18T16:24:52.000Z",
                "Answer_score":0,
                "Answer_body":"Ah, great. That was exactly what I was missing. I simply changed the template to {{ task.input.taskObject | skip_autoescape }} and left out the pre-processing lambda.\n\nEdited by: nicolasdoodle on Apr 28, 2019 11:56 PM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-29T04:49:25.000Z",
                "Answer_score":0,
                "Answer_body":"I'm trying to do the same thing. But couldn't make it work. Did you still use the post-processing lambda? Or did you just change the template.liquid file\n\nEdited by: apoorvsrivastava on Jun 18, 2021 9:28 AM",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"ground truth text format hello try setup label text ground truth possibl format sourc text label wai html markdown new line help lot document http doc aw amazon com latest sm data input html thank nicola",
        "Question_preprocessed_content":"ground truth text format hello try setup label text ground truth possibl format sourc text label wai html markdown new line help lot document thank nicola",
        "Question_gpt_summary_original":"The user is facing challenges in setting up labeling for text with ground truth. They are seeking information on whether it is possible to format the source text to be labeled using html or markdown, as they could not find any documentation on this.",
        "Question_gpt_summary":"user face challeng set label text ground truth seek inform possibl format sourc text label html markdown document",
        "Answer_original_content":"post task ground truth displai worker web interfac html best bet format text want annot exampl simpli us follow creat multi line input sourc lorem ipsum dolor sit amet unfortun default input html escap prevent confus variabl text html result us text classif widget text displai worker lorem ipsum dolor sit amet pass valu escap need creat custom templat includ filter variabl prevent escap setup custom task start creat lambda handl pre post process requir inform set http doc aw amazon com latest sm custom templat step html includ simpl pre post lambda exampl good place start text annot note post lambda answer consolid simpli pass answer provid worker creat custom templat ground truth us sentiment analysi templat creat starter task text annot prevent escap html valu updat liquid variabl includ skip autoescap filter task input text skip autoescap info liquid templat valu http doc aw amazon com latest sm custom templat step html sm custom templat step autom pre def lambda handler event context print event sourc event dataobject sourc sourc print text format sourc print miss text dataobject return respons taskinput text sourc print respons return respons post import json import boto urllib pars import urlpars def lambda handler event context print json dump event payload payload event print json dump payload consolid respons dataset payload annot dataset annot respons annot annot respons json load annot annotationdata content annotatedresult respons respons respons annotatedresult respons append workerid annot workerid annot respons consolid respons append datasetobjectid dataset datasetobjectid consolidatedannot content event labelattributenam respons respons print json dump consolid respons return consolid respons def payload event payload event pars url urlpars event payload suri boto client text file object bucket pars url netloc kei pars url path return json load text file bodi read return event test payload great exactli miss simpli chang templat task input taskobject skip autoescap left pre process lambda edit nicolasdoodl apr try thing couldn work us post process lambda chang templat liquid file edit apoorvsrivastava jun",
        "Answer_preprocessed_content":"post task ground truth displai worker web interfac html best bet format text want annot exampl simpli us follow creat input sourc lorem ipsum dolor sit amet unfortun default input html escap prevent confus variabl text html result us text classif widget text displai worker lorem ipsum dolor sit amet pass valu escap need creat custom templat includ filter variabl prevent escap setup custom task start creat lambda handl pre post process requir inform set includ simpl pre post lambda exampl good place start text annot note post lambda answer consolid simpli pass answer provid worker creat custom templat ground truth us sentiment analysi templat creat starter task text annot prevent escap html valu updat liquid variabl includ filter info liquid templat valu pre def context print sourc sourc print print return respons print return respons post import json import boto import urlpars def context payload dataset payload annot dataset respons annot annot respons annotatedresult respons respons respons workerid annot annot respons datasetobjectid dataset consolidatedannot return def payload event urlpars return return great exactli miss simpli chang templat left lambda edit nicolasdoodl apr try thing couldn work us lambda chang file edit apoorvsrivastava jun",
        "Answer_gpt_summary_original":"the answer suggests using html formatting for text annotation tasks in ground truth. it also provides instructions on how to prevent html values from being escaped and how to create a custom template for text annotation. the solution involves creating pre and post-processing lambdas and updating the liquid variables to include the skip_autoescape filter. the post-processing lambda is optional and is used for answer consolidation.",
        "Answer_gpt_summary":"answer suggest html format text annot task ground truth provid instruct prevent html valu escap creat custom templat text annot solut involv creat pre post process lambda updat liquid variabl includ skip autoescap filter post process lambda option answer consolid"
    },
    {
        "Question_id":null,
        "Question_title":"How to compare (parameter & gradient) histograms from different runs?",
        "Question_body":"<p>Usually when logging a metric from different runs wandb will plot the results in the same graph (for different runs).<\/p>\n<p>This is not the case for histograms, is there a way to compare the histograms from different runs other than manually switching between tabs that contain the histogram plots for different runs?<\/p>\n<p>In general I\u2019m curious what the best way to compare gradients between different runs is. Maybe just tracking the magnitude is enough and one doesn\u2019t need histograms (I presume in this case, since the logging value is just a scalar the graphs from different runs would end up in the same plot)? Curious whether other engineers may have found this to be the case.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1656234575899,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":98.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-compare-parameter-gradient-histograms-from-different-runs\/2660",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-29T16:29:40.882Z",
                "Answer_body":"<p>Hi Rudolf, I feel like you are asking two different questions here? Are you wanting to compare histograms or gradients. If you want to compare histograms, you can do so by creating a custom chart (<a href=\"https:\/\/wandb.ai\/wandb\/plots\/reports\/Extend-a-Preset-Histogram-Bins--VmlldzozMTAxMjU\" class=\"inline-onebox-loading\">https:\/\/wandb.ai\/wandb\/plots\/reports\/Extend-a-Preset-Histogram-Bins--VmlldzozMTAxMjU<\/a>). However regarding gradients through wandb.watch() we currently don\u2019t have a way to compare those charts. I can create a feature request ticket regarding this if you would like?<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-06T13:00:56.192Z",
                "Answer_body":"<p>Hi Rudolf, do you still need help here?<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-12T16:10:38.317Z",
                "Answer_body":"<p>Hi again Rudolf, since you haven\u2019t responded I\u2019m going to close out this ticket. However, please let me know if you want to continue this conversation!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T09:09:44.323Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"compar paramet gradient histogram differ run usual log metric differ run plot result graph differ run case histogram wai compar histogram differ run manual switch tab contain histogram plot differ run gener curiou best wai compar gradient differ run mayb track magnitud doesnt need histogram presum case log valu scalar graph differ run end plot curiou engin case",
        "Question_preprocessed_content":"compar histogram differ run usual log metric differ run plot result graph case histogram wai compar histogram differ run manual switch tab contain histogram plot differ run gener curiou best wai compar gradient differ run mayb track magnitud doesnt need histogram curiou engin case",
        "Question_gpt_summary_original":"The user is facing challenges in comparing histograms from different runs while logging a metric. They are looking for a way to compare histograms without manually switching between tabs. Additionally, the user is curious about the best way to compare gradients between different runs and whether tracking the magnitude is enough.",
        "Question_gpt_summary":"user face challeng compar histogram differ run log metric look wai compar histogram manual switch tab addition user curiou best wai compar gradient differ run track magnitud",
        "Answer_original_content":"rudolf feel like ask differ question want compar histogram gradient want compar histogram creat custom chart http plot report extend preset histogram bin vmlldzozmtaxmju gradient watch current dont wai compar chart creat featur request ticket like rudolf need help rudolf havent respond go close ticket let know want continu convers topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"rudolf feel like ask differ question want compar histogram gradient want compar histogram creat custom chart gradient watch current dont wai compar chart creat featur request ticket like rudolf need help rudolf havent respond go close ticket let know want continu convers topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are two possible solutions mentioned in the answer. the first solution is to compare histograms by creating a custom chart. the second solution is to request a feature to compare gradients through .watch(). however, it is not clear if the second solution is currently available or not.",
        "Answer_gpt_summary":"possibl solut mention answer solut compar histogram creat custom chart second solut request featur compar gradient watch clear second solut current avail"
    },
    {
        "Question_id":56466592.0,
        "Question_title":"SageMaker: ClientError: .lst file missing in the train_lst channel. (customized image classification)",
        "Question_body":"<p><strong><em>Problem:<\/em><\/strong><\/p>\n\n<p>Jobs repeatedly fail after 5 minutes with the error<\/p>\n\n<blockquote>\n  <p>ClientError: .lst file missing in the train_lst channel.<\/p>\n<\/blockquote>\n\n<p><strong><em>Context:<\/em><\/strong><\/p>\n\n<p>Working within the AWS console, I have a binary classification task of images. I have labeled the classes in their filenames, per a guide.<\/p>\n\n<p>Eventually I started hitting errors that revealed that for this particular algorithm a <code>.lst<\/code> file is required for gathering the labels, since \"Content Type\" is specified as image, which apparently requires a lst file.<\/p>\n\n<p><strong><em>Example Data:<\/em><\/strong><\/p>\n\n<p>I am trying to match the examples I see on <a href=\"https:\/\/stackoverflow.com\/questions\/51670563\/invalid-lst-file-in-sagemaker\">StackOverflow<\/a> and elsewhere online. The current iteration of <code>trn_list.lst<\/code> looks like this:<\/p>\n\n<pre><code>292 \\t 1 \\t dog-292.jpeg\n214 \\t 1 \\t dog-214.jpeg\n290 \\t 0 \\t cat-290.jpeg\n288 \\t 1 \\t dog-288.jpeg\n160 \\t 1 \\t dog-160.jpeg\n18 \\t 0 \\t cat-18.jpeg\n215 \\t 1 \\t dog-215.jpeg\n254 \\t 1 \\t dog-254.jpeg\n53 \\t 1 \\t dog-53.jpeg\n337 \\t 0 \\t cat-337.jpeg\n284 \\t 0 \\t cat-284.jpeg\n177 \\t 1 \\t dog-177.jpeg\n192 \\t 1 \\t dog-192.jpeg\n228 \\t 0 \\t cat-228.jpeg\n305 \\t 0 \\t cat-305.jpeg\n258 \\t 1 \\t dog-258.jpeg\n75 \\t 0 \\t cat-75.jpeg\n148 \\t 0 \\t cat-148.jpeg\n268 \\t 1 \\t dog-268.jpeg\n281 \\t 1 \\t dog-281.jpeg\n24 \\t 1 \\t dog-24.jpeg\n328 \\t 1 \\t dog-328.jpeg\n99 \\t 1 \\t dog-99.jpeg\n<\/code><\/pre>\n\n<p>The bucket has no sub-folders, so I just put the .lst on the <\/p>\n\n<p>In one iteration I allowed my R program that creates the .lst to replace the <code>\\t<\/code> with actual tabs when it writes it out. In other iterations I left the actual delimiters (<code>\\t<\/code>) in there. Didn't seem to affect it (?).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559761618273,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":608.0,
        "Answer_body":"<p>When you are using SageMaker training jobs you are actually deploying a Docker image to a cluster of EC2 instances. The Docker has a python file that is running the training code in a similar way that you train it on your machine. In the training code you are referring to local folders when it expects to find the data such as the images to train on and the meta-data to use for that training. <\/p>\n\n<p>The \"magic\" is how to get the data from S3 to be available locally for the training instances. This is done using the definition of the channels in your training job configuration. Each channel definition creates a local folder on the training instance and copies the data from S3 to that local folder. You need to match the names and the S3 location and file formats.<\/p>\n\n<p>Here is the documentation of the definition of a channel in SageMaker: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_Channel.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_Channel.html<\/a><\/p>\n\n<p>For the specific example of the built-in algorithm for image classification and if you use the Image format for training, specify <code>train<\/code>, <code>validation<\/code>, <code>train_lst<\/code>, and <code>validation_lst<\/code> channels as values for the <code>InputDataConfig<\/code> parameter of the <code>CreateTrainingJob<\/code> request. Specify the individual image data (.jpg or .png files) for the train and validation channels. Specify one .lst file in each of the train_lst and validation_lst channels. Set the content type for all four channels to <code>application\/x-image<\/code>.<\/p>\n\n<p>See more details here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html#IC-inputoutput\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html#IC-inputoutput<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1559985734928,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56466592",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1559907535367,
        "Question_original_content":"clienterror lst file miss train lst channel custom imag classif problem job repeatedli fail minut error clienterror lst file miss train lst channel context work aw consol binari classif task imag label class filenam guid eventu start hit error reveal particular algorithm lst file requir gather label content type specifi imag appar requir lst file exampl data try match exampl stackoverflow onlin current iter trn list lst look like dog jpeg dog jpeg cat jpeg dog jpeg dog jpeg cat jpeg dog jpeg dog jpeg dog jpeg cat jpeg cat jpeg dog jpeg dog jpeg cat jpeg cat jpeg dog jpeg cat jpeg cat jpeg dog jpeg dog jpeg dog jpeg dog jpeg dog jpeg bucket sub folder lst iter allow program creat lst replac actual tab write iter left actual delimit affect",
        "Question_preprocessed_content":"clienterror lst file miss channel problem job repeatedli fail minut error clienterror lst file miss channel context work aw consol binari classif task imag label class filenam guid eventu start hit error reveal particular algorithm file requir gather label content type specifi imag appar requir lst file exampl data try match exampl stackoverflow onlin current iter look like bucket lst iter allow program creat lst replac actual tab write iter left actual delimit affect",
        "Question_gpt_summary_original":"The user is encountering a problem with AWS SageMaker where jobs repeatedly fail after 5 minutes with the error \"ClientError: .lst file missing in the train_lst channel.\" The user is working on a binary classification task of images and has labeled the classes in their filenames. However, the algorithm requires a .lst file for gathering the labels, which the user did not initially provide. The user has tried creating a .lst file and placing it in the bucket, but the error persists. The user has also tried different delimiters in the .lst file, but it did not seem to affect the error.",
        "Question_gpt_summary":"user encount problem job repeatedli fail minut error clienterror lst file miss train lst channel user work binari classif task imag label class filenam algorithm requir lst file gather label user initi provid user tri creat lst file place bucket error persist user tri differ delimit lst file affect error",
        "Answer_original_content":"train job actual deploi docker imag cluster instanc docker python file run train code similar wai train machin train code refer local folder expect data imag train meta data us train magic data avail local train instanc definit channel train job configur channel definit creat local folder train instanc copi data local folder need match name locat file format document definit channel http doc aw amazon com latest api channel html specif exampl built algorithm imag classif us imag format train specifi train valid train lst valid lst channel valu inputdataconfig paramet createtrainingjob request specifi individu imag data jpg png file train valid channel specifi lst file train lst valid lst channel set content type channel applic imag detail http doc aw amazon com latest imag classif html inputoutput",
        "Answer_preprocessed_content":"train job actual deploi docker imag cluster instanc docker python file run train code similar wai train machin train code refer local folder expect data imag train us train magic data avail local train instanc definit channel train job configur channel definit creat local folder train instanc copi data local folder need match name locat file format document definit channel specif exampl algorithm imag classif us imag format train specifi channel valu paramet request specifi individu imag data train valid channel specifi lst file channel set content type channel detail",
        "Answer_gpt_summary_original":"the solution to the challenge of repeated job failures in a customized image classification task on aws console is to ensure that the data is available locally for the training instances by defining channels in the training job configuration. the names and s3 location and file formats should match, and the documentation of the definition of a channel should be followed. for the specific example of the built-in algorithm for image classification, the inputdataconfig parameter of the createtrainingjob request should be specified with train, validation, train_lst, and validation_lst channels as values for the image format for training. the individual image data (.jpg or .png files) should be specified for the train and validation channels, and one .lst file should be specified in each of the train_lst and validation_lst channels. the content type for all four channels should be set to application\/x-image.",
        "Answer_gpt_summary":"solut challeng repeat job failur custom imag classif task aw consol ensur data avail local train instanc defin channel train job configur name locat file format match document definit channel follow specif exampl built algorithm imag classif inputdataconfig paramet createtrainingjob request specifi train valid train lst valid lst channel valu imag format train individu imag data jpg png file specifi train valid channel lst file specifi train lst valid lst channel content type channel set applic imag"
    },
    {
        "Question_id":null,
        "Question_title":"Synapse - 'No Azure Cognitive Service Linked Service are available'",
        "Question_body":"I am following this guide\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-cognitive-services-sentiment\n\nBut when I get to selecting 'Machine Learning - Predict with a Model', then I choose 'Sentiment Analysis'\n\nBut the first dropdown is greyed out for me and reads:\nAzure Cognitive Services linked service: No Azure Cognitive Service Linked Service are available\n\nI had one created, then created another. AKV and link service tested with specific key.\n\nAnyone know how I can get it to recognize the Cognitive Service Linked Server?\n\nThanks\n\n][2]\n\n\n\n\n\n\n\n[2]: \/answers\/storage\/attachments\/171659-image.png",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644147139677,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/724155\/synapse-39no-azure-cognitive-service-linked-servic.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-06T20:12:21.55Z",
                "Answer_score":0,
                "Answer_body":"The problem was I needed to create a Language specific Cognitive Service and not the general Cognitive Service resource.\n\nThere is no distinction for this from Synapse, when creating the Cognitive Service Linked Service, but the Predict a Model cares.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":19.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"synaps azur cognit servic link servic avail follow guid http doc microsoft com azur synaps analyt machin learn tutori cognit servic sentiment select machin learn predict model choos sentiment analysi dropdown grei read azur cognit servic link servic azur cognit servic link servic avail creat creat akv link servic test specif kei know recogn cognit servic link server thank answer storag attach imag png",
        "Question_preprocessed_content":"synaps azur cognit servic link servic avail follow guid select machin learn predict model choos sentiment analysi dropdown grei read azur cognit servic link servic azur cognit servic link servic avail creat creat akv link servic test specif kei know recogn cognit servic link server thank",
        "Question_gpt_summary_original":"The user is facing a challenge with Synapse Analytics while following a guide for cognitive services sentiment analysis. The first dropdown for Azure Cognitive Services linked service is greyed out, and the system is not recognizing the Cognitive Service Linked Server. The user has tried creating another linked service and tested AKV and link service with a specific key, but the issue persists.",
        "Question_gpt_summary":"user face challeng synaps analyt follow guid cognit servic sentiment analysi dropdown azur cognit servic link servic grei recogn cognit servic link server user tri creat link servic test akv link servic specif kei issu persist",
        "Answer_original_content":"problem need creat languag specif cognit servic gener cognit servic resourc distinct synaps creat cognit servic link servic predict model care",
        "Answer_preprocessed_content":"problem need creat languag specif cognit servic gener cognit servic resourc distinct synaps creat cognit servic link servic predict model care",
        "Answer_gpt_summary_original":"solution: the user needs to create a language-specific cognitive service instead of a general cognitive service resource to use the sentiment analysis feature in synapse. although there is no distinction for this from synapse when creating the cognitive service linked service, the predict a model cares.",
        "Answer_gpt_summary":"solut user need creat languag specif cognit servic instead gener cognit servic resourc us sentiment analysi featur synaps distinct synaps creat cognit servic link servic predict model care"
    },
    {
        "Question_id":49689216.0,
        "Question_title":"How can I print the Canonical String which aws-requests-auth sends?",
        "Question_body":"<p>I want to have a lambda calling a Sagemaker instance in another region. If both are in the same region, everything works fine. If they are not, I get the following error:<\/p>\n\n<pre><code>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\n\nThe Canonical String for this request should have been\n'POST\n\/endpoints\/foo-endpoint\/invocations\n\nhost:runtime.sagemaker.us-east-1.amazonaws.com\nx-amz-date:20180406T082536Z\n\nhost;x-amz-date\n1234567890foobarfoobarfoobarboofoobarfoobarfoobarfoobarfoobarfoo'\n\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20180406T082536Z\n20180406\/us-east-1\/sagemaker\/aws4_request\n987654321abcdeffoobarfoobarfoobarfoobarfoobarfoobarfoobarfoobarf'\n<\/code><\/pre>\n\n<p>I use <a href=\"https:\/\/github.com\/DavidMuller\/aws-requests-auth\" rel=\"nofollow noreferrer\"><code>aws-requests-auth<\/code><\/a> (0.4.1) with boto3 (1.5.15 - updating to 1.7.1 didn't change anything, <a href=\"https:\/\/github.com\/boto\/boto3\/blob\/develop\/CHANGELOG.rst\" rel=\"nofollow noreferrer\">changelog<\/a>) like this:<\/p>\n\n<pre><code>import requests\nfrom aws_requests_auth.aws_auth import AWSRequestsAuth\nauth = AWSRequestsAuth(aws_access_key=config['AWS']['ACCESS_KEY'],\n                       aws_secret_access_key=(\n                           config['AWS']['SECRET_ACCESS_KEY']),\n                       aws_host=config['AWS']['HOST'],\n                       aws_region=config['AWS']['REGION'],\n                       aws_service=config['AWS']['SERVICE'])\n\npayload = {'foo': 'bar'}\nresponse = requests.post(post_url,\n                         data=json.dumps(payload),\n                         headers={'content-type': 'application\/json'},\n                         auth=auth)\n<\/code><\/pre>\n\n<p>printing <code>auth<\/code> only gives <code>&lt;aws_requests_auth.aws_auth.AWSRequestsAuth object at 0x7f9d00c98390&gt;<\/code>.<\/p>\n\n<p>Is there a way to print the \"Canonical String\" mentioned in the error message?<\/p>\n\n<p>(Any other ideas how to fix this are appreciated as well)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1523005580303,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1531211963023,
        "Question_score":3.0,
        "Question_view_count":827.0,
        "Answer_body":"<p>A work-around for the asked question:<\/p>\n\n<pre><code>req = requests.request('POST', 'http:\/\/httpbin.org\/get')\nreq.body = b''\nreq.method = ''\nprint(auth.get_aws_request_headers(req,\n                                   aws_access_key=auth.aws_access_key,\n                                   aws_secret_access_key=auth.aws_secret_access_key,\n                                   aws_token=auth.aws_token))\n<\/code><\/pre>\n\n<p>The problem is not solved, though. And now I wonder what the first argument of <code>auth.get_aws_request_headers<\/code> is.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49689216",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1523012141480,
        "Question_original_content":"print canon string aw request auth send want lambda call instanc region region work fine follow error request signatur calcul match signatur provid check aw secret access kei sign method consult servic document detail canon string request post endpoint foo endpoint invoc host runtim east amazonaw com amz date host amz date foobarfoobarfoobarboofoobarfoobarfoobarfoobarfoobarfoo string sign aw hmac sha east aw request abcdeffoobarfoobarfoobarfoobarfoobarfoobarfoobarfoobarf us aw request auth boto updat chang changelog like import request aw request auth aw auth import awsrequestsauth auth awsrequestsauth aw access kei config aw access kei aw secret access kei config aw secret access kei aw host config aw host aw region config aw region aw servic config aw servic payload foo bar respons request post post url data json dump payload header content type applic json auth auth print auth give wai print canon string mention error messag idea fix appreci",
        "Question_preprocessed_content":"print canon string send want lambda call instanc region region work fine follow error us boto like print give wai print canon string mention error messag idea fix appreci",
        "Question_gpt_summary_original":"The user is encountering an error when trying to call a Sagemaker instance in another region using a lambda. The error message indicates that the request signature does not match the signature provided and provides a Canonical String and String-to-Sign that should have been used. The user is using aws-requests-auth and boto3 to make the request and is looking for a way to print the Canonical String mentioned in the error message.",
        "Question_gpt_summary":"user encount error try instanc region lambda error messag indic request signatur match signatur provid provid canon string string sign user aw request auth boto request look wai print canon string mention error messag",
        "Answer_original_content":"work ask question req request request post http httpbin org req bodi req method print auth aw request header req aw access kei auth aw access kei aw secret access kei auth aw secret access kei aw token auth aw token problem solv wonder argument auth aw request header",
        "Answer_preprocessed_content":"ask question problem solv wonder argument",
        "Answer_gpt_summary_original":"the answer provides a work-around for the challenge of printing the canonical string which aws-requests-auth sends when calling a lambda instance in another region. the work-around involves setting the request body to an empty byte string, setting the request method to an empty string, and then printing the aws request headers using the auth.get_aws_request_headers() function. however, the problem is not fully solved and the answer raises a new question about the first argument of the auth.get_aws_request_headers() function.",
        "Answer_gpt_summary":"answer provid work challeng print canon string aw request auth send call lambda instanc region work involv set request bodi byte string set request method string print aw request header auth aw request header function problem fulli solv answer rais new question argument auth aw request header function"
    },
    {
        "Question_id":null,
        "Question_title":"C# handwritten text detection using google.cloud.Vision.v1 Api. why can't detect language?",
        "Question_body":"hello mam\/sir,I was used google.cloud.Visiion.v1 for handwritten text recognition for Indian languages .  this code is work but only for Marathi, Hindi languages. but other language like Malayalam, Tamil, Kaneda Telegu its not return a 100 percent result.For Example- i have a 12 months handwritten name(Malayalam, Kaneda etc.) but its recognize only 7 to 8 correct word detection .can you please help me to 100 percent  accurate word detection.please give me a solution .thanks and regards,Bhagyashri ",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1664755140000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":74.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/C-handwritten-text-detection-using-google-cloud-Vision-v1-Api\/td-p\/473437\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-03T13:10:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi Bjagyashri, I think that you would find a better help on StackOverflow\u00a0since your issue seems to be on your code and people there is specialized on programming."
            },
            {
                "Answer_creation_time":"2022-10-03T23:56:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"okay\n\nthank you for advice."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"handwritten text detect googl cloud vision api detect languag hello mam sir googl cloud visiion handwritten text recognit indian languag code work marathi hindi languag languag like malayalam tamil kaneda telegu return percent result exampl month handwritten malayalam kaneda recogn correct word detect help percent accur word detect solut thank regard bhagyashri",
        "Question_preprocessed_content":"handwritten text detect api detect languag hello handwritten text recognit indian languag code work marathi hindi languag languag like malayalam tamil kaneda telegu return percent exampl month handwritten recogn correct word detect help percent accur word solut thank regard bhagyashri",
        "Question_gpt_summary_original":"The user is facing challenges with the Google Cloud Vision v1 API for handwritten text recognition in Indian languages. The API is only able to accurately detect Marathi and Hindi languages, but not Malayalam, Tamil, Kaneda, and Telegu. The user is seeking a solution to achieve 100% accurate word detection for all languages.",
        "Question_gpt_summary":"user face challeng googl cloud vision api handwritten text recognit indian languag api abl accur detect marathi hindi languag malayalam tamil kaneda telegu user seek solut achiev accur word detect languag",
        "Answer_original_content":"bjagyashri think better help stackoverflowsinc issu code peopl special program okai thank advic",
        "Answer_preprocessed_content":"bjagyashri think better help stackoverflowsinc issu code peopl special program okai thank advic",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the responder suggests seeking help on stack overflow as the issue seems to be related to the user's code.",
        "Answer_gpt_summary":"solut provid answer respond suggest seek help stack overflow issu relat user code"
    },
    {
        "Question_id":null,
        "Question_title":"How to connect Elasticsearch to Jupyter Notebook on GCP",
        "Question_body":"Hi,I am working on an ML project which requires to use Transformers and Elasticsearch.For Transformers, I have created a Jupyter Notebook instance on GCP.For Elasticsearch I will create another instance on CGP.As part of project requirement, I need to access Elasticsearch in Jupyter Notebook through port 9200. I need to ingest data in Elasticsearch and run search queries which can fetch relevant information from Elasticsearch DB and give it in Notebook.My question is that if I create two separate instances of Notebook and Elasticsearch and if i try to connect Elasticsearch through Jupyter Notebook via port 9200, will i be able to connect and perform the above mentioned operations ?If NO, then what is the procedure to do so ?",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1665196080000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":105.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-connect-Elasticsearch-to-Jupyter-Notebook-on-GCP\/td-p\/475882\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-10T15:02:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Can you elaborate on how you have set up the Elasticsearch and Jupyter notebook instances? On which compute service is your Elasticsearch instance running?"
            },
            {
                "Answer_creation_time":"2022-10-12T02:37:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi,\n\nBelow is the Screenshots of the way i have setup Jupyter Notebook instance and Elasticsearch instance.\n\n1.\n\nVertex AI Workbench - Jupyter Notebook\n\nThis is a link of the screenshot of the Vertex AI workbench where I have created a Jupyter Notebook instance.\n\n2. Below is the screenshots of the way I have accessed Elasticsearch through marketplace. Once I click on the service provider, the second screenshot is the landing page of Elasticsearch Cloud where also, I have signed and created the account.\n\nElasticsearch in Market Place\nLanding Page of Elasticsearch once we click on provider\n\n\u00a03. I want to access the Elasticsearch through \"http\" request made through my Jupyter Notebook and the screenshot of the codes which I want to run in my Jupyter Notebook is attached in below links.\n\nCode Snippet 3\n\n\u00a0\n\nCode Snippet 2\n\n\u00a0\n\nCode Snippet 1\n\n\u00a0Please Note that I want to access Elasticsearch version 8 onwards, as version 8 onwards, there is a facility to ingest \"Dense Vector\" in the database."
            },
            {
                "Answer_creation_time":"2022-10-14T15:48:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Using the Elasticsearch managed service from the GCP marketplace, you would need to use the [provided guide\u00a0to connect using a Python environment (in this case being the Jupyter notebook). Additional methods of connecting to the JSON API of the service are also included\u00a0in the documentation.\n\nIf you would like to exclusively connect directly to the deployment using port 9200, creating a deployment within a VM instance in GCE\u00a0should be more appropriate. This is due to the fact that a Vertex Workbench Notebook is also hosted on a GCE instance, and can be set up to be in the same VPC as a VM hosting Elasticsearch."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"connect elasticsearch jupyt notebook gcp work project requir us transform elasticsearch transform creat jupyt notebook instanc gcp elasticsearch creat instanc cgp project requir need access elasticsearch jupyt notebook port need ingest data elasticsearch run search queri fetch relev inform elasticsearch notebook question creat separ instanc notebook elasticsearch try connect elasticsearch jupyt notebook port abl connect perform mention oper procedur",
        "Question_preprocessed_content":"connect elasticsearch jupyt notebook gcp work project requir us transform transform creat jupyt notebook instanc elasticsearch creat instanc project requir need access elasticsearch jupyt notebook port need ingest data elasticsearch run search queri fetch relev inform elasticsearch question creat separ instanc notebook elasticsearch try connect elasticsearch jupyt notebook port abl connect perform mention oper procedur",
        "Question_gpt_summary_original":"The user is working on an ML project that requires the use of Transformers and Elasticsearch. They have created a Jupyter Notebook instance on GCP for Transformers and plan to create another instance for Elasticsearch. However, they are unsure if they can connect Elasticsearch to Jupyter Notebook through port 9200 to ingest data and run search queries. They are seeking guidance on how to connect the two instances and perform the required operations.",
        "Question_gpt_summary":"user work project requir us transform elasticsearch creat jupyt notebook instanc gcp transform plan creat instanc elasticsearch unsur connect elasticsearch jupyt notebook port ingest data run search queri seek guidanc connect instanc perform requir oper",
        "Answer_original_content":"elabor set elasticsearch jupyt notebook instanc comput servic elasticsearch instanc run screenshot wai setup jupyt notebook instanc elasticsearch instanc workbench jupyt notebook link screenshot workbench creat jupyt notebook instanc screenshot wai access elasticsearch marketplac click servic provid second screenshot land page elasticsearch cloud sign creat account elasticsearch market place land page elasticsearch click provid want access elasticsearch http request jupyt notebook screenshot code want run jupyt notebook attach link code snippet code snippet code snippet note want access elasticsearch version onward version onward facil ingest dens vector databas elasticsearch manag servic gcp marketplac need us provid guideto connect python environ case jupyt notebook addit method connect json api servic includedin document like exclus connect directli deploy port creat deploy instanc gceshould appropri fact vertex workbench notebook host gce instanc set vpc host elasticsearch",
        "Answer_preprocessed_content":"elabor set elasticsearch jupyt notebook instanc comput servic elasticsearch instanc run screenshot wai setup jupyt notebook instanc elasticsearch instanc workbench jupyt notebook link screenshot workbench creat jupyt notebook instanc screenshot wai access elasticsearch marketplac click servic provid second screenshot land page elasticsearch cloud sign creat account elasticsearch market place land page elasticsearch click provid want access elasticsearch http request jupyt notebook screenshot code want run jupyt notebook attach link code snippet code snippet code snippet note want access elasticsearch version onward version onward facil ingest dens vector databas elasticsearch manag servic gcp marketplac need us provid guideto connect python environ addit method connect json api servic includedin document like exclus connect directli deploy port creat deploy instanc gceshould appropri fact vertex workbench notebook host gce instanc set vpc host elasticsearch",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- use the provided guide to connect elasticsearch using a python environment (in this case being the jupyter notebook).\n- additional methods of connecting to the json api of the service are also included in the documentation.\n- create a deployment within a vm instance in gce to exclusively connect directly to the deployment using port 9200. this is more appropriate if the jupyter notebook and elasticsearch are not in the same vpc.",
        "Answer_gpt_summary":"possibl solut answer us provid guid connect elasticsearch python environ case jupyt notebook addit method connect json api servic includ document creat deploy instanc gce exclus connect directli deploy port appropri jupyt notebook elasticsearch vpc"
    },
    {
        "Question_id":null,
        "Question_title":"Legend ordering",
        "Question_body":"<p>Hey all,<br>\nI was just wondering if its possible to rearrange\/order elements in the legend of a chart? I have different data series that each go by a number and right now, the legend displays these numbers in random order\u2026<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1661988930181,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":120.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/legend-ordering\/3041",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-02T13:05:44.621Z",
                "Answer_body":"<p>Hi Christopher,<\/p>\n<p>Thanks for your question! Currently this is not possible, but I can create a request for this feature. The order in which the legend is shown is by the time each run has been logged. Please let me know if I can help you in any other way!<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-30T23:36:22.374Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"legend order hei wonder possibl rearrang order element legend chart differ data seri number right legend displai number random order",
        "Question_preprocessed_content":"legend order hei wonder possibl element legend chart differ data seri number right legend displai number random order",
        "Question_gpt_summary_original":"The user is facing a challenge with the ordering of elements in the legend of a chart, as the numbers representing different data series are currently displayed in a random order.",
        "Question_gpt_summary":"user face challeng order element legend chart number repres differ data seri current displai random order",
        "Answer_original_content":"christoph thank question current possibl creat request featur order legend shown time run log let know help wai best lui topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"christoph thank question current possibl creat request featur order legend shown time run log let know help wai best lui topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"possible solutions: currently, it is not possible to rearrange\/order elements in the legend of a chart. however, the user can request this feature to be added. the order in which the legend is shown is based on the time each run has been logged.",
        "Answer_gpt_summary":"possibl solut current possibl rearrang order element legend chart user request featur ad order legend shown base time run log"
    },
    {
        "Question_id":null,
        "Question_title":"Using DVC for end-to-end pipeline",
        "Question_body":"<p>HI,<\/p>\n<p>I am trying to create an end-to-end pipeline ( I am new to it \u2026 ), and trying to use dvc as well.<\/p>\n<p>I have create 5 script corresponding to each phase<\/p>\n<ol>\n<li>preprocessing<\/li>\n<li>1st Feature engineering<\/li>\n<li>train-test data splite<\/li>\n<li>2nd Feature engineering<\/li>\n<li>Model training<br>\nI could do 'dvc run bla bla bla \u2019 for each program file. and I could reproduce the result.<\/li>\n<\/ol>\n<p>However if I would like to re-run the \u201cWHOLE\u201d pipeline after I have twitched something, is there any \u201cbest practice\u201d or recommendation or guide line  (e.g. use shell script to wrap up all \u2018dvc run\u2019 commands and run it ?  use dvc run -d xx.data -d yy.data \u2026 -d fe1.py -d fe2.py -d main.py \u2026, to run a very bulky command to make a dvc file ?  ) ?<\/p>\n<p>and. \u2026 do creating separate script for each phase a good idea to manage the pipeline ?<\/p>\n<p>Looking forward for answer ! Many thanks !<\/p>\n<p>Best Regards,<br>\nSolomon Leung<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1545748275249,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":7.0,
        "Question_view_count":1158.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/using-dvc-for-end-to-end-pipeline\/111",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-12-25T15:02:46.968Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/solomonleung\">@solomonleung<\/a> !<\/p>\n<p>So your pipeline consists of only 5 stages(i.e. 5 <code>dvc run<\/code>s), right? <code>dvc repro 5.dvc<\/code> would look through all of the dependency stages(1.dvc \u2026 4.dvc) and reproduce anything that has changed, effectively re-running the whole pipeline. You can also force re-running by adding a <code>--force<\/code> flag to <code>dvc repro<\/code> command. We also have <code>-P|--all-pipelines<\/code> option for <code>dvc repro<\/code> that will reproduce all pipelines present in your project. Please feel free to correct me, if I didn\u2019t get your scenario right <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=6\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<aside class=\"quote no-group\" data-post=\"1\" data-topic=\"111\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/solomonleung\/40\/26_1.png\" class=\"avatar\"> solomonleung:<\/div>\n<blockquote>\n<p>and. \u2026 do creating separate script for each phase a good idea to manage the pipeline ?<\/p>\n<\/blockquote>\n<\/aside>\n<p>Yes, it is a recommended way of organizing your pipeline, so each <code>dvc run<\/code> has its own script to run that is corresponding to the action(e.g. preprocess, train, etc). <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=6\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":32.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-01-01T13:06:56.943Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a><\/p>\n<p>Thanks <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=6\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><br>\nIf I have created a \u201cconfig.py\u201d and \u201cutil.py\u201d scripts and every phrase of my script reference to it.<\/p>\n<ol>\n<li>do I need to declare dependency with config.py and util.py for each dvc run ?<\/li>\n<li>If I make changes on util.py (e.g. add a new class which be used in final phase \u2026  ) would the whole pipeline re-run \u2026 ?<\/li>\n<\/ol>\n<p>Looking forward to your reply <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=6\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>Thanks,<br>\nSolomon<\/p>",
                "Answer_score":27.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-01-01T13:47:41.317Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/solomonleung\">@solomonleung<\/a> !<\/p>\n<ol>\n<li>Yes, if every stage of your pipeline is using it. But you could also declare it once in one of the first stages, and use -p|\u2013pipeline with <code>dvc repro<\/code> to re-run the whole pipeline, it is clearly a trade-of though.<\/li>\n<li>Yes, it would. Dvc only checks checksums for the dependencies, so if there is anything changed in the dependency then it will re-run the stage that depends on it.<\/li>\n<\/ol>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":22.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-01-01T17:28:08.514Z",
                "Answer_body":"<p>HI <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a> ,<\/p>\n<p>Thanks again :)<img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/muscle.png?v=6\" title=\":muscle:\" class=\"emoji\" alt=\":muscle:\"><br>\nAnd got some question again \u2026<\/p>\n<ol>\n<li>\n<p>if I accidentally run \u201cdvc run\u201d in a wrong place \u2026 (e.g. \/data), the .dvc file will created at the path where I have run \u201cdvc run\u201d, what would happen if I have the dvc file to another location (e.g. \/ ) ? will it break the pipeline ?<\/p>\n<\/li>\n<li>\n<p>My pipeline is still improving \u2026 and the output -o and dependency -d of some stages would change  \u2026  ( e.g. change from .csv to .pk1 , or add new output in some stages ) , when that happen, can the dvc file be modify ? or I need to re-create the whole pipeline ? :(<img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/scream.png?v=6\" title=\":scream:\" class=\"emoji\" alt=\":scream:\"> , or there is a way to remove some stages in pipeline ?<\/p>\n<\/li>\n<\/ol>\n<p>Regards,<br>\nSolomon<\/p>",
                "Answer_score":61.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-01-01T20:21:10.621Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/solomonleung\">@solomonleung<\/a><\/p>\n<ol>\n<li>As long as outputs of those stages don\u2019t overlap, you\u2019ll just have two similar stages in your pipeline.<\/li>\n<li>Sure, you can just open any .dvc file with your favorite editor and add\/remove\/modify parameters in it(command, outputs, dependencies, etc). Any .dvc file is in a simple YAML format, so it is pretty easy to modify it. See <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-file-format\" rel=\"nofollow noopener\">https:\/\/dvc.org\/doc\/user-guide\/dvc-file-format<\/a> . You can also remove any stage with <code>dvc remove --purge stage.dvc<\/code>. See <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/remove\" rel=\"nofollow noopener\">https:\/\/dvc.org\/doc\/commands-reference\/remove<\/a> .<\/li>\n<\/ol>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":32.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-01-05T02:41:22.495Z",
                "Answer_body":"<p>Thanks Rusian <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=6\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":76.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"end end pipelin try creat end end pipelin new try us creat script correspond phase preprocess featur engin train test data splite featur engin model train run bla bla bla program file reproduc result like run pipelin twitch best practic recommend guid line us shell script wrap run command run us run data data main run bulki command file creat separ script phase good idea manag pipelin look forward answer thank best regard solomon leung",
        "Question_preprocessed_content":"pipelin try creat pipelin try us creat script correspond phase preprocess featur engin data splite featur engin model train run bla bla bla program file reproduc result like pipelin twitch best practic recommend guid line creat separ script phase good idea manag pipelin look forward answer thank best regard solomon leung",
        "Question_gpt_summary_original":"The user is facing challenges in creating an end-to-end pipeline using DVC. They have created 5 scripts for each phase but are unsure of the best practice for re-running the entire pipeline after making changes. They are also seeking guidance on whether creating separate scripts for each phase is a good idea for managing the pipeline.",
        "Question_gpt_summary":"user face challeng creat end end pipelin creat script phase unsur best practic run entir pipelin make chang seek guidanc creat separ script phase good idea manag pipelin",
        "Answer_original_content":"solomonleung pipelin consist stage run right repro look depend stage reproduc chang effect run pipelin forc run ad forc flag repro command pipelin option repro reproduc pipelin present project feel free correct didnt scenario right solomonleung creat separ script phase good idea manag pipelin ye recommend wai organ pipelin run script run correspond action preprocess train thank ruslan kuprus thank creat config util script phrase script refer need declar depend config util run chang util add new class final phase pipelin run look forward repli thank solomon solomonleung ye stage pipelin declar stage us pipelin repro run pipelin clearli trade ye check checksum depend chang depend run stage depend thank ruslan kuprus thank got question accident run run wrong place data file creat path run run happen file locat break pipelin pipelin improv output depend stage chang chang csv add new output stage happen file modifi need creat pipelin wai remov stage pipelin regard solomon solomonleung long output stage dont overlap youll similar stage pipelin sure open file favorit editor add remov modifi paramet command output depend file simpl yaml format pretti easi modifi http org doc user guid file format remov stage remov purg stage http org doc command refer remov thank ruslan thank rusian",
        "Answer_preprocessed_content":"pipelin consist right look depend stage reproduc chang effect pipelin forc ad flag command option reproduc pipelin present project feel free correct didnt scenario right solomonleung creat separ script phase good idea manag pipelin ye recommend wai organ pipelin script run correspond preprocess train thank ruslan thank creat script phrase script refer need declar depend run chang pipelin look forward repli thank solomon ye stage pipelin declar stage us pipelin pipelin clearli ye check checksum depend chang depend stage depend thank ruslan thank got question accident run run wrong place file creat path run run happen file locat break pipelin pipelin improv output depend stage chang happen file modifi need pipelin wai remov stage pipelin regard solomon long output stage dont overlap youll similar stage pipelin sure open file favorit editor paramet file simpl yaml format pretti easi modifi remov stage thank ruslan thank rusian",
        "Answer_gpt_summary_original":"possible solutions from the answer include: using the \"repro\" command to reproduce the pipeline, creating separate scripts for each phase of the pipeline, declaring dependencies for each run, modifying the . file to add\/remove\/modify parameters, and removing stages with the \"remove --purge stage\" command.",
        "Answer_gpt_summary":"possibl solut answer includ repro command reproduc pipelin creat separ script phase pipelin declar depend run modifi file add remov modifi paramet remov stage remov purg stage command"
    },
    {
        "Question_id":null,
        "Question_title":"Custom Vision for Canada region",
        "Question_body":"I used Custom Vision to create a small proof of concept project, and it works super nice; however, now that we are ready for the next steps to see if it is doable to use it, I have got some questions that might be roadblocks if I were to go ahead and work on the business implementation:\n\nThe service cannot be deployed in Canada regions. Is this something that will be considered in the future? This is a huge block because the items classified may contain data that should not leave the Canadian space\n\n\nWhat's the privacy terms of using custom vision or where can I find them to read? As the previous item describes, the items classified could contain compromised data, so it would be unfeasible to use the custom vision service if the data is going to be \"shared\" or \"used\" by Microsoft or other parties for other purposes.\n\nAs a side question, is this service capable of classifying PDF documents as images? And if not, is there a known Azure\/Microsoft service that does so?\n\nThanks a bunch! :)\n\nKenny Perroni",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606512423207,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@KennyPerroni-1017 Thanks for the feedback. We have forwarded this feedback to our product team, You can also raise a user voice request here so the community can vote and provide their feedback, the product team then checks this feedback and implements the same for Canada region. For Region availability please check the following link\n\nFor custom vision service you control over the storage and deletion of any customer data that store as part of the service. Please follow the below for privacy and compliance. As with all of the Cognitive Services, developers using the Custom Vision service should be aware of Microsoft's policies on customer data. See the below Cognitive Services page on the Microsoft Trust Center to learn more.\nhttps:\/\/azure.microsoft.com\/en-us\/support\/legal\/cognitive-services-compliance-and-privacy\/\n\n\n\n\nThe Computer vision Read service support PDF document as images. Here is the link for vision best practice and samples. Also the Form Recognizer supports OCR and PDF documents with AI builder.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/178721\/custom-vision-for-canada-region.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-30T04:09:44.577Z",
                "Answer_score":1,
                "Answer_body":"@KennyPerroni-1017 Thanks for the feedback. We have forwarded this feedback to our product team, You can also raise a user voice request here so the community can vote and provide their feedback, the product team then checks this feedback and implements the same for Canada region. For Region availability please check the following link\n\nFor custom vision service you control over the storage and deletion of any customer data that store as part of the service. Please follow the below for privacy and compliance. As with all of the Cognitive Services, developers using the Custom Vision service should be aware of Microsoft's policies on customer data. See the below Cognitive Services page on the Microsoft Trust Center to learn more.\nhttps:\/\/azure.microsoft.com\/en-us\/support\/legal\/cognitive-services-compliance-and-privacy\/\n\n\n\n\nThe Computer vision Read service support PDF document as images. Here is the link for vision best practice and samples. Also the Form Recognizer supports OCR and PDF documents with AI builder.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1606709384576,
        "Question_original_content":"custom vision canada region custom vision creat small proof concept project work super nice readi step doabl us got question roadblock ahead work busi implement servic deploi canada region consid futur huge block item classifi contain data leav canadian space privaci term custom vision read previou item describ item classifi contain compromis data unfeas us custom vision servic data go share microsoft parti purpos question servic capabl classifi pdf document imag known azur microsoft servic thank bunch kenni perroni",
        "Question_preprocessed_content":"custom vision canada region custom vision creat small proof concept project work super nice readi step doabl us got question roadblock ahead work busi implement servic deploi canada region consid futur huge block item classifi contain data leav canadian space privaci term custom vision read previou item describ item classifi contain compromis data unfeas us custom vision servic data go share microsoft parti purpos question servic capabl classifi pdf document imag known servic thank bunch kenni perroni",
        "Question_gpt_summary_original":"The user is facing challenges with using Custom Vision for a business implementation due to the service not being deployable in Canada regions, which is a concern for data privacy. The user is also seeking information on the privacy terms of using Custom Vision and whether the service is capable of classifying PDF documents as images.",
        "Question_gpt_summary":"user face challeng custom vision busi implement servic deploy canada region concern data privaci user seek inform privaci term custom vision servic capabl classifi pdf document imag",
        "Answer_original_content":"kennyperroni thank feedback forward feedback product team rais user voic request commun vote provid feedback product team check feedback implement canada region region avail check follow link custom vision servic control storag delet custom data store servic follow privaci complianc cognit servic develop custom vision servic awar microsoft polici custom data cognit servic page microsoft trust center learn http azur microsoft com support legal cognit servic complianc privaci vision read servic support pdf document imag link vision best practic sampl form recogn support ocr pdf document builder",
        "Answer_preprocessed_content":"thank feedback forward feedback product team rais user voic request commun vote provid feedback product team check feedback implement canada region region avail check follow link custom vision servic control storag delet custom data store servic follow privaci complianc cognit servic develop custom vision servic awar microsoft polici custom data cognit servic page microsoft trust center learn vision read servic support pdf document imag link vision best practic sampl form recogn support ocr pdf document builder",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. the user can raise a user voice request for the custom vision service to be implemented in canada regions.\n2. the user has control over the storage and deletion of any customer data that is stored as part of the service.\n3. the user should follow the privacy and compliance guidelines provided by microsoft for using cognitive services.\n4. the custom vision service supports pdf documents as images.\n5. the user can check out the best practices and samples for the custom vision service.\n6. the form recognizer supports ocr and pdf documents with ai builder. \n\nin summary, the answer provides possible solutions for the user's challenges with deploying the custom vision service in canada regions, understanding the privacy terms of using the service, and determining if the service is capable of classifying pdf documents as images.",
        "Answer_gpt_summary":"possibl solut answer user rais user voic request custom vision servic implement canada region user control storag delet custom data store servic user follow privaci complianc guidelin provid microsoft cognit servic custom vision servic support pdf document imag user check best practic sampl custom vision servic form recogn support ocr pdf document builder summari answer provid possibl solut user challeng deploi custom vision servic canada region understand privaci term servic determin servic capabl classifi pdf document imag"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning (AutoML) export data to SharePoint",
        "Question_body":"I am using Azure Machine Learning Studio to design pipelines to analyze data.\nIs there any possibility to export data to sharepoint?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1631064917827,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi @MiaZhangWHQWistron-2092\nPer my research, there is no way to export data from Azure Machine Learning Studio to SharePoint directly.\n\nAs an alternative, you could export data to Azure SQL database first:\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/export-to-azure-sql-database\n\nThen export data from Azure SQL database to SharePoint list:\nhttps:\/\/social.technet.microsoft.com\/wiki\/contents\/articles\/39170.azure-sql-db-with-sharepoint-online-as-external-list-using-business-connectivity-services.aspx\n\n\nIf an Answer is helpful, please click \"Accept Answer\" and upvote it.\n\nNote: Please follow the steps in our documentation to enable e-mail notifications if you want to receive the related email notification for this thread.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/543361\/azure-machine-learning-automl-export-data-to-share.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-08T07:07:22.44Z",
                "Answer_score":2,
                "Answer_body":"Hi @MiaZhangWHQWistron-2092\nPer my research, there is no way to export data from Azure Machine Learning Studio to SharePoint directly.\n\nAs an alternative, you could export data to Azure SQL database first:\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/export-to-azure-sql-database\n\nThen export data from Azure SQL database to SharePoint list:\nhttps:\/\/social.technet.microsoft.com\/wiki\/contents\/articles\/39170.azure-sql-db-with-sharepoint-online-as-external-list-using-business-connectivity-services.aspx\n\n\nIf an Answer is helpful, please click \"Accept Answer\" and upvote it.\n\nNote: Please follow the steps in our documentation to enable e-mail notifications if you want to receive the related email notification for this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1631084842440,
        "Question_original_content":"automl export data sharepoint studio design pipelin analyz data possibl export data sharepoint",
        "Question_preprocessed_content":"export data sharepoint studio design pipelin analyz data possibl export data sharepoint",
        "Question_gpt_summary_original":"The user is facing a challenge in exporting data from Azure Machine Learning Studio to SharePoint.",
        "Question_gpt_summary":"user face challeng export data studio sharepoint",
        "Answer_original_content":"miazhangwhqwistron research wai export data studio sharepoint directli altern export data azur sql databas http doc microsoft com azur machin learn studio modul refer export azur sql databas export data azur sql databas sharepoint list http social technet microsoft com wiki content articl azur sql sharepoint onlin extern list busi connect servic aspx answer help click accept answer upvot note follow step document enabl mail notif want receiv relat email notif thread",
        "Answer_preprocessed_content":"research wai export data studio sharepoint directli altern export data azur sql databas export data azur sql databas sharepoint list answer help click accept answer upvot note follow step document enabl notif want receiv relat email notif thread",
        "Answer_gpt_summary_original":"possible solutions to export data from studio to sharepoint are to first export data to azure sql database and then export data from azure sql database to sharepoint list. there is no direct way to export data from studio to sharepoint.",
        "Answer_gpt_summary":"possibl solut export data studio sharepoint export data azur sql databas export data azur sql databas sharepoint list direct wai export data studio sharepoint"
    },
    {
        "Question_id":null,
        "Question_title":"My Azure ML studio is not loading",
        "Question_body":"Hello,\n\nI am trying to enter my ML studio, however when I launch it, the window opens and appears to be loading (it says: loading workspace) but it reamins this way for a long time and I cannot access it. I have had this problem for a couple of days now and have tried it on different web browser and the result is the same.\n\nPlease your help or feedback to solve this as soon as possible.\n\nThank you!",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_time":1651477263057,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/833332\/my-azure-ml-studio-is-not-loading.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-02T16:00:03.39Z",
                "Answer_score":0,
                "Answer_body":"Hi @CASTANOSANCHEZMariaJoseA-3582,\n\nHave you tried it by using a Chrome browser?\n\nHere is a quite similar issue.\n\nhttps:\/\/docs.microsoft.com\/en-us\/answers\/questions\/576536\/not-able-to-access-to-microsoft-machine-learning-s.html",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-03T07:45:11.707Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nYes I am still experiencing this issue. The region of my resource is West Europe. I can access my account perfectly but then when I try to launch the ML Studio it goes on a loop saying \"Loading workspace\" and I can never enter. I have tried with different browsers (Chrome, Firefox, Safari, and microsoft edge) and also private windows and nothing is working.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-03T08:24:37.73Z",
                "Answer_score":0,
                "Answer_body":"Also the problem is only with my account, as there is other people that work with me in the same resource group (azrgy-ml4p-01) and their accounts are working perfectly, they can log in into their Azure ML studio and even mine without an issue. However, on my side I cannot log into any Azure ML Studio, nor mine and theirs. I have also checked with the IT department at my organization and from this side my account should be working just fine, I can enter other Azure platforms except for Azure ML studios, which is the focus of my projects.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":21.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"studio load hello try enter studio launch window open appear load sai load workspac reamin wai long time access problem coupl dai tri differ web browser result help feedback solv soon possibl thank",
        "Question_preprocessed_content":"studio load hello try enter studio launch window open appear load reamin wai long time access problem coupl dai tri differ web browser result help feedback solv soon possibl thank",
        "Question_gpt_summary_original":"The user is experiencing difficulty accessing their Azure ML studio as the workspace is not loading despite trying different web browsers for a couple of days.",
        "Question_gpt_summary":"user experienc difficulti access studio workspac load despit try differ web browser coupl dai",
        "Answer_original_content":"castanosanchezmariajosea tri chrome browser similar issu http doc microsoft com answer question abl access microsoft machin learn html hello ye experienc issu region resourc west europ access account perfectli try launch studio goe loop sai load workspac enter tri differ browser chrome firefox safari microsoft edg privat window work problem account peopl work resourc group azrgi mlp account work perfectli log studio issu log studio their check depart organ account work fine enter azur platform studio focu project",
        "Answer_preprocessed_content":"tri chrome browser similar issu hello ye experienc issu region resourc west europ access account perfectli try launch studio goe loop sai load workspac enter tri differ browser privat window work problem account peopl work resourc group account work perfectli log studio issu log studio their check depart organ account work fine enter azur platform studio focu project",
        "Answer_gpt_summary_original":"possible solutions to the issue of the user not being able to access their ml studio include trying to access it using a chrome browser and checking with the it department to ensure that their account is working fine. the user has already tried accessing it using different browsers and private windows, but the issue persists. the problem seems to be specific to the user's account, as other people in the same resource group can access their accounts without any issues.",
        "Answer_gpt_summary":"possibl solut issu user abl access studio includ try access chrome browser check depart ensur account work fine user tri access differ browser privat window issu persist problem specif user account peopl resourc group access account issu"
    },
    {
        "Question_id":null,
        "Question_title":"Sigopt for RL DQN",
        "Question_body":"<p>I found an old example on DQN on the github page. But it was using TF v1 and the old sigopt. It would have been nice to get a new tutorial replacing it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1664314630467,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":58.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.sigopt.com\/t\/sigopt-for-rl-dqn\/120",
        "Tool":"SigOpt",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-28T19:42:04.217Z",
                "Answer_body":"<p>Hi there,<\/p>\n<p>Thanks for checking out our examples\u2026we will keep your request in mind as we continue to improve and update elements of SigOpt!<br>\nIn case you missed it, we\u2019ve re-enabled the classic SigOpt experience for users, now rebranded as the \u201ccore module.\u201d Check out the differences between the core module (original SigOpt), and the AI module <a href=\"https:\/\/docs.sigopt.com\/intro\/sigopt-api-modules\">here!<\/a><\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"dqn old exampl dqn github page old nice new tutori replac",
        "Question_preprocessed_content":"dqn old exampl dqn github page old nice new tutori replac",
        "Question_gpt_summary_original":"The user encountered challenges with an outdated example on DQN using TF v1 and old sigopt, and expressed a need for a new tutorial to replace it.",
        "Question_gpt_summary":"user encount challeng outdat exampl dqn old express need new tutori replac",
        "Answer_original_content":"thank check examplesw request mind continu improv updat element case miss weve enabl classic experi user rebrand core modul check differ core modul origin modul",
        "Answer_preprocessed_content":"thank check examplesw request mind continu improv updat element case miss weve classic experi user rebrand core modul check differ core modul modul",
        "Answer_gpt_summary_original":"there are no specific solutions provided in the answer to the user's challenge of finding an up-to-date tutorial on dqn using tensorflow v1 and the old api. however, the answer suggests that the request will be kept in mind for future updates and improvements. additionally, the answer mentions that the classic experience has been re-enabled for users, now rebranded as the core module, and provides a link to check out the differences between the core module and the ai module.",
        "Answer_gpt_summary":"specif solut provid answer user challeng find date tutori dqn tensorflow old api answer suggest request kept mind futur updat improv addition answer mention classic experi enabl user rebrand core modul provid link check differ core modul modul"
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker MultiDataModel deployment error during inference. ValueError: Exactly one .pth or .pt file is required for PyTorch models: []",
        "Question_body":"Hello, I've been trying to deploy multiple PyTorch models on one endpoint on SageMaker from a SageMaker Notebook. First I tested deployment of single models on single endpoints, to check if everything works smoothly and it did. I would create a PyTorchModel first:\n\nimport sagemaker\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker import get_execution_role\nfrom sagemaker.multidatamodel import MultiDataModel\nfrom sagemaker.serializers import JSONSerializer\nfrom sagemaker.deserializers import JSONDeserializer\nimport boto3\n\nrole = get_execution_role()\nsagemaker_session = sagemaker.Session()\n\npytorch_model = PyTorchModel(\n            entry_point='inference.py',\n            source_dir='code',\n            role=role,\n            model_data='s3:\/\/***\/model\/model.tar.gz',\n            framework_version='1.11.0',\n            py_version='py38',\n            name='***-model',\n            sagemaker_session=sagemaker_session\n        )\n\n\nMultiDataModel inherits properties from Model classes, so I used the same PyTorch model that I used for single model deployment. Then I would define the MultiDataModel the following way:\n\nmodels = MultiDataModel(name='***-multi-model',\n                       model_data_prefix='s3:\/\/***-sagemaker\/model\/',\n                       model=pytorch_model,\n                       sagemaker_session=sagemaker_session\n                       )\n\n\nAll it should need is the prefix to the S3 bucket of the model artifacts saved as tar.gz files (the same files used for single model deployment), the previously defined PyTorch model, a name and a sagemaker_session.\n\nTo deploy it:\n\nmodels.deploy(initial_instance_count =1,\n             instance_type='ml.m4.xlarge',\n             serializer=JSONSerializer(),\n             deserializer=JSONDeserializer(),\n             endpoint_name='***-multi-model-deployment',\n             )\n\n\nThe deployment goes well, as there are no failures and the endpoint is InService by the end of this step. However the error occurs when I try to run inference on one of the models:\n\nimport json\nbody = {\"url\":\"https:\/\/***image.jpg\"} #url to an image online\npayload = json.dumps(body)\nclient = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(\n    EndpointName = \"***-multi-model-deployment\",\n    ContentType  = \"application\/json\",\n    TargetModel  = \"\/model.tar.gz\",\n    Body         = payload)\n\n\nThis prompts an error message:\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"{\n  \"code\": 500,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Failed to start workers for model ec1cd509c40ca81ffc3fb09deb4599e2 version: 1.0\"\n}\n\". See https:\/\/***.console.aws.amazon.com\/cloudwatch\/home?region=***#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/***-multi-model-deployment in account ***** for more information.\n\n\nThe Cloudwatch logs show this error in particular:\n\n22-09-26T15:51:40,494 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/ts\/model_service_worker.py\", line 210, in <module>\n2022-09-26T15:51:40,494 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\n2022-09-26T15:51:40,494 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/ts\/model_service_worker.py\", line 181, in run_server\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/ts\/model_service_worker.py\", line 139, in handle_connection\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/ts\/model_service_worker.py\", line 104, in load_model\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/ts\/model_loader.py\", line 151, in load\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker_pytorch_serving_container\/handler_service.py\", line 51, in initialize\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker_inference\/default_handler_service.py\", line 66, in initialize\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker_inference\/transformer.py\", line 162, in validate_and_initialize\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._model = self._model_fn(model_dir)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker_pytorch_serving_container\/default_pytorch_inference_handler.py\", line 73, in default_model_fn\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     raise ValueError(\n2022-09-26T15:51:40,496 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ValueError: Exactly one .pth or .pt file is required for PyTorch models: []\n\n\nIt seems like it's having problems loading the model, saying only one .pth file is required, however in the invocation function i point to the exact model artifact present at that S3 bucket prefix. I'm having a hard time trying to fix this issue, so it would be very helpful if anyone had some suggestions!\n\nInstead of giving the MultiDataModel a model, I also tried providing it an ECR docker image with the same inference code, but I would get the same error during invocation of the endpoint.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1664208836417,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":112.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUe7ia8vHWRmukMsV_i5SzpA\/sage-maker-multi-data-model-deployment-error-during-inference-value-error-exactly-one-pth-or-pt-file-is-required-for-py-torch-models",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-26T17:18:04.935Z",
                "Answer_score":0,
                "Answer_body":"Hey! I see a couple of potential issues that you might want to carefully check.\n\n1\/ Note that the CloudWatch the logs you're looking at are the errors for the default model worker W-9000-model_1.0. So these messages are irrelevant to you prediction request. See the Fix: Don't load default model in MME mode for detailed description of the issue. When you make predictions, the models are lazy-loaded from your model_data_prefix . Carefully check the further logs and timings and see what happens after it tries to load your TargetModel = \"\/model.tar.gz\". According to the InternalServerException exception, you are executing the model ec1cd509c40ca81ffc3fb09deb4599e2, so look for the logs of the worker W-9001-ec1cd509c40ca81ffc3fb09deb4599e2 and you might see some other errors.\n\n2\/ There are two types of model.tar.gz in SageMaker - one is plain model produced by an estimator when you call fit(), another when you deploy the model and it's repackaged with your inference code (see the SageMaker Python SDK source code fragments: 1 and 2).\n\nMake sure that your model_data_prefix contains repackaged models and they are not repackaged twice. Look into your model.tar.gz and make sure it contains both your PyTorch model as well as inference.py with the code dir.\n\nThe location of repackaged model should be accessible as pytorch_model.repacked_model_data after you deployed the endpoint.\n\nFor better clarity I recommend to create a separate path for models in your multi-model endpoint and copy the models to it with the following API:\n\nmodels.add_model(model_data_source=pytorch_model.repacked_model_data, model_data_path=model_name)\n\n\nHere the model_name can be something like model_1.tar.gz, model_2.tar.gz etc. Note that leading slash \/ is not necessary in the model name.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"multidatamodel deploy error infer valueerror exactli pth file requir pytorch model hello try deploi multipl pytorch model endpoint notebook test deploy singl model singl endpoint check work smoothli creat pytorchmodel import pytorch import pytorchmodel import execut role multidatamodel import multidatamodel serial import jsonseri deseri import jsondeseri import boto role execut role session session pytorch model pytorchmodel entri point infer sourc dir code role role model data model model tar framework version version model session session multidatamodel inherit properti model class pytorch model singl model deploy defin multidatamodel follow wai model multidatamodel multi model model data prefix model model pytorch model session session need prefix bucket model artifact save tar file file singl model deploy previous defin pytorch model session deploi model deploi initi instanc count instanc type xlarg serial jsonseri deseri jsondeseri endpoint multi model deploy deploy goe failur endpoint inservic end step error occur try run infer model import json bodi url http imag jpg url imag onlin payload json dump bodi client boto client runtim respons client invok endpoint endpointnam multi model deploy contenttyp applic json targetmodel model tar bodi payload prompt error messag modelerror error occur modelerror call invokeendpoint oper receiv server error model messag code type internalserverexcept messag fail start worker model eccdccaffcfbdeb version http consol aw amazon com cloudwatch home region logeventview group aw endpoint multi model deploy account inform cloudwatch log error particular info model stdout model log file opt conda lib python site packag model servic worker line info model stdout model log worker run server info model stdout model log file opt conda lib python site packag model servic worker line run server info model stdout model log self handl connect socket info model stdout model log file opt conda lib python site packag model servic worker line handl connect info model stdout model log servic result code self load model msg info model stdout model log file opt conda lib python site packag model servic worker line load model info model stdout model log servic model loader load info model stdout model log file opt conda lib python site packag model loader line load info model stdout model log initi servic context info model stdout model log file opt conda lib python site packag pytorch serv contain handler servic line initi info model stdout model log super initi context info model stdout model log file opt conda lib python site packag infer default handler servic line initi info model stdout model log self servic valid initi model dir model dir info model stdout model log file opt conda lib python site packag infer transform line valid initi info model stdout model log self model self model model dir info model stdout model log file opt conda lib python site packag pytorch serv contain default pytorch infer handler line default model info model stdout model log rais valueerror info model stdout model log valueerror exactli pth file requir pytorch model like have problem load model sai pth file requir invoc function point exact model artifact present bucket prefix have hard time try fix issu help suggest instead give multidatamodel model tri provid ecr docker imag infer code error invoc endpoint",
        "Question_preprocessed_content":"multidatamodel deploy error infer valueerror exactli pth file requir pytorch model hello try deploi multipl pytorch model endpoint notebook test deploy singl model singl endpoint check work smoothli creat pytorchmodel import pytorch import pytorchmodel import multidatamodel import multidatamodel serial import jsonseri deseri import jsondeseri import boto role session pytorchmodel multidatamodel inherit properti model class pytorch model singl model deploy defin multidatamodel follow wai model need prefix bucket model artifact save file previous defin pytorch model deploi serial jsonseri deseri jsondeseri deploy goe failur endpoint inservic end step error occur try run infer model import json bodi url imag onlin payload client respons endpointnam contenttyp targetmodel bodi payload prompt error messag modelerror error occur call invokeendpoint oper receiv server error model messag code type internalserverexcept messag fail start worker model ffc deb version account inform cloudwatch log error particular file line file line file line servic result code file line servic file line load file line initi file line initi file line file line rais valueerror valueerror exactli pth file requir pytorch model like have problem load model sai pth file requir invoc function point exact model artifact present bucket prefix have hard time try fix issu help suggest instead give multidatamodel model tri provid ecr docker imag infer code error invoc endpoint",
        "Question_gpt_summary_original":"The user is encountering an error while trying to deploy multiple PyTorch models on one endpoint on SageMaker from a SageMaker Notebook. The deployment goes well, but the error occurs when the user tries to run inference on one of the models. The Cloudwatch logs show that the model is having problems loading, saying only one .pth file is required, however, in the invocation function, the user points to the exact model artifact present at that S3 bucket prefix. The user has tried providing the MultiDataModel an ECR docker image with the same inference code, but the same error occurs during invocation of the endpoint.",
        "Question_gpt_summary":"user encount error try deploi multipl pytorch model endpoint notebook deploy goe error occur user tri run infer model cloudwatch log model have problem load sai pth file requir invoc function user point exact model artifact present bucket prefix user tri provid multidatamodel ecr docker imag infer code error occur invoc endpoint",
        "Answer_original_content":"hei coupl potenti issu want carefulli check note cloudwatch log look error default model worker model messag irrelev predict request fix load default model mme mode detail descript issu predict model lazi load model data prefix carefulli check log time happen tri load targetmodel model tar accord internalserverexcept except execut model eccdccaffcfbdeb look log worker eccdccaffcfbdeb error type model tar plain model produc estim fit deploi model repackag infer code python sdk sourc code fragment sure model data prefix contain repackag model repackag twice look model tar sure contain pytorch model infer code dir locat repackag model access pytorch model repack model data deploi endpoint better clariti recommend creat separ path model multi model endpoint copi model follow api model add model model data sourc pytorch model repack model data model data path model model like model tar model tar note lead slash necessari model",
        "Answer_preprocessed_content":"hei coupl potenti issu want carefulli check note cloudwatch log look error default model worker messag irrelev predict request fix load default model mme mode detail descript issu predict model carefulli check log time happen tri load targetmodel accord internalserverexcept except execut model ffc deb look log worker error type plain model produc estim fit deploi model repackag infer code sure contain repackag model repackag twice look sure contain pytorch model code dir locat repackag model access deploi endpoint better clariti recommend creat separ path model endpoint copi model follow api like note lead slash necessari model",
        "Answer_gpt_summary_original":"the answer suggests two potential solutions to the error encountered when deploying multiple pytorch models on one endpoint. firstly, the user should carefully check the logs and timings to identify any errors after the target model is loaded. secondly, the user should ensure that their model_data_prefix contains repackaged models and that they are not repackaged twice. the answer recommends creating a separate path for models in the multi-model endpoint and copying the models to it using the add_model api.",
        "Answer_gpt_summary":"answer suggest potenti solut error encount deploi multipl pytorch model endpoint firstli user carefulli check log time identifi error target model load secondli user ensur model data prefix contain repackag model repackag twice answer recommend creat separ path model multi model endpoint copi model add model api"
    },
    {
        "Question_id":50032795.0,
        "Question_title":"prevent access to s3 buckets for sagemaker users",
        "Question_body":"<p>I'm trying to add an IAM user for using sagemaker. I used the <code>AmazonSageMakerFullAccess<\/code> policy. But when I log in as this user I can see all of the s3 buckets of the root account and download files from them.<\/p>\n<p>The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html\" rel=\"nofollow noreferrer\">sagemaker documentation<\/a> states<\/p>\n<blockquote>\n<p>When attaching the AmazonSageMakerFullAccess policy to a role, you must do one of the following to allow Amazon SageMaker to access your S3 bucket:<\/p>\n<p>Include the string &quot;SageMaker&quot; or &quot;sagemaker&quot; in the name of the bucket where you store training data, or the model artifacts resulting from model training, or both.<\/p>\n<p>Include the string &quot;SageMaker&quot; or &quot;sagemaker&quot; in the object name of the training data object(s).<\/p>\n<p>Tag the S3 object with &quot;sagemaker=true&quot;. The key and value are case sensitive. For more information, see Object Tagging in the Amazon Simple Storage Service Developer Guide.<\/p>\n<p>Add a bucket policy that allows access for the execution role. For more information, see Using Bucket Policies and User Policies in the Amazon Simple Storage Service Developer Guide.<\/p>\n<\/blockquote>\n<p>This seems to be inaccurate the user can access s3 buckets lacking <code>sagemaker<\/code> in the name. How do I limit the access?<\/p>\n<p>the full policy is below<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;sagemaker:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;ecr:GetAuthorizationToken&quot;,\n                &quot;ecr:GetDownloadUrlForLayer&quot;,\n                &quot;ecr:BatchGetImage&quot;,\n                &quot;ecr:BatchCheckLayerAvailability&quot;,\n                &quot;cloudwatch:PutMetricData&quot;,\n                &quot;cloudwatch:PutMetricAlarm&quot;,\n                &quot;cloudwatch:DescribeAlarms&quot;,\n                &quot;cloudwatch:DeleteAlarms&quot;,\n                &quot;ec2:CreateNetworkInterface&quot;,\n                &quot;ec2:CreateNetworkInterfacePermission&quot;,\n                &quot;ec2:DeleteNetworkInterface&quot;,\n                &quot;ec2:DeleteNetworkInterfacePermission&quot;,\n                &quot;ec2:DescribeNetworkInterfaces&quot;,\n                &quot;ec2:DescribeVpcs&quot;,\n                &quot;ec2:DescribeDhcpOptions&quot;,\n                &quot;ec2:DescribeSubnets&quot;,\n                &quot;ec2:DescribeSecurityGroups&quot;,\n                &quot;application-autoscaling:DeleteScalingPolicy&quot;,\n                &quot;application-autoscaling:DeleteScheduledAction&quot;,\n                &quot;application-autoscaling:DeregisterScalableTarget&quot;,\n                &quot;application-autoscaling:DescribeScalableTargets&quot;,\n                &quot;application-autoscaling:DescribeScalingActivities&quot;,\n                &quot;application-autoscaling:DescribeScalingPolicies&quot;,\n                &quot;application-autoscaling:DescribeScheduledActions&quot;,\n                &quot;application-autoscaling:PutScalingPolicy&quot;,\n                &quot;application-autoscaling:PutScheduledAction&quot;,\n                &quot;application-autoscaling:RegisterScalableTarget&quot;,\n                &quot;logs:CreateLogGroup&quot;,\n                &quot;logs:CreateLogStream&quot;,\n                &quot;logs:DescribeLogStreams&quot;,\n                &quot;logs:GetLogEvents&quot;,\n                &quot;logs:PutLogEvents&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetObject&quot;,\n                &quot;s3:PutObject&quot;,\n                &quot;s3:DeleteObject&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::*SageMaker*&quot;,\n                &quot;arn:aws:s3:::*Sagemaker*&quot;,\n                &quot;arn:aws:s3:::*sagemaker*&quot;\n            ]\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:CreateBucket&quot;,\n                &quot;s3:GetBucketLocation&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:ListAllMyBuckets&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetObject&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEqualsIgnoreCase&quot;: {\n                    &quot;s3:ExistingObjectTag\/SageMaker&quot;: &quot;true&quot;\n                }\n            }\n        },\n        {\n            &quot;Action&quot;: &quot;iam:CreateServiceLinkedRole&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:iam::*:role\/aws-service-role\/sagemaker.application-autoscaling.amazonaws.com\/AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringLike&quot;: {\n                    &quot;iam:AWSServiceName&quot;: &quot;sagemaker.application-autoscaling.amazonaws.com&quot;\n                }\n            }\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;iam:PassRole&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEquals&quot;: {\n                    &quot;iam:PassedToService&quot;: &quot;sagemaker.amazonaws.com&quot;\n                }\n            }\n        }\n    ]\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1524699817877,
        "Question_favorite_count":null,
        "Question_last_edit_time":1592644375060,
        "Question_score":1.0,
        "Question_view_count":1703.0,
        "Answer_body":"<p>looks like the sagemaker notebook wizard has you create a role that has limited s3 access. If I add this and the default <code>AmazonSageMakerFullAccess<\/code> the user is properly restricted. <a href=\"https:\/\/i.stack.imgur.com\/9BjRD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9BjRD.png\" alt=\"Amazon make sagemaker role\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/IM7WW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IM7WW.png\" alt=\"choose iam roles\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50032795",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1525129267447,
        "Question_original_content":"prevent access bucket user try add iam user amazonfullaccess polici log user bucket root account download file document state attach amazonfullaccess polici role follow allow access bucket includ string bucket store train data model artifact result model train includ string object train data object tag object true kei valu case sensit inform object tag amazon simpl storag servic develop guid add bucket polici allow access execut role inform bucket polici user polici amazon simpl storag servic develop guid inaccur user access bucket lack limit access polici version statement effect allow action resourc effect allow action ecr getauthorizationtoken ecr getdownloadurlforlay ecr batchgetimag ecr batchchecklayeravail cloudwatch putmetricdata cloudwatch putmetricalarm cloudwatch describealarm cloudwatch deletealarm createnetworkinterfac createnetworkinterfacepermiss deletenetworkinterfac deletenetworkinterfacepermiss describenetworkinterfac describevpc describedhcpopt describesubnet describesecuritygroup applic autosc deletescalingpolici applic autosc deletescheduledact applic autosc deregisterscalabletarget applic autosc describescalabletarget applic autosc describescalingact applic autosc describescalingpolici applic autosc describescheduledact applic autosc putscalingpolici applic autosc putscheduledact applic autosc registerscalabletarget log createloggroup log createlogstream log describelogstream log getlogev log putlogev resourc effect allow action getobject putobject deleteobject resourc arn aw arn aw arn aw effect allow action createbucket getbucketloc listbucket listallmybucket resourc effect allow action getobject resourc condit stringequalsignorecas existingobjecttag true action iam createservicelinl effect allow resourc arn aw iam role aw servic role applic autosc amazonaw com awsserviceroleforapplicationautosc endpoint condit stringlik iam awsservicenam applic autosc amazonaw com effect allow action iam passrol resourc condit stringequ iam passedtoservic amazonaw com",
        "Question_preprocessed_content":"prevent access bucket user try add iam user polici log user bucket root account download file document state attach amazonfullaccess polici role follow allow access bucket includ string bucket store train data model artifact result model train includ string object train data object tag object true kei valu case sensit inform object tag amazon simpl storag servic develop guid add bucket polici allow access execut role inform bucket polici user polici amazon simpl storag servic develop guid inaccur user access bucket lack limit access polici",
        "Question_gpt_summary_original":"The user is encountering a challenge in preventing access to S3 buckets for SageMaker users. Despite using the AmazonSageMakerFullAccess policy, the user is able to access S3 buckets lacking \"SageMaker\" in the name and download files from them. The user is seeking guidance on how to limit access.",
        "Question_gpt_summary":"user encount challeng prevent access bucket user despit amazonfullaccess polici user abl access bucket lack download file user seek guidanc limit access",
        "Answer_original_content":"look like notebook wizard creat role limit access add default amazonfullaccess user properli restrict",
        "Answer_preprocessed_content":"look like notebook wizard creat role limit access add default user properli restrict",
        "Answer_gpt_summary_original":"the answer suggests creating a role with limited s3 access and adding it along with the default amazonfullaccess policy to restrict user access to s3 buckets.",
        "Answer_gpt_summary":"answer suggest creat role limit access ad default amazonfullaccess polici restrict user access bucket"
    },
    {
        "Question_id":56863907.0,
        "Question_title":"how to set path of bucket in amazonsagemaker jupyter notebook?",
        "Question_body":"<p>I'm new to the aws how to set path of my bucket and access file of that bucket?<\/p>\n\n<p>Is there anything i need to change with prefix ?<\/p>\n\n<pre><code>import os\nimport boto3\nimport re\nimport copy\nimport time\nfrom time import gmtime, strftime\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\n\nregion = boto3.Session().region_name\n\nbucket='ltfs1' # Replace with your s3 bucket name\nprefix = 'sagemaker\/ltfs1' # Used as part of the path in the bucket where you store data\n# bucket_path = 'https:\/\/s3-{}.amazonaws.com\/{}'.format(region,bucket) # The URL to access the bucket\n<\/code><\/pre>\n\n<p>I'm using the above code but it's showing file not found error<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1562134154127,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":852.0,
        "Answer_body":"<p>If the file you are accessing is in the root directory of your s3 bucket, you can access the file like this:<\/p>\n\n<pre><code>import pandas as pd\n\nbucket='ltfs1'\ndata_key = 'data.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\ntraining_data = pd.read_csv(data_location)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56863907",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1562166882852,
        "Question_original_content":"set path bucket amazon jupyt notebook new aw set path bucket access file bucket need chang prefix import import boto import import copi import time time import gmtime strftime import execut role role execut role region boto session region bucket ltf replac bucket prefix ltf path bucket store data bucket path http amazonaw com format region bucket url access bucket code show file error",
        "Question_preprocessed_content":"set path bucket amazon jupyt notebook new aw set path bucket access file bucket need chang prefix code show file error",
        "Question_gpt_summary_original":"The user is facing challenges in setting the path of their bucket in Amazon SageMaker Jupyter Notebook and accessing files from that bucket. They have provided a code snippet that they are using, but it is resulting in a \"file not found\" error. The user is seeking guidance on whether they need to change anything with the prefix.",
        "Question_gpt_summary":"user face challeng set path bucket jupyt notebook access file bucket provid code snippet result file error user seek guidanc need chang prefix",
        "Answer_original_content":"file access root directori bucket access file like import panda bucket ltf data kei data csv data locat format bucket data kei train data read csv data locat",
        "Answer_preprocessed_content":"file access root directori bucket access file like",
        "Answer_gpt_summary_original":"possible solution: if the file is in the root directory of the s3 bucket, the user can access it by importing pandas and using the code provided in the answer to set the path and read the file.",
        "Answer_gpt_summary":"possibl solut file root directori bucket user access import panda code provid answer set path read file"
    },
    {
        "Question_id":null,
        "Question_title":"AI scientist",
        "Question_body":"I'm from Ukraine. Therefore, I write with the help of a translator. I immediately apologize for any mistakes.I am a doctor. I am interested in many areas of science that are related to medicine. But because of their volume and complexity, it is impossible to learn by one person.I propose to create an AI that will analyze information on the Internet (video lectures, articles, books, audio books, images ...) and find relationships. For example, the electrophysical properties of DNA are analyzed through all known theories of physics. And a concrete example: Academician P. Garyaev's \"Linguistic Wave Genome\" through V. Atsyukovsky's \"Ether Theory\".This tool needs to be made multifunctional and accessible to all users. This will revolutionize science by combining all knowledge.\nIt is important that there is a convenient voice interface and a personal account where studies are saved.Thank you for attention. Sincerely, Sukhachov Denis.\u042f \u0441 \u0423\u043a\u0440\u0430\u0438\u043d\u044b. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u044f \u043f\u0438\u0448\u0443 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0447\u0438\u043a\u0430. \u0421\u0440\u0430\u0437\u0443 \u0438\u0437\u0432\u0438\u043d\u044f\u044e\u0441\u044c \u0437\u0430 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u043e\u0448\u0438\u0431\u043a\u0438.\u042f \u0434\u043e\u043a\u0442\u043e\u0440. \u041c\u0435\u043d\u044f \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0443\u044e\u0442 \u043c\u043d\u043e\u0433\u0438\u0435 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u043d\u0430\u0443\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u0432\u044f\u0437\u0430\u043d\u044b \u0441 \u043c\u0435\u0434\u0438\u0446\u0438\u043d\u043e\u0439. \u041d\u043e \u0438\u0437-\u0437\u0430 \u0438\u0445 \u043e\u0431\u044a\u0435\u043c\u0430 \u0438 \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0438\u0445 \u043d\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u043e\u0441\u0432\u043e\u0438\u0442\u044c \u043e\u0434\u043d\u043e\u043c\u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0443.\u041f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u044e \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0418\u0418, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0432 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0435 (\u0432\u0438\u0434\u0435\u043e\u043b\u0435\u043a\u0446\u0438\u0438, \u0441\u0442\u0430\u0442\u044c\u0438, \u043a\u043d\u0438\u0433\u0438, \u0430\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0438, \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f...) \u0438 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0438. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0441\u0432\u043e\u0439\u0441\u0442\u0432\u0430 \u0414\u041d\u041a \u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0432\u0441\u0435\u0445 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0442\u0435\u043e\u0440\u0438\u0439. \u0418 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u044b\u0439 \u043f\u0440\u0438\u043c\u0435\u0440: \u00ab\u041b\u0438\u043d\u0433\u0432\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0432\u043e\u043b\u043d\u043e\u0432\u043e\u0439 \u0433\u0435\u043d\u043e\u043c\u00bb \u0430\u043a\u0430\u0434\u0435\u043c\u0438\u043a\u0430 \u041f. \u0413\u0430\u0440\u044f\u0435\u0432\u0430 \u0447\u0435\u0440\u0435\u0437 \u00ab\u0422\u0435\u043e\u0440\u0438\u044e \u044d\u0444\u0438\u0440\u0430\u00bb \u0412. \u0410\u0446\u044e\u043a\u043e\u0432\u0441\u043a\u043e\u0433\u043e.\u042d\u0442\u043e\u0442 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043c\u043d\u043e\u0433\u043e\u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u043c \u0438 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u043c \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439. \u042d\u0442\u043e \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u0442 \u0440\u0435\u0432\u043e\u043b\u044e\u0446\u0438\u044e \u0432 \u043d\u0430\u0443\u043a\u0435, \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0438\u0432 \u0432\u0441\u0435 \u0437\u043d\u0430\u043d\u0438\u044f.\n\u0412\u0430\u0436\u043d\u043e \u043d\u0430\u043b\u0438\u0447\u0438\u0435 \u0443\u0434\u043e\u0431\u043d\u043e\u0433\u043e \u0433\u043e\u043b\u043e\u0441\u043e\u0432\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441\u0430 \u0438 \u043b\u0438\u0447\u043d\u043e\u0433\u043e \u043a\u0430\u0431\u0438\u043d\u0435\u0442\u0430, \u0433\u0434\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u0437\u0430\u043d\u044f\u0442\u0438\u044f.\u0421\u043f\u0430\u0441\u0438\u0431\u043e \u0437\u0430 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435. \u0421 \u0443\u0432\u0430\u0436\u0435\u043d\u0438\u0435\u043c, \u0421\u0443\u0445\u0430\u0447\u0435\u0432 \u0414\u0435\u043d\u0438\u0441.",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1655652120000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":44.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AI-scientist\/td-p\/432867\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-19T15:22:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"I'm from Ukraine. Therefore, I write with the help of a translator. I immediately apologize for any mistakes.\n\nI am a doctor. I am interested in many areas of science that are related to medicine. But because of their volume and complexity, it is impossible to learn by one person.\n\nI propose to create an AI that will analyze information on the Internet (video lectures, articles, books, audio books, images ...) and find relationships. For example, the electrophysical properties of DNA are analyzed through all known theories of physics. And a concrete example: Academician P. Garyaev's \"Linguistic Wave Genome\" through V. Atsyukovsky's \"Ether Theory\".\n\nThis tool needs to be made multifunctional and accessible to all users. This will revolutionize science by combining all knowledge.\nIt is important that there is a convenient voice interface and a personal account where studies are saved.\n\nThank you for attention. Sincerely, Sukhachov Denis.\n\n\u042f \u0441 \u0423\u043a\u0440\u0430\u0438\u043d\u044b. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u044f \u043f\u0438\u0448\u0443 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0447\u0438\u043a\u0430. \u0421\u0440\u0430\u0437\u0443 \u0438\u0437\u0432\u0438\u043d\u044f\u044e\u0441\u044c \u0437\u0430 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u043e\u0448\u0438\u0431\u043a\u0438.\n\n\u042f \u0434\u043e\u043a\u0442\u043e\u0440. \u041c\u0435\u043d\u044f \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0443\u044e\u0442 \u043c\u043d\u043e\u0433\u0438\u0435 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u043d\u0430\u0443\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u0432\u044f\u0437\u0430\u043d\u044b \u0441 \u043c\u0435\u0434\u0438\u0446\u0438\u043d\u043e\u0439. \u041d\u043e \u0438\u0437-\u0437\u0430 \u0438\u0445 \u043e\u0431\u044a\u0435\u043c\u0430 \u0438 \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0438\u0445 \u043d\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u043e\u0441\u0432\u043e\u0438\u0442\u044c \u043e\u0434\u043d\u043e\u043c\u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0443.\n\n\u041f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u044e \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0418\u0418, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0432 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0435 (\u0432\u0438\u0434\u0435\u043e\u043b\u0435\u043a\u0446\u0438\u0438, \u0441\u0442\u0430\u0442\u044c\u0438, \u043a\u043d\u0438\u0433\u0438, \u0430\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0438, \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f...) \u0438 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0438. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0441\u0432\u043e\u0439\u0441\u0442\u0432\u0430 \u0414\u041d\u041a \u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0432\u0441\u0435\u0445 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0442\u0435\u043e\u0440\u0438\u0439. \u0418 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u044b\u0439 \u043f\u0440\u0438\u043c\u0435\u0440: \u00ab\u041b\u0438\u043d\u0433\u0432\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0432\u043e\u043b\u043d\u043e\u0432\u043e\u0439 \u0433\u0435\u043d\u043e\u043c\u00bb \u0430\u043a\u0430\u0434\u0435\u043c\u0438\u043a\u0430 \u041f. \u0413\u0430\u0440\u044f\u0435\u0432\u0430 \u0447\u0435\u0440\u0435\u0437 \u00ab\u0422\u0435\u043e\u0440\u0438\u044e \u044d\u0444\u0438\u0440\u0430\u00bb \u0412. \u0410\u0446\u044e\u043a\u043e\u0432\u0441\u043a\u043e\u0433\u043e.\n\n\u042d\u0442\u043e\u0442 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043c\u043d\u043e\u0433\u043e\u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u043c \u0438 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u043c \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439. \u042d\u0442\u043e \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u0442 \u0440\u0435\u0432\u043e\u043b\u044e\u0446\u0438\u044e \u0432 \u043d\u0430\u0443\u043a\u0435, \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0438\u0432 \u0432\u0441\u0435 \u0437\u043d\u0430\u043d\u0438\u044f.\n\u0412\u0430\u0436\u043d\u043e \u043d\u0430\u043b\u0438\u0447\u0438\u0435 \u0443\u0434\u043e\u0431\u043d\u043e\u0433\u043e \u0433\u043e\u043b\u043e\u0441\u043e\u0432\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441\u0430 \u0438 \u043b\u0438\u0447\u043d\u043e\u0433\u043e \u043a\u0430\u0431\u0438\u043d\u0435\u0442\u0430, \u0433\u0434\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u0437\u0430\u043d\u044f\u0442\u0438\u044f.\n\n\u0421\u043f\u0430\u0441\u0438\u0431\u043e \u0437\u0430 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435. \u0421 \u0443\u0432\u0430\u0436\u0435\u043d\u0438\u0435\u043c, \u0421\u0443\u0445\u0430\u0447\u0435\u0432 \u0414\u0435\u043d\u0438\u0441."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"scientist ukrain write help translat immedi apolog mistak doctor interest area scienc relat medicin volum complex imposs learn person propos creat analyz inform internet video lectur articl book audio book imag relationship exampl electrophys properti dna analyz known theori physic concret exampl academician garyaev linguist wave genom atsyukovski ether theori tool need multifunct access user revolution scienc combin knowledg import conveni voic interfac person account studi save thank attent sincer sukhachov deni",
        "Question_preprocessed_content":"scientist ukrain write help translat immedi apolog doctor interest area scienc relat medicin volum complex imposs learn propos creat analyz inform internet relationship exampl electrophys properti dna analyz known theori physic concret exampl academician garyaev linguist wave genom atsyukovski ether tool need multifunct access user revolution scienc combin knowledg import conveni voic interfac person account studi attent sincer sukhachov deni",
        "Question_gpt_summary_original":"The user, a doctor from Ukraine, is interested in various areas of science related to medicine but finds it impossible to learn everything due to their complexity and volume. They propose creating an AI that can analyze information on the internet and find relationships between different scientific theories. The tool needs to be multifunctional and accessible to all users, with a convenient voice interface and a personal account where studies can be saved. The user believes that this tool will revolutionize science by combining all knowledge.",
        "Question_gpt_summary":"user doctor ukrain interest area scienc relat medicin find imposs learn complex volum propos creat analyz inform internet relationship differ scientif theori tool need multifunct access user conveni voic interfac person account studi save user believ tool revolution scienc combin knowledg",
        "Answer_original_content":"ukrain write help translat immedi apolog mistak doctor interest area scienc relat medicin volum complex imposs learn person propos creat analyz inform internet video lectur articl book audio book imag relationship exampl electrophys properti dna analyz known theori physic concret exampl academician garyaev linguist wave genom atsyukovski ether theori tool need multifunct access user revolution scienc combin knowledg import conveni voic interfac person account studi save thank attent sincer sukhachov deni",
        "Answer_preprocessed_content":"ukrain write help translat immedi apolog mistak doctor interest area scienc relat medicin volum complex imposs learn person propos creat analyz inform internet relationship exampl electrophys properti dna analyz known theori physic concret exampl academician garyaev linguist wave genom atsyukovski ether theori tool need multifunct access user revolution scienc combin knowledg import conveni voic interfac person account studi save thank attent sincer sukhachov deni",
        "Answer_gpt_summary_original":"possible solutions from the answer include creating an ai that can analyze information from the internet, finding relationships between different scientific theories, making the tool multifunctional and accessible to all users, having a convenient voice interface, and a personal account where studies are saved. the goal is to revolutionize science by combining all knowledge.",
        "Answer_gpt_summary":"possibl solut answer includ creat analyz inform internet find relationship differ scientif theori make tool multifunct access user have conveni voic interfac person account studi save goal revolution scienc combin knowledg"
    },
    {
        "Question_id":63500377.0,
        "Question_title":"memory issues for sparse one hot encoded features",
        "Question_body":"<p>I want to create sparse matrix for one hot encoded features from data frame <code>df<\/code>. But I am getting memory issue for code given below. Shape of <code>sparse_onehot<\/code> is  (450138, 1508)<\/p>\n<pre><code>sp_features = ['id', 'video_id', 'genre']\nsparse_onehot = pd.get_dummies(df[sp_features], columns = sp_features)\nimport scipy\nX = scipy.sparse.csr_matrix(sparse_onehot.values)\n<\/code><\/pre>\n<p>I get memory error as shown below.<\/p>\n<pre><code>MemoryError: Unable to allocate 647. MiB for an array with shape (1508, 450138) and data type uint8\n<\/code><\/pre>\n<p>I have tried <code>scipy.sparse.lil_matrix<\/code> and get same error as above.<\/p>\n<p>Is there any efficient way of handling this?\nThanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1597908911857,
        "Question_favorite_count":null,
        "Question_last_edit_time":1597911320856,
        "Question_score":2.0,
        "Question_view_count":97.0,
        "Answer_body":"<p>Try setting to <code>True<\/code> the <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html\" rel=\"nofollow noreferrer\"><code>sparse<\/code> parameter<\/a>:<\/p>\n<blockquote>\n<p>sparsebool, default False\nWhether the dummy-encoded columns should be backed by a SparseArray (True) or a regular NumPy array (False).<\/p>\n<\/blockquote>\n<pre><code>sparse_onehot = pd.get_dummies(df[sp_features], columns = sp_features, sparse = True)\n<\/code><\/pre>\n<p>This will use a much more memory efficient (but somewhat slower) representation than the default one.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63500377",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1597911539563,
        "Question_original_content":"memori issu spars hot encod featur want creat spars matrix hot encod featur data frame get memori issu code given shape spars onehot featur video genr spars onehot dummi featur column featur import scipi scipi spars csr matrix spars onehot valu memori error shown memoryerror unabl alloc mib arrai shape data type uint tri scipi spars lil matrix error effici wai handl thank advanc",
        "Question_preprocessed_content":"memori issu spars hot encod featur want creat spars matrix hot encod featur data frame get memori issu code given shape memori error shown tri error effici wai handl thank advanc",
        "Question_gpt_summary_original":"The user is encountering memory issues while trying to create a sparse matrix for one hot encoded features from a data frame. The shape of the resulting sparse matrix is (450138, 1508), and the user is getting a memory error while trying to allocate memory for an array with shape (1508, 450138) and data type uint8. The user has tried using scipy.sparse.lil_matrix but is still encountering the same error. The user is seeking an efficient way to handle this issue.",
        "Question_gpt_summary":"user encount memori issu try creat spars matrix hot encod featur data frame shape result spars matrix user get memori error try alloc memori arrai shape data type uint user tri scipi spars lil matrix encount error user seek effici wai handl issu",
        "Answer_original_content":"try set true spars paramet sparsebool default fals dummi encod column back sparsearrai true regular numpi arrai fals spars onehot dummi featur column featur spars true us memori effici somewhat slower represent default",
        "Answer_preprocessed_content":"try set paramet sparsebool default fals column back sparsearrai regular numpi arrai us memori effici represent default",
        "Answer_gpt_summary_original":"the solution to the memory issue when creating a sparse matrix for one hot encoded features from a data frame is to set the sparse parameter to true. this will use a more memory-efficient representation than the default one. the code to implement this solution is: sparse_onehot = pd.get_dummies(df[sp_features], columns = sp_features, sparse = true).",
        "Answer_gpt_summary":"solut memori issu creat spars matrix hot encod featur data frame set spars paramet true us memori effici represent default code implement solut spars onehot dummi featur column featur spars true"
    },
    {
        "Question_id":68237132.0,
        "Question_title":"No module named 'Cython' setting up Azure ML docker instance",
        "Question_body":"<p>I'm trying to install the following library in my Azure ML instance:<\/p>\n<p><a href=\"https:\/\/github.com\/philferriere\/cocoapi#egg=pycocotools&amp;subdirectory=PythonAPI\" rel=\"nofollow noreferrer\">https:\/\/github.com\/philferriere\/cocoapi#egg=pycocotools&amp;subdirectory=PythonAPI<\/a><\/p>\n<p>My Dockerfile looks like this:<\/p>\n<pre><code>FROM mcr.microsoft.com\/azureml\/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04:20210615.v1\n\nENV AZUREML_CONDA_ENVIRONMENT_PATH \/azureml-envs\/pytorch-1.7\n\n# Create conda environment\nRUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\n    python=3.7 \\\n    pip=20.2.4 \\\n    pytorch=1.7.1 \\\n    torchvision=0.8.2 \\\n    torchaudio=0.7.2 \\\n    cudatoolkit=11.0 \\\n    nvidia-apex=0.1.0 \\\n    -c anaconda -c pytorch -c conda-forge\n\n# Prepend path to AzureML conda environment\nENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/bin:$PATH\n\n# Install pip dependencies\nRUN HOROVOD_WITH_PYTORCH=1 \\\n    pip install 'matplotlib&gt;=3.3,&lt;3.4' \\\n                'psutil&gt;=5.8,&lt;5.9' \\\n                'tqdm&gt;=4.59,&lt;4.60' \\\n                'pandas&gt;=1.1,&lt;1.2' \\\n                'scipy&gt;=1.5,&lt;1.6' \\\n                'numpy&gt;=1.10,&lt;1.20' \\\n                'azureml-core==1.31.0' \\\n                'azureml-defaults==1.31.0' \\\n                'azureml-mlflow==1.31.0' \\\n                'azureml-telemetry==1.31.0' \\\n                'tensorboard==2.4.0' \\\n                'tensorflow-gpu==2.4.1' \\\n                'onnxruntime-gpu&gt;=1.7,&lt;1.8' \\\n                'horovod[pytorch]==0.21.3' \\\n                'future==0.17.1' \\\n                'git+https:\/\/github.com\/philferriere\/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI'\n\n# This is needed for mpi to locate libpython\nENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/lib:$LD_LIBRARY_PATH\n<\/code><\/pre>\n<p>An error is thrown when the library is being installed:<\/p>\n<pre><code>  Cloning https:\/\/github.com\/philferriere\/cocoapi.git to \/tmp\/pip-install-_i3sjryy\/pycocotools\n[91m    ERROR: Command errored out with exit status 1:\n     command: \/azureml-envs\/pytorch-1.7\/bin\/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' egg_info --egg-base \/tmp\/pip-pip-egg-info-o68by1_q\n         cwd: \/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\n    Complete output (5 lines):\n    Traceback (most recent call last):\n      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n      File &quot;\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py&quot;, line 2, in &lt;module&gt;\n        from Cython.Build import cythonize\n    ModuleNotFoundError: No module named 'Cython'\n<\/code><\/pre>\n<p>I've tried adding Cython as a dependecy in both the pip section and as part of the conda environment but the error is still thrown.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625322434170,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":209.0,
        "Answer_body":"<p>Solution was to add the following to the Dockerfile:<\/p>\n<pre><code># Install Cython\nRUN pip3 install Cython\n\n# Install pip dependencies\nRUN HOROVOD_WITH_PYTORCH=1 \\\n    pip install 'matplotlib&gt;=3.3,&lt;3.4' \\\n                ...\n                'git+https:\/\/github.com\/philferriere\/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68237132",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1625324240192,
        "Question_original_content":"modul name cython set docker instanc try instal follow librari instanc http github com philferrier cocoapi egg pycocotool subdirectori pythonapi dockerfil look like mcr microsoft com openmpi cuda cudnn ubuntu env conda environ path env pytorch creat conda environ run conda creat conda environ path python pip pytorch torchvis torchaudio cudatoolkit nvidia apex anaconda pytorch conda forg prepend path conda environ env path conda environ path bin path instal pip depend run horovod pytorch pip instal matplotlib line file tmp pip instal isjryi pycocotool pythonapi setup line cython build import cython modulenotfounderror modul name cython tri ad cython dependeci pip section conda environ error thrown",
        "Question_preprocessed_content":"modul name cython set docker instanc try instal follow librari instanc dockerfil look like error thrown librari instal tri ad cython dependeci pip section conda environ error thrown",
        "Question_gpt_summary_original":"The user is encountering an error while trying to install a library in their Azure ML instance. The error message indicates that the installation failed due to a missing module named 'Cython'. The user has attempted to add Cython as a dependency in both the pip section and as part of the conda environment, but the error persists.",
        "Question_gpt_summary":"user encount error try instal librari instanc error messag indic instal fail miss modul name cython user attempt add cython depend pip section conda environ error persist",
        "Answer_original_content":"solut add follow dockerfil instal cython run pip instal cython instal pip depend run horovod pytorch pip instal matplotlib git http github com philferrier cocoapi git egg pycocotool subdirectori pythonapi",
        "Answer_preprocessed_content":"solut add follow dockerfil",
        "Answer_gpt_summary_original":"the solution to the \"modulenotfounderror: no module named 'cython'\" error when setting up a docker instance is to add the following lines to the dockerfile: \n\n- install cython: run pip3 install cython \n- install pip dependencies: run horovod_with_pytorch=1 \\ pip install 'matplotlib>=3.3,<3.4' \\ ... 'git+https:\/\/github.com\/philferriere\/cocoapi.git#egg=pycocotools&subdirectory=pythonapi'",
        "Answer_gpt_summary":"solut modulenotfounderror modul name cython error set docker instanc add follow line dockerfil instal cython run pip instal cython instal pip depend run horovod pytorch pip instal matplotlib git http github com philferrier cocoapi git egg pycocotool subdirectori pythonapi"
    },
    {
        "Question_id":52876202.0,
        "Question_title":"How to bulk test the Sagemaker Object detection model with a .mat dataset or S3 folder of images?",
        "Question_body":"<p>I have trained the following Sagemaker model: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco<\/a><\/p>\n\n<p>I've tried both the JSON and RecordIO version. In both, the algorithm is tested on ONE sample image. However, I have a dataset of 2000 pictures, which I would like to test. I have saved the 2000 jpg pictures in a folder within an S3 bucket and I also have two .mat files (pics + ground truth). How can I apply this model to all 2000 pictures at once and then save the results, rather than doing it one picture at a time?<\/p>\n\n<p>I am using the code below to load a single picture from my S3 bucket:<\/p>\n\n<pre><code>object = bucket.Object('pictures\/pic1.jpg')\nobject.download_file('pic1.jpg')\nimg=mpimg.imread('pic1.jpg')\nimg_name = 'pic1.jpg'\nimgplot = plt.imshow(img)\nplt.show(imgplot)\n\nwith open(img_name, 'rb') as image:\n    f = image.read()\n    b = bytearray(f)\n    ne = open('n.txt','wb')\n    ne.write(b)\n\nimport json\nobject_detector.content_type = 'image\/jpeg'\nresults = object_detector.predict(b)\ndetections = json.loads(results)\nprint (detections['prediction'])\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1539872573950,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":112.0,
        "Answer_body":"<p>I'm not sure if I understood your question correctly. However, if you want to feed multiple images to the model at once, you can create a multi-dimensional array of images (byte arrays) to feed the model.<\/p>\n\n<p>The code would look something like this.<\/p>\n\n<pre><code>import numpy as np\n...\n\n#  predict_images_list is a Python list of byte arrays\npredict_images = np.stack(predict_images_list)\n\nwith graph.as_default():\n    #  results is an list of typical results you'd get.\n    results = object_detector.predict(predict_images)\n<\/code><\/pre>\n\n<p>But, I'm not sure if it's a good idea to feed 2000 images at once. Better to batch them in 20-30 images at a time and predict. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52876202",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1539965456440,
        "Question_original_content":"bulk test object detect model mat dataset folder imag train follow model http github com awslab amazon exampl tree master introduct amazon algorithm object detect pascalvoc coco tri json recordio version algorithm test sampl imag dataset pictur like test save jpg pictur folder bucket mat file pic ground truth appli model pictur save result pictur time code load singl pictur bucket object bucket object pictur pic jpg object download file pic jpg img mpimg imread pic jpg img pic jpg imgplot plt imshow img plt imgplot open img imag imag read bytearrai open txt write import json object detector content type imag jpeg result object detector predict detect json load result print detect predict",
        "Question_preprocessed_content":"bulk test object detect model mat dataset folder imag train follow model tri json recordio version algorithm test sampl imag dataset pictur like test save jpg pictur folder bucket mat file appli model pictur save result pictur time code load singl pictur bucket",
        "Question_gpt_summary_original":"The user has encountered a challenge in testing their Sagemaker Object detection model on a dataset of 2000 pictures. The algorithm is currently tested on one sample image at a time, and the user has saved the 2000 jpg pictures in a folder within an S3 bucket and also has two .mat files (pics + ground truth). The user is seeking a way to apply the model to all 2000 pictures at once and save the results.",
        "Question_gpt_summary":"user encount challeng test object detect model dataset pictur algorithm current test sampl imag time user save jpg pictur folder bucket mat file pic ground truth user seek wai appli model pictur save result",
        "Answer_original_content":"sure understood question correctli want feed multipl imag model creat multi dimension arrai imag byte arrai feed model code look like import numpi predict imag list python list byte arrai predict imag stack predict imag list graph default result list typic result result object detector predict predict imag sure good idea feed imag better batch imag time predict",
        "Answer_preprocessed_content":"sure understood question correctli want feed multipl imag model creat arrai imag feed model code look like sure good idea feed imag better batch imag time predict",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- create a multi-dimensional array of images (byte arrays) to feed the model.\n- batch the images in 20-30 images at a time and predict.",
        "Answer_gpt_summary":"possibl solut answer creat multi dimension arrai imag byte arrai feed model batch imag imag time predict"
    },
    {
        "Question_id":71752458.0,
        "Question_title":"MLflow Experiments Tracking : local (dev tools - vscode) to databricks workspace",
        "Question_body":"<p>I had configured my databricks workspace in local using,<\/p>\n<p><code>databricks configure --profile &lt;profile_name&gt; --token<\/code><\/p>\n<p>by which I am able to list the clusters and create secret scope.<\/p>\n<p>But I am unable to create mlflow experiments. I had set the tracking uri to &quot;databricks&quot; and also tested with &quot;databricks\/&lt;profile_name&quot; and tested but i am unable to create or track any experiments on my databricks workspace.<\/p>\n<p>I get this following error;<\/p>\n<p><code>from mlflow.tracking import MlflowClient client = MlflowClient() mlflow.set_tracking_uri(&quot;databricks&quot;) experiment =  client.get_experiment_by_name('\/Shared\/test')<\/code><\/p>\n<p>MlflowException: API request to endpoint was successful but the response body was not in a valid JSON format. Response body: '&lt;!doctype html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;\/&gt;&lt;meta http-equiv=&quot;Content-Language&quot; content=&quot;en&quot;\/&gt;&lt;title&gt;Databricks - Sign In&lt;\/title&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=960&quot;\/&gt;&lt;link rel=&quot;icon&quot; type=&quot;image\/png&quot; href=&quot;\/favicon.ico&quot;\/&gt;&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text\/html; charset=UTF8&quot;\/&gt;&lt;link rel=&quot;icon&quot; href=&quot;\/favicon.ico&quot;&gt;&lt;script defer=&quot;defer&quot; src=&quot;\/login\/login.0ceb14c0.js&quot;&gt;&lt;\/script&gt;&lt;\/head&gt;&lt;body class=&quot;light-mode&quot;&gt;&lt;uses-legacy-bootstrap&gt;&lt;div id=&quot;login-page&quot;&gt;&lt;\/div&gt;&lt;\/uses-legacy-bootstrap&gt;&lt;\/body&gt;&lt;\/html&gt;'<\/p>\n<p>Could someone help me on what I am missing here?<\/p>\n<p>I am expecting to create\/track mlflow experiements in databricks workspace via dev-tools(vscode).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649164485623,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":220.0,
        "Answer_body":"<p>I had the same problem while trying to load a model from model registry with mismatching versions (client 1.22.0).<\/p>\n<p>I had to downgrade the client version to make it work.<\/p>\n<p>Downgraded first the client to 1.21 and then server to 1.20<\/p>\n<p>Refer - <a href=\"https:\/\/docs.databricks.com\/dev-tools\/api\/latest\/mlflow.html#operation\/transition-model-version-stage\" rel=\"nofollow noreferrer\">https:\/\/docs.databricks.com\/dev-tools\/api\/latest\/mlflow.html#operation\/transition-model-version-stage<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71752458",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1649225844710,
        "Question_original_content":"experi track local dev tool vscode databrick workspac configur databrick workspac local databrick configur profil token abl list cluster creat secret scope unabl creat experi set track uri databrick test databrick databrick sign help miss expect creat track experi databrick workspac dev tool vscode",
        "Question_preprocessed_content":"experi track local databrick workspac configur databrick workspac local abl list cluster creat secret scope unabl creat experi set track uri databrick test test unabl creat track experi databrick workspac follow error except api request endpoint success respons bodi valid json format respons bodi databrick sign viewport rel icon rel icon defer defer help miss expect experi databrick workspac",
        "Question_gpt_summary_original":"The user has configured their Databricks workspace in local using a command-line interface and is able to list clusters and create secret scopes. However, they are unable to create MLflow experiments and have set the tracking URI to \"databricks\" and \"databricks\/<profile_name>\" but are still unable to create or track any experiments. They receive an error message indicating that the response body was not in a valid JSON format. The user is seeking help to create and track MLflow experiments in their Databricks workspace via dev-tools (VSCode).",
        "Question_gpt_summary":"user configur databrick workspac local command line interfac abl list cluster creat secret scope unabl creat experi set track uri databrick databrick unabl creat track experi receiv error messag indic respons bodi valid json format user seek help creat track experi databrick workspac dev tool vscode",
        "Answer_original_content":"problem try load model model registri mismatch version client downgrad client version work downgrad client server refer http doc databrick com dev tool api latest html oper transit model version stage",
        "Answer_preprocessed_content":"problem try load model model registri mismatch version downgrad client version work downgrad client server refer",
        "Answer_gpt_summary_original":"possible solution: downgrade the client version to 1.21 and then the server to 1.20 to resolve the issue of loading a model from the model registry with mismatching versions. refer to the documentation for more information.",
        "Answer_gpt_summary":"possibl solut downgrad client version server resolv issu load model model registri mismatch version refer document inform"
    },
    {
        "Question_id":null,
        "Question_title":"DVC and AWS EFS",
        "Question_body":"<p>Hi. Can DVC work with an Amazon Web Services EFS volume? I looked at the list of supported storage types and did not see EFS, but figured I\u2019d ask here to be sure because my boss is asking me to be sure <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1646678462276,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":225.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-aws-efs\/1103",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-07T18:47:58.056Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cesc\">@cesc<\/a><br>\nIsn\u2019t EFS a drive that you mount to your EC2 machine? If so, you could probably use it as a <code>local<\/code> remote type.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-08T13:31:42.607Z",
                "Answer_body":"<p>That makes a lot of sense. Thanks!<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"aw ef work amazon web servic ef volum look list support storag type ef figur ask sure boss ask sure thank",
        "Question_preprocessed_content":"aw ef work amazon web servic ef volum look list support storag type ef figur ask sure boss ask sure thank",
        "Question_gpt_summary_original":"The user is inquiring about whether DVC can work with an Amazon Web Services EFS volume, as they did not see EFS on the list of supported storage types. They are seeking confirmation as their boss has requested them to be sure.",
        "Question_gpt_summary":"user inquir work amazon web servic ef volum ef list support storag type seek confirm boss request sure",
        "Answer_original_content":"cesc isnt ef drive mount machin probabl us local remot type make lot sens thank",
        "Answer_preprocessed_content":"isnt ef drive mount machin probabl us remot type make lot sens thank",
        "Answer_gpt_summary_original":"possible solution: the user can mount the amazon web services efs volume to their ec2 machine and use it as a local remote type.",
        "Answer_gpt_summary":"possibl solut user mount amazon web servic ef volum machin us local remot type"
    },
    {
        "Question_id":53660590.0,
        "Question_title":"PySpark Using collect_list to collect Arrays of Varying Length",
        "Question_body":"<p>I am attempting to use collect_list to collect arrays (and maintain order) from two different data frames.<\/p>\n\n<p>Test_Data and Train_Data have the same format.<\/p>\n\n<pre><code>from pyspark.sql import functions as F\nfrom pyspark.sql import Window\n\nw = Window.partitionBy('Group').orderBy('date')\n\n# Train_Data has 4 data points\n# Test_Data has 7 data points\n# desired target array:         [1, 1, 2, 3]\n# desired MarchMadInd array:    [0, 0, 0, 1, 0, 0, 1]\n\nsorted_list_diff_array_lens = train_data.withColumn('target', \nF.collect_list('target').over(w)\n                                  )\\\ntest_data.withColumn('MarchMadInd', F.collect_list('MarchMadInd').over(w))\\\n   .groupBy('Group')\\\n   .agg(F.max('target').alias('target'), \n    F.max('MarchMadInd').alias('MarchMadInd')\n)\n<\/code><\/pre>\n\n<p>I realize the syntax is incorrect with \"test_data.withColumn\", but I want to select the array for the <em>MarchMadInd<\/em> from the <strong>test_date<\/strong>, but the array for the <em>target<\/em> from the <strong>train_data<\/strong>. The desired output would look like the following:<\/p>\n\n<pre><code>{\"target\":[1, 1, 2, 3], \"MarchMadInd\":[0, 0, 0, 1, 0, 0, 1]}\n<\/code><\/pre>\n\n<p>Context: this is for a DeepAR time series model (using AWS) that requires dynamic features to include the prediction period, but the target should be historical data.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1544134968250,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1544534805888,
        "Question_score":0.0,
        "Question_view_count":345.0,
        "Answer_body":"<p>The solution involves using a join as recommended by pault. <\/p>\n\n<ol>\n<li>Create a dataframe with dynamic features of length equal to Training + Prediction period<\/li>\n<li>Create a dataframe with target values of length equal to just the Training period.<\/li>\n<li>Use a LEFT JOIN (with the dynamic feature data on LEFT) to bring these dataframes together<\/li>\n<\/ol>\n\n<p>Now, using collect_list will create the desired result.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53660590",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1555600483376,
        "Question_original_content":"pyspark collect list collect arrai vari length attempt us collect list collect arrai maintain order differ data frame test data train data format pyspark sql import function pyspark sql import window window partitionbi group orderbi date train data data point test data data point desir target arrai desir marchmadind arrai sort list diff arrai len train data withcolumn target collect list target test data withcolumn marchmadind collect list marchmadind groupbi group agg max target alia target max marchmadind alia marchmadind realiz syntax incorrect test data withcolumn want select arrai marchmadind test date arrai target train data desir output look like follow target marchmadind context deepar time seri model aw requir dynam featur includ predict period target histor data",
        "Question_preprocessed_content":"pyspark collect arrai vari length attempt us collect arrai differ data frame format realiz syntax incorrect want select arrai marchmadind arrai target desir output look like follow context deepar time seri model requir dynam featur includ predict period target histor data",
        "Question_gpt_summary_original":"The user is attempting to use PySpark's collect_list function to collect arrays from two different data frames while maintaining order. The desired output is to have the target array from the train_data and the MarchMadInd array from the test_data. However, the syntax used is incorrect, and the user is seeking help to achieve the desired output. This is for a DeepAR time series model that requires dynamic features to include the prediction period, but the target should be historical data.",
        "Question_gpt_summary":"user attempt us pyspark collect list function collect arrai differ data frame maintain order desir output target arrai train data marchmadind arrai test data syntax incorrect user seek help achiev desir output deepar time seri model requir dynam featur includ predict period target histor data",
        "Answer_original_content":"solut involv join recommend pault creat datafram dynam featur length equal train predict period creat datafram target valu length equal train period us left join dynam featur data left bring datafram collect list creat desir result",
        "Answer_preprocessed_content":"solut involv join recommend pault creat datafram dynam featur length equal train predict period creat datafram target valu length equal train period us left join bring datafram creat desir result",
        "Answer_gpt_summary_original":"the solution to the challenge of using pyspark's collect_list to collect arrays of varying length from two different data frames involves creating two data frames, one with dynamic features of length equal to training + prediction period and another with target values of length equal to just the training period. then, a left join is used to bring these data frames together, and collect_list can be used to create the desired output of two arrays.",
        "Answer_gpt_summary":"solut challeng pyspark collect list collect arrai vari length differ data frame involv creat data frame dynam featur length equal train predict period target valu length equal train period left join bring data frame collect list creat desir output arrai"
    },
    {
        "Question_id":null,
        "Question_title":"Local Deployment Azure ML failed with error",
        "Question_body":"I am a beginner in the Azure. I am using this tutorial https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python of setting a dummy script for a local web service but many errors are coming up. It is strange because I am using an h5 file (model involving Keras and tensor flow) in place of onxx file. I used the code\n\n from azureml.core import Environment\n from azureml.core.model import InferenceConfig\n    \n env = Environment(name=\"myenv\")\n conda_dep = CondaDependencies()\n conda_dep.add_conda_package(\"tensorflow\")\n conda_dep.add_conda_package(\"pip\")\n conda_dep.add_pip_package(\"azureml-core\")\n conda_dep.add_pip_package(\"azureml-contrib-services\")\n conda_dep.add_pip_package(\"azureml.api\")\n env.python.conda_dependencies=conda_dep\n inference_config = InferenceConfig(\n     environment=env,\n     source_directory=\".\/source_dir\",\n     entry_script=\".\/echo_score.py\",\n\n)\n\nI am trying to deploy the model local using Webservice. But always getting some error. I have tried many times but does not work. I am always getting some error. It is bizarre.\n\n\n\n\n$ conda update -n base -c defaults conda\n\n\n\n\nPip subprocess error:\nERROR: Could not find a version that satisfies the requirement azureml.api (from -r \/azureml-environment-setup\/condaenv.811vr6y8.requirements.txt (line 4)) (from versions: none)\nERROR: No matching distribution found for azureml.api (from -r \/azureml-environment-setup\/condaenv.811vr6y8.requirements.txt (line 4))\n\n\n\n\nCondaEnvException: Pip failed",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1631297380170,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/547716\/local-deployment-azure-ml-failed-with-error.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-13T07:04:56.04Z",
                "Answer_score":1,
                "Answer_body":"@santra-4408 The error indicates that the environment setup is not done or the start of the environment setup is failing. I think you need to add the following line since you are using conda package installation along with pip packages.\n\n  from azureml.core.environment import CondaDependencies\n\nYou can also use this sample notebook for localwebservice deployment using Azure notebooks feature for ml.azure.com\nThis notebook or repo can be cloned directly from the portal and can be run to deploy the webservice.",
                "Answer_comment_count":4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-23T14:06:07.737Z",
                "Answer_score":0,
                "Answer_body":"Hi I am trying to deploy Prophet Model.\nModel is logged as mlflow.prophet flavour.\nHere the code for deployment:\n\n import azureml\n import mlflow.azureml\n from azureml.core import Workspace\n from azureml.core.authentication import ServicePrincipalAuthentication\n from azureml.core.webservice import AciWebservice, Webservice\n    \n    \n    \n principal_auth = ServicePrincipalAuthentication(tenant_id, principal_id, app_secret)\n workspace = Workspace.get(name=workspace_name, subscription_id=subscription_id, auth=principal_auth, resource_group=workspace_rg)\n model_image, azureml_model = mlflow.azureml.build_image(model_uri='runs:\/016fc3f2d5c34c8eadbeb20ad16d3\/082358timeseriesforecasting', workspace=workspace)\n model_image.wait_for_creation(show_output=True)\n\n\n\nWhen I am creating image from Model Artifact this error came :\n\nWebserviceException: WebserviceException:\nMessage: Image creation polling reached non-successful terminal state, current state: Failed\nError response from server:\nStatusCode: 400\nMessage: Docker image build failed.\nInnerException None\nErrorResponse\n{\n\"error\": {\n\"message\": \"Image creation polling reached non-successful terminal state, current state: Failed\\nError response from server:\\nStatusCode: 400\\nMessage: Docker image build failed.\"\n}\n}\n\n\n\n\nIn Container logs error is shown below:\n\n[0mThe command '\/bin\/sh -c CONDA_ROOT_DIR=$(conda info --root) && if [ -n \"$AZUREML_CONDA_ENVIRONMENT_PATH\" ]; then conda env update -p \"$AZUREML_CONDA_ENVIRONMENT_PATH\" -f '\/var\/azureml-app\/conda.yaml'; else conda env update -n base -f '\/var\/azureml-app\/conda.yaml'; fi && conda clean -aqy && rm -rf \/root\/.cache\/pip && rm -rf \"$CONDA_ROOT_DIR\/pkgs\" && find \"$CONDA_ROOT_DIR\" -type d -name pycache -exec rm -rf {} +' returned a non-zero code: 137\n2021\/11\/23 12:21:39 Container failed during run: acb_step_0. No retries remaining.\nfailed to run step ID: acb_step_0: exit status 137\n\nRun ID: cj58 failed after 6m19s. Error: failed during run, err: exit status 1\n\n\n\n\n\n\n\nI have tried different yaml dependencies even defaults but of no use.\nDo anyone have a clue what is wrong here? Any solution?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"local deploy fail error beginn azur tutori http doc microsoft com azur machin learn deploi tab python set dummi script local web servic error come strang file model involv kera tensor flow place onxx file code core import environ core model import inferenceconfig env environ myenv conda dep condadepend conda dep add conda packag tensorflow conda dep add conda packag pip conda dep add pip packag core conda dep add pip packag contrib servic conda dep add pip packag api env python conda depend conda dep infer config inferenceconfig environ env sourc directori sourc dir entri script echo score try deploi model local webservic get error tri time work get error bizarr conda updat base default conda pip subprocess error error version satisfi requir api environ setup condaenv vry requir txt line version error match distribut api environ setup condaenv vry requir txt line condaenvexcept pip fail",
        "Question_preprocessed_content":"local deploy fail error beginn azur tutori set dummi script local web servic error come strang file place onxx file code core import environ import inferenceconfig env environ condadepend inferenceconfig try deploi model local webservic get error tri time work get error bizarr conda updat base default conda pip subprocess error error version satisfi requir api error match distribut api condaenvexcept pip fail",
        "Question_gpt_summary_original":"The user is facing challenges while trying to deploy a local web service using Azure ML. They are encountering errors while using an h5 file instead of an onxx file. The user has tried to deploy the model multiple times but is still getting errors. The error message suggests that there is an issue with the azureml.api package.",
        "Question_gpt_summary":"user face challeng try deploi local web servic encount error file instead onxx file user tri deploi model multipl time get error error messag suggest issu api packag",
        "Answer_original_content":"santra error indic environ setup start environ setup fail think need add follow line conda packag instal pip packag core environ import condadepend us sampl notebook localwebservic deploy azur notebook featur azur com notebook repo clone directli portal run deploi webservic try deploi prophet model model log prophet flavour code deploy import import core import workspac core authent import serviceprincipalauthent core webservic import aciwebservic webservic princip auth serviceprincipalauthent tenant princip app secret workspac workspac workspac subscript subscript auth princip auth resourc group workspac model imag model build imag model uri run fcfdcceadbebadd timeseriesforecast workspac workspac model imag wait creation output true creat imag model artifact error came webserviceexcept webserviceexcept messag imag creation poll reach non success termin state current state fail error respons server statuscod messag docker imag build fail innerexcept errorrespons error messag imag creation poll reach non success termin state current state fail nerror respons server nstatuscod nmessag docker imag build fail contain log error shown mthe command bin conda root dir conda info root conda environ path conda env updat conda environ path var app conda yaml conda env updat base var app conda yaml conda clean aqi root cach pip conda root dir pkg conda root dir type pycach exec return non zero code contain fail run acb step retri remain fail run step acb step exit statu run fail error fail run err exit statu tri differ yaml depend default us clue wrong solut",
        "Answer_preprocessed_content":"error indic environ setup start environ setup fail think need add follow line conda packag instal pip packag import condadepend us sampl notebook localwebservic deploy azur notebook featur notebook repo clone directli portal run deploi webservic try deploi prophet model model log prophet flavour code deploy import import core import workspac import serviceprincipalauthent import aciwebservic webservic workspac workspac workspac creat imag model artifact error came webserviceexcept webserviceexcept messag imag creation poll reach termin state current state fail error respons server statuscod messag docker imag build fail innerexcept errorrespons error contain log error shown mthe command info conda env updat conda env updat base conda clean aqi type pycach exec return code contain fail run retri remain fail run step exit statu run fail error fail run err exit statu tri differ yaml depend default us clue wrong solut",
        "Answer_gpt_summary_original":"the answer suggests that the error encountered during local web service deployment may be due to a failure in environment setup. the solution proposed is to add a specific line of code and use a sample notebook for local web service deployment. the answer also includes code for deploying a prophet model and building an image from the model artifact, but it seems that the image creation failed due to an error in the container logs. the answer does not provide a clear solution to this specific error.",
        "Answer_gpt_summary":"answer suggest error encount local web servic deploy failur environ setup solut propos add specif line code us sampl notebook local web servic deploy answer includ code deploi prophet model build imag model artifact imag creation fail error contain log answer provid clear solut specif error"
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker batch transform not loading CSV correctly",
        "Question_body":"I am running a batch transform job that us uploading data from a CSV. The CSV is formatted as such\n\n\"joe annes rifle accesories discount\"\n\"cute puppies for sale\"\n\"Two dudes talk about sports\"\n\"Smith & Wesson M&P 500 review\"\n\"Glock vs 1911 handgun\"\n\n\nMy code for creating the batch transform is below\n\nelec_model = PyTorchModel(model_data='s3:\/\/some_path\/binary-models\/tar_models\/14_10_2022__19_54_23_arms_ammunition.tar.gz',\n                         role=role,\n                         entry_point='torchserve_.py',\n                         source_dir='source_dir',\n                          framework_version='1.12.0',\n                          py_version='py38')\n\nnl_detector = elec_model.transformer(\n                     instance_count = 1,\n                     instance_type = 'ml.g4dn.xlarge', strategy=\"MultiRecord\", assemble_with=\"Line\", output_path = \"s3:\/\/some_path\/trash_output\")\n\nnl_detector.transform(\"s3:\/\/brand-safety-training-data\/trash\", content_type=\"text\/csv\", split_type=\"Line\")\n\n\nWhen I run this code instead of the batch job taking the CSV and breaking up the examples with every space, which is what\n\nsplit_type=\"Line\" \n\n\nis telling the algorithm to do, but instead it just ingests all of the sentences in the above CSV, and outputs 1 probability. Also, if I do the same thing with the same code, but switch\n\nstrategy=\"MultiRecord\"\n\n\nto\n\nstrategy=\"SingleRecord\"\n\n\nso the one code block would look like this\n\nnl_detector = elec_model.transformer(\n                     instance_count = 1,\n                     instance_type = 'ml.g4dn.xlarge', strategy=\"SingleRecord\", assemble_with=\"Line\", output_path = \"s3:\/\/some_path\/trash_output\")\n\n\nThe algorithm works correctly, and performs inference on all of the above sentences in the CSV correctly. Any reason why this is happening?\n\nEDIT 1: When I print the input payload it looks like this\n\n\"joe annes rifle accesories discount\"\n\n2022-10-26T21:03:04,265 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n\n\"cute puppies for sale\"\n\n2022-10-26T21:03:04,265 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n\n\"Two dudes talk about sports\"\n\n2022-10-26T21:03:04,265 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\n\n\nWhere each sentence is a inference example, and is separated by this statement\n\n2022-10-26T21:03:04,265 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\n\n\nSo it seems like sagemaker is separating the inference examples. But when I try and pass these sentences into a huggingface tokenizer, the tokenizer tokenizes them like they are one inference example, when they should be 3 distinct inference examples.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1666732676105,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":96.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEci4LOFuRdeRj40V-koySw\/sage-maker-batch-transform-not-loading-csv-correctly",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-30T04:11:00.002Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nThis issue is not quite related to SageMaker but how you pass the data to transformer tokenizer.\n\nYou are right on split_type=\"Line\" which splits your CSV files by lines. However, MultiRecord will ask SageMaker to pack as many lines as possible, up to MaxPayloadInMB as described in this doc. The default value is 6 MB. SingleRecord, on the other hand, will pass lines one by one.\n\nThe text will be from Byte\/IO stream, which essentially something like a string as follows, \"joe annes rifle accesories discount\\ncute puppies for sale\\nTwo dudes talk about sports\\nSmith & Wesson M&P 500 review\\nGlock vs 1911 handgun\" If we pass this directly to tokenizer, it will be treated as a single string.\n\nYou can firstly parse the string to a list like\n\n[\"joe annes rifle accesories discount\",  \"cute puppies for sale\", \"Two dudes talk about sports\", \"Smith & Wesson M&P 500 review\", \"Glock vs 1911 handgun\"]\n\n\nbefore passing data to tokenizer, which will result in a (5, xxx) tensor. This could ensure transformer understand sentences individually.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"batch transform load csv correctli run batch transform job upload data csv csv format joe ann rifl accesori discount cute puppi sale dude talk sport smith wesson review glock handgun code creat batch transform elec model pytorchmodel model data path binari model tar model arm ammunit tar role role entri point torchserv sourc dir sourc dir framework version version detector elec model transform instanc count instanc type gdn xlarg strategi multirecord assembl line output path path trash output detector transform brand safeti train data trash content type text csv split type line run code instead batch job take csv break exampl space split type line tell algorithm instead ingest sentenc csv output probabl thing code switch strategi multirecord strategi singlerecord code block look like detector elec model transform instanc count instanc type gdn xlarg strategi singlerecord assembl line output path path trash output algorithm work correctli perform infer sentenc csv correctli reason happen edit print input payload look like joe ann rifl accesori discount info model stdout com amazonaw mm wlm workerlifecycl cute puppi sale info model stdout com amazonaw mm wlm workerlifecycl dude talk sport info model stdout com amazonaw mm wlm workerlifecycl sentenc infer exampl separ statement info model stdout com amazonaw mm wlm workerlifecycl like separ infer exampl try pass sentenc huggingfac token token token like infer exampl distinct infer exampl",
        "Question_preprocessed_content":"batch transform load csv correctli run batch transform job upload data csv csv format joe ann rifl accesori discount cute puppi sale dude talk sport smith wesson review glock handgun code creat batch transform role role strategi multirecord run code instead batch job take csv break exampl space tell algorithm instead ingest sentenc csv output probabl thing code switch strategi multirecord strategi singlerecord code block look like strategi singlerecord algorithm work correctli perform infer sentenc csv correctli reason happen edit print input payload look like joe ann rifl accesori discount cute puppi sale dude talk sport sentenc infer exampl separ statement like separ infer exampl try pass sentenc huggingfac token token token like infer exampl distinct infer exampl",
        "Question_gpt_summary_original":"The user is encountering challenges with SageMaker batch transform not loading CSV correctly. The CSV file is not being broken up into separate examples as expected, resulting in only one probability output. The issue seems to be related to the \"split_type\" and \"strategy\" parameters used in the code. The user has tried different strategies and observed that the algorithm works correctly with \"SingleRecord\" strategy but not with \"MultiRecord\" strategy. When the input payload is printed, it appears that SageMaker is separating the inference examples, but the Huggingface tokenizer is tokenizing them as one example instead of three distinct examples.",
        "Question_gpt_summary":"user encount challeng batch transform load csv correctli csv file broken separ exampl expect result probabl output issu relat split type strategi paramet code user tri differ strategi observ algorithm work correctli singlerecord strategi multirecord strategi input payload print appear separ infer exampl huggingfac token token exampl instead distinct exampl",
        "Answer_original_content":"issu relat pass data transform token right split type line split csv file line multirecord ask pack line possibl maxpayloadinmb describ doc default valu singlerecord hand pass line text byte stream essenti like string follow joe ann rifl accesori discount ncute puppi sale ntwo dude talk sport nsmith wesson review nglock handgun pass directli token treat singl string firstli pars string list like joe ann rifl accesori discount cute puppi sale dude talk sport smith wesson review glock handgun pass data token result tensor ensur transform understand sentenc individu",
        "Answer_preprocessed_content":"issu relat pass data transform token right split csv file line multirecord ask pack line possibl maxpayloadinmb describ doc default valu singlerecord hand pass line text stream essenti like string follow joe ann rifl accesori puppi dude talk wesson handgun pass directli token treat singl string firstli pars string list like joe ann rifl accesori discount cute puppi sale dude talk sport smith wesson review glock handgun pass data token result tensor ensur transform understand sentenc individu",
        "Answer_gpt_summary_original":"the solution to the issue of the batch transform job not correctly loading data from a csv is to parse the string to a list before passing data to tokenizer. this will ensure that the transformer understands sentences individually. the user can use multirecord or singlerecord to pass lines one by one or pack as many lines as possible, respectively.",
        "Answer_gpt_summary":"solut issu batch transform job correctli load data csv pars string list pass data token ensur transform understand sentenc individu user us multirecord singlerecord pass line pack line possibl respect"
    },
    {
        "Question_id":63807950.0,
        "Question_title":"Letter Recognition Error in Azure ML Studio",
        "Question_body":"<p>I'm having troubles with a Letter Recognition model I'm creating in Azure ML Studio.<\/p>\n<p>I'm running a few algorithms - Decision Jungle, Neural Network, Decision Forest, Logistic Regression, One vs. All Multiclass, and then I append them using the Add rows method (Neural Network and Desicion Jungle\/ Decision Forest and Logistic Regression), until I append them all.<\/p>\n<p>However, appending Decision Forest and Logistic Regression I get the following error:<\/p>\n<pre><code>requestId = 9292bc066f51404eb5e0d0d219d3a072 errorComponent=Module. taskStatusCode=400. {&quot;Exception&quot;:{&quot;ErrorId&quot;:&quot;NotInRangeValue&quot;,&quot;ErrorCode&quot;:&quot;0008&quot;,&quot;ExceptionType&quot;:&quot;ModuleException&quot;,&quot;Message&quot;:&quot;Error 0008: Parameter \\&quot;Dataset2(number of columns)\\&quot; value should be in the range of [3, 3].&quot;}}Error: Error 0008: Parameter &quot;Dataset2(number of columns)&quot; value should be in the range of [3, 3]. Process exited with error code -2\n<\/code><\/pre>\n<p>Any advice what should I do? Huge thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1599640980777,
        "Question_favorite_count":null,
        "Question_last_edit_time":1599642150896,
        "Question_score":0.0,
        "Question_view_count":37.0,
        "Answer_body":"<p>This error occurs when there is a mismatch of number of columns of the two dataset you are appending.<\/p>\n<p>Looking at the error :<\/p>\n<p>The output of one model is returning rows with 3 columns and other one is having either more or less than 3 columns.<\/p>\n<p>Before this step &quot;Add Rows&quot; step -&gt; Do quick Visualize<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PsYQT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PsYQT.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This will give a view of the dataset that you are planning to append.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/x442d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/x442d.png\" alt=\"![enter image description here\" \/><\/a><\/p>\n<p>Ensure for both, the columns numbers are same.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63807950",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1599653828448,
        "Question_original_content":"letter recognit error studio have troubl letter recognit model creat studio run algorithm decis jungl neural network decis forest logist regress multiclass append add row method neural network desicion jungl decis forest logist regress append append decis forest logist regress follow error requestid bcfebeddda errorcompon modul taskstatuscod except errorid notinrangevalu errorcod exceptiontyp moduleexcept messag error paramet dataset number column valu rang error error paramet dataset number column valu rang process exit error code advic huge thank advanc",
        "Question_preprocessed_content":"letter recognit error studio have troubl letter recognit model creat studio run algorithm decis jungl neural network decis forest logist regress multiclass append add row method append append decis forest logist regress follow error advic huge thank advanc",
        "Question_gpt_summary_original":"The user is encountering an error while creating a Letter Recognition model in Azure ML Studio. They are using several algorithms and appending them using the Add rows method, but when they try to append Decision Forest and Logistic Regression, they receive an error message stating that the parameter \"Dataset2(number of columns)\" value should be in the range of [3, 3]. The user is seeking advice on how to resolve this issue.",
        "Question_gpt_summary":"user encount error creat letter recognit model studio algorithm append add row method try append decis forest logist regress receiv error messag state paramet dataset number column valu rang user seek advic resolv issu",
        "Answer_original_content":"error occur mismatch number column dataset append look error output model return row column have column step add row step quick visual view dataset plan append ensur column number",
        "Answer_preprocessed_content":"error occur mismatch number column dataset append look error output model return row column have column step add row step quick visual view dataset plan append ensur column number",
        "Answer_gpt_summary_original":"possible solutions to the \"notinrangevalue\" error when appending decision forest and logistic regression models in studio are to check for a mismatch of the number of columns in the two datasets being appended. before the \"add rows\" step, it is recommended to do a quick visualization to ensure that both datasets have the same number of columns.",
        "Answer_gpt_summary":"possibl solut notinrangevalu error append decis forest logist regress model studio check mismatch number column dataset append add row step recommend quick visual ensur dataset number column"
    },
    {
        "Question_id":null,
        "Question_title":"Comparing different artifact versions visually",
        "Question_body":"<p>I am running an image generative model and logging the generated images at each step as a new version of the same artifact. I would like to compare the generated images over time. I was able to compare two versions of the same artifact using \u201ccompare\u201d in the artifact view (and doing an inner join between the tables containing the images). However I was not able to compare more than two versions. I have tried using weave in a reportbut it seems to be very buggy.<br>\nAny ideas how I can compare multiple versions of an artifact (to view the improvement in image generation over time).<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1652183776601,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":375.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/comparing-different-artifact-versions-visually\/2394",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-11T08:56:51.110Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/mmabrouk_modelme\">@mmabrouk_modelme<\/a>. I am checking whether that\u2019s possible<\/p>",
                "Answer_score":16.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-18T08:52:19.194Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/mmabrouk_modelme\">@mmabrouk_modelme<\/a>, apologies about the delay. At the moment comparing more that two versions of an artifact is not possible.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-17T08:52:55.515Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"compar differ artifact version visual run imag gener model log gener imag step new version artifact like compar gener imag time abl compar version artifact compar artifact view inner join tabl contain imag abl compar version tri weav reportbut buggi idea compar multipl version artifact view improv imag gener time",
        "Question_preprocessed_content":"compar differ artifact version visual run imag gener model log gener imag step new version artifact like compar gener imag time abl compar version artifact compar artifact view abl compar version tri weav reportbut buggi idea compar multipl version artifact",
        "Question_gpt_summary_original":"The user is facing challenges in comparing multiple versions of an artifact generated by an image generative model. They were able to compare two versions using \"compare\" in the artifact view but were unable to compare more than two versions. The user tried using weave in a report but found it to be buggy. They are seeking ideas on how to compare multiple versions to view the improvement in image generation over time.",
        "Question_gpt_summary":"user face challeng compar multipl version artifact gener imag gener model abl compar version compar artifact view unabl compar version user tri weav report buggi seek idea compar multipl version view improv imag gener time",
        "Answer_original_content":"hei mmabrouk modelm check that possibl hei mmabrouk modelm apolog delai moment compar version artifact possibl topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"hei check that possibl hei apolog delai moment compar version artifact possibl topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the user is informed that comparing more than two versions of an artifact is not possible at the moment.",
        "Answer_gpt_summary":"solut provid answer user inform compar version artifact possibl moment"
    },
    {
        "Question_id":67925860.0,
        "Question_title":"Kedro Data Modelling",
        "Question_body":"<p>We are struggling to model our data correctly for use in Kedro - we are using the recommended Raw\\Int\\Prm\\Ft\\Mst model but are struggling with some of the concepts....e.g.<\/p>\n<ul>\n<li>When is a dataset a feature rather than a primary dataset? The distinction seems vague...<\/li>\n<li>Is it OK for a primary dataset to consume data from another primary dataset?<\/li>\n<li>Is it good practice to build a feature dataset from the INT layer? or should it always pass through Primary?<\/li>\n<\/ul>\n<p>I appreciate there are no hard &amp; fast rules with data modelling but these are big modelling decisions &amp; any guidance or best practice on Kedro modelling would be really helpful, I can find just one table defining the layers in the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/12_faq\/01_faq.html#what-is-data-engineering-convention\" rel=\"nofollow noreferrer\">Kedro docs<\/a><\/p>\n<p>If anyone can offer any further advice or blogs\\docs talking about Kedro Data Modelling that would be awesome!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1623345871000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":180.0,
        "Answer_body":"<p>Great question. As you say, there are no hard and fast rules here and opinions do vary, but let me share my perspective as a QB data scientist and kedro maintainer who has used the layering convention you referred to several times.<\/p>\n<p>For a start, let me emphasise that there's absolutely no reason to stick to the data engineering convention suggested by kedro if it's not suitable for your needs. 99% of users don't change the folder structure in <code>data<\/code>. This is not because the kedro default is the right structure for them but because they just don't think of changing it. You should absolutely add\/remove\/rename layers to suit yourself. The most important thing is to choose a set of layers (or even a non-layered structure) that works for your project rather than trying to shoehorn your datasets to fit the kedro default suggestion.<\/p>\n<p>Now, assuming you are following kedro's suggested structure - onto your questions:<\/p>\n<blockquote>\n<p>When is a dataset a feature rather than a primary dataset? The distinction seems vague...<\/p>\n<\/blockquote>\n<p>In the case of simple features, a feature dataset can be very similar to a primary one. The distinction is maybe clearest if you think about more complex features, e.g. formed by aggregating over time windows. A primary dataset would have a column that gives a cleaned version of the original data, but without doing any complex calculations on it, just simple transformations. Say the raw data is the colour of all cars driving past your house over a week. By the time the data is in primary, it will be clean (e.g. correcting &quot;rde&quot; to &quot;red&quot;, maybe mapping &quot;crimson&quot; and &quot;red&quot; to the same colour). Between primary and the feature layer, we will have done some less trivial calculations on it, e.g. to find one-hot encoded most common car colour each day.<\/p>\n<blockquote>\n<p>Is it OK for a primary dataset to consume data from another primary dataset?<\/p>\n<\/blockquote>\n<p>In my opinion, yes. This might be necessary if you want to join multiple primary tables together. In general if you are building complex pipelines it will become very difficult if you don't allow this. e.g. in the feature layer I might want to form a dataset containing <code>composite_feature = feature_1 * feature_2<\/code> from the two inputs <code>feature_1<\/code> and <code>feature_2<\/code>. There's no way of doing this without having multiple sub-layers within the feature layer.<\/p>\n<p>However, something that is generally worth avoiding is a node that consumes data from many different layers. e.g. a node that takes in one dataset from the feature layer and one from the intermediate layer. This seems a bit strange (why has the latter dataset not passed through the feature layer?).<\/p>\n<blockquote>\n<p>Is it good practice to build a feature dataset from the INT layer? or should it always pass through Primary?<\/p>\n<\/blockquote>\n<p>Building features from the intermediate layer isn't unheard of, but it seems a bit weird. The primary layer is typically an important one which forms the basis for all feature engineering. If your data is in a shape that you can build features then that means it's probably primary layer already. In this case, maybe you don't need an intermediate layer.<\/p>\n<p>The above points might be summarised by the following rules (which should no doubt be broken when required):<\/p>\n<ol>\n<li>The input datasets for a node in layer <code>L<\/code> should all be in the same layer, which can be either <code>L<\/code> or <code>L-1<\/code><\/li>\n<li>The output datasets for a node in layer <code>L<\/code> should all be in the same layer <code>L<\/code>, which can be either <code>L<\/code> or <code>L+1<\/code><\/li>\n<\/ol>\n<blockquote>\n<p>If anyone can offer any further advice or blogs\\docs talking about Kedro Data Modelling that would be awesome!<\/p>\n<\/blockquote>\n<p>I'm also interested in seeing what others think here! One possibly useful thing to note is that kedro was inspired by cookiecutter data science, and the kedro layer structure is an extended version of <a href=\"http:\/\/drivendata.github.io\/cookiecutter-data-science\/#directory-structure\" rel=\"nofollow noreferrer\">what's suggested there<\/a>. Maybe other projects have taken this directory structure and adapted it in different ways.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67925860",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1623349806340,
        "Question_original_content":"data model struggl model data correctli us recommend raw int prm mst model struggl concept dataset featur primari dataset distinct vagu primari dataset consum data primari dataset good practic build featur dataset int layer pass primari appreci hard fast rule data model big model decis guidanc best practic model help tabl defin layer doc offer advic blog doc talk data model awesom",
        "Question_preprocessed_content":"data model struggl model data correctli us recommend model struggl dataset featur primari dataset distinct primari dataset consum data primari dataset good practic build featur dataset int layer pass primari appreci hard fast rule data model big model decis guidanc best practic model help tabl defin layer doc offer advic talk data model awesom",
        "Question_gpt_summary_original":"The user is facing challenges in correctly modeling their data for use in Kedro. They are using the recommended Raw\\Int\\Prm\\Ft\\Mst model but are struggling with concepts such as when a dataset is a feature rather than a primary dataset, whether it is okay for a primary dataset to consume data from another primary dataset, and whether it is good practice to build a feature dataset from the INT layer or if it should always pass through Primary. The user is seeking guidance and best practices on Kedro modeling as they are big modeling decisions.",
        "Question_gpt_summary":"user face challeng correctli model data us recommend raw int prm mst model struggl concept dataset featur primari dataset okai primari dataset consum data primari dataset good practic build featur dataset int layer pass primari user seek guidanc best practic model big model decis",
        "Answer_original_content":"great question hard fast rule opinion vari let share perspect data scientist maintain layer convent refer time start let emphasis absolut reason stick data engin convent suggest suitabl need user chang folder structur data default right structur think chang absolut add remov renam layer suit import thing choos set layer non layer structur work project try shoehorn dataset fit default suggest assum follow suggest structur question dataset featur primari dataset distinct vagu case simpl featur featur dataset similar primari distinct mayb clearest think complex featur form aggreg time window primari dataset column give clean version origin data complex calcul simpl transform raw data colour car drive past hous week time data primari clean correct rde red mayb map crimson red colour primari featur layer trivial calcul hot encod common car colour dai primari dataset consum data primari dataset opinion ye necessari want join multipl primari tabl gener build complex pipelin difficult allow featur layer want form dataset contain composit featur featur featur input featur featur wai have multipl sub layer featur layer gener worth avoid node consum data differ layer node take dataset featur layer intermedi layer bit strang dataset pass featur layer good practic build featur dataset int layer pass primari build featur intermedi layer isn unheard bit weird primari layer typic import form basi featur engin data shape build featur mean probabl primari layer case mayb need intermedi layer point summaris follow rule doubt broken requir input dataset node layer layer output dataset node layer layer offer advic blog doc talk data model awesom interest see think possibl us thing note inspir cookiecutt data scienc layer structur extend version suggest mayb project taken directori structur adapt differ wai",
        "Answer_preprocessed_content":"great question hard fast rule opinion vari let share perspect data scientist maintain layer convent refer time start let emphasis absolut reason stick data engin convent suggest suitabl need user chang folder structur default right structur think chang absolut layer suit import thing choos set layer work project try shoehorn dataset fit default suggest assum follow suggest structur question dataset featur primari dataset distinct case simpl featur featur dataset similar primari distinct mayb clearest think complex featur form aggreg time window primari dataset column give clean version origin data complex calcul simpl transform raw data colour car drive past hous week time data primari clean primari featur layer trivial calcul encod common car colour dai primari dataset consum data primari dataset opinion ye necessari want join multipl primari tabl gener build complex pipelin difficult allow featur layer want form dataset contain input wai have multipl featur layer gener worth avoid node consum data differ layer node take dataset featur layer intermedi layer bit strang good practic build featur dataset int layer pass primari build featur intermedi layer isn unheard bit weird primari layer typic import form basi featur engin data shape build featur mean probabl primari layer case mayb need intermedi layer point summaris follow rule input dataset node layer layer output dataset node layer layer offer advic talk data model awesom interest see think possibl us thing note inspir cookiecutt data scienc layer structur extend version suggest mayb project taken directori structur adapt differ wai",
        "Answer_gpt_summary_original":"the answer provides some possible solutions for the user struggling to model their data. the most important thing is to choose a set of layers that work for the project rather than trying to fit the default suggestion. the distinction between a dataset being a feature or a primary dataset is vague, but the primary dataset is typically an important one that forms the basis for all feature engineering. it is generally good practice to avoid a node that consumes data from many different layers. the input datasets for a node in layer l should all be in the same layer, which can be either l or l-1, and the output datasets for a node in layer l should all be in the same layer l, which can be either l or l+1.",
        "Answer_gpt_summary":"answer provid possibl solut user struggl model data import thing choos set layer work project try fit default suggest distinct dataset featur primari dataset vagu primari dataset typic import form basi featur engin gener good practic avoid node consum data differ layer input dataset node layer layer output dataset node layer layer"
    },
    {
        "Question_id":null,
        "Question_title":"Waiter TrainingJobCompletedOrStopped failed: Waiter encountered a terminal",
        "Question_body":"I am trying to launch a job using the low level api in boto3 sagemaker client. After calling sagemaker.create_training_job(**params) I try to get a waiter. This code is directly from the documentation for creating a training job (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html)\nI get this error:\n\nTraceback (most recent call last):\r\n  File \"traindeploy.py\", line 97, in create_training_job\r\n    sagemaker.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\r\n  File \"\/path\/to\/lib\/Python\/3.6\/lib\/python\/site-packages\/botocore\/waiter.py\", line 53, in wait\r\n    Waiter.wait(self, **kwargs)\r\n  File \"\/path\/to\/lib\/Python\/3.6\/lib\/python\/site-packages\/botocore\/waiter.py\", line 323, in wait\r\n    last_response=response,\r\nbotocore.exceptions.WaiterError: Waiter TrainingJobCompletedOrStopped failed: Waiter encountered a terminal failure state\n\n\nThese are my job params:\n\n{\r\n  \"AlgorithmSpecification\": {\r\n    \"TrainingImage\": \"<image-url-from-ecr>\",\r\n    \"TrainingInputMode\": \"File\"\r\n  },\r\n  \"RoleArn\": \"<role-arn>\",\r\n  \"OutputDataConfig\": {\r\n    \"S3OutputPath\": \"s3:\/\/path-to-bucket\/some-folder-output\/\"\r\n  },\r\n  \"ResourceConfig\": {\r\n    \"InstanceCount\": 2,\r\n    \"InstanceType\": \"ml.c4.8xlarge\",\r\n    \"VolumeSizeInGB\": 50\r\n  },\r\n  \"TrainingJobName\": \"some-jobname\",\r\n  \"HyperParameters\": {},\r\n  \"StoppingCondition\": {\r\n    \"MaxRuntimeInSeconds\": 3600\r\n  },\r\n  \"InputDataConfig\": [\r\n    {\r\n      \"ChannelName\": \"train\",\r\n      \"DataSource\": {\r\n        \"S3DataSource\": {\r\n          \"S3DataType\": \"S3Prefix\",\r\n          \"S3Uri\": \"s3:\/\/path-to-bucket\/some-folder-input\/\",\r\n          \"S3DataDistributionType\": \"FullyReplicated\"\r\n        }\r\n      },\r\n      \"CompressionType\": \"None\",\r\n      \"RecordWrapperType\": \"None\"\r\n    }\r\n  ]\r\n}\n\n\nCan someone please advise what is causing this and how will I get a waiter on a training job?",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1526422450000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":401.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeYBDZTVwQlWovkq2eE1CkQ\/waiter-training-job-completed-or-stopped-failed-waiter-encountered-a-terminal",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-05-17T23:27:35.000Z",
                "Answer_score":0,
                "Answer_body":"Hello rks,\n\nYou received that error message because the waiter you created was expecting the training job to finish in either a Completed or Stopped state, but it reached the Failed state instead. This means there was an issue with your job and it could not be completed correctly.\n\nTo understand what caused your training job to fail, you can follow these steps:\n\nGo to the Amazon SageMaker console in your account\nClick on \"Jobs\" in the navigation bar to the left\nSearch for your training job in the list. You can use the \"Search jobs\" text form to quickly find the job by its name, or filter them by status.\nThe training job details should explain why it failed.\n\nIf you need more help with this issue, please don't hesitate to contact us.\n\nBest regards,\nAmazon SageMaker team",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-05-18T01:37:08.000Z",
                "Answer_score":0,
                "Answer_body":"Hi Rodrigo,\n\nThanks for looking into this. Actually the training job does eventually succeed. I tried putting time.wait(seconds=N) before I get call get_waiter, but no matter what wait time I chose the waiter still failed giving the same exception. I waited 5 seconds, I waited 2 minutes, I even waited 3 minutes which was more than the time my training job took to complete successfully, but I always got the same exception.\n\nSo, I want to emphasize that there is nothing wrong with the training jobs I create. They succeed. But the get_waiter method always fails for me. What am I doing wrong here?\n\nEdited by: rks on May 17, 2018 4:34 PM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-05-17T22:39:01.000Z",
                "Answer_score":0,
                "Answer_body":"Hi rks,\n\nI'm sorry for the confusion, I misunderstood your issue. I attempted to reproduce it by running the example code myself in a SageMaker notebook, but I was able to run the job and the waiter worked correctly. For the record, I used the low-level KMeans MNIST sample notebook that comes bundled with all SageMaker notebooks. You can find it in \"\/sample-notebooks\/sagemaker-python-sdk\/1P_kmeans_lowlevel\/kmeans_mnist_lowlevel.ipynb\". That notebook should be very similar to the code you tried to run.\n\nCould you tell us more about how the environment you're running the job in? In particular, we'd like to know the version of Python and BOTO you're using. You can run the following commands to get them:\n\nimport sys, boto3\r\nprint(\"boto version = \" + boto3.__version__)\r\nprint(\"python version = \" + sys.version)\n\n\nThank you for your patience.\n\nRodrigo",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-06-12T02:41:29.000Z",
                "Answer_score":0,
                "Answer_body":"Hi Roberto,\n\nThank you for helping me out. This is the version of boto and python I am running:\n\n16:30 $ python3.6\r\nPython 3.6.4 (v3.6.4:d48ecebad5, Dec 18 2017, 21:07:28) \r\n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\nimport sys, boto3\r\nprint(\"boto version = \" + boto3.__version__)\r\n    boto version = 1.5.18\r\nprint(\"python version = \" + sys.version)\r\n    python version = 3.6.4 (v3.6.4:d48ecebad5, Dec 18 2017, 21:07:28) \r\n    [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]\n\n\nI'll review my code based on the notebook you've pointed out.\n\nEdited by: rks on May 18, 2018 4:35 PM\n\nEdited by: rks on May 18, 2018 4:35 PM\n\nEdited by: rks on May 18, 2018 4:36 PM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-05-18T23:34:52.000Z",
                "Answer_score":0,
                "Answer_body":"Hi rks,\n\nWe have tried to troubleshoot this issue but it doesn't seem like it's reproducible. It's likely that the training-job name was invalid. Is this issue still occurring?\n\nThanks,\nIngrid",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"waiter trainingjobcompletedorstop fail waiter encount termin try launch job low level api boto client call creat train job param try waiter code directli document creat train job http doc aw amazon com latest train model creat train job html error traceback recent file traindeploi line creat train job waiter train job complet stop wait trainingjobnam job file path lib python lib python site packag botocor waiter line wait waiter wait self kwarg file path lib python lib python site packag botocor waiter line wait respons respons botocor except waitererror waiter trainingjobcompletedorstop fail waiter encount termin failur state job param algorithmspecif trainingimag traininginputmod file rolearn outputdataconfig soutputpath path bucket folder output resourceconfig instancecount instancetyp xlarg volumesizeingb trainingjobnam jobnam hyperparamet stoppingcondit maxruntimeinsecond inputdataconfig channelnam train datasourc sdatasourc sdatatyp sprefix suri path bucket folder input sdatadistributiontyp fullyrepl compressiontyp recordwrappertyp advis caus waiter train job",
        "Question_preprocessed_content":"waiter trainingjobcompletedorstop fail waiter encount termin try launch job low level api boto client call try waiter code directli document creat train job error traceback file line file line wait kwarg file line wait waiter trainingjobcompletedorstop fail waiter encount termin failur state job param algorithmspecif rolearn outputdataconfig resourceconfig trainingjobnam hyperparamet stoppingcondit inputdataconfig advis caus waiter train job",
        "Question_gpt_summary_original":"The user is encountering an error while trying to launch a job using the low level API in boto3 sagemaker client. After calling sagemaker.create_training_job(**params), the user tries to get a waiter but encounters a terminal failure state. The user has provided the job parameters and is seeking advice on what is causing the error and how to get a waiter on a training job.",
        "Question_gpt_summary":"user encount error try launch job low level api boto client call creat train job param user tri waiter encount termin failur state user provid job paramet seek advic caus error waiter train job",
        "Answer_original_content":"hello rk receiv error messag waiter creat expect train job finish complet stop state reach fail state instead mean issu job complet correctli understand caus train job fail follow step consol account click job navig bar left search train job list us search job text form quickli job filter statu train job detail explain fail need help issu hesit contact best regard team rodrigo thank look actual train job eventu succe tri put time wait second waiter matter wait time chose waiter fail give except wait second wait minut wait minut time train job took complet successfulli got except want emphas wrong train job creat succe waiter method fail wrong edit rk rk sorri confus misunderstood issu attempt reproduc run exampl code notebook abl run job waiter work correctli record low level kmean mnist sampl notebook come bundl notebook sampl notebook python sdk kmean lowlevel kmean mnist lowlevel ipynb notebook similar code tri run tell environ run job particular like know version python boto run follow command import sy boto print boto version boto version print python version sy version thank patienc rodrigo roberto thank help version boto python run python python decebad dec gcc appl build dot darwin type help copyright credit licens inform import sy boto print boto version boto version boto version print python version sy version python version decebad dec gcc appl build dot review code base notebook point edit rk edit rk edit rk rk tri troubleshoot issu like reproduc like train job invalid issu occur thank ingrid",
        "Answer_preprocessed_content":"hello rk receiv error messag waiter creat expect train job finish complet stop state reach fail state instead mean issu job complet correctli understand caus train job fail follow step consol account click job navig bar left search train job list us search job text form quickli job filter statu train job detail explain fail need help issu hesit contact best regard team rodrigo thank look actual train job eventu succe tri put matter wait time chose waiter fail give except wait second wait minut wait minut time train job took complet successfulli got except want emphas wrong train job creat succe method fail wrong edit rk rk sorri confus misunderstood issu attempt reproduc run exampl code notebook abl run job waiter work correctli record kmean mnist sampl notebook come bundl notebook notebook similar code tri run tell environ run job particular like know version python boto run follow command import sy boto print print thank patienc rodrigo roberto thank help version boto python run python gcc darwin type help copyright credit licens inform import sy boto print boto version print python version review code base notebook point edit rk edit rk edit rk rk tri troubleshoot issu like reproduc like invalid issu occur thank ingrid",
        "Answer_gpt_summary_original":"the answer provides a solution to the user's challenge of getting a waiter on a training job. the solution involves going to the console, clicking on \"jobs,\" searching for the training job, and reviewing the job details to understand why it failed. additionally, the answer suggests checking the version of python and boto being used and reviewing the code based on a sample notebook. the answer also mentions that the issue may be due to an invalid training-job name.",
        "Answer_gpt_summary":"answer provid solut user challeng get waiter train job solut involv go consol click job search train job review job detail understand fail addition answer suggest check version python boto review code base sampl notebook answer mention issu invalid train job"
    },
    {
        "Question_id":null,
        "Question_title":"Data Labeling Data name",
        "Question_body":"Is there a possibility to show the data name of an image while labeling?\nSo I mean in this step https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-labeling-projects\nIt would be much easier to choose the label if we know the data name",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1622641393617,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/419384\/data-labeling-data-name.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-02T23:26:11.54Z",
                "Answer_score":0,
                "Answer_body":"Hi, there's no way to show the dataset name while labeling, it only shows the images. However, I'd be more than happy to share your feedback with the product team as a feature request.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-28T07:04:28.687Z",
                "Answer_score":0,
                "Answer_body":"Update: This is a supported feature now. whether allowing labeler to check the image name is controlled by the project owner.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"data label data possibl data imag label mean step http doc microsoft com azur machin learn creat label project easier choos label know data",
        "Question_preprocessed_content":"data label data possibl data imag label mean step easier choos label know data",
        "Question_gpt_summary_original":"The user is facing a challenge while labeling data and is seeking a way to display the data name of an image during the labeling process to make it easier to choose the appropriate label.",
        "Question_gpt_summary":"user face challeng label data seek wai displai data imag label process easier choos appropri label",
        "Answer_original_content":"wai dataset label show imag happi share feedback product team featur request updat support featur allow label check imag control project owner",
        "Answer_preprocessed_content":"wai dataset label show imag happi share feedback product team featur request updat support featur allow label check imag control project owner",
        "Answer_gpt_summary_original":"solution: initially, there was no way to show the dataset name while labeling in the  labeling project. however, the product team has now added this feature, and the project owner can control whether the labeler can check the image name.",
        "Answer_gpt_summary":"solut initi wai dataset label label project product team ad featur project owner control label check imag"
    },
    {
        "Question_id":73360734.0,
        "Question_title":"Programmatically enable installed extensions in Vertex AI Managed Notebook instance",
        "Question_body":"<p>I am working in JupyterLab within a Managed Notebook instance, accessed through the Vertex AI workbench, as part of a Google Cloud Project. When the instance is created, there are a number of JupyterLab extensions that are installed by default. In the web GUI, one can click the puzzle piece icon and enable\/disable all extensions with a single button click. I currently run a post-startup bash script to manage environments and module installations, and I would like to add to this script whatever commands would turn on the existing extensions. My understanding is that I can do this with<\/p>\n<pre><code># Status of extensions\njupyter labextension list\n# Enable\/disable some extension\njupyter labextension enable extensionIdentifierHere\n<\/code><\/pre>\n<p>However, when I test the enable\/disable command in an instance Terminal window, I receive, for example<\/p>\n<pre><code>[Errno 13] Permission denied: '\/opt\/conda\/etc\/jupyter\/labconfig\/page_config.json'\n<\/code><\/pre>\n<p>If I try to run this with <code>sudo<\/code>, I am asked for a password, but have no idea what that would be, given that I just built the environment and didn't set any password.<\/p>\n<p>Any insights on how to set this up, what the command(s) may be, or how else to approach this, would be appreciated.<\/p>\n<p>Potentially relevant:<\/p>\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/65950610\/not-able-to-install-jupyterlab-extensions-on-gcp-ai-platform-notebooks\">Not able to install Jupyterlab extensions on GCP AI Platform Notebooks<\/a><\/p>\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/52753205\/unable-to-sudo-to-deep-learning-image\">Unable to sudo to Deep Learning Image<\/a><\/p>\n<p><a href=\"https:\/\/jupyterlab.readthedocs.io\/en\/stable\/user\/extensions.html#enabling-and-disabling-extensions\" rel=\"nofollow noreferrer\">https:\/\/jupyterlab.readthedocs.io\/en\/stable\/user\/extensions.html#enabling-and-disabling-extensions<\/a><\/p>\n<p>Edit 1:\nAdding more detail in response to answers and comments (@gogasca, @kiranmathew). My goal is to use ipyleaft-based mapping, through the geemap and earthengine-api python modules, within the notebook. If I create a Managed Notebook instance (service account, Networks shared with me, Enable terminal, all other defaults), launch JupyterLab, open the Terminal from the Launcher, and then run a bash script that creates a venv virtual environment, exposes a custom kernel, and performs the installations, I can use geemap and ipywidgets to visualize and modify (e.g., widget sliders that change map properties) Google Earth Engine assets in a Notebook. If I try to replicate this using a Docker image, it seems to break the connection with ipyleaflet, such that when I start the instance and use a Notebook, I have access to the modules (they can be imported) but can't use ipyleaflet to do the visualization. I thought the issue was that I was not properly enabling the extensions, per the &quot;Error displaying widget: model not found&quot; error, addressed in <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/504\" rel=\"nofollow noreferrer\">this<\/a>, <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/889\" rel=\"nofollow noreferrer\">this<\/a>, <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/547\" rel=\"nofollow noreferrer\">this<\/a>, <a href=\"https:\/\/leafmap.org\/faq\/\" rel=\"nofollow noreferrer\">this<\/a>, etc. -- hence the title of my post. I tried using and modifying @TylerErickson 's Dockerfile that modifies a Google deep learning container and should handle all of this (<a href=\"https:\/\/github.com\/gee-community\/ee-jupyter-contrib\/blob\/master\/docker\/gcp_ai_deep_learning_platform\/Dockerfile\" rel=\"nofollow noreferrer\">here<\/a>), but both the original and modifications break the ipyleaflet connection when booting the Managed Notebook instance from the Docker image.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1660564868170,
        "Question_favorite_count":null,
        "Question_last_edit_time":1660731534620,
        "Question_score":2.0,
        "Question_view_count":202.0,
        "Answer_body":"<p>Google Managed Notebooks do not support third-party JL extensions.  Most of these extensions require a rebuild of the JupyterLab static assets bundle. This requires root access which our Managed Notebooks do not support.<\/p>\n<p>Untangling this limitation would require a significant change to the permission and security model that Managed Notebooks provides. It would also have implications for the supportability of the product itself since a user could effectively break their Managed Notebook by installing something rogue.<\/p>\n<p>I would suggest to use User Managed Notebooks.<\/p>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1660634341887,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73360734",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1660628912063,
        "Question_original_content":"programmat enabl instal extens manag notebook instanc work jupyterlab manag notebook instanc access workbench googl cloud project instanc creat number jupyterlab extens instal default web gui click puzzl piec icon enabl disabl extens singl button click current run post startup bash script manag environ modul instal like add script command turn exist extens understand statu extens jupyt labextens list enabl disabl extens jupyt labextens enabl extensionidentifierher test enabl disabl command instanc termin window receiv exampl errno permiss deni opt conda jupyt labconfig page config json try run sudo ask password idea given built environ set password insight set command approach appreci potenti relev abl instal jupyterlab extens gcp platform notebook unabl sudo deep learn imag http jupyterlab readthedoc stabl user extens html enabl disabl extens edit ad respons answer comment gogasca kiranmathew goal us ipyleaft base map geemap earthengin api python modul notebook creat manag notebook instanc servic account network share enabl termin default launch jupyterlab open termin launcher run bash script creat venv virtual environ expos custom kernel perform instal us geemap ipywidget visual modifi widget slider chang map properti googl earth engin asset notebook try replic docker imag break connect ipyleaflet start instanc us notebook access modul import us ipyleaflet visual thought issu properli enabl extens error displai widget model error address titl post tri modifi tylererickson dockerfil modifi googl deep learn contain handl origin modif break ipyleaflet connect boot manag notebook instanc docker imag",
        "Question_preprocessed_content":"programmat enabl instal extens manag notebook instanc work jupyterlab manag notebook instanc access workbench googl cloud project instanc creat number jupyterlab extens instal default web gui click puzzl piec icon extens singl button click current run bash script manag environ modul instal like add script command turn exist extens understand test command instanc termin window receiv exampl try run ask password idea given built environ set password insight set command approach appreci potenti relev abl instal jupyterlab extens gcp platform notebook unabl sudo deep learn imag edit ad respons answer comment goal us map geemap python modul notebook creat manag notebook instanc launch jupyterlab open termin launcher run bash script creat venv virtual environ expos custom kernel perform instal us geemap ipywidget visual modifi googl earth engin asset notebook try replic docker imag break connect ipyleaflet start instanc us notebook access modul us ipyleaflet visual thought issu properli enabl extens error displai widget model error address titl post tri modifi dockerfil modifi googl deep learn contain handl origin modif break ipyleaflet connect boot manag notebook instanc docker imag",
        "Question_gpt_summary_original":"The user is facing challenges in programmatically enabling installed JupyterLab extensions in a Managed Notebook instance accessed through the Vertex AI workbench. The user is encountering a permission denied error when trying to enable\/disable extensions using a post-startup bash script and is unable to use sudo as they do not have a password. The user is seeking insights on how to set this up, what the command(s) may be, or how else to approach this. The user has also provided potentially relevant links for further information.",
        "Question_gpt_summary":"user face challeng programmat enabl instal jupyterlab extens manag notebook instanc access workbench user encount permiss deni error try enabl disabl extens post startup bash script unabl us sudo password user seek insight set command approach user provid potenti relev link inform",
        "Answer_original_content":"googl manag notebook support parti extens extens requir rebuild jupyterlab static asset bundl requir root access manag notebook support untangl limit requir signific chang permiss secur model manag notebook provid implic support product user effect break manag notebook instal rogu suggest us user manag notebook",
        "Answer_preprocessed_content":"googl manag notebook support extens extens requir rebuild jupyterlab static asset bundl requir root access manag notebook support untangl limit requir signific chang permiss secur model manag notebook provid implic support product user effect break manag notebook instal rogu suggest us user manag notebook",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n- google managed notebooks do not support third-party jl extensions, so the user may need to switch to user-managed notebooks.\n- rebuilding the jupyterlab static assets bundle requires root access, which is not supported by managed notebooks.\n- untangling this limitation would require a significant change to the permission and security model that managed notebooks provides, which could have implications for the supportability of the product itself.",
        "Answer_gpt_summary":"possibl solut answer googl manag notebook support parti extens user need switch user manag notebook rebuild jupyterlab static asset bundl requir root access support manag notebook untangl limit requir signific chang permiss secur model manag notebook provid implic support product"
    },
    {
        "Question_id":72425907.0,
        "Question_title":"How to log a tensorflow model with mlflow.tensorflow.log_model (error module 'tensorflow._api.v2.saved_model' has no attribute 'tag_constants')",
        "Question_body":"<p>I am trying to log a trained model with MLFlow using mlflow.tensorflow.log_model.<\/p>\n<p>After training a simple sequential tf model<\/p>\n<pre><code>history = binary_model.fit(train_ds, validation_data=val_ds, epochs=num_epochs)\n<\/code><\/pre>\n<p>I am trying to log it:<\/p>\n<pre><code>    from tensorflow.python.saved_model import signature_constants\n    tag=[tf.saved_model.tag_constants.SERVING]\n    key=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n\n    mlflow.tensorflow.log_model(tf_saved_model_dir=saved_model_path,\n                                tf_meta_graph_tags=tag,\n                                tf_signature_def_key=key,\n                                artifact_path=&quot;tf-models&quot;,\n                                registered_model_name=model_name)\n<\/code><\/pre>\n<p>but I get the error:<\/p>\n<pre><code>    AttributeError                            Traceback (most recent call last)\n    \/var\/folders\/2k\/g7p7j2gx6v54vkwv3v401h2m0000gn\/T\/ipykernel_73638\/562549064.py in &lt;module&gt;\n          1 from tensorflow.python.saved_model import signature_constants\n    ----&gt; 2 tag=[tf.saved_model.tag_constants.SERVING]\n          3 key=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n          4 \n          5 mlflow.tensorflow.log_model(tf_saved_model_dir=saved_model_path,\n\n    AttributeError: module 'tensorflow._api.v2.saved_model' has no attribute 'tag_constants'\n<\/code><\/pre>\n<p>Any idea how to get the tags and keys correctly from the model to log it in MLFlow?<\/p>\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1653845789630,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":319.0,
        "Answer_body":"<p>The <code>tag_constants<\/code> is in <code>tf.compat.v1.saved_model<\/code>.<\/p>\n<p>To resolve the error replace this line<\/p>\n<pre><code>tag=[tf.saved_model.tag_constants.SERVING]\n<\/code><\/pre>\n<p>with this<\/p>\n<pre><code>tag=[tf.compat.v1.saved_model.tag_constants.SERVING]\n<\/code><\/pre>\n<p>Please refer <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/compat\/v1\/saved_model\/tag_constants\" rel=\"nofollow noreferrer\">this<\/a> for more details.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72425907",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1655094569432,
        "Question_original_content":"log tensorflow model tensorflow log model error modul tensorflow api save model attribut tag constant try log train model tensorflow log model train simpl sequenti model histori binari model fit train valid data val epoch num epoch try log tensorflow python save model import signatur constant tag save model tag constant serv kei signatur constant default serv signatur def kei tensorflow log model save model dir save model path meta graph tag tag signatur def kei kei artifact path model regist model model error attributeerror traceback recent var folder gpjgxvvkwvvhmgn ipykernel tensorflow python save model import signatur constant tag save model tag constant serv kei signatur constant default serv signatur def kei tensorflow log model save model dir save model path attributeerror modul tensorflow api save model attribut tag constant idea tag kei correctli model log thank advanc",
        "Question_preprocessed_content":"log tensorflow model try log train model train simpl sequenti model try log error idea tag kei correctli model log thank advanc",
        "Question_gpt_summary_original":"The user is encountering an error while trying to log a trained TensorFlow model with MLFlow using mlflow.tensorflow.log_model. The error message indicates that the module 'tensorflow._api.v2.saved_model' has no attribute 'tag_constants'. The user is seeking assistance in resolving this issue and successfully logging the model in MLFlow.",
        "Question_gpt_summary":"user encount error try log train tensorflow model tensorflow log model error messag indic modul tensorflow api save model attribut tag constant user seek assist resolv issu successfulli log model",
        "Answer_original_content":"tag constant compat save model resolv error replac line tag save model tag constant serv tag compat save model tag constant serv refer detail",
        "Answer_preprocessed_content":"resolv error replac line refer detail",
        "Answer_gpt_summary_original":"the solution to resolve the attributeerror encountered when logging a trained tensorflow model using .tensorflow.log_model is to replace the line tag=[tf.saved_model.tag_constants.serving] with tag=[tf.compat.v1.saved_model.tag_constants.serving]. this change is required because the tag_constants is in tf.compat.v1.saved_model.",
        "Answer_gpt_summary":"solut resolv attributeerror encount log train tensorflow model tensorflow log model replac line tag save model tag constant serv tag compat save model tag constant serv chang requir tag constant compat save model"
    },
    {
        "Question_id":null,
        "Question_title":"Api key + entity verification",
        "Question_body":"<p>Hello Everyone,<\/p>\n<p>I\u2019ve been using this package for the past year to keep track of all my phd experiments (it is awesome!!!). I am in the process of developing an application (using streamlit) that makes use of my neural network framework more accessible to users.  For that reason, I want to provide users with the ability to use their wandb credentials to start logging results to their accounts. As far as I understand you need a valid API key and entity. Is there a way to verify that this API_key+entity combination exists?? Passing a random 40 character string in the key parameter of wandb.login() returns true, so I suspect that it only checks the length of the key and not if it actually exists. I guess I can try logging to a dummy project and then catch an exception (this means that the API key or the entity name is wrong) but I\u2019m looking for something more elegant.<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641324610203,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":267.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/api-key-entity-verification\/1687",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-20T18:02:05.401Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"api kei entiti verif hello iv packag past year track phd experi awesom process develop applic streamlit make us neural network framework access user reason want provid user abil us credenti start log result account far understand need valid api kei entiti wai verifi api kei entiti combin exist pass random charact string kei paramet login return true suspect check length kei actual exist guess try log dummi project catch except mean api kei entiti wrong look eleg thank",
        "Question_preprocessed_content":"api kei entiti verif hello iv packag past year track phd experi process develop applic make us neural network framework access user reason want provid user abil us credenti start log result account far understand need valid api kei entiti wai verifi combin exist pass random charact string kei paramet login return true suspect check length kei actual exist guess try log dummi project catch except look eleg thank",
        "Question_gpt_summary_original":"The user is developing an application using streamlit that allows users to log results to their wandb accounts. However, the user is facing challenges in verifying the API key and entity combination. The user suspects that the API key verification only checks the length of the key and not if it actually exists. The user is looking for a more elegant solution than logging to a dummy project and catching an exception.",
        "Question_gpt_summary":"user develop applic streamlit allow user log result account user face challeng verifi api kei entiti combin user suspect api kei verif check length kei actual exist user look eleg solut log dummi project catch except",
        "Answer_original_content":"topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer as it is not related to the question.",
        "Answer_gpt_summary":"solut provid answer relat question"
    },
    {
        "Question_id":null,
        "Question_title":"Does dvc work for live streaming data versioning and batch data versioning ? If yes, can someone explain briefly",
        "Question_body":"<p>I couldn\u2019t find any dvc documentation for batch data, and live streaming data versioning.<br>\nIs it possible in dvc to track streaming data and also fetch data in batch or time travel data?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1619414992269,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":796.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/does-dvc-work-for-live-streaming-data-versioning-and-batch-data-versioning-if-yes-can-someone-explain-briefly\/738",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-26T05:52:19.927Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/veeresh\">@veeresh<\/a>,<\/p>\n<p>DVC could be used to maintain snapshots of a growing dataset every X time (prob not real-time for a raw stream though) IF the dataset growth looks like adding\/removing files. That would work efficiently because DVC de-duplicates data storage at the file level (see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a>). So in that narrow case, it\u2019s technically possible.<\/p>\n<p>That said, there may be better tools for timed data back-ups. And I would question whether snapshots of a data stream should be considered versions: You could say they\u2019re all parts of the same version, which you just haven\u2019t obtained completely yet (maybe never will). If the stream includes updates\/corrections to previously received data points though, then that would more clearly represent versioning IMO.<\/p>\n<p>This is all pretty conceptual though. Feel free to share a more specific use case you have in mind for more concrete answer <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>",
                "Answer_score":51.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-26T06:40:24.955Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a> ,<\/p>\n<p>Thanks for explaining.<\/p>\n<p>I was looking if something similar to delta lake time travel is possible in dvc. (<a href=\"https:\/\/databricks.com\/blog\/2019\/02\/04\/introducing-delta-time-travel-for-large-scale-data-lakes.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Introducing Delta Time Travel for Large Scale Data Lakes - The Databricks Blog<\/a>)<\/p>\n<p>Lets say there are 2 folders which keep updating every day (steaming),<br>\n-dogs<br>\n-cats<br>\nBasically, I want to get data between some time period (between 2 dates) , this is not possible in dvc right?<\/p>",
                "Answer_score":46.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-26T06:44:01.795Z",
                "Answer_body":"<p>DVC uses Git as the underlying versioning layer. In Git you decide when and what to include in your commits. You can only switch between the commits that you have registered yourself.<\/p>\n<p>The new Experiments features do include some automatic tracking of multiple project versions but again that has a different purpose (ML <a href=\"https:\/\/dvc.org\/doc\/user-guide\/experiment-management\">experiment management<\/a>).<\/p>\n<p>Thanks<\/p>",
                "Answer_score":51.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"work live stream data version batch data version ye explain briefli document batch data live stream data version possibl track stream data fetch data batch time travel data",
        "Question_preprocessed_content":"work live stream data version batch data version ye explain briefli document batch data live stream data version possibl track stream data fetch data batch time travel data",
        "Question_gpt_summary_original":"The user is facing challenges in finding documentation for using dvc for batch data versioning and live streaming data versioning. They are unsure if it is possible to track streaming data and fetch data in batch or time travel data using dvc.",
        "Question_gpt_summary":"user face challeng find document batch data version live stream data version unsur possibl track stream data fetch data batch time travel data",
        "Answer_original_content":"veeresh maintain snapshot grow dataset time prob real time raw stream dataset growth look like ad remov file work effici duplic data storag file level http org doc user guid larg dataset optim narrow case technic possibl said better tool time data up question snapshot data stream consid version theyr part version havent obtain complet mayb stream includ updat correct previous receiv data point clearli repres version imo pretti conceptu feel free share specif us case mind concret answer jorgeorpinel thank explain look similar delta lake time travel possibl introduc delta time travel larg scale data lake databrick blog let folder updat dai steam dog cat basic want data time period date possibl right us git underli version layer git decid includ commit switch commit regist new experi featur includ automat track multipl project version differ purpos experi manag thank",
        "Answer_preprocessed_content":"maintain snapshot grow dataset time dataset growth look like file work effici data storag file level narrow case technic possibl said better tool time data question snapshot data stream consid version theyr part version havent obtain complet stream includ previous receiv data point clearli repres version imo pretti conceptu feel free share specif us case mind concret answer thank explain look similar delta lake time travel possibl let folder updat dai dog cat basic want data time period possibl right us git underli version layer git decid includ commit switch commit regist new experi featur includ automat track multipl project version differ purpos thank",
        "Answer_gpt_summary_original":"possible solutions mentioned in the answer include using snapshots to maintain versions of a growing dataset every x time, which could work efficiently for adding\/removing files. however, there may be better tools for timed data backups. the answer also suggests that if the stream includes updates\/corrections to previously received data points, then that would more clearly represent versioning. additionally, the answer mentions that uses git as the underlying versioning layer, but switching between commits is limited to those registered by the user. the new experiments features include some automatic tracking of multiple project versions, but that has a different purpose (ml experiment management).",
        "Answer_gpt_summary":"possibl solut mention answer includ snapshot maintain version grow dataset time work effici ad remov file better tool time data backup answer suggest stream includ updat correct previous receiv data point clearli repres version addition answer mention us git underli version layer switch commit limit regist user new experi featur includ automat track multipl project version differ purpos experi manag"
    },
    {
        "Question_id":null,
        "Question_title":"Wandb login not working for sweep",
        "Question_body":"<p>Hi folks<br>\nI have been seeing this since Friday Feb 3 afternoon PST. I am using wandb version <code>0.13.7<\/code>. I am logged in at wandb, but when I do<\/p>\n<pre><code class=\"lang-auto\">wandb sweep sweep.yaml\n<\/code><\/pre>\n<p>I see below error<\/p>\n<pre><code class=\"lang-auto\">wandb: ERROR Error while calling W&amp;B API: permission denied (&lt;Response [403]&gt;)\nwandb: ERROR Find detailed error logs at: \/tmp\/debug-cli.ec2-user.log\nError: permission denied\n<\/code><\/pre>\n<p>Below is the complete error log<\/p>\n<pre><code class=\"lang-auto\">2023-02-05 00:45:38 ERROR 403 response executing GraphQL.\n2023-02-05 00:45:38 ERROR {\"errors\":[{\"message\":\"permission denied\",\"path\":[\"upsertSweep\"],\"extensions\":{\"code\":\"PERMISSION_ERROR\"}}],\"data\":{\"upsertSweep\":null}}\n2023-02-05 00:45:38 ERROR Traceback (most recent call last):\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/retry.py\", line 113, in __call__\n    result = self._call_fn(*args, **kwargs)\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 209, in execute\n    return self.client.execute(*args, **kwargs)  # type: ignore\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 52, in execute\n    result = self._get_result(document, *args, **kwargs)\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 60, in _get_result\n    return self.transport.execute(document, *args, **kwargs)\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/transport\/requests.py\", line 39, in execute\n    request.raise_for_status()\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/requests\/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https:\/\/api.wandb.ai\/graphql\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/apis\/normalize.py\", line 26, in wrapper\n    return func(*args, **kwargs)\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2218, in upsert_sweep\n    raise e\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2215, in upsert_sweep\n    check_retry_fn=util.no_retry_4xx,\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/retry.py\", line 129, in __call__\n    retry_timedelta_triggered = check_retry_fn(e)\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/util.py\", line 969, in no_retry_4xx\n    raise UsageError(body[\"errors\"][0][\"message\"])\nwandb.errors.UsageError: permission denied\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/cli\/cli.py\", line 97, in wrapper\n    return func(*args, **kwargs)\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/cli\/cli.py\", line 941, in sweep\n    launch_scheduler=_launch_scheduler_spec,\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/apis\/internal.py\", line 102, in upsert_sweep\n    return self.api.upsert_sweep(*args, **kwargs)\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/apis\/normalize.py\", line 62, in wrapper\n    raise CommError(message, err).with_traceback(sys.exc_info()[2])\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/apis\/normalize.py\", line 26, in wrapper\n    return func(*args, **kwargs)\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2218, in upsert_sweep\n    raise e\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2215, in upsert_sweep\n    check_retry_fn=util.no_retry_4xx,\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/retry.py\", line 129, in __call__\n    retry_timedelta_triggered = check_retry_fn(e)\n  File \"\/home\/ec2-user\/anaconda3\/envs\/clip_env\/lib\/python3.7\/site-packages\/wandb\/util.py\", line 969, in no_retry_4xx\n    raise UsageError(body[\"errors\"][0][\"message\"])\nwandb.errors.CommError: permission denied\n<\/code><\/pre>\n<p>Please suggest<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1675558286979,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":56.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-login-not-working-for-sweep\/3822",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-08T15:38:00.938Z",
                "Answer_body":"<p>Hello Nahid,<\/p>\n<p>Thank you for contacting us and for your patience. Could you please type <code>wandb login --relogin<\/code> then try to run your sweep command again and let me know if it works? If not please share any error message or result.<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-13T14:18:58.004Z",
                "Answer_body":"<p>Hello Nahid,<\/p>\n<p>Just following up if you tried my suggestion for the issue you had.<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-15T20:40:59.030Z",
                "Answer_body":"<p>Hello Nahid,<\/p>\n<p>I will be closing this ticket as there is no answer. Feel free to contact us any time if you have the same or any other issue.<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"login work sweep folk see fridai feb afternoon pst version log sweep sweep yaml error error error call api permiss deni error detail error log tmp debug cli user log error permiss deni complet error log error respons execut graphql error error messag permiss deni path upsertsweep extens code permiss error data upsertsweep null error traceback recent file home user anaconda env clip env lib python site packag sdk lib retri line result self arg kwarg file home user anaconda env clip env lib python site packag sdk intern intern api line execut return self client execut arg kwarg type ignor file home user anaconda env clip env lib python site packag vendor gql gql client line execut result self result document arg kwarg file home user anaconda env clip env lib python site packag vendor gql gql client line result return self transport execut document arg kwarg file home user anaconda env clip env lib python site packag vendor gql gql transport request line execut request rais statu file home user anaconda env clip env lib python site packag request model line rais statu rais httperror http error msg respons self request except httperror client error forbidden url http api graphql handl except except occur traceback recent file home user anaconda env clip env lib python site packag api normal line wrapper return func arg kwarg file home user anaconda env clip env lib python site packag sdk intern intern api line upsert sweep rais file home user anaconda env clip env lib python site packag sdk intern intern api line upsert sweep check retri util retri file home user anaconda env clip env lib python site packag sdk lib retri line retri timedelta trigger check retri file home user anaconda env clip env lib python site packag util line retri rais usageerror bodi error messag error usageerror permiss deni handl except except occur traceback recent file home user anaconda env clip env lib python site packag cli cli line wrapper return func arg kwarg file home user anaconda env clip env lib python site packag cli cli line sweep launch schedul launch schedul spec file home user anaconda env clip env lib python site packag api intern line upsert sweep return self api upsert sweep arg kwarg file home user anaconda env clip env lib python site packag api normal line wrapper rais commerror messag err traceback sy exc info file home user anaconda env clip env lib python site packag api normal line wrapper return func arg kwarg file home user anaconda env clip env lib python site packag sdk intern intern api line upsert sweep rais file home user anaconda env clip env lib python site packag sdk intern intern api line upsert sweep check retri util retri file home user anaconda env clip env lib python site packag sdk lib retri line retri timedelta trigger check retri file home user anaconda env clip env lib python site packag util line retri rais usageerror bodi error messag error commerror permiss deni suggest",
        "Question_preprocessed_content":"login work sweep folk see fridai feb afternoon pst version log error complet error log suggest",
        "Question_gpt_summary_original":"The user is encountering an error while trying to use the Wandb sweep feature. The error message indicates that there is a permission issue with the Wandb API, and the user is unable to execute the command. The error log shows that the API is returning a 403 error, and the user has tried using the latest version of Wandb. The user is seeking suggestions to resolve the issue.",
        "Question_gpt_summary":"user encount error try us sweep featur error messag indic permiss issu api user unabl execut command error log show api return error user tri latest version user seek suggest resolv issu",
        "Answer_original_content":"hello nahid thank contact patienc type login relogin try run sweep command let know work share error messag result hello nahid follow tri suggest issu hello nahid close ticket answer feel free contact time issu",
        "Answer_preprocessed_content":"hello nahid thank contact patienc type try run sweep command let know work share error messag result hello nahid follow tri suggest issu hello nahid close ticket answer feel free contact time issu",
        "Answer_gpt_summary_original":"the solution suggested in the answer is to type \"login --relogin\" and try running the sweep command again. if the issue persists, the user is asked to share any error message or result. if the user does not respond, the ticket will be closed.",
        "Answer_gpt_summary":"solut suggest answer type login relogin try run sweep command issu persist user ask share error messag result user respond ticket close"
    },
    {
        "Question_id":null,
        "Question_title":"How to implment the Azure ML model in the .NET Core web service",
        "Question_body":"][1]\n\n\n\n\n\nI want to run this model on the website. The website is running now but I do not know how can I put models on.",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1613442662890,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/274165\/how-to-implment-the-azure-ml-model-in-the-net-core.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-16T04:16:58.453Z",
                "Answer_score":1,
                "Answer_body":"HI @YuxuanTian-3512\n\nThank You for posting in Q & A.\n\nBased on your request to deploy your machine learning or deep learning model as a web service in the Azure cloud, refer the below URL.\nDeploy machine learning models to Azure\nDeploy your existing model with Azure Machine Learning\nConsuming Azure Machine Learning in ASP.NET Core\n\n\n\n\nIf the Answer is helpful, please click Accept Answer and up-vote, this can be beneficial to other community members.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"implment model net core web servic want run model websit websit run know model",
        "Question_preprocessed_content":"implment model net core web servic want run model websit websit run know model",
        "Question_gpt_summary_original":"The user is facing challenges in implementing an Azure ML model in a .NET Core web service and is unsure of how to integrate the model into their website.",
        "Question_gpt_summary":"user face challeng implement model net core web servic unsur integr model websit",
        "Answer_original_content":"yuxuantian thank post base request deploi machin learn deep learn model web servic azur cloud refer url deploi machin learn model azur deploi exist model consum asp net core answer help click accept answer vote benefici commun member",
        "Answer_preprocessed_content":"thank post base request deploi machin learn deep learn model web servic azur cloud refer url deploi machin learn model azur deploi exist model consum core answer help click accept answer benefici commun member",
        "Answer_gpt_summary_original":"possible solutions from the answer include referring to a url for deploying machine learning models to azure and deploying an existing model with consuming in asp.net core. the answer also suggests accepting the answer and up-voting if it is helpful.",
        "Answer_gpt_summary":"possibl solut answer includ refer url deploi machin learn model azur deploi exist model consum asp net core answer suggest accept answer vote help"
    },
    {
        "Question_id":null,
        "Question_title":"AWS AI\/ML integration with Power BI",
        "Question_body":"Customer wants to know if AWS AI\/ML services integrate with Power BI. The customer currently uses Power BI that integrates with Azure ML for sentiment analysis, opinion mining, etc. Customer is looking for a push button solution where the business analyst can do text analytics on the response from the model. Is there a way to do this on AWS or a marketplace solution?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1607495476000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":148.0,
        "Answer_body":"PowerBI can connect to Amazon Redshift and leverage the new SQL based ML capability in Redshift that uses Sagemaker under the hood.\n\nAs an alternative thought the customer can integrate Amazon Sagemaker Model with Amazon Quicksight to achieve functionality very similar to PowerBI with Azure ML. Quicksight does have some embedded ML capability like forecasting and anomaly detection but Opinion mining is not one of them yet.\n\nYou should be able to leverage Blazing Text Algorithm in Sagemaker or some market place solution like Twinword sentiment model in sagemaker for sentiment analysis for Text mining after the integration.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4VexAnfiSFi4Jf5i9RyO_A\/aws-ai-ml-integration-with-power-bi",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-09T15:40:38.000Z",
                "Answer_score":0,
                "Answer_body":"PowerBI can connect to Amazon Redshift and leverage the new SQL based ML capability in Redshift that uses Sagemaker under the hood.\n\nAs an alternative thought the customer can integrate Amazon Sagemaker Model with Amazon Quicksight to achieve functionality very similar to PowerBI with Azure ML. Quicksight does have some embedded ML capability like forecasting and anomaly detection but Opinion mining is not one of them yet.\n\nYou should be able to leverage Blazing Text Algorithm in Sagemaker or some market place solution like Twinword sentiment model in sagemaker for sentiment analysis for Text mining after the integration.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-03-21T18:11:56.219Z",
                "Answer_score":0,
                "Answer_body":"Please take a look at this AWS solution - https:\/\/aws.amazon.com\/solutions\/implementations\/text-analysis-with-amazon-opensearch-service-and-amazon-comprehend\/\n\nComprehend also has an integration with AWS Aurora - https:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/AuroraUserGuide\/mysql-ml.html",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1607528438000,
        "Question_original_content":"aw integr power custom want know aw servic integr power custom current us power integr sentiment analysi opinion mine custom look push button solut busi analyst text analyt respons model wai aw marketplac solut",
        "Question_preprocessed_content":"aw integr power custom want know aw servic integr power custom current us power integr sentiment analysi opinion mine custom look push button solut busi analyst text analyt respons model wai aw marketplac solut",
        "Question_gpt_summary_original":"The user is facing a challenge of finding out if AWS AI\/ML services can integrate with Power BI, as they currently use Power BI with Azure ML for sentiment analysis and opinion mining. They are looking for a push button solution that allows business analysts to perform text analytics on the model's response. The user is seeking to know if there is a way to achieve this on AWS or through a marketplace solution.",
        "Question_gpt_summary":"user face challeng find aw servic integr power current us power sentiment analysi opinion mine look push button solut allow busi analyst perform text analyt model respons user seek know wai achiev aw marketplac solut",
        "Answer_original_content":"powerbi connect amazon redshift leverag new sql base capabl redshift us hood altern thought custom integr model amazon quicksight achiev function similar powerbi quicksight embed capabl like forecast anomali detect opinion mine abl leverag blaze text algorithm market place solut like twinword sentiment model sentiment analysi text mine integr",
        "Answer_preprocessed_content":"powerbi connect amazon redshift leverag new sql base capabl redshift us hood altern thought custom integr model amazon quicksight achiev function similar powerbi quicksight embed capabl like forecast anomali detect opinion mine abl leverag blaze text algorithm market place solut like twinword sentiment model sentiment analysi text mine integr",
        "Answer_gpt_summary_original":"possible solutions to integrate aws ai\/ml services with power bi for text analytics include connecting power bi to amazon redshift and using its sql-based ml capability, or integrating the model with amazon quicksight for similar functionality. quicksight has some embedded ml capabilities, but not opinion mining. after integration, sentiment analysis for text mining can be performed using algorithms like blazing text or marketplace solutions like twinword sentiment model.",
        "Answer_gpt_summary":"possibl solut integr aw servic power text analyt includ connect power amazon redshift sql base capabl integr model amazon quicksight similar function quicksight embed capabl opinion mine integr sentiment analysi text mine perform algorithm like blaze text marketplac solut like twinword sentiment model"
    },
    {
        "Question_id":73200116.0,
        "Question_title":"AWS Sagemaker integration with mongodb and lambda",
        "Question_body":"<p>I'm looking for some advice from anyone who's tried aws Sagemaker. I'm very new to this and would appreciate anyone kind enough to help me out.<\/p>\n<p>I have created a basic time series project in a Sagemaker notebook. It trains the model on CSV file data and tests it, with good results.<\/p>\n<p>The data I am using is based on store profits. I am predicting the profit each week.<\/p>\n<p>However, my question is, how can I pass new store sales data into this model each week (only one day a week), and retrain it with the new weeks data (so it can notice any new patterns), then for it to predict the next week profit for each store?<\/p>\n<p>All my store data is synced into mongodb, so I'm presuming I would need a lambda function to get this data and pass it over to the Sagemaker model.<\/p>\n<p>Is it worth retraining the model every week? As I have years worth of store data? Or should I just pass over the old data with the new data added in for it to predict? How do I pass over this data? In a lambda function with a cloud event to make it run automatically every week?<\/p>\n<p>Can I write the predictions back into mongodb in a new table, or are they saved somewhere else first and this would have to be another lambda function?<\/p>\n<p>I have looked at so many tutorials, but none of them seem to explain how I can connect everything up and have the model make predictions automatically and then save them in a dB.<\/p>\n<p>Many thanks in advance to anyone who can explain this to me! Sorry for such a long question!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1659392227390,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":58.0,
        "Answer_body":"<p>Recently I have completed similar use case and here's my answers -<\/p>\n<p><strong>Q1  : Is there need of retraining every week?<\/strong><\/p>\n<p>Ans : Yes, you need to do continuous training and continuous forecasting steps (tie using sagemaker pipeline) in prod to make it work perfectly automated for stable MAE, MAPE etc.<\/p>\n<p><strong>Q2  : How can I pass new data and forecast for next week? How to get input data from mongodb?<\/strong><\/p>\n<p>Ans : You could use Lambda, or Glue job (designed for ETL so better) to drop in S3 bucket. This will could become input raw data bucket for sagemaker pipeline.<\/p>\n<p><strong>Q3  : Can I write the predictions back into mongodb in a new table, or are they saved somewhere else first and this would have to be another lambda function?<\/strong><\/p>\n<p>Ans : Yes you can, both ways.<\/p>\n<p>I would suggest to start small i.e. First drop a csv file to s3 location in say YYMMDD folder. Use this as input and develop completely in one notebook (continuous train, continuous forecast).<\/p>\n<p>Later, learn about pipelines - how to write different steps, pass objects between steps etc and go modify your code to fit in pipeline.<\/p>\n<p>Create a sagemaker pipeline with steps : (Refer links below )<\/p>\n<ol>\n<li>Preprocess ( any transformations, cleansing )<\/li>\n<li>Training ( use prebuilt image or need to build one, depends)<\/li>\n<li>Forecast ( either do batch transform or deploy to endpoint and later delete )<\/li>\n<li>Post processing ( if required )<\/li>\n<\/ol>\n<p>Take the output the from sagemaker pipelines to mongodb. <strong>Sagemaker Pipelines help automate scheduled execution using AWS Event Bridge<\/strong><\/p>\n<p>Some references :<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/abalone_build_train_deploy\/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.ipynb\" rel=\"nofollow noreferrer\">Example Pipelines<\/a> Look here in to know about pipelines<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-forecasting-air-pollution-with-deepar\/blob\/5e29057a8c9d7f8db6cb1143f7838b6614f44ef1\/01_train_and_evaluate_air_quality_deepar_model.ipynb\" rel=\"nofollow noreferrer\">Example1 DeepAR<\/a><\/p>\n<p><a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/introduction_to_amazon_algorithms\/deepar_electricity\/DeepAR-Electricity.html?highlight=deepar\" rel=\"nofollow noreferrer\">Example2 DeepAR<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73200116",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1659526594223,
        "Question_original_content":"integr mongodb lambda look advic tri new appreci kind help creat basic time seri project notebook train model csv file data test good result data base store profit predict profit week question pass new store sale data model week dai week retrain new week data notic new pattern predict week profit store store data sync mongodb presum need lambda function data pass model worth retrain model week year worth store data pass old data new data ad predict pass data lambda function cloud event run automat week write predict mongodb new tabl save lambda function look tutori explain connect model predict automat save thank advanc explain sorri long question",
        "Question_preprocessed_content":"integr mongodb lambda look advic tri new appreci kind help creat basic time seri project notebook train model csv file data test good result data base store profit predict profit week question pass new store sale data model week retrain new week data predict week profit store store data sync mongodb presum need lambda function data pass model worth retrain model week year worth store data pass old data new data ad predict pass data lambda function cloud event run automat week write predict mongodb new tabl save lambda function look tutori explain connect model predict automat save thank advanc explain sorri long question",
        "Question_gpt_summary_original":"The user is facing challenges in integrating AWS Sagemaker with MongoDB and Lambda to pass new store sales data into the model every week, retrain it with the new data, and predict the next week's profit for each store. The user is unsure whether to retrain the model every week or just pass over the old data with the new data added in for prediction. The user is also unsure how to write the predictions back into MongoDB and automate the process. The user has looked at many tutorials but has not found a solution.",
        "Question_gpt_summary":"user face challeng integr mongodb lambda pass new store sale data model week retrain new data predict week profit store user unsur retrain model week pass old data new data ad predict user unsur write predict mongodb autom process user look tutori solut",
        "Answer_original_content":"recent complet similar us case answer need retrain week an ye need continu train continu forecast step tie pipelin prod work perfectli autom stabl mae mape pass new data forecast week input data mongodb an us lambda glue job design etl better drop bucket input raw data bucket pipelin write predict mongodb new tabl save lambda function an ye wai suggest start small drop csv file locat yymmdd folder us input develop complet notebook continu train continu forecast later learn pipelin write differ step pass object step modifi code fit pipelin creat pipelin step refer link preprocess transform cleans train us prebuilt imag need build depend forecast batch transform deploi endpoint later delet post process requir output pipelin mongodb pipelin help autom schedul execut aw event bridg refer exampl pipelin look know pipelin exampl deepar exampl deepar",
        "Answer_preprocessed_content":"recent complet similar us case answer need retrain week an ye need continu train continu forecast step prod work perfectli autom stabl mae mape pass new data forecast week input data mongodb an us lambda glue job drop bucket input raw data bucket pipelin write predict mongodb new tabl save lambda function an ye wai suggest start small drop csv file locat yymmdd folder us input develop complet notebook later learn pipelin write differ step pass object step modifi code fit pipelin creat pipelin step preprocess train forecast post process output pipelin mongodb pipelin help autom schedul execut aw event bridg refer exampl pipelin look know pipelin exampl deepar exampl deepar",
        "Answer_gpt_summary_original":"possible solutions from the answer include: \n- continuous training and forecasting using a pipeline in production for stable mae, mape, etc.\n- using lambda or glue job to drop new data in an s3 bucket, which can become the input raw data bucket for the pipeline.\n- writing predictions back into mongodb in a new table or using another lambda function.\n- starting small by dropping a csv file to an s3 location and developing the pipeline in one notebook before modifying the code to fit in the pipeline.\n- creating a pipeline with steps such as preprocessing, training, forecasting, and post-processing.\n- using aws event bridge to automate scheduled execution of the pipeline.",
        "Answer_gpt_summary":"possibl solut answer includ continu train forecast pipelin product stabl mae mape lambda glue job drop new data bucket input raw data bucket pipelin write predict mongodb new tabl lambda function start small drop csv file locat develop pipelin notebook modifi code fit pipelin creat pipelin step preprocess train forecast post process aw event bridg autom schedul execut pipelin"
    },
    {
        "Question_id":null,
        "Question_title":"Speech to text work",
        "Question_body":"  I am a user of Speech to Text. I use it in order to get a written text from the interviews and courses I shoot myself. After that I correct the text manually. So, in Russian it works fine, however, 20-30 percents of the words are incorrect. Moreover, there are no Russian punctuation at all.  So I get the speech to text transcript, then I create the perfect transcript out of this with correct words and punctuations. All I want to know is how I can improve Speech toText by using the perfect transcript I have already corrected? Where I can send that data to?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636330440000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":325.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-to-text-work\/td-p\/175139\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-09T09:36:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi,\u00a0\n\nIf you're using the Google Cloud Speech-to-Text API [1] and encounter text quality problem, I would suggest that you can report the issue at the Issue Tracker [2] with the reproduction details for the support to further look into issue with you to improve the quality.\u00a0\n\n[1]\u00a0https:\/\/cloud.google.com\/speech-to-text\n[2]\u00a0https:\/\/cloud.google.com\/support\/docs\/issue-trackers"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"speech text work user speech text us order written text interview cours shoot correct text manual russian work fine percent word incorrect russian punctuat speech text transcript creat perfect transcript correct word punctuat want know improv speech totext perfect transcript correct send data",
        "Question_preprocessed_content":"speech text work user speech text us order written text interview cours shoot correct text manual russian work fine percent word incorrect russian punctuat speech text transcript creat perfect transcript correct word punctuat want know improv speech totext perfect transcript correct send data",
        "Question_gpt_summary_original":"The user is facing challenges with the accuracy of Speech to Text software when transcribing interviews and courses in Russian. They report that 20-30% of the words are incorrect and there is no Russian punctuation. The user manually corrects the transcript and is seeking ways to improve the software using their corrected transcript. They are looking for information on where to send this data.",
        "Question_gpt_summary":"user face challeng accuraci speech text softwar transcrib interview cours russian report word incorrect russian punctuat user manual correct transcript seek wai improv softwar correct transcript look inform send data",
        "Answer_original_content":"googl cloud speech text api encount text qualiti problem suggest report issu issu tracker reproduct detail support look issu improv qualiti http cloud googl com speech text http cloud googl com support doc issu tracker",
        "Answer_preprocessed_content":"googl cloud api encount text qualiti problem suggest report issu issu tracker reproduct detail support look issu improv qualiti",
        "Answer_gpt_summary_original":"possible solution: the user can report the speech-to-text accuracy and punctuation issues they are facing while using the google cloud speech-to-text api at the issue tracker with reproduction details. the support team can then work with the user to improve the quality.",
        "Answer_gpt_summary":"possibl solut user report speech text accuraci punctuat issu face googl cloud speech text api issu tracker reproduct detail support team work user improv qualiti"
    },
    {
        "Question_id":71744288.0,
        "Question_title":"wandb getting logged without initiating",
        "Question_body":"<p>I do not want to use wandb. I don't even have an account. I am simply following <a href=\"https:\/\/colab.research.google.com\/github\/huggingface\/notebooks\/blob\/master\/examples\/summarization.ipynb#scrollTo=UmvbnJ9JIrJd\" rel=\"nofollow noreferrer\">this notebook<\/a> for finetuning. I am not running the 2nd and 3 cells because I do not want to push the model to the hub.<\/p>\n<p>However, when I do trainer.train() I get the following error : <a href=\"https:\/\/i.stack.imgur.com\/VPIZm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VPIZm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I don't understand where wandb.log is being called.\nI even tried os.environ[&quot;WANDB_DISABLED&quot;]  = &quot;true&quot; but I still get the error.\nPlease help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649110110203,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":289.0,
        "Answer_body":"<p>posting the same message as <a href=\"https:\/\/github.com\/huggingface\/transformers\/issues\/16594\" rel=\"nofollow noreferrer\">over on <code>transformers<\/code><\/a>:<\/p>\n<hr \/>\n<p>You can turn off all external logger logging, including wandb logging by passing <code>report_to=&quot;none&quot;<\/code> in your <code>Seq2SeqTrainingArguments<\/code>.<\/p>\n<p>You might have noticed the following warning when setting up your TrainingArguments:<\/p>\n<pre><code>The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-)\n<\/code><\/pre>\n<p>Right now the default is to run all loggers that you have installed, so maybe you installed wandb on your machine since the last time you ran the script?<\/p>\n<p>If you would like to log with wandb, best practice would already be to start setting <code>report_to=&quot;wandb&quot;<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71744288",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1649156563120,
        "Question_original_content":"get log initi want us account simpli follow notebook finetun run cell want push model hub trainer train follow error understand log call tri environ disabl true error help",
        "Question_preprocessed_content":"get log initi want us account simpli follow notebook finetun run cell want push model hub follow error understand log call tri true error help",
        "Question_gpt_summary_original":"The user is encountering an issue where wandb is getting logged without initiating it. They do not want to use wandb and are following a notebook for finetuning, but when they run trainer.train(), they get an error related to wandb.log being called. They have tried disabling wandb but still get the error and are seeking help.",
        "Question_gpt_summary":"user encount issu get log initi want us follow notebook finetun run trainer train error relat log call tri disabl error seek help",
        "Answer_original_content":"post messag transform turn extern logger log includ log pass report seqseqtrainingargu notic follow warn set trainingargu default valu train argument report chang instal integr need us report behavior start updat code info disappear right default run logger instal mayb instal machin time ran script like log best practic start set report",
        "Answer_preprocessed_content":"post messag turn extern logger log includ log pass notic follow warn set trainingargu right default run logger instal mayb instal machin time ran script like log best practic start set",
        "Answer_gpt_summary_original":"possible solutions from the answer include turning off all external logger logging by passing report_to=\"none\" in the seq2seqtrainingarguments and updating the code to use `--report_to all` to get the same behavior as now. the user may have installed new loggers on their machine since the last time they ran the script, and if they would like to log with best practice, they should start setting report_to=\"\".",
        "Answer_gpt_summary":"possibl solut answer includ turn extern logger log pass report seqseqtrainingargu updat code us report behavior user instal new logger machin time ran script like log best practic start set report"
    },
    {
        "Question_id":null,
        "Question_title":"Accessing logged values in a callback during run",
        "Question_body":"<p>Hi-<\/p>\n<p>I\u2019m trying  to access a logged value during a run (in a callback). When I examine <code>run.summary<\/code> it does not seem to have any of the values logged by <code>self.log<\/code> in either <code>train_step()<\/code> or <code>validation_step()<\/code>. Is there a correct pattern for accessing logged values during a run.<\/p>\n<p>My use case is trying to keep track of the minimum of a metric. I already use <code>define_metric<\/code>, but I want to see the minimum as a plot over <code>step<\/code> or <code>epoch<\/code> as opposed to just in the summary. Essentially trying to call <code>torchmetrics.MinMetric<\/code> in the <code>on_validation_epoch_end()<\/code> callback - but to update that metric I need to feed it the previously logged value for another metric.<\/p>\n<p>Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654107981576,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":60.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/accessing-logged-values-in-a-callback-during-run\/2527",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-31T18:27:19.938Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"access log valu callback run try access log valu run callback examin run summari valu log self log train step valid step correct pattern access log valu run us case try track minimum metric us defin metric want minimum plot step epoch oppos summari essenti try torchmetr minmetr valid epoch end callback updat metric need feed previous log valu metric thank",
        "Question_preprocessed_content":"access log valu callback run try access log valu run examin valu log correct pattern access log valu run us case try track minimum metric us want minimum plot oppos summari essenti try callback updat metric need feed previous log valu metric thank",
        "Question_gpt_summary_original":"The user is facing challenges in accessing logged values during a run in a callback. They are unable to access the values logged by self.log in either train_step() or validation_step() when examining run.summary. The user wants to keep track of the minimum of a metric and see it as a plot over step or epoch, but they need to feed the previously logged value for another metric to update the metric using torchmetrics.MinMetric in the on_validation_epoch_end() callback.",
        "Question_gpt_summary":"user face challeng access log valu run callback unabl access valu log self log train step valid step examin run summari user want track minimum metric plot step epoch need feed previous log valu metric updat metric torchmetr minmetr valid epoch end callback",
        "Answer_original_content":"topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer as it is simply stating that the topic has been closed.",
        "Answer_gpt_summary":"solut provid answer simpli state topic close"
    },
    {
        "Question_id":null,
        "Question_title":"AzureMLCompute job failed: container registry failed unexpectedly: container setup task failed",
        "Question_body":"Hi,\n\nCould you please help me with running python script in azureml environment? I created the workspace and azure container registry and pushed docker image to the container. This is the example of dockerfile:\n\n FROM python:3.7\n    \n RUN pip install --upgrade pip\n    \n RUN pip install virtualenv\n    \n ENV VIRTUAL_ENV=\/venv\n    \n RUN virtualenv venv -p python3\n    \n ENV PATH=\"VIRTUAL_ENV\/bin:$PATH\"\n    \n WORKDIR \/app\n    \n ADD . \/app\n    \n ENV PYTHON_PACKAGES=\"\\\n      numpy \\\n  pandas \\\n  seaborn \\\n  matplotlib \\\n  sklearn \\\n  scipy \\\n  imbalanced-learn \\\n  xgboost \\\n  joblib \\\n \" \n    \n RUN pip install --no-cache-dir $PYTHON_PACKAGES\n    \n ENTRYPOINT [\"python3\",\"train.py\"]\n\n\n\nWhen I run the experiment I get this error:\n\n\"Message\": \"AzureMLCompute job failed.\\nJobContainerConfigFailed: Container configuration failed unexpectedly\\n\\tJobContainerConfigFailed: Container configuration failed unexpectedly\\n\\terr: container setup task failed: exit status 1\\n\\tReason: container setup task failed: exit status 1\\n\\tInfo: Failed to prepare an environment for the job execution: Job environment preparation failed on 10.0.0.5 with err exit status 1.\"\n\n\n\n\nI do not understand what this error mean.\n\nThank you!",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1610999729247,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/235234\/azuremlcompute-job-failed-container-registry-faile.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-20T10:39:50.733Z",
                "Answer_score":0,
                "Answer_body":"@MomirBeljic-9741 Thanks for the question. Could you please try the following solution given below.\nSolution:\nUpdated storage account key with below command.\nChange storage account access keys - Azure Machine Learning | Microsoft Docs\n\n\n\n\naz ml workspace sync-keys -w myworkspace -g myresourcegroup\n\nThis message occurs when the AML storage account restricts access to specific VNETs and the Compute Cluster isn\u2019t in that VNET.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"comput job fail contain registri fail unexpectedli contain setup task fail help run python script environ creat workspac azur contain registri push docker imag contain exampl dockerfil python run pip instal upgrad pip run pip instal virtualenv env virtual env venv run virtualenv venv python env path virtual env bin path workdir app add app env python packag numpi panda seaborn matplotlib sklearn scipi imbalanc learn xgboost joblib run pip instal cach dir python packag entrypoint python train run experi error messag comput job fail njobcontainerconfigfail contain configur fail unexpectedli tjobcontainerconfigfail contain configur fail unexpectedli terr contain setup task fail exit statu treason contain setup task fail exit statu tinfo fail prepar environ job execut job environ prepar fail err exit statu understand error mean thank",
        "Question_preprocessed_content":"comput job fail contain registri fail unexpectedli contain setup task fail help run python script environ creat workspac azur contain registri push docker imag contain exampl dockerfil run pip instal pip run pip instal virtualenv env run virtualenv venv python env workdir add env numpi panda seaborn matplotlib sklearn scipi xgboost joblib run pip instal entrypoint run experi error messag comput job contain configur fail contain configur fail contain setup task fail exit statu contain setup task fail exit statu fail prepar environ job execut job environ prepar fail err exit statu understand error mean thank",
        "Question_gpt_summary_original":"The user is encountering challenges while running a Python script in an AzureML environment. They have created a workspace and Azure container registry and pushed a Docker image to the container. However, when running the experiment, they receive an error message indicating that the AzureMLCompute job failed due to container configuration issues. The user is unsure about the meaning of the error message.",
        "Question_gpt_summary":"user encount challeng run python script environ creat workspac azur contain registri push docker imag contain run experi receiv error messag indic comput job fail contain configur issu user unsur mean error messag",
        "Answer_original_content":"momirbelj thank question try follow solut given solut updat storag account kei command chang storag account access kei microsoft doc workspac sync kei myworkspac myresourcegroup messag occur aml storag account restrict access specif vnet comput cluster isnt vnet",
        "Answer_preprocessed_content":"thank question try follow solut given solut updat storag account kei command chang storag account access kei microsoft doc workspac myworkspac myresourcegroup messag occur aml storag account restrict access specif vnet comput cluster isnt vnet",
        "Answer_gpt_summary_original":"the solution to the \"compute job failed\" error due to a container registry failure is to update the storage account key using the command \"az ml workspace sync-keys -w myworkspace -g myresourcegroup\". this error occurs when the aml storage account restricts access to specific vnets and the compute cluster is not in that vnet.",
        "Answer_gpt_summary":"solut comput job fail error contain registri failur updat storag account kei command workspac sync kei myworkspac myresourcegroup error occur aml storag account restrict access specif vnet comput cluster vnet"
    },
    {
        "Question_id":71499094.0,
        "Question_title":"Python AzureML Hello world - Can't find module azureml",
        "Question_body":"<p>Python 3.10, Pip install azureml-sdk 1.39.0.<br \/>\nEnvironments: Win10 PS, VS2022, and a docker image- all same results . Pip show shows the azureml-core package.<\/p>\n<p>Simple (I thought) script, but it can't find &quot;azureml.core&quot;   No module named azureml is the error.\nHow do I make it &quot;find&quot; it? I'm new at python so it could be syntax.<\/p>\n<pre><code>import os\nfrom azureml.core import Workspace, Experiment, Environment, Model,Dataset,Datastore,ScriptRunConfig\n     \n    # check core SDK version number\n    print(&quot;Azure ML SDK Version: &quot;, azureml.core.VERSION)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1647441787870,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":310.0,
        "Answer_body":"<p>azureml python sdk does not support py3.10 yet, AutoML sdk supports py&lt;=3.8.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71499094",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1648177993987,
        "Question_original_content":"python hello world modul python pip instal sdk environ win docker imag result pip show core packag simpl thought script core modul name error new python syntax import core import workspac experi environ model dataset datastor scriptrunconfig check core sdk version number print sdk version core version",
        "Question_preprocessed_content":"python hello world modul python pip instal sdk environ win docker imag result pip show core packag simpl script modul name error new python syntax",
        "Question_gpt_summary_original":"The user is facing challenges in running a Python script that uses the AzureML SDK. Despite installing the required package, the script is unable to find the \"azureml.core\" module, resulting in a \"No module named azureml\" error. The user is seeking guidance on how to resolve this issue.",
        "Question_gpt_summary":"user face challeng run python script us sdk despit instal requir packag script unabl core modul result modul name error user seek guidanc resolv issu",
        "Answer_original_content":"python sdk support automl sdk support",
        "Answer_preprocessed_content":"python sdk support automl sdk support",
        "Answer_gpt_summary_original":"possible solutions to the user's challenge with a python script that cannot find the \".core\" module are not explicitly mentioned in the answer. however, the answer suggests that the issue may be related to the python version being used. specifically, the python sdk does not support version 3.10 yet, and the automl sdk only supports versions up to 3.8. therefore, the user may need to downgrade their python version to resolve the issue.",
        "Answer_gpt_summary":"possibl solut user challeng python script core modul explicitli mention answer answer suggest issu relat python version specif python sdk support version automl sdk support version user need downgrad python version resolv issu"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning: I cannot find experiment's user logs located in logs\/user folder",
        "Question_body":"I am running experiments in Azure Machine Learning using ParallelRunStep, and I cannot get the user folder with logs as defined in readme.txt file with the log folder structure.\nI cannot find log\/user folder with \"Logs generated when loading and running user's scripts.\"\n\nreadme.txt file states:\nParallelRunStep has two major parts:\n1. Scheduling, progress tracking and file concatenation for append_row.\n2. Processing mini batch by calling the entry script.\nThe agent manager on each node start agents.\nAn agent gets mini batch and calls the entry script against the mini batch.\n\n The \"logs\" folder has user, sys and perf sub folders.\n The user folder includes messages from the entry script in processing mini batches.\n The sys folder includes messages from #1 and non-entry script log from #2.\n The perf folder includes periodical checking result of resource usage.\n\n\n\nIn majority case, users can find the processing messages from the user folder.\nUsers need to check sys folder for messages beyond processing mini batches.\nlogs\/\nazureml\/: Logs from azureml dependencies. e.g. azureml.dataprep\nuser\/ : Logs generated when loading and running user's scripts.\nerror\/ : Logs of errors encountered while loading and running entry script.\nstderr\/ : stderr output of user's scripts.\nstdout\/ : stdout output of user's scripts.\nentry_script_log\/ : Logs generated by loggers of EntryScript()\n<node seq> :\nprocessNNN.log.txt : Logs generated by loggers of EntryScript() from each process.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1645621539517,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@CalabriaMonteroSalvadorSGRESEDFPDC-5704 Thanks for the question. Please follow the doc to view and log files for a run. Interactive logging sessions are typically used in notebook environments. The method Experiment.start_logging() starts an interactive logging session. Any metrics logged during the session are added to the run record in the experiment. The method run.complete() ends the sessions and marks the run as completed.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics#view-and-download-log-files-for-a-run",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/747549\/azure-machine-learning-i-cannot-find-experiment39s.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-24T02:37:03.417Z",
                "Answer_score":0,
                "Answer_body":"@CalabriaMonteroSalvadorSGRESEDFPDC-5704 Thanks for the question. Please follow the doc to view and log files for a run. Interactive logging sessions are typically used in notebook environments. The method Experiment.start_logging() starts an interactive logging session. Any metrics logged during the session are added to the run record in the experiment. The method run.complete() ends the sessions and marks the run as completed.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics#view-and-download-log-files-for-a-run",
                "Answer_comment_count":4,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1645670223416,
        "Question_original_content":"experi user log locat log user folder run experi parallelrunstep user folder log defin readm txt file log folder structur log user folder log gener load run user script readm txt file state parallelrunstep major part schedul progress track file concaten append row process mini batch call entri script agent manag node start agent agent get mini batch call entri script mini batch log folder user sy perf sub folder user folder includ messag entri script process mini batch sy folder includ messag non entri script log perf folder includ period check result resourc usag major case user process messag user folder user need check sy folder messag process mini batch log log depend dataprep user log gener load run user script error log error encount load run entri script stderr stderr output user script stdout stdout output user script entri script log log gener logger entryscript process log txt log gener logger entryscript process",
        "Question_preprocessed_content":"experi user log locat folder run experi parallelrunstep user folder log defin file log folder structur folder log gener load run user file state parallelrunstep major part schedul progress track file concaten process mini batch call entri script agent manag node start agent agent get mini batch call entri script mini batch log folder user sy perf sub folder user folder includ messag entri script process mini batch sy folder includ messag script log perf folder includ period check result resourc usag major case user process messag user folder user need check sy folder messag process mini batch log log depend dataprep user log gener load run user script error log error encount load run entri script stderr stderr output user script stdout stdout output user script log gener logger entryscript log gener logger entryscript process",
        "Question_gpt_summary_original":"The user is encountering challenges in finding the user logs folder located in logs\/user while running experiments in Azure Machine Learning using ParallelRunStep. The readme.txt file provides information on the log folder structure, which includes user, sys, and perf subfolders. The user folder contains messages from the entry script in processing mini batches, while the sys folder includes messages from scheduling, progress tracking, and non-entry script logs. The user needs to check the sys folder for messages beyond processing mini batches.",
        "Question_gpt_summary":"user encount challeng find user log folder locat log user run experi parallelrunstep readm txt file provid inform log folder structur includ user sy perf subfold user folder contain messag entri script process mini batch sy folder includ messag schedul progress track non entri script log user need check sy folder messag process mini batch",
        "Answer_original_content":"calabriamonterosalvadorsgresedfpdc thank question follow doc view log file run interact log session typic notebook environ method experi start log start interact log session metric log session ad run record experi method run complet end session mark run complet http doc microsoft com azur machin learn log view metric view download log file run",
        "Answer_preprocessed_content":"thank question follow doc view log file run interact log session typic notebook environ method start interact log session metric log session ad run record experi method end session mark run complet",
        "Answer_gpt_summary_original":"the answer suggests following the documentation to view and log files for a run. it also recommends using interactive logging sessions, which can be started with the method experiment.start_logging(). any metrics logged during the session are added to the run record in the experiment. the method run.complete() ends the session and marks the run as completed. the link to the documentation is provided for further guidance.",
        "Answer_gpt_summary":"answer suggest follow document view log file run recommend interact log session start method experi start log metric log session ad run record experi method run complet end session mark run complet link document provid guidanc"
    },
    {
        "Question_id":null,
        "Question_title":"How does one save a plot in wandb with wandb.log?",
        "Question_body":"<p>I\u2019m trying to save a plot with wandb.log. Their <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log\/plots\">docs<\/a> say to do:<\/p>\n<pre><code class=\"lang-auto\">    wandb.log({\"chart\": plt})\n<\/code><\/pre>\n<p>but this fails for me.<\/p>\n<p>I get two errors, 1st error (when I do NOT do <code>plt.show()<\/code> before trying to do wand.log):<\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/Applications\/PyCharm.app\/Contents\/plugins\/python\/helpers\/pydev\/_pydevd_bundle\/pydevd_exec2.py\", line 3, in Exec\n    exec(exp, global_vars, local_vars)\n  File \"&lt;input&gt;\", line 1, in &lt;module&gt;\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 256, in wrapper\n    return func(self, *args, **kwargs)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 222, in wrapper\n    return func(self, *args, **kwargs)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1548, in log\n    self._log(data=data, step=step, commit=commit)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1339, in _log\n    self._partial_history_callback(data, step, commit)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1228, in _partial_history_callback\n    self._backend.interface.publish_partial_history(\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 541, in publish_partial_history\n    data = history_dict_to_json(run, data, step=user_step, ignore_copy_err=True)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/utils.py\", line 54, in history_dict_to_json\n    payload[key] = val_to_json(\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/utils.py\", line 82, in val_to_json\n    val = Plotly.make_plot_media(val)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/plotly.py\", line 48, in make_plot_media\n    val = util.matplotlib_to_plotly(val)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/util.py\", line 560, in matplotlib_to_plotly\n    return tools.mpl_to_plotly(obj)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/tools.py\", line 112, in mpl_to_plotly\n    matplotlylib.Exporter(renderer).run(fig)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 53, in run\n    self.crawl_fig(fig)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 124, in crawl_fig\n    self.crawl_ax(ax)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 146, in crawl_ax\n    self.draw_collection(ax, collection)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 289, in draw_collection\n    offset_order = offset_dict[collection.get_offset_position()]\nAttributeError: 'LineCollection' object has no attribute 'get_offset_position'\n<\/code><\/pre>\n<p>I get two errors, 2nd error (when I DO <code>plt.show()<\/code> before trying to do wand.log):<\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/Applications\/PyCharm.app\/Contents\/plugins\/python\/helpers\/pydev\/_pydevd_bundle\/pydevd_exec2.py\", line 3, in Exec\n    exec(exp, global_vars, local_vars)\n  File \"&lt;input&gt;\", line 1, in &lt;module&gt;\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 256, in wrapper\n    return func(self, *args, **kwargs)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 222, in wrapper\n    return func(self, *args, **kwargs)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1548, in log\n    self._log(data=data, step=step, commit=commit)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1339, in _log\n    self._partial_history_callback(data, step, commit)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1228, in _partial_history_callback\n    self._backend.interface.publish_partial_history(\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 541, in publish_partial_history\n    data = history_dict_to_json(run, data, step=user_step, ignore_copy_err=True)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/utils.py\", line 54, in history_dict_to_json\n    payload[key] = val_to_json(\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/utils.py\", line 82, in val_to_json\n    val = Plotly.make_plot_media(val)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/plotly.py\", line 48, in make_plot_media\n    val = util.matplotlib_to_plotly(val)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/util.py\", line 560, in matplotlib_to_plotly\n    return tools.mpl_to_plotly(obj)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/tools.py\", line 112, in mpl_to_plotly\n    matplotlylib.Exporter(renderer).run(fig)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 53, in run\n    self.crawl_fig(fig)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 122, in crawl_fig\n    with self.renderer.draw_figure(fig=fig, props=utils.get_figure_properties(fig)):\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/contextlib.py\", line 119, in __enter__\n    return next(self.gen)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/renderers\/base.py\", line 45, in draw_figure\n    self.open_figure(fig=fig, props=props)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/renderer.py\", line 90, in open_figure\n    self.mpl_x_bounds, self.mpl_y_bounds = mpltools.get_axes_bounds(fig)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mpltools.py\", line 265, in get_axes_bounds\n    x_min, y_min, x_max, y_max = min(x_min), min(y_min), max(x_max), max(y_max)\nValueError: min() arg is an empty sequence\n<\/code><\/pre>\n<p>Note that their trivial example DOES work:<\/p>\n<pre><code class=\"lang-auto\">import matplotlib.pyplot as plt\n\nplt.plot([1, 2, 3, 4])\nplt.ylabel(\"some interesting numbers\")\nwandb.log({\"chart\": plt})\n<\/code><\/pre>\n<p>for me.<\/p>\n<hr>\n<p>cross posted: <a href=\"https:\/\/stackoverflow.com\/questions\/72134168\/how-does-one-save-a-plot-in-wandb-with-wandb-log\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">python - How does one save a plot in wandb with wandb.log? - Stack Overflow<\/a><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1651788927254,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":188.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-does-one-save-a-plot-in-wandb-with-wandb-log\/2373",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-09T21:30:52.813Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/brando\">@brando<\/a>,<\/p>\n<p>I\u2019m sorry you are facing this. Could you share the code snippet you were using to generate this chart? I\u2019ll test this on my end and see how this can be resolved.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-12T22:18:46.528Z",
                "Answer_body":"<p>Hi Brando,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-08T21:31:40.625Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"save plot log try save plot log doc log chart plt fail error error plt try wand log traceback recent file applic pycharm app content plugin python helper pydev pydevd bundl pydevd exec line exec exec exp global var local var file line file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line wrapper return func self arg kwarg file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line wrapper return func self arg kwarg file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line log self log data data step step commit commit file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line log self partial histori callback data step commit file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line partial histori callback self backend interfac publish partial histori file user brandomiranda opt anaconda env meta learn lib python site packag sdk interfac interfac line publish partial histori data histori dict json run data step user step ignor copi err true file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type util line histori dict json payload kei val json file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type util line val json val plotli plot media val file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type plotli line plot media val util matplotlib plotli val file user brandomiranda opt anaconda env meta learn lib python site packag util line matplotlib plotli return tool mpl plotli obj file user brandomiranda opt anaconda env meta learn lib python site packag plotli tool line mpl plotli matplotlylib export render run fig file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line run self crawl fig fig file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line crawl fig self crawl file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line crawl self draw collect collect file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line draw collect offset order offset dict collect offset posit attributeerror linecollect object attribut offset posit error error plt try wand log traceback recent file applic pycharm app content plugin python helper pydev pydevd bundl pydevd exec line exec exec exp global var local var file line file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line wrapper return func self arg kwarg file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line wrapper return func self arg kwarg file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line log self log data data step step commit commit file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line log self partial histori callback data step commit file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line partial histori callback self backend interfac publish partial histori file user brandomiranda opt anaconda env meta learn lib python site packag sdk interfac interfac line publish partial histori data histori dict json run data step user step ignor copi err true file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type util line histori dict json payload kei val json file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type util line val json val plotli plot media val file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type plotli line plot media val util matplotlib plotli val file user brandomiranda opt anaconda env meta learn lib python site packag util line matplotlib plotli return tool mpl plotli obj file user brandomiranda opt anaconda env meta learn lib python site packag plotli tool line mpl plotli matplotlylib export render run fig file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line run self crawl fig fig file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line crawl fig self render draw figur fig fig prop util figur properti fig file user brandomiranda opt anaconda env meta learn lib python contextlib line enter return self gen file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport render base line draw figur self open figur fig fig prop prop file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib render line open figur self mpl bound self mpl bound mpltool ax bound fig file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mpltool line ax bound min min max max min min min min max max max max valueerror min arg sequenc note trivial exampl work import matplotlib pyplot plt plt plot plt ylabel interest number log chart plt cross post python save plot log stack overflow",
        "Question_preprocessed_content":"save plot log try save plot log doc fail error error error error note trivial exampl work cross post python save plot log stack overflow",
        "Question_gpt_summary_original":"The user is trying to save a plot with wandb.log but is encountering errors. The first error occurs when plt.show() is not done before trying to do wand.log, and the second error occurs when plt.show() is done before trying to do wand.log. The trivial example provided by wandb works for the user.",
        "Question_gpt_summary":"user try save plot log encount error error occur plt try wand log second error occur plt try wand log trivial exampl provid work user",
        "Answer_original_content":"brando sorri face share code snippet gener chart ill test end resolv thank ramit brando want follow support request heard let know assist issu resolv best topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"sorri face share code snippet gener chart ill test end resolv thank ramit brando want follow support request heard let know assist issu resolv best topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the responder is asking for more information to help resolve the issue and is following up with the user to see if they still need assistance.",
        "Answer_gpt_summary":"solut provid answer respond ask inform help resolv issu follow user need assist"
    },
    {
        "Question_id":56558552.0,
        "Question_title":"How to save your fitted transformer into blob, so your prediction pipeline can use it in AML Service?",
        "Question_body":"<p>I am building a data transformation and training pipeline on Azure Machine Leaning Service. I'd like to save my fitted transformer (e.g. tf-idf) to the blob, so my prediction pipeline can later access it. <\/p>\n\n<pre><code>transformed_data = PipelineData(\"transformed_data\", \n                               datastore = default_datastore,\n                               output_path_on_compute=\"my_project\/tfidf\")\n\nstep_tfidf = PythonScriptStep(name = \"tfidf_step\",\n                              script_name = \"transform.py\",\n                              arguments = ['--input_data', blob_train_data, \n                                           '--output_folder', transformed_data],\n                              inputs = [blob_train_data],\n                              outputs = [transformed_data],\n                              compute_target = aml_compute,\n                              source_directory = project_folder,\n                              runconfig = run_config,\n                              allow_reuse = False)\n\n<\/code><\/pre>\n\n<p>The above code saves the transformer to a current run's folder, which is dynamically generated during each run. <\/p>\n\n<p>I want to save the transformer to a fixed location on blob, so I can access it later, when calling a prediction pipeline.<\/p>\n\n<p>I tried to use an instance of <code>DataReference<\/code> class as <code>PythonScriptStep<\/code> output, but it results in an error: \n<code>ValueError: Unexpected output type: &lt;class 'azureml.data.data_reference.DataReference'&gt;<\/code> <\/p>\n\n<p>It's because <code>PythonScriptStep<\/code> only accepts <code>PipelineData<\/code> or <code>OutputPortBinding<\/code> objects as outputs.<\/p>\n\n<p>How could I save my fitted transformer so it's later accessible by any aribitraly process (e.g. my prediction pipeline)?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1560330676657,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":1056.0,
        "Answer_body":"<p>Another solution is to pass <code>DataReference<\/code> as an input to your <code>PythonScriptStep<\/code>. <\/p>\n\n<p>Then inside <code>transform.py<\/code> you're able to read this <code>DataReference<\/code> as a command line argument. <\/p>\n\n<p>You can parse it and use it just as any regular path to save your vectorizer to.<\/p>\n\n<p>E.g. you can:<\/p>\n\n<pre><code>step_tfidf = PythonScriptStep(name = \"tfidf_step\",\n                              script_name = \"transform.py\",\n                              arguments = ['--input_data', blob_train_data, \n                                           '--output_folder', transformed_data,\n                                           '--transformer_path', trained_transformer_path],\n                              inputs = [blob_train_data, trained_transformer_path],\n                              outputs = [transformed_data],\n                              compute_target = aml_compute,\n                              source_directory = project_folder,\n                              runconfig = run_config,\n                              allow_reuse = False)\n<\/code><\/pre>\n\n<p>Then inside your script (<code>transform.py<\/code> in the example above) you can e.g.:<\/p>\n\n<pre><code>import argparse\nimport joblib as jbl\nimport os\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--transformer_path', dest=\"transformer_path\", required=True)\nargs = parser.parse_args()\n\ntfidf = ### HERE CREATE AND TRAIN YOUR VECTORIZER ###\n\nvect_filename = os.path.join(args.transformer_path, 'my_vectorizer.jbl')\n\n<\/code><\/pre>\n\n<hr>\n\n<p>EXTRA: The third way would be to just register the vectorizer as another model in your workspace. You can then use it exactly as any other registered model. (Though this option does not involve explicit writing to blob - as specified in the question above)<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1578692352196,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56558552",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1562586040652,
        "Question_original_content":"save fit transform blob predict pipelin us aml servic build data transform train pipelin azur machin lean servic like save fit transform idf blob predict pipelin later access transform data pipelinedata transform data datastor default datastor output path comput project tfidf step tfidf pythonscriptstep tfidf step script transform argument input data blob train data output folder transform data input blob train data output transform data comput target aml comput sourc directori project folder runconfig run config allow reus fals code save transform current run folder dynam gener run want save transform fix locat blob access later call predict pipelin tri us instanc datarefer class pythonscriptstep output result error valueerror unexpect output type pythonscriptstep accept pipelinedata outputportbind object output save fit transform later access aribitrali process predict pipelin",
        "Question_preprocessed_content":"save fit transform blob predict pipelin us aml servic build data transform train pipelin azur machin lean servic like save fit transform blob predict pipelin later access code save transform current run folder dynam gener run want save transform fix locat blob access later call predict pipelin tri us instanc class output result error accept object output save fit transform later access aribitrali process",
        "Question_gpt_summary_original":"The user is facing a challenge in saving a fitted transformer (e.g. tf-idf) to a fixed location on blob, so it can be accessed later by a prediction pipeline. The current code saves the transformer to a dynamically generated folder during each run, but the user wants to save it to a fixed location. The user tried using an instance of the DataReference class as PythonScriptStep output, but it resulted in an error because PythonScriptStep only accepts PipelineData or OutputPortBinding objects as outputs. The user is seeking a solution to save the fitted transformer so it can be accessed by any arbitrary process.",
        "Question_gpt_summary":"user face challeng save fit transform idf fix locat blob access later predict pipelin current code save transform dynam gener folder run user want save fix locat user tri instanc datarefer class pythonscriptstep output result error pythonscriptstep accept pipelinedata outputportbind object output user seek solut save fit transform access arbitrari process",
        "Answer_original_content":"solut pass datarefer input pythonscriptstep insid transform abl read datarefer command line argument pars us regular path save vector step tfidf pythonscriptstep tfidf step script transform argument input data blob train data output folder transform data transform path train transform path input blob train data train transform path output transform data comput target aml comput sourc directori project folder runconfig run config allow reus fals insid script transform exampl import argpars import joblib jbl import sklearn featur extract text import tfidfvector parser argpars argumentpars parser add argument transform path dest transform path requir true arg parser pars arg tfidf creat train vector vect filenam path join arg transform path vector jbl extra wai regist vector model workspac us exactli regist model option involv explicit write blob specifi question",
        "Answer_preprocessed_content":"solut pass input insid abl read command line argument pars us regular path save vector insid script extra wai regist vector model workspac us exactli regist model",
        "Answer_gpt_summary_original":"the answer suggests three possible solutions to the challenge of saving a fitted transformer into a blob for use in  service. one solution is to pass datareference as an input to the pythonscriptstep and read it as a command line argument in transform.py. another solution is to register the vectorizer as another model in the workspace. the third solution is to use the tfidfvectorizer and save it as a regular path.",
        "Answer_gpt_summary":"answer suggest possibl solut challeng save fit transform blob us servic solut pass datarefer input pythonscriptstep read command line argument transform solut regist vector model workspac solut us tfidfvector save regular path"
    },
    {
        "Question_id":null,
        "Question_title":"How cache is maintained for big data size locally",
        "Question_body":"<p>Hi,<br>\nI have a VM on which my local git repo exists. On top of that i have installed dvc on same machine. Now when i add data to dvc it will be in dvc cache and on git push, commit same data will go to git repo as well. Is my understanding correct? If yes then their will be two copies of data and size will keep increasing as data grows. I am not using any remote repo for data.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1568253347446,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":876.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-cache-is-maintained-for-big-data-size-locally\/217",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-09-12T03:36:35.541Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/writetoneeraj\">@writetoneeraj<\/a> Could you please have a look at this tutorial: <a href=\"https:\/\/katacoda.com\/dvc\" rel=\"nofollow noopener\">https:\/\/katacoda.com\/dvc<\/a> ? It explains how DVC manages the data. If you still have any questions please come back and ask.<\/p>",
                "Answer_score":57.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-09-13T02:44:01.763Z",
                "Answer_body":"<p>Thanks <a class=\"mention\" href=\"\/u\/dashohoxha\">@dashohoxha<\/a> , that looks great!<\/p>\n<p><a class=\"mention\" href=\"\/u\/writetoneeraj\">@writetoneeraj<\/a> No, data won\u2019t go into your git repo, only tiny metafiles (DVC-files), so there won\u2019t be duplication happening. E.g. if you <code>dvc add data<\/code>, then <code>data.dvc<\/code>(tiny yaml metafile) will be stored by git, but the <code>data<\/code> itself will be stored by dvc, so on <code>git push<\/code> only that <code>data.dvc<\/code> will be uploaded, but the <code>data<\/code> itself will stay on your machine.<\/p>",
                "Answer_score":37.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-09-19T17:56:25.101Z",
                "Answer_body":"<aside class=\"quote no-group\" data-post=\"2\" data-topic=\"217\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/dashohoxha\/40\/45_2.png\" class=\"avatar\"> dashohoxha:<\/div>\n<blockquote>\n<p>Could you please have a look at this tutorial: <a href=\"https:\/\/katacoda.com\/dvc\" rel=\"nofollow noopener\">https:\/\/katacoda.com\/dvc<\/a><\/p>\n<\/blockquote>\n<\/aside>\n<p>p.s. <a class=\"mention\" href=\"\/u\/writetoneeraj\">@writetoneeraj<\/a> the official get started guide is at <a href=\"https:\/\/dvc.org\/doc\/get-started\" rel=\"nofollow noopener\">https:\/\/dvc.org\/doc\/get-started<\/a>. Your question has to do with the <strong>Add Files<\/strong> chapter.<\/p>",
                "Answer_score":22.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-15T06:22:30.965Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/dashohoxha\">@dashohoxha<\/a> this tutorial is really good.<\/p>",
                "Answer_score":56.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"cach maintain big data size local local git repo exist instal machin add data cach git push commit data git repo understand correct ye copi data size increas data grow remot repo data",
        "Question_preprocessed_content":"cach maintain big data size local local git repo exist instal machin add data cach git push commit data git repo understand correct ye copi data size increas data grow remot repo data",
        "Question_gpt_summary_original":"The user is facing a challenge in maintaining cache for big data size locally. They are concerned about the duplication of data and the increase in size as the data grows, as they are not using any remote repository for data.",
        "Question_gpt_summary":"user face challeng maintain cach big data size local concern duplic data increas size data grow remot repositori data",
        "Answer_original_content":"writetoneeraj look tutori http katacoda com explain manag data question come ask thank dashohoxha look great writetoneeraj data wont git repo tini metafil file wont duplic happen add data data tini yaml metafil store git data store git push data upload data stai machin dashohoxha look tutori http katacoda com writetoneeraj offici start guid http org doc start question add file chapter dashohoxha tutori good",
        "Answer_preprocessed_content":"look tutori explain manag data question come ask thank look great data wont git repo tini metafil wont duplic happen store git store upload stai machin dashohoxha look tutori offici start guid question add file chapter tutori good",
        "Answer_gpt_summary_original":"the answer suggests that the user's data won't go into their git repository, only tiny metafiles, so there won't be duplication happening. the data itself will be stored locally, and on git push, only the metafile will be uploaded, but the data itself will stay on the user's machine. the answer also provides links to tutorials and the official git documentation for further information.",
        "Answer_gpt_summary":"answer suggest user data won git repositori tini metafil won duplic happen data store local git push metafil upload data stai user machin answer provid link tutori offici git document inform"
    },
    {
        "Question_id":null,
        "Question_title":"Data Wrangler Full Outer Join Not Working As Expected Nor Concatenate",
        "Question_body":"I've got two CSV files that are loaded into Data Wrangler that are intended to augment each other. The tables have some columns that are the same (in name) and some that are not, many of the rows are missing entries for many of the columns. The two tables represent separate datasets. Consider the example below: Table 1:\n\nFilename\tLabelA\tLabelB\n.\/A\/001.dat\t1\t1\n.\/A\/002.dat\t0\t1\n\nTable 2:\n\nFilename\tLabelB\tLabelC\n.\/B\/001.dat\t\t0\n.\/B\/002.dat\t0\t1\n\nI am looking to merge \/ concatenate the two table. The problem is that neither Data Wrangler join nor concatenate seems to work (at least as expected).\n\nDesired result:\n\nFilename\tLabelA\tLabelB\tLabelC\n.\/A\/001.dat\t1\t1\t\n.\/A\/002.dat\t0\t1\t\n.\/B\/001.dat\t\t\t0\n.\/B\/002.dat\t\t0\t1\n\nWhen using a \"Full Outer\" join and ask to combine \"Filename\" and \"LabelB\" columns, it will take all the values from Table 1 OR Table 2 even if Table 1 does not have that entry (for example, some rows will have Filename = <nothing> rather than Filename = .\/B\/001.dat).\n\nWhen using concatenate, Data Wrangler errors on the fact that it cannot match EVERY column between the tables.\n\nNow in my example there are many columns and many rows which precludes a manual process of joining without merging columns and then going through a renaming and merging process one-by-one. How do get these tables to simply merge? I feel I must be missing something obvious. I am about to give up on Data Wrangler and do it all in a python script using pandas, but I thought I should give Data Wrangler a try while learning the MLops process.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651799011602,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":56.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4DJAYgTvQQKdl-jplyRtmw\/data-wrangler-full-outer-join-not-working-as-expected-nor-concatenate",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-09T11:29:24.682Z",
                "Answer_score":0,
                "Answer_body":"Hi, @b33fcafe\n\nShouldn't you use Concatenate instead of Join because you don't want to join data horizontally but vertically?\n\nAfter importing Table1 and Table2, add the missing columns and then Concatenate to get the expected result.\n\nIf duplicate data occurs, additional processing is possible in subsequent steps.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"data wrangler outer join work expect concaten got csv file load data wrangler intend augment tabl column row miss entri column tabl repres separ dataset consid exampl tabl filenam labela labelb dat dat tabl filenam labelb labelc dat dat look merg concaten tabl problem data wrangler join concaten work expect desir result filenam labela labelb labelc dat dat dat dat outer join ask combin filenam labelb column valu tabl tabl tabl entri exampl row filenam filenam dat concaten data wrangler error fact match column tabl exampl column row preclud manual process join merg column go renam merg process tabl simpli merg feel miss obviou data wrangler python script panda thought data wrangler try learn mlop process",
        "Question_preprocessed_content":"data wrangler outer join work expect concaten got csv file load data wrangler intend augment tabl column row miss entri column tabl repres separ dataset consid exampl tabl filenam labela labelb tabl filenam labelb labelc look merg concaten tabl problem data wrangler join concaten work desir result filenam labela labelb labelc outer join ask combin filenam labelb column valu tabl tabl tabl entri concaten data wrangler error fact match column tabl exampl column row preclud manual process join merg column go renam merg process tabl simpli merg feel miss obviou data wrangler python script panda thought data wrangler try learn mlop process",
        "Question_gpt_summary_original":"The user is facing challenges in merging two CSV files in Data Wrangler. The tables have some common columns and some missing entries. The user has tried using a \"Full Outer\" join and concatenate, but neither seems to work as expected. The Full Outer join takes all the values from both tables, even if Table 1 does not have that entry, and concatenate errors on the fact that it cannot match every column between the tables. The user is looking for a way to merge the tables without a manual process and is considering using a python script with pandas instead.",
        "Question_gpt_summary":"user face challeng merg csv file data wrangler tabl common column miss entri user tri outer join concaten work expect outer join take valu tabl tabl entri concaten error fact match column tabl user look wai merg tabl manual process consid python script panda instead",
        "Answer_original_content":"bfcafe shouldn us concaten instead join want join data horizont vertic import tabl tabl add miss column concaten expect result duplic data occur addit process possibl subsequ step",
        "Answer_preprocessed_content":"shouldn us concaten instead join want join data horizont vertic import tabl tabl add miss column concaten expect result duplic data occur addit process possibl subsequ step",
        "Answer_gpt_summary_original":"possible solutions to the user's challenge with data wrangler's full outer join and concatenate functions when merging two csv files with shared columns and many missing entries are: \n1. use concatenate instead of join because the user wants to merge data vertically.\n2. import table1 and table2, add the missing columns, and then concatenate to get the expected result.\n3. if duplicate data occurs, additional processing is possible in subsequent steps.",
        "Answer_gpt_summary":"possibl solut user challeng data wrangler outer join concaten function merg csv file share column miss entri us concaten instead join user want merg data vertic import tabl tabl add miss column concaten expect result duplic data occur addit process possibl subsequ step"
    },
    {
        "Question_id":null,
        "Question_title":"How to pass data to an endpoint",
        "Question_body":"Hello,\nI have followed the DeepAR Chicago Traffic violations notebook example. The Model and Endpoint has been created and the forecasting is working.\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/deepar_chicago_traffic_violations\/deepar_chicago_traffic_violations.ipynb\n\nHowevr, I haven't deleted the model nor the endpoint in order to use it externally. I have created a Python script on an EC2 that tries to load the endpoint and passes the data to it to get a prediction, and here is what I am doing:\n\nLoading the CSV exactly the way I did it on the notebook\nParsing the CSV the same way I did on the notebook for the \"predictor.predict\" command\nInstead of using the \"predictor.predict\", I am using \"invoke_endpoint\" to load the endpoint and passing the data from the previous point\nInstead of getting the same response I got on the notebook, I am getting the following message:\n\"type: <class 'list'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object\"\n\nNot sure what the issue is, seems that it requires a byte data... I guess I cannot send the data as a list to the endpoint and I need to serialize it or to encode it? convert to to JSON? to Bytes?\n\nAny help will be appreciated.\nRegards",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1625081705000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":277.0,
        "Answer_body":"Hello,\n\nSo the issue here is the predictor.predict command converts the data to the format necessary for the endpoint to understand, thus you need to serialize or encode the payload by yourself. To do this you can work with something like json.dumps(payload) or for a byte array json.dumps(payload).encode().\n\nIf you want to use the predictor class this is taken care of by the serializer option. The serializer encodes\/decodes the data for us and lets you simply call the endpoint through the predictor class. An example of this is the following code snippet:\n\nfrom sagemaker.serializers import IdentitySerializer\nfrom sagemaker.deserializers import JSONDeserializer\nserializer=IdentitySerializer(content_type=\"application\/json\")\n\nHope this helps!\n\nTo check out the various serializer options that can work for your different use cases check the following link.\nSerializers: https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\n\nEdited by: rvegira-aws on Jul 22, 2021 9:22 AM\n\nEdited by: rvegira-aws on Jul 22, 2021 9:24 AM",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUpamBayk2RT6c6KuNop0DQQ\/how-to-pass-data-to-an-endpoint",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-15T13:25:47.348Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nSo the issue here is the predictor.predict command converts the data to the format necessary for the endpoint to understand, thus you need to serialize or encode the payload by yourself. To do this you can work with something like json.dumps(payload) or for a byte array json.dumps(payload).encode().\n\nIf you want to use the predictor class this is taken care of by the serializer option. The serializer encodes\/decodes the data for us and lets you simply call the endpoint through the predictor class. An example of this is the following code snippet:\n\nfrom sagemaker.serializers import IdentitySerializer\nfrom sagemaker.deserializers import JSONDeserializer\nserializer=IdentitySerializer(content_type=\"application\/json\")\n\nHope this helps!\n\nTo check out the various serializer options that can work for your different use cases check the following link.\nSerializers: https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\n\nEdited by: rvegira-aws on Jul 22, 2021 9:22 AM\n\nEdited by: rvegira-aws on Jul 22, 2021 9:24 AM",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-07-22T16:21:57.000Z",
                "Answer_score":0,
                "Answer_body":"Thanks rvegira-aws,\n\nI changed the approach, instead of using the \"invoke_endpoint\" method, I have used the predictor class as you suggested and this has fixed the issue.\n\nRegards.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-17T16:50:37.000Z",
                "Answer_score":0,
                "Answer_body":"I faced the exact problem when building models for my website, thanks for the question",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1639574747348,
        "Question_original_content":"pass data endpoint hello follow deepar chicago traffic violat notebook exampl model endpoint creat forecast work http github com aw amazon exampl blob master introduct appli machin learn deepar chicago traffic violat deepar chicago traffic violat ipynb howevr haven delet model endpoint order us extern creat python script tri load endpoint pass data predict load csv exactli wai notebook pars csv wai notebook predictor predict command instead predictor predict invok endpoint load endpoint pass data previou point instead get respons got notebook get follow messag type valid type file like object sure issu requir byte data guess send data list endpoint need serial encod convert json byte help appreci regard",
        "Question_preprocessed_content":"pass data endpoint hello follow deepar chicago traffic violat notebook exampl model endpoint creat forecast work howevr haven delet model endpoint order us extern creat python script tri load endpoint pass data predict load csv exactli wai notebook pars csv wai notebook command instead load endpoint pass data previou point instead get respons got notebook get follow messag type valid type object sure issu requir byte guess send data list endpoint need serial encod convert json byte help appreci regard",
        "Question_gpt_summary_original":"The user is facing challenges while trying to pass data to an endpoint. They have followed the DeepAR Chicago Traffic violations notebook example and created a model and endpoint, which is working fine. However, when they try to load the endpoint and pass data to it using a Python script on an EC2, they are getting an error message that requires byte data. They are unsure about how to serialize or encode the data and are seeking help to resolve the issue.",
        "Question_gpt_summary":"user face challeng try pass data endpoint follow deepar chicago traffic violat notebook exampl creat model endpoint work fine try load endpoint pass data python script get error messag requir byte data unsur serial encod data seek help resolv issu",
        "Answer_original_content":"hello issu predictor predict command convert data format necessari endpoint understand need serial encod payload work like json dump payload byte arrai json dump payload encod want us predictor class taken care serial option serial encod decod data let simpli endpoint predictor class exampl follow code snippet serial import identityseri deseri import jsondeseri serial identityseri content type applic json hope help check serial option work differ us case check follow link serial http readthedoc stabl api infer serial html edit rvegira aw jul edit rvegira aw jul",
        "Answer_preprocessed_content":"hello issu command convert data format necessari endpoint understand need serial encod payload work like byte arrai want us predictor class taken care serial option serial data let simpli endpoint predictor class exampl follow code snippet serial import identityseri deseri import jsondeseri hope help check serial option work differ us case check follow link serial edit jul edit jul",
        "Answer_gpt_summary_original":"the solution to the user's challenge in passing data to an endpoint created from a deepar chicago traffic violations notebook example is to serialize or encode the payload by themselves. they can work with something like json.dumps(payload) or for a byte array json.dumps(payload).encode(). if they want to use the predictor class, the serializer option takes care of encoding\/decoding the data for them and lets them simply call the endpoint through the predictor class. they can check out the various serializer options that can work for their different use cases at the serializers link provided.",
        "Answer_gpt_summary":"solut user challeng pass data endpoint creat deepar chicago traffic violat notebook exampl serial encod payload work like json dump payload byte arrai json dump payload encod want us predictor class serial option take care encod decod data let simpli endpoint predictor class check serial option work differ us case serial link provid"
    },
    {
        "Question_id":null,
        "Question_title":"Provide Autopilot own data",
        "Question_body":"Is there any way to provide your own (already labeled) data to sagemaker ground truth?\nMy plan is to use sagemaker autopilot with my own data that I have already labeled such that the folder structure represents the labels (rekognition provides such functionality).\nThe problem is that AutoPilot can only accept manifest files or csv,txt, ect. Well I have images with labels, so my only option is to use a manifest file. HOWEVER GroundTruth create manifest file option only accepts images from only 1 folder without any option to give it labels by default, therefore forcing me to repeat the labeling on an already labeled dataset.\n\nAny idea how to use already labeled data as input???",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591879538000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":25.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfEXvtHQmR0egOGgOSG-b0A\/provide-autopilot-own-data",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-13T11:35:14.000Z",
                "Answer_score":0,
                "Answer_body":"Hello... How are your images labeled? What format are the labels in?\n\nIf you have multiple folders each containing images, and the folder is the label then you could create a quick python (etc) script that will create a manifest file, using the folders as labels. You can then upload all the folders and the manifest file to S3 and your set.\n\nGroundtruth is a labeling pipeline tool. Its designed to organize people with domain knowledge to label data. Also its pretty pricey so you definitely don't want to use it to re-label anything thats already labeled.\n\nHope that helps.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"provid autopilot data wai provid label data ground truth plan us autopilot data label folder structur repres label rekognit provid function problem autopilot accept manifest file csv txt ect imag label option us manifest file groundtruth creat manifest file option accept imag folder option label default forc repeat label label dataset idea us label data input",
        "Question_preprocessed_content":"provid autopilot data wai provid data ground truth plan us autopilot data label folder structur repres label problem autopilot accept manifest file csv txt ect imag label option us manifest file groundtruth creat manifest file option accept imag folder option label default forc repeat label label dataset idea us label data input",
        "Question_gpt_summary_original":"The user is facing a challenge in providing their own labeled data to Sagemaker Ground Truth for use with Autopilot. While Autopilot can only accept manifest files or csv, txt, etc., the user's data consists of labeled images. The GroundTruth create manifest file option only accepts images from one folder without any option to give it labels by default, forcing the user to repeat the labeling on an already labeled dataset. The user is seeking a solution to use their already labeled data as input.",
        "Question_gpt_summary":"user face challeng provid label data ground truth us autopilot autopilot accept manifest file csv txt user data consist label imag groundtruth creat manifest file option accept imag folder option label default forc user repeat label label dataset user seek solut us label data input",
        "Answer_original_content":"hello imag label format label multipl folder contain imag folder label creat quick python script creat manifest file folder label upload folder manifest file set groundtruth label pipelin tool design organ peopl domain knowledg label data pretti pricei definit want us label that label hope help",
        "Answer_preprocessed_content":"imag label format label multipl folder contain imag folder label creat quick python script creat manifest file folder label upload folder manifest file set groundtruth label pipelin tool design organ peopl domain knowledg label data pretti pricei definit want us that label hope help",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. create a quick python script that will create a manifest file using the folders as labels if the images are labeled and stored in multiple folders.\n2. upload all the folders and the manifest file to s3.\n3. groundtruth is a labeling pipeline tool designed to organize people with domain knowledge to label data, and it is not recommended to use it to re-label anything that is already labeled.",
        "Answer_gpt_summary":"possibl solut answer creat quick python script creat manifest file folder label imag label store multipl folder upload folder manifest file groundtruth label pipelin tool design organ peopl domain knowledg label data recommend us label label"
    },
    {
        "Question_id":57147396.0,
        "Question_title":"Amazon Sage Maker: How to authenticate AWS SageMaker End-Point Request",
        "Question_body":"<p>I have an aws sagemaker end-point which need to be called from .Net core client, I have used the AWS SDK that deals with SageMaker and provided the required credentials however, always it keeps saying : <\/p>\n\n<p>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.<\/p>\n\n<p>var requestBody = \"{'url':'\"+\"<a href=\"https:\/\/cdn.pixabay.com\/photo\/2018\/05\/28\/22\/11\/message-in-a-bottle-3437294_960_720.jpg\" rel=\"nofollow noreferrer\">https:\/\/cdn.pixabay.com\/photo\/2018\/05\/28\/22\/11\/message-in-a-bottle-3437294_960_720.jpg<\/a>\" + \"'}\";<\/p>\n\n<pre><code>        var request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest()\n        {\n            EndpointName = \"CG-model-v1-endpoint\",\n            ContentType = \"application\/json;utf-8\",\n            Body = new MemoryStream(Encoding.ASCII.GetBytes(JsonConvert.SerializeObject(requestBody)))\n\n        };\n\n\n        var awsClient = new AmazonSageMakerRuntimeClient(awsAccessKeyId: \"XXXX\", awsSecretAccessKey: \"XXX\", region: RegionEndpoint.EUCentral1);\n\n        try\n        {\n            var resposnse = await awsClient.InvokeEndpointAsync(request);\n\n        }\n        catch (Exception ex)\n        {\n\n            return ApiResponse&lt;bool&gt;.Create(false);\n        }\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1563803017410,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":222.0,
        "Answer_body":"<p>I found the error , it was simply because of the request content-type,it had to be application\/json instead of application\/json;utf-8<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57147396",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1563872105436,
        "Question_original_content":"amazon sage maker authent end point request end point need call net core client aw sdk deal provid requir credenti keep sai request signatur calcul match signatur provid check aw secret access kei sign method consult servic document detail var requestbodi url http cdn pixabai com photo messag bottl jpg var request new amazon runtim model invokeendpointrequest endpointnam model endpoint contenttyp applic json utf bodi new memorystream encod ascii getbyt jsonconvert serializeobject requestbodi var awsclient new amazonruntimecli awsaccesskeyid awssecretaccesskei region regionendpoint eucentr try var resposns await awsclient invokeendpointasync request catch except return apirespons creat fals",
        "Question_preprocessed_content":"amazon sage maker authent request need call net core client aw sdk deal provid requir credenti keep sai request signatur calcul match signatur provid check aw secret access kei sign method consult servic document detail var requestbodi",
        "Question_gpt_summary_original":"The user is facing challenges in authenticating AWS SageMaker End-Point requests from a .Net core client. Despite providing the required credentials, the user is receiving an error message indicating that the request signature does not match the provided signature. The user has shared a code snippet that includes the request body and the AWS SDK used to deal with SageMaker.",
        "Question_gpt_summary":"user face challeng authent end point request net core client despit provid requir credenti user receiv error messag indic request signatur match provid signatur user share code snippet includ request bodi aw sdk deal",
        "Answer_original_content":"error simpli request content type applic json instead applic json utf",
        "Answer_preprocessed_content":"error simpli request instead",
        "Answer_gpt_summary_original":"solution: the error encountered while authenticating an end-point request using the aws sdk and provided credentials was due to the request content-type being set to \"application\/json;utf-8\" instead of \"application\/json\". changing the content-type to \"application\/json\" should resolve the issue.",
        "Answer_gpt_summary":"solut error encount authent end point request aw sdk provid credenti request content type set applic json utf instead applic json chang content type applic json resolv issu"
    },
    {
        "Question_id":null,
        "Question_title":"When will Hebrew language be available in Text-To-Speech API?",
        "Question_body":"",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1652854440000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":111.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/When-will-Hebrew-language-be-available-in-Text-To-Speech-API\/td-p\/424088\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-19T08:46:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"There is no information about when Hebrew will be available in Cloud Text-to-Speech you can file a feature request in Issue Tracker[1].\n\n[1] https:\/\/issuetracker.google.com\/issues\/new?component=451645&template=1161363"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"hebrew languag avail text speech api ",
        "Question_preprocessed_content":"hebrew languag avail api ",
        "Question_gpt_summary_original":"The user is facing a challenge with the availability of Hebrew language in the Text-To-Speech API and is seeking information on when it will be made available.",
        "Question_gpt_summary":"user face challeng avail hebrew languag text speech api seek inform avail",
        "Answer_original_content":"inform hebrew avail cloud text speech file featur request issu tracker http issuetrack googl com issu new compon templat",
        "Answer_preprocessed_content":"inform hebrew avail cloud file featur request issu tracker",
        "Answer_gpt_summary_original":"the answer suggests that there is currently no hebrew language support in the cloud text-to-speech api. however, the user can file a feature request in the issue tracker to request for the addition of hebrew language support.",
        "Answer_gpt_summary":"answer suggest current hebrew languag support cloud text speech api user file featur request issu tracker request addit hebrew languag support"
    },
    {
        "Question_id":null,
        "Question_title":"Artifact.add_reference('s3:\/\/...'): error in finding credentials",
        "Question_body":"<p>Hi, I am trying to track an artifact that lives in AWS S3. I have a <code>~\/.aws\/credentials<\/code> file with two profiles and I would like to talk to AWS and store the reference with one of those profiles. I rather not use environment variables (i.e. AWS_ACCESS_KEY_ID, etc), but stick with the credentials folder and maintain the existing profiles. Are there any ways of making this work? Thanks!<\/p>\n<p>Problem code:<\/p>\n<pre><code class=\"lang-auto\">    raw_data_uri = os.path.join('s3:\/\/', cfg.bucket_name, cfg.prefix)\n    bucket = S3Bucket(cfg.bucket_name, cfg.aws_profile) # my handler to talk to S3\n    num_items = len(bucket.ls(cfg.prefix))\n    \n    with wandb.init(project=cfg.project, job_type=cfg.job_type, name=cfg.run_stamp) as run:\n        artifact = wandb.Artifact(name='raw_data', type='raw_data',\n                                    metadata={\n                                        'aws_profile': cfg.aws_profile,\n                                        'num_items': num_items\n                                        })\n            \n        artifact.add_reference(raw_data_uri)\n        run.log_artifact(artifact)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code class=\"lang-auto\">botocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1633402137833,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":320.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/artifact-add-reference-s3-error-in-finding-credentials\/866",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-05T02:54:49.016Z",
                "Answer_body":"<p>Solved my own problem. Forgot I could set a temp env variable in the script itself:<\/p>\n<pre><code class=\"lang-auto\">os.environ['AWS_PROFILE'] = cfg.aws_profile\n<\/code><\/pre>",
                "Answer_score":47.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-05T13:54:20.420Z",
                "Answer_body":"<p>Thanks for writing an answer to your own question for any future readers!!<\/p>\n<p><img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/clap.png?v=10\" title=\":clap:\" class=\"emoji\" alt=\":clap:\"><img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/raised_hands.png?v=10\" title=\":raised_hands:\" class=\"emoji\" alt=\":raised_hands:\"> <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/raised_hands.png?v=10\" title=\":raised_hands:\" class=\"emoji\" alt=\":raised_hands:\"> <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/raised_hands.png?v=10\" title=\":raised_hands:\" class=\"emoji\" alt=\":raised_hands:\"> <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/raised_hands.png?v=10\" title=\":raised_hands:\" class=\"emoji\" alt=\":raised_hands:\"> <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/clap.png?v=10\" title=\":clap:\" class=\"emoji\" alt=\":clap:\">  <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/clap.png?v=10\" title=\":clap:\" class=\"emoji\" alt=\":clap:\"><\/p>",
                "Answer_score":47.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-20T18:02:06.476Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"artifact add refer error find credenti try track artifact live aw aw credenti file profil like talk aw store refer profil us environ variabl aw access kei stick credenti folder maintain exist profil wai make work thank problem code raw data uri path join cfg bucket cfg prefix bucket sbucket cfg bucket cfg aw profil handler talk num item len bucket cfg prefix init project cfg project job type cfg job type cfg run stamp run artifact artifact raw data type raw data metadata aw profil cfg aw profil num item num item artifact add refer raw data uri run log artifact artifact error botocor except nocredentialserror unabl locat credenti",
        "Question_preprocessed_content":"error find credenti try track artifact live aw file profil like talk aw store refer profil us environ variabl stick credenti folder maintain exist profil wai make work thank problem code error",
        "Question_gpt_summary_original":"The user is encountering an error while trying to track an artifact that is stored in AWS S3. The error message indicates that the credentials cannot be located, even though the user has a ~\/.aws\/credentials file with two profiles. The user wants to avoid using environment variables and instead use the existing profiles in the credentials folder.",
        "Question_gpt_summary":"user encount error try track artifact store aw error messag indic credenti locat user aw credenti file profil user want avoid environ variabl instead us exist profil credenti folder",
        "Answer_original_content":"solv problem forgot set temp env variabl script environ aw profil cfg aw profil thank write answer question futur reader topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"solv problem forgot set temp env variabl script thank write answer question futur reader topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the user solved their problem by setting a temporary environment variable in the script itself using the code \"os.environ['aws_profile'] = cfg.aws_profile\". this solution may be helpful for others encountering a similar issue.",
        "Answer_gpt_summary":"user solv problem set temporari environ variabl script code environ aw profil cfg aw profil solut help encount similar issu"
    },
    {
        "Question_id":50473170.0,
        "Question_title":"Embedded Azure MLmodel",
        "Question_body":"<p>Suppose I have a trained model in <a href=\"https:\/\/studio.azureml.net\" rel=\"nofollow noreferrer\">Azure ML<\/a> and I deployed it as a Web Service. Is it possible to export the model, embed it in an Android app and use it <em>locally<\/em>, without making any requests to Azure Web service?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1527009105097,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":85.0,
        "Answer_body":"<p>From <a href=\"https:\/\/stackoverflow.com\/questions\/41236871\/how-to-download-the-trained-models-from-azure-machine-studio\">this answer<\/a> you won't be able to save the model locally if you do everything within Azure ML Studio.<\/p>\n\n<p>If you create the model using Python or R and execute it within Azure ML Studio, then you can save it from the library that you use.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50473170",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1527019984807,
        "Question_original_content":"embed model suppos train model deploi web servic possibl export model emb android app us local make request azur web servic",
        "Question_preprocessed_content":"embed model suppos train model deploi web servic possibl export model emb android app us local make request azur web servic",
        "Question_gpt_summary_original":"The user is facing a challenge of whether it is possible to export a trained model from Azure ML, embed it in an Android app, and use it locally without making any requests to Azure Web service.",
        "Question_gpt_summary":"user face challeng possibl export train model emb android app us local make request azur web servic",
        "Answer_original_content":"answer won abl save model local studio creat model python execut studio save librari us",
        "Answer_preprocessed_content":"answer won abl save model local studio creat model python execut studio save librari us",
        "Answer_gpt_summary_original":"possible solution: the user can create the model using python or r and execute it within azure studio. then, they can save the model from the library that they use and embed it in their android app to use it locally without making any requests to azure web service.",
        "Answer_gpt_summary":"possibl solut user creat model python execut azur studio save model librari us emb android app us local make request azur web servic"
    },
    {
        "Question_id":65889143.0,
        "Question_title":"upload image dataset to S3 sagemaker",
        "Question_body":"<p>my dataset is 3 folders (train, validation and test) of images. each folder has two subfolders (cat1 and cat2). I am using AWS sage maker to preprocess my data and train my model. we all know that we have to upload the training data to S3 bucket before starting the &quot;.fit&quot; process.\nI want to know how to upload my data set to S3<\/p>\n<pre><code># general prefix\nprefix='chest-xray'\n#unique train\/test prefixes\ntrain_prefix   = '{}\/{}'.format(prefix, 'train')\nval_prefix   = '{}\/{}'.format(prefix, 'validation')\ntest_prefix    = '{}\/{}'.format(prefix, 'test')\n\n# uploading data to S3, and saving locations\ntrain_path  = sagemaker_session.upload_data(train_data, bucket=bucket, key_prefix=train_prefix)\n<\/code><\/pre>\n<p>what the train_data parameters should look like<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611594075077,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":444.0,
        "Answer_body":"<p>According to the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/utility\/session.html#sagemaker.session.Session.upload_data\" rel=\"nofollow noreferrer\">documentation<\/a> <code>train_data<\/code> is the local path of the file to upload to S3, so you need this file locally where you are launching the training job. If you are using a notebook this is not the way to do. You have instead to manually upload your dataset in a S3 bucket. I suggest to preprocess your dataset in a single file (tfrecord for example if you are using TF) and upload that file to S3. You can do it using the AWS web console or using the AWS-CLI with the <code>aws s3 cp yourfile s3:\/\/your-bucket <\/code>command.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65889143",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1611678605710,
        "Question_original_content":"upload imag dataset dataset folder train valid test imag folder subfold cat cat aw sage maker preprocess data train model know upload train data bucket start fit process want know upload data set gener prefix prefix chest xrai uniqu train test prefix train prefix format prefix train val prefix format prefix valid test prefix format prefix test upload data save locat train path session upload data train data bucket bucket kei prefix train prefix train data paramet look like",
        "Question_preprocessed_content":"upload imag dataset dataset folder imag folder subfold aw sage maker preprocess data train model know upload train data bucket start process want know upload data set paramet look like",
        "Question_gpt_summary_original":"The user is facing a challenge of uploading their image dataset, which consists of three folders (train, validation, and test) with two subfolders each (cat1 and cat2), to S3 bucket using AWS SageMaker. They are unsure about the format of the train_data parameters required for the upload process.",
        "Question_gpt_summary":"user face challeng upload imag dataset consist folder train valid test subfold cat cat bucket unsur format train data paramet requir upload process",
        "Answer_original_content":"accord document train data local path file upload need file local launch train job notebook wai instead manual upload dataset bucket suggest preprocess dataset singl file tfrecord exampl upload file aw web consol aw cli aw yourfil bucket command",
        "Answer_preprocessed_content":"accord document local path file upload need file local launch train job notebook wai instead manual upload dataset bucket suggest preprocess dataset singl file upload file aw web consol command",
        "Answer_gpt_summary_original":"the answer suggests that the user needs to have the file locally where they are launching the training job. if they are using a notebook, they need to manually upload their dataset to an s3 bucket. the answer recommends preprocessing the dataset in a single file, such as tfrecord, and uploading that file to s3 using either the aws web console or the aws cli.",
        "Answer_gpt_summary":"answer suggest user need file local launch train job notebook need manual upload dataset bucket answer recommend preprocess dataset singl file tfrecord upload file aw web consol aw cli"
    },
    {
        "Question_id":null,
        "Question_title":"Save_code in Google Colab",
        "Question_body":"<p>I tried saving code using <code>save_code=True<\/code> in wandb.init() running on Google Colab, but can\u2019t see any code files in the run dashboard (there is no code section).<br>\nTaking a look in the debug log, I see \"\u2026Unable to probe notebook: \u2018NoneType\u2019 object has no attribute \u2018get\u2019 \" - can I assume save_code doesn\u2019t work on Google colab?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1652861284020,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":77.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/save-code-in-google-colab\/2439",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-19T10:44:04.711Z",
                "Answer_body":"<p>Hey John, could you share the debug logs? Also which wandb version are you using?<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-20T12:59:23.676Z",
                "Answer_body":"<p>I tried to re-create in a fresh notebook and it seems to mostly work. If I can find what it was about the previous test that triggered the error I\u2019ll share more but my question (does save_code) work on colab has been answered by this second test with a resounding yes, and you can consider this issue closed.<\/p>",
                "Answer_score":45.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-23T13:27:14.262Z",
                "Answer_body":"<p>Thanks for the update!<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-19T13:00:16.400Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"save code googl colab tri save code save code true init run googl colab code file run dashboard code section take look debug log unabl probe notebook nonetyp object attribut assum save code doesnt work googl colab",
        "Question_preprocessed_content":"googl colab tri save code init run googl colab code file run dashboard take look debug log unabl probe notebook nonetyp object attribut assum doesnt work googl colab",
        "Question_gpt_summary_original":"The user encountered a challenge while trying to save code using \"save_code=True\" in wandb.init() on Google Colab. The user cannot see any code files in the run dashboard, and the debug log shows an error message \"Unable to probe notebook: 'NoneType' object has no attribute 'get'\". The user is unsure if save_code works on Google Colab.",
        "Question_gpt_summary":"user encount challeng try save code save code true init googl colab user code file run dashboard debug log show error messag unabl probe notebook nonetyp object attribut user unsur save code work googl colab",
        "Answer_original_content":"hei john share debug log version tri creat fresh notebook work previou test trigger error ill share question save code work colab answer second test resound ye consid issu close thank updat topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"hei john share debug log version tri fresh notebook work previou test trigger error ill share question work colab answer second test resound ye consid issu close thank updat topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no specific solutions mentioned in the answer. however, the answer suggests that the issue might have been resolved in a new test, and the user can consider the issue closed. the answer also requests the user to share the debug logs and the version they are using.",
        "Answer_gpt_summary":"specif solut mention answer answer suggest issu resolv new test user consid issu close answer request user share debug log version"
    },
    {
        "Question_id":null,
        "Question_title":"Cannot use artifact when in offline mode",
        "Question_body":"<p>Hi,<\/p>\n<p>How can I use artifacts without actually enabling wandb syncing? Sometimes I want just to play around my notebook without logging anything, but using data\/models logged as artifacts in my project. I think I can do it via cli but I would like to know if there\u2019s something I\u2019m missing in the API.<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1632313502310,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":287.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/cannot-use-artifact-when-in-offline-mode\/739",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-23T20:10:20.762Z",
                "Answer_body":"<p>Hello!<\/p>\n<p>Here\u2019s a copy pasta from the <a href=\"https:\/\/docs.wandb.ai\/guides\/technical-faq\">docs-faq<\/a>, please lmk if this doesn\u2019t answer your Q:<\/p>\n<h2><a name=\"heading-1\" class=\"anchor\" href=\"#heading-1\"><\/a><\/h2>\n<p>Can I run wandb offline?<\/p>\n<p>If you\u2019re training on an offline machine and want to upload your results to our servers afterwards, we have a feature for you!<\/p>\n<ol>\n<li>\n<p>Set the environment variable <code>WANDB_MODE=offline<\/code> to save the metrics locally, no internet required.<\/p>\n<\/li>\n<li>\n<p>When you\u2019re ready, run <code>wandb init<\/code> in your directory to set the project name.<\/p>\n<\/li>\n<li>\n<p>Run <code>wandb sync YOUR_RUN_DIRECTORY<\/code> to push the metrics to our cloud service and see your results in our hosted web app.<\/p>\n<\/li>\n<\/ol>",
                "Answer_score":3.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-24T03:44:17.983Z",
                "Answer_body":"<p>You can do something like this:<\/p>\n<pre><code class=\"lang-auto\">artifact =  wandb.Artifact(name=\"folds\", type=\"dataset\")\nartifact.add_file('.\/df_train.csv')\nartifact.add_file('.\/df_valid.csv')\n<\/code><\/pre>",
                "Answer_score":47.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-24T07:54:15.442Z",
                "Answer_body":"<p>The thing is that I want to be online, but just to use\/download artifacts, but not for logging anything. In other words, I want to be able to download from my project but not to upload.<\/p>",
                "Answer_score":7.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-24T09:38:30.540Z",
                "Answer_body":"<p>You could try the following pseudocode:<\/p>\n<ol>\n<li>wandb.login()<\/li>\n<li>wandb.init()<\/li>\n<li>Download from Artifacts<\/li>\n<li>wandb.finish()<\/li>\n<li>set WANDB_MODE=offline<\/li>\n<li>do training<\/li>\n<\/ol>\n<p>An alternative to 4 + 5 would be to  turn on <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/anon?q=disable\">anonymous mode<\/a> for your training, everything will  be tracked and synced to a temporary anon account and not linked to your account<\/p>\n<p><code>wandb.init(anonymous=\"allow\")<\/code><\/p>",
                "Answer_score":72.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-24T10:52:16.926Z",
                "Answer_body":"<p>Thanks! The anonymus mode could do the trick for me\u2026however, I can\u2019t udnerstand how it exactly works. I\u2019m trying the <a href=\"http:\/\/bit.ly\/anon-mode\" rel=\"noopener nofollow ugc\">colab notebook they provide<\/a>,  setting <code>anonymous=must<\/code> but  it still creates a run linked to my account<\/p>",
                "Answer_score":7.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-24T11:43:51.750Z",
                "Answer_body":"<p>hmmm ok, I guess maybe you have to log out\u2026will forward that feedback. Maybe instead of anonymous you could use set the <code>mode<\/code> parameter to <code>offline<\/code>, or <code>disabled<\/code> then<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/docs.wandb.ai\/ref\/python\/init\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/41cd209363aac9340aa990ad198a67c63ad5a47b.png\" class=\"site-icon\" width=\"256\" height=\"256\">\n\n      <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/init\" target=\"_blank\" rel=\"noopener\">docs.wandb.ai<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/75ad225cd4a0f29cec0e6dd7859f017ee0df3c7a_2_690x362.png\" class=\"thumbnail\" width=\"690\" height=\"362\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/75ad225cd4a0f29cec0e6dd7859f017ee0df3c7a_2_690x362.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/75ad225cd4a0f29cec0e6dd7859f017ee0df3c7a_2_1035x543.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/75ad225cd4a0f29cec0e6dd7859f017ee0df3c7a.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/75ad225cd4a0f29cec0e6dd7859f017ee0df3c7a_2_10x10.png\"><\/div>\n\n<h3><a href=\"https:\/\/docs.wandb.ai\/ref\/python\/init\" target=\"_blank\" rel=\"noopener\">wandb.init<\/a><\/h3>\n\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":7.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-24T12:24:22.844Z",
                "Answer_body":"<p>Yes that\u2019s what I used to do (<code>mode=disabled<\/code>) when my run does not use artifacts as input datasets. The problem is that with mode=disabled I cannot use artifacts<\/p>",
                "Answer_score":2.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-20T18:02:07.422Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"us artifact offlin mode us artifact actual enabl sync want plai notebook log data model log artifact project think cli like know there miss api thank",
        "Question_preprocessed_content":"us artifact offlin mode us artifact actual enabl sync want plai notebook log log artifact project think cli like know there miss api thank",
        "Question_gpt_summary_original":"The user is facing a challenge of not being able to use artifacts without enabling wandb syncing. They want to use data\/models logged as artifacts in their project without logging anything and are looking for a way to do it via the API.",
        "Question_gpt_summary":"user face challeng abl us artifact enabl sync want us data model log artifact project log look wai api",
        "Answer_original_content":"hello here copi pasta doc faq lmk doesnt answer run offlin your train offlin machin want upload result server featur set environ variabl mode offlin save metric local internet requir your readi run init directori set project run sync run directori push metric cloud servic result host web app like artifact artifact fold type dataset artifact add file train csv artifact add file valid csv thing want onlin us download artifact log word want abl download project upload try follow pseudocod login init download artifact finish set mode offlin train altern turn anonym mode train track sync temporari anon account link account init anonym allow thank anonymu mode trick mehowev udnerstand exactli work try colab notebook provid set anonym creat run link account guess mayb log outwil forward feedback mayb instead anonym us set mode paramet offlin disabl doc init ye that mode disabl run us artifact input dataset problem mode disabl us artifact topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"hello here copi pasta lmk doesnt answer run offlin your train offlin machin want upload result server featur set environ variabl save metric local internet requir your readi run directori set project run push metric cloud servic result host web app like thing want onlin artifact log word want abl download project upload try follow pseudocod login init download artifact finish set train altern turn anonym mode train track sync temporari anon account link account thank anonymu mode trick mehowev udnerstand exactli work try colab notebook provid set creat run link account guess mayb log outwil forward feedback mayb instead anonym us set paramet init ye that run us artifact input dataset problem mode disabl us artifact topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the user wants to use artifacts in offline mode without enabling syncing. one solution is to set the environment variable _mode=offline to save metrics locally and upload them later. another solution is to use anonymous mode for training, which tracks and syncs everything to a temporary anonymous account. alternatively, the user can set the mode parameter to offline or disabled in the .init function, but this will not allow the use of artifacts.",
        "Answer_gpt_summary":"user want us artifact offlin mode enabl sync solut set environ variabl mode offlin save metric local upload later solut us anonym mode train track sync temporari anonym account altern user set mode paramet offlin disabl init function allow us artifact"
    },
    {
        "Question_id":null,
        "Question_title":"No more Wavenet for fr-FR lang",
        "Question_body":"Hi,I have been using Google cloud API for text-to-speech to generate audio based on text for some days using the Wavenet voices and it worked great. The vast majority of my text is French and I have been using the fr-FR-Wavenet-C voice for it. I can't find it anymore. Even the page https:\/\/cloud.google.com\/text-to-speech\/ doesn't show up in the demo section. That's seems to be the case for all fr-FR-Wavenet voices. Have they been deleted?",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1644366720000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":60.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/No-more-Wavenet-for-fr-FR-lang\/td-p\/391415\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-09T00:32:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi,\n\nI have been using Google cloud API for text-to-speech to generate audio based on text for some days using the Wavenet voices and it worked great. The vast majority of my text is French and I have been using the fr-FR-Wavenet-C voice for it. I can't find it anymore. Even the page https:\/\/cloud.google.com\/text-to-speech\/ doesn't show up in the demo section. That's seems to be the case for all fr-FR-Wavenet voices. Have they been deleted?"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"wavenet lang googl cloud api text speech gener audio base text dai wavenet voic work great vast major text french wavenet voic anymor page http cloud googl com text speech demo section case wavenet voic delet",
        "Question_preprocessed_content":"wavenet lang googl cloud api gener audio base text dai wavenet voic work great vast major text french voic anymor page demo section case voic delet",
        "Question_gpt_summary_original":"The user is facing challenges with the Google Cloud API for text-to-speech as the fr-FR-Wavenet-C voice, which was previously used for generating audio based on French text, is no longer available. The user is unable to find any fr-FR-Wavenet voices on the demo section of the Google Cloud API for text-to-speech. The user is unsure if the voices have been deleted.",
        "Question_gpt_summary":"user face challeng googl cloud api text speech wavenet voic previous gener audio base french text longer avail user unabl wavenet voic demo section googl cloud api text speech user unsur voic delet",
        "Answer_original_content":"googl cloud api text speech gener audio base text dai wavenet voic work great vast major text french wavenet voic anymor page http cloud googl com text speech demo section case wavenet voic delet",
        "Answer_preprocessed_content":"googl cloud api gener audio base text dai wavenet voic work great vast major text french voic anymor page demo section case voic delet",
        "Answer_gpt_summary_original":"possible solutions: \n- check if the fr-fr-wavenet-c voice has been replaced by a different voice or if it has been removed entirely.\n- contact google cloud support to inquire about the availability of the fr-fr-wavenet-c voice or any alternative options for generating french audio using the text-to-speech api.",
        "Answer_gpt_summary":"possibl solut check wavenet voic replac differ voic remov entir contact googl cloud support inquir avail wavenet voic altern option gener french audio text speech api"
    },
    {
        "Question_id":null,
        "Question_title":"azure ML no kernel connected jupyter notebook",
        "Question_body":"I Setup a new ML Resource, cloned a tutorial notebook from azure ml and when i try to get &#34;Jupyter Kernel Failure&#34;. I do see at the top right no kernel is connected. i have set a compute. How am i able to start or restart the jupyter kernel?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1593201636980,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/40474\/azure-ml-no-kernel-connected-jupyter-notebook.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-29T11:50:51.187Z",
                "Answer_score":0,
                "Answer_body":"@Orange-7012 If you are using ml.azure.com for Azure ML notebooks you should see an option to interrupt the kernel and restart it or use the menu option of your notebook for Kernel operations to perform the same.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-24T16:00:18.113Z",
                "Answer_score":0,
                "Answer_body":"hello romungi,\ni do not see the option to interrupt and restart the kernel, the kernel options is greyed out and does not let me view the menu.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"kernel connect jupyt notebook setup new resourc clone tutori notebook try jupyt kernel failur right kernel connect set comput abl start restart jupyt kernel",
        "Question_preprocessed_content":"kernel connect jupyt notebook setup new resourc clone tutori notebook try jupyt kernel failur right kernel connect set comput abl start restart jupyt kernel",
        "Question_gpt_summary_original":"The user encountered a challenge with Azure ML where they were unable to connect to the Jupyter kernel and received a \"Jupyter Kernel Failure\" message. Despite setting up a compute, the user was unable to start or restart the kernel and noticed that no kernel was connected in the top right corner of the screen.",
        "Question_gpt_summary":"user encount challeng unabl connect jupyt kernel receiv jupyt kernel failur messag despit set comput user unabl start restart kernel notic kernel connect right corner screen",
        "Answer_original_content":"orang azur com notebook option interrupt kernel restart us menu option notebook kernel oper perform hello romungi option interrupt restart kernel kernel option grei let view menu",
        "Answer_preprocessed_content":"notebook option interrupt kernel restart us menu option notebook kernel oper perform hello romungi option interrupt restart kernel kernel option grei let view menu",
        "Answer_gpt_summary_original":"possible solutions to the challenge of being unable to start or restart the kernel in a jupyter notebook include interrupting and restarting the kernel through the ml.azure.com platform or using the kernel operations menu within the notebook. however, if the kernel options are greyed out and inaccessible, these solutions may not be applicable.",
        "Answer_gpt_summary":"possibl solut challeng unabl start restart kernel jupyt notebook includ interrupt restart kernel azur com platform kernel oper menu notebook kernel option grei inaccess solut applic"
    },
    {
        "Question_id":null,
        "Question_title":"How to predict train model time",
        "Question_body":"How to predict the time for train model for data with 10 millions lines and zero columns with decision tree?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653903513477,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/869641\/how-to-predict-train-model-time.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-30T20:34:28.677Z",
                "Answer_score":0,
                "Answer_body":"Hello @Jack-1854\n\nI am a little bit confused about your zero columns. Let's say x columns.\n\nFirst, I don't think there is an official way to estimate the time for now, since the data and the algorithms will both effect it. But for your model, you can absolutely estimat it by reudce the dataset as a test to see the time, like, you can give 10k data first to see how long the studio takes to train it, then you can estimate the time.\n\nPlease let me know if you are doing something special since you are mentioning zero column.\n\nI hope this helps.\n\nRegards,\nYutong\n\n\n\n\n-Please kindly accept the answer if you feel helpful to help the community, thanks.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"predict train model time predict time train model data million line zero column decis tree",
        "Question_preprocessed_content":"predict train model time predict time train model data million line zero column decis tree",
        "Question_gpt_summary_original":"The user is facing a challenge in predicting the time required to train a decision tree model for a large dataset with 10 million lines and zero columns.",
        "Question_gpt_summary":"user face challeng predict time requir train decis tree model larg dataset million line zero column",
        "Answer_original_content":"hello jack littl bit confus zero column let column think offici wai estim time data algorithm effect model absolut estimat reudc dataset test time like data long studio take train estim time let know special mention zero column hope help regard yutong kindli accept answer feel help help commun thank",
        "Answer_preprocessed_content":"hello littl bit confus zero column let column think offici wai estim time data algorithm effect model absolut estimat reudc dataset test time like data long studio take train estim time let know special mention zero column hope help regard yutong kindli accept answer feel help help commun thank",
        "Answer_gpt_summary_original":"the answer suggests that there is no official way to estimate the time it will take to train a model for data with 10 million lines and x columns using a decision tree. however, the user can estimate it by reducing the dataset as a test to see the time. for instance, the user can give 10k data first to see how long the studio takes to train it, then estimate the time.",
        "Answer_gpt_summary":"answer suggest offici wai estim time train model data million line column decis tree user estim reduc dataset test time instanc user data long studio take train estim time"
    },
    {
        "Question_id":null,
        "Question_title":"How Do I Create a ModelDirectory Type FileDataset",
        "Question_body":"I am trying to build a solution that automates part of the model deployment within the Azure ML designer. I am able to build a model with the designer, and then execute a python script block to extract the trained_model_outputs folder from the model training block. I have precisely matched the folder structure that Azure ML designer assigns to the model's FileDataset\n\nWhen I register the trained_model_outputs as a FileDataset, it assigns it the type AnyDirectory. This is a problem, as when I try to build it into the inference pipeline, the designer rejects it, saying it must be a ModelDirectory, even though there shouldn't be any functional difference between the two.\n\nI have seen that I can expose the ModelDirectory class as below, however I cannot find the API documentation online about this class anywhere, and I can't review it's source code as it isn't in the standard SDK:\n\nfrom azureml.studio.core.io.model_directory import ModelDirectory\n\nCan you provide a code snippet or similar that I can use to leverage this class when creating the FileDataset so that the model dataset gains the ModelDirectory type attribute?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1621299075643,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/398468\/how-do-i-create-a-modeldirectory-type-filedataset.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-18T13:56:26.373Z",
                "Answer_score":0,
                "Answer_body":"@LeeHarper-5286 Thanks for the question. Can you please add more details about the use case.\n\nOutFileDatasetConfig is a control plane concept to pass data between pipeline steps. PipelineData was intended to represent \"transient\" data from one step to the next one, while OutputDatasetConfig was intended for capturing the final state of a dataset. PipelineData always outputs data in a folder structure like {run_id}{output_name}. OutputDatasetConfig allows to decouple the data from the run and hence it allows you to control where to land the data (although by default it will produce similar folder structure). The OutputDatasetConfig allows even to register the output as a Dataset, where getting rid of such folder structure makes sense. From the docs itself: \"Represent how to copy the output of a run and be promoted as a FileDataset. The OutputFileDatasetConfig allows you to specify how you want a particular local path on the compute target to be uploaded to the specified destination\".\n\nPlease follow the below link to use the upload API.\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.dataset_factory.filedatasetfactory?view=azure-ml-py#upload-directory-src-dir--target--pattern-none--overwrite-false--show-progress-true-",
                "Answer_comment_count":10,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"creat modeldirectori type filedataset try build solut autom model deploy design abl build model design execut python script block extract train model output folder model train block precis match folder structur design assign model filedataset regist train model output filedataset assign type anydirectori problem try build infer pipelin design reject sai modeldirectori shouldn function differ seen expos modeldirectori class api document onlin class review sourc code isn standard sdk studio core model directori import modeldirectori provid code snippet similar us leverag class creat filedataset model dataset gain modeldirectori type attribut",
        "Question_preprocessed_content":"creat modeldirectori type filedataset try build solut autom model deploy design abl build model design execut python script block extract folder model train block precis match folder structur design assign model filedataset regist filedataset assign type anydirectori problem try build infer pipelin design reject sai modeldirectori shouldn function differ seen expos modeldirectori class api document onlin class review sourc code isn standard sdk import modeldirectori provid code snippet similar us leverag class creat filedataset model dataset gain modeldirectori type attribut",
        "Question_gpt_summary_original":"The user is facing a challenge in creating a ModelDirectory type FileDataset for their solution that automates part of the model deployment within the Azure ML designer. They have built a model with the designer and extracted the trained_model_outputs folder from the model training block, but when they register it as a FileDataset, it assigns it the type AnyDirectory instead of ModelDirectory. The designer rejects it when they try to build it into the inference pipeline, and they are unable to find API documentation or source code for the ModelDirectory class. The user is seeking a code snippet or similar to leverage this class when creating the FileDataset to give the model dataset the ModelDirectory type attribute.",
        "Question_gpt_summary":"user face challeng creat modeldirectori type filedataset solut autom model deploy design built model design extract train model output folder model train block regist filedataset assign type anydirectori instead modeldirectori design reject try build infer pipelin unabl api document sourc code modeldirectori class user seek code snippet similar leverag class creat filedataset model dataset modeldirectori type attribut",
        "Answer_original_content":"leeharp thank question add detail us case outfiledatasetconfig control plane concept pass data pipelin step pipelinedata intend repres transient data step outputdatasetconfig intend captur final state dataset pipelinedata output data folder structur like run output outputdatasetconfig allow decoupl data run allow control land data default produc similar folder structur outputdatasetconfig allow regist output dataset get rid folder structur make sens doc repres copi output run promot filedataset outputfiledatasetconfig allow specifi want particular local path comput target upload specifi destin follow link us upload api http doc microsoft com python api core data dataset factori filedatasetfactori view azur upload directori src dir target pattern overwrit fals progress true",
        "Answer_preprocessed_content":"thank question add detail us case outfiledatasetconfig control plane concept pass data pipelin step pipelinedata intend repres transient data step outputdatasetconfig intend captur final state dataset pipelinedata output data folder structur like outputdatasetconfig allow decoupl data run allow control land data outputdatasetconfig allow regist output dataset get rid folder structur make sens doc repres copi output run promot filedataset outputfiledatasetconfig allow specifi want particular local path comput target upload specifi destin follow link us upload api",
        "Answer_gpt_summary_original":"the answer provides some clarification on the use of outfiledatasetconfig and pipelinedata in passing data between pipeline steps. it suggests using outputdatasetconfig to capture the final state of a dataset and register it as a filedataset. the answer also provides a link to the upload api for further assistance.",
        "Answer_gpt_summary":"answer provid clarif us outfiledatasetconfig pipelinedata pass data pipelin step suggest outputdatasetconfig captur final state dataset regist filedataset answer provid link upload api assist"
    },
    {
        "Question_id":null,
        "Question_title":"The new languages are missing",
        "Question_body":"Google cloud translation have added new languages. About 24 new languages has been added to Google Translate. Very good job, well done. But they are not listed on this link.\nhttps:\/\/cloud.google.com\/translate\/docs\/languages\n\nI tried to access it using basic v2 API code, but no response came to my translation request. When will this new languages be available to be accessed by v2 APIs? ",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1652751780000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":89.0,
        "Answer_body":"These are the new 24 languages[1].\n\nIn that post there is a research paper[2] where you can see the codes it begins on page 57.\n\nThe document that you shared it is in an internal Work in Progress with no launch date yet.\n\n[1]https:\/\/blog.google\/products\/translate\/24-new-languages\/\u00a0\n\n[2]https:\/\/arxiv.org\/pdf\/2205.03983.pdf\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/The-new-languages-are-missing\/td-p\/423648\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-18T16:00:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"These are the new 24 languages[1].\n\nIn that post there is a research paper[2] where you can see the codes it begins on page 57.\n\nThe document that you shared it is in an internal Work in Progress with no launch date yet.\n\n[1]https:\/\/blog.google\/products\/translate\/24-new-languages\/\u00a0\n\n[2]https:\/\/arxiv.org\/pdf\/2205.03983.pdf\n\nView solution in original post"
            },
            {
                "Answer_creation_time":"2022-05-18T16:00:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"These are the new 24 languages[1].\n\nIn that post there is a research paper[2] where you can see the codes it begins on page 57.\n\nThe document that you shared it is in an internal Work in Progress with no launch date yet.\n\n[1]https:\/\/blog.google\/products\/translate\/24-new-languages\/\u00a0\n\n[2]https:\/\/arxiv.org\/pdf\/2205.03983.pdf"
            },
            {
                "Answer_creation_time":"2022-05-22T09:33:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Thank you for the update. I hope they do it soon. That would be good."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1652889600000,
        "Question_original_content":"new languag miss googl cloud translat ad new languag new languag ad googl translat good job list link http cloud googl com translat doc languag tri access basic api code respons came translat request new languag avail access api",
        "Question_preprocessed_content":"new languag miss googl cloud translat ad new languag new languag ad googl translat good job list link tri access basic api code respons came translat request new languag avail access api",
        "Question_gpt_summary_original":"The user is facing challenges accessing the new languages added to Google Translate as they are not listed on the provided link and are not available to be accessed by v2 APIs.",
        "Question_gpt_summary":"user face challeng access new languag ad googl translat list provid link avail access api",
        "Answer_original_content":"new languag post research paper code begin page document share intern work progress launch date http blog googl product translat new languag http arxiv org pdf pdf view solut origin post",
        "Answer_preprocessed_content":"new languag post research paper code begin page document share intern work progress launch date view solut origin post",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer to the user's challenge with accessing new languages added to google cloud translation using the v2 api code. the answer only provides information about the new languages and a research paper with codes for the languages. the document shared by the user is also mentioned to be an internal work in progress with no launch date yet.",
        "Answer_gpt_summary":"solut provid answer user challeng access new languag ad googl cloud translat api code answer provid inform new languag research paper code languag document share user mention intern work progress launch date"
    },
    {
        "Question_id":null,
        "Question_title":"Adding tfRecords files to artifacts doesn't work?",
        "Question_body":"<p>Hello,<\/p>\n<p>I am trying logging my tfRecords files to artefact, but it seems to not be working (I get an error: \u201cwandb: Network error (TransientError), entering retry loop.\u201d).<\/p>\n<p>I am providing the code I use below. I am pretty sure it is something regarding the tfRecords file since I tried changing the contents of my folders to contain only .csv and .paqruet and it worked nicely. Do you have any ideas what could be happening here?<\/p>\n<pre><code class=\"lang-auto\">with wandb.init(project=\"----\", entity='----', job_type='saving_processed_files') as run:\n    train_data_art = wandb.Artifact(\n        name='train_data',\n        type='train_data'  \n    )\n\n    files_train = os.listdir(final_path_train)\n    files_train=[x  for x in files_train if x[0]!='.']\n\n    for file in files_train:\n        file_path = os.path.join(final_path_train, file)\n        train_data_art.add_file(file_path, name=file)\n\n    run.log_artifact(train_data_art)\n<\/code><\/pre>",
        "Question_answer_count":9,
        "Question_comment_count":0,
        "Question_creation_time":1645200359797,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":201.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/adding-tfrecords-files-to-artifacts-doesnt-work\/1948",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-18T23:21:12.490Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/drlje\">@drlje<\/a>,<\/p>\n<p>Is this error still popping up with TF Record files? Usually TransientErrors are minor network issues that automatically get resolved after a while.<\/p>\n<p>Please let me know if this error still persists and I will dig in further into what might be happening here in that case.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":7.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-22T18:02:59.790Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/drlje\">@drlje<\/a>,<\/p>\n<p>We wanted to follow up with you regarding this issue as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":6.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-25T20:23:30.807Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/drlje\">@drlje<\/a>,<\/p>\n<p>Since we have not heard back from you, I am closing out this request. If you would like to re-open this conversation, please let us know!<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-26T14:09:58.492Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/ramit_goolry\">@ramit_goolry<\/a> ,<\/p>\n<p>Sorry for not being prompt. I tried again and I got the same error. However, I also tries doing this on a small fraction of data (also saved as a TFRecords) and it went through. So I am guessing this has something to do with the size - the total size of my files is around 10gb. Do you think that could be the issue?<\/p>\n<p>Thanks a lot!<\/p>\n<p>Marin<\/p>",
                "Answer_score":6.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-22T09:00:11.736Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ramit_goolry\">@ramit_goolry<\/a>,<\/p>\n<p>Do you have a feedback regarding the limit size of the files being uploaded? We are thinking to upgrade our account and this issue is really important for us.<\/p>\n<p>Thanks!<\/p>\n<p>Marin<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-13T14:29:02.000Z",
                "Answer_body":"<p>Hi,<\/p>\n<p>Could you please re-look at this issue - I have left additional comment a while ago, and in a couple of days we will face this issue again, so I would love to get to the bottom of it.<\/p>\n<p><a href=\"https:\/\/community.wandb.ai\/t\/adding-tfrecords-files-to-artifacts-doesnt-work\/1948\/6\">https:\/\/community.wandb.ai\/t\/adding-tfrecords-files-to-artifacts-doesnt-work\/1948\/6<\/a><\/p>\n<p>Thanks!<\/p>\n<p>Marin<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-14T17:33:53.146Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/drlje\">@drlje<\/a>,<\/p>\n<p>I\u2019m sorry about not responding here sooner. I\u2019m sorry to hear you are still facing this issue, and I will definitely assist you here. You said you are seeing an error with a lot of data : Could you share a little bit more  information about the structure of this data and the behavior you see? More specifically:<\/p>\n<ul>\n<li>How many files do you have?<\/li>\n<li>Are you seeing a lot of time delay before these errors show up?<\/li>\n<li>Could you try uploading this same scale of information but using some other file format? (like a set of <code>.txt<\/code> files)<\/li>\n<\/ul>\n<p>Additionally, the <code>debug.log<\/code> and <code>debug-internal.log<\/code> files associated with the run where you are facing this issue would be highly appreciated since it would give us some more visibility into this issue.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-22T06:54:30.577Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ramit_goolry\">@ramit_goolry<\/a>,<\/p>\n<p>In reproducing the issue today, the artifact was saved without any problems; so I guess the issue can be closed. If we experience the same problematic again, I will follow the steps above and supply you with the log files.<\/p>\n<p>Many thanks!<\/p>\n<p>Best,<\/p>\n<p>Marin<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-21T06:55:17.151Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"ad tfrecord file artifact work hello try log tfrecord file artefact work error network error transienterror enter retri loop provid code us pretti sure tfrecord file tri chang content folder contain csv paqruet work nice idea happen init project entiti job type save process file run train data art artifact train data type train data file train listdir final path train file train file train file file train file path path join final path train file train data art add file file path file run log artifact train data art",
        "Question_preprocessed_content":"ad tfrecord file artifact work hello try log tfrecord file artefact work enter retri provid code us pretti sure tfrecord file tri chang content folder contain csv paqruet work nice idea happen",
        "Question_gpt_summary_original":"The user is encountering an error while trying to log their tfRecords files to an artifact using WandB. They suspect that the issue is related to the tfRecords file format since they were able to successfully log other file formats. The user has provided their code and is seeking assistance in resolving the issue.",
        "Question_gpt_summary":"user encount error try log tfrecord file artifact suspect issu relat tfrecord file format abl successfulli log file format user provid code seek assist resolv issu",
        "Answer_original_content":"drlje error pop record file usual transienterror minor network issu automat resolv let know error persist dig happen case thank ramit drlje want follow issu heard let know assist issu resolv thank ramit drlje heard close request like open convers let know thank ramit hello ramit goolri sorri prompt tri got error tri small fraction data save tfrecord went guess size total size file think issu thank lot marin ramit goolri feedback limit size file upload think upgrad account issu import thank marin look issu left addit comment ago coupl dai face issu love http commun ad tfrecord file artifact doesnt work thank marin drlje sorri respond sooner sorri hear face issu definit assist said see error lot data share littl bit inform structur data behavior specif file see lot time delai error try upload scale inform file format like set txt file addition debug log debug intern log file associ run face issu highli appreci visibl issu thank ramit ramit goolri reproduc issu todai artifact save problem guess issu close experi problemat follow step suppli log file thank best marin topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"error pop record file usual transienterror minor network issu automat resolv let know error persist dig happen case thank ramit want follow issu heard let know assist issu resolv thank ramit heard close request like convers let know thank ramit hello sorri prompt tri got error tri small fraction data went guess size total size file think issu thank lot marin feedback limit size file upload think upgrad account issu import thank marin issu left addit comment ago coupl dai face issu love thank marin sorri respond sooner sorri hear face issu definit assist said see error lot data share littl bit inform structur data behavior specif file see lot time delai error try upload scale inform file format addition file associ run face issu highli appreci visibl issu thank ramit reproduc issu todai artifact save problem guess issu close experi problemat follow step suppli log file thank best marin topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the user was encountering an error when attempting to log their tfrecords files to an artifact, despite successfully logging other file types. the error was related to the size of the files, which were around 10gb. the possible solutions suggested were to try uploading the same scale of information but using some other file format, like a set of .txt files, and to provide the debug.log and debug-internal.log files associated with the run where the issue occurred. the issue was eventually resolved, and the user was advised to follow the same steps if they experience the same problem again.",
        "Answer_gpt_summary":"user encount error attempt log tfrecord file artifact despit successfulli log file type error relat size file possibl solut suggest try upload scale inform file format like set txt file provid debug log debug intern log file associ run issu occur issu eventu resolv user advis follow step experi problem"
    },
    {
        "Question_id":62422682.0,
        "Question_title":"sagemaker notebook instance Elastic Inference tensorflow model local deployment",
        "Question_body":"<p>I am trying to replicate <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_using_elastic_inference_with_your_own_model\/tensorflow_serving_pretrained_model_elastic_inference.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_using_elastic_inference_with_your_own_model\/tensorflow_serving_pretrained_model_elastic_inference.ipynb<\/a><\/p>\n\n<p>My elastic inference accelerator is attached to notebook instance. I am using conda_amazonei_tensorflow_p36 kernel. According to documentation I made the changes for local EI:<\/p>\n\n<pre><code>%%time\nimport boto3\n\nregion = boto3.Session().region_name\nsaved_model = 's3:\/\/sagemaker-sample-data-{}\/tensorflow\/model\/resnet\/resnet_50_v2_fp32_NCHW.tar.gz'.format(region)\n\nimport sagemaker\nfrom sagemaker.tensorflow.serving import Model\n\nrole = sagemaker.get_execution_role()\n\ntensorflow_model = Model(model_data=saved_model,\nrole=role,\nframework_version='1.14')\ntf_predictor = tensorflow_model.deploy(initial_instance_count=1,\ninstance_type='local',\naccelerator_type='local_sagemaker_notebook')\n<\/code><\/pre>\n\n<p>I am getting following log in the notebook:<\/p>\n\n<pre><code>Attaching to tmp6uqys1el_algo-1-7ynb1_1\nalgo-1-7ynb1_1 | INFO:main:starting services\nalgo-1-7ynb1_1 | INFO:main:using default model name: Servo\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving model config:\nalgo-1-7ynb1_1 | model_config_list: {\nalgo-1-7ynb1_1 | config: {\nalgo-1-7ynb1_1 | name: \"Servo\",\nalgo-1-7ynb1_1 | base_path: \"\/opt\/ml\/model\/export\/Servo\",\nalgo-1-7ynb1_1 | model_platform: \"tensorflow\"\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | INFO:main:nginx config:\nalgo-1-7ynb1_1 | load_module modules\/ngx_http_js_module.so;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | worker_processes auto;\nalgo-1-7ynb1_1 | daemon off;\nalgo-1-7ynb1_1 | pid \/tmp\/nginx.pid;\nalgo-1-7ynb1_1 | error_log \/dev\/stderr error;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | worker_rlimit_nofile 4096;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | events {\nalgo-1-7ynb1_1 | worker_connections 2048;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | http {\nalgo-1-7ynb1_1 | include \/etc\/nginx\/mime.types;\nalgo-1-7ynb1_1 | default_type application\/json;\nalgo-1-7ynb1_1 | access_log \/dev\/stdout combined;\nalgo-1-7ynb1_1 | js_include tensorflow-serving.js;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | upstream tfs_upstream {\nalgo-1-7ynb1_1 | server localhost:8501;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | upstream gunicorn_upstream {\nalgo-1-7ynb1_1 | server unix:\/tmp\/gunicorn.sock fail_timeout=1;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | server {\nalgo-1-7ynb1_1 | listen 8080 deferred;\nalgo-1-7ynb1_1 | client_max_body_size 0;\nalgo-1-7ynb1_1 | client_body_buffer_size 100m;\nalgo-1-7ynb1_1 | subrequest_output_buffer_size 100m;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | set $tfs_version 1.14;\nalgo-1-7ynb1_1 | set $default_tfs_model Servo;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/tfs {\nalgo-1-7ynb1_1 | rewrite ^\/tfs\/(.) \/$1 break;\nalgo-1-7ynb1_1 | proxy_redirect off;\nalgo-1-7ynb1_1 | proxy_pass_request_headers off;\nalgo-1-7ynb1_1 | proxy_set_header Content-Type 'application\/json';\nalgo-1-7ynb1_1 | proxy_set_header Accept 'application\/json';\nalgo-1-7ynb1_1 | proxy_pass http:\/\/tfs_upstream;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/ping {\nalgo-1-7ynb1_1 | js_content ping;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/invocations {\nalgo-1-7ynb1_1 | js_content invocations;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location ~ ^\/models\/(.)\/invoke {\nalgo-1-7ynb1_1 | js_content invocations;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/models {\nalgo-1-7ynb1_1 | proxy_pass http:\/\/gunicorn_upstream\/models;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/ {\nalgo-1-7ynb1_1 | return 404 '{\"error\": \"Not Found\"}';\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | keepalive_timeout 3;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | INFO:main:tensorflow version info:\nalgo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-7ynb1_1 | TensorFlow Library: 1.14.0\nalgo-1-7ynb1_1 | EI Version: EI-1.4\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 8)\nalgo-1-7ynb1_1 | INFO:main:nginx version info:\nalgo-1-7ynb1_1 | nginx version: nginx\/1.16.1\nalgo-1-7ynb1_1 | built by gcc 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11)\nalgo-1-7ynb1_1 | built with OpenSSL 1.0.2g 1 Mar 2016\nalgo-1-7ynb1_1 | TLS SNI support enabled\nalgo-1-7ynb1_1 | configure arguments: --prefix=\/etc\/nginx --sbin-path=\/usr\/sbin\/nginx --modules-path=\/usr\/lib\/nginx\/modules --conf-path=\/etc\/nginx\/nginx.conf --error-log-path=\/var\/log\/nginx\/error.log --http-log-path=\/var\/log\/nginx\/access.log --pid-path=\/var\/run\/nginx.pid --lock-path=\/var\/run\/nginx.lock --http-client-body-temp-path=\/var\/cache\/nginx\/client_temp --http-proxy-temp-path=\/var\/cache\/nginx\/proxy_temp --http-fastcgi-temp-path=\/var\/cache\/nginx\/fastcgi_temp --http-uwsgi-temp-path=\/var\/cache\/nginx\/uwsgi_temp --http-scgi-temp-path=\/var\/cache\/nginx\/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\nalgo-1-7ynb1_1 | INFO:main:started nginx (pid: 10)\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.888114: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.888186: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988623: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988688: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988728: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988762: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988783: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.001922: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.082734: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.613725: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\nalgo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1\nalgo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0\nalgo-1-7ynb1_1 |\n!algo-1-7ynb1_1 | 172.18.0.1 - - [17\/Jun\/2020:05:02:10 +0000] \"GET \/ping HTTP\/1.1\" 200 3 \"-\" \"-\"\nalgo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662569us] [Execution Engine] Error getting application context for [TensorFlow][2]\nalgo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662722us] [Execution Engine][TensorFlow][2] Failed - Last Error:\nalgo-1-7ynb1_1 | EI Error Code: [3, 16, 8]\nalgo-1-7ynb1_1 | EI Error Description: Unable to authenticate with accelerator\nalgo-1-7ynb1_1 | EI Request ID: TF-D66B9810-D81A-448F-ACE2-703FFFA0F194 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | EI Client Version: 1.5.3\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.668412: F external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.\nalgo-1-7ynb1_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.\nalgo-1-7ynb1_1 | INFO:main:tensorflow version info:\nalgo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-7ynb1_1 | TensorFlow Library: 1.14.0\nalgo-1-7ynb1_1 | EI Version: EI-1.4\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 38)`enter code here`\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.759706: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.759783: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860242: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860309: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860333: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860365: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860382: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.873381: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.949421: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:12.512935: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\nalgo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1\nalgo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0\n`\n<\/code><\/pre>\n\n<p>The log never stops in notebook. It keeps throwing in notebook cells. I am not sure whether the model is deployed correctly.<\/p>\n\n<p>I can see the docker of the model running\n<a href=\"https:\/\/i.stack.imgur.com\/YRXKL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YRXKL.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When I try to infer\/predict from that model, I get error:<\/p>\n\n<pre><code>algo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761607us] [Execution Engine] Error getting application context for [TensorFlow][2]\n\nalgo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761691us] [Execution Engine][TensorFlow][2] Failed - Last Error:\nalgo-1-iikpj_1 | EI Error Code: [3, 16, 8]\nalgo-1-iikpj_1 | EI Error Description: Unable to authenticate with accelerator\nalgo-1-iikpj_1 | EI Request ID: TF-ADECD8EF-7138-4B5F-9C37-ADFDC8122DF1 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-iikpj_1 | EI Client Version: 1.5.3\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.768249: F external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.\nalgo-1-iikpj_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.\nalgo-1-iikpj_1 | INFO:main:tensorflow version info:\nalgo-1-iikpj_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-iikpj_1 | TensorFlow Library: 1.14.0\nalgo-1-iikpj_1 | EI Version: EI-1.4\nalgo-1-iikpj_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-iikpj_1 | INFO:main:started tensorflow serving (pid: 1052)\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.854331: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.854405: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"POST \/invocations HTTP\/1.1\", subrequest: \"\/v1\/models\/Servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/Servo:predict\", host: \"localhost:8080\"\nalgo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"POST \/invocations HTTP\/1.1\", subrequest: \"\/v1\/models\/Servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/Servo:predict\", host: \"localhost:8080\"\nalgo-1-iikpj_1 | 172.18.0.1 - - [17\/Jun\/2020:05:29:47 +0000] \"POST \/invocations HTTP\/1.1\" 502 157 \"-\" \"-\"\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.954825: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.954887: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955448: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955494: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955859: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.969511: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nJSONDecodeError Traceback (most recent call last)\nin ()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/tensorflow\/serving.py in predict(self, data, initial_args)\n116 args[\"CustomAttributes\"] = self._model_attributes\n117\n--&gt; 118 return super(Predictor, self).predict(data, args)\n119\n120\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model)\n109 request_args = self._create_request_args(data, initial_args, target_model)\n110 response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n--&gt; 111 return self._handle_response(response)\n112\n113 def _handle_response(self, response):\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in _handle_response(self, response)\n119 if self.deserializer is not None:\n120 # It's the deserializer's responsibility to close the stream\n--&gt; 121 return self.deserializer(response_body, response[\"ContentType\"])\n122 data = response_body.read()\n123 response_body.close()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in call(self, stream, content_type)\n578 \"\"\"\n579 try:\n--&gt; 580 return json.load(codecs.getreader(\"utf-8\")(stream))\n581 finally:\n582 stream.close()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n297 cls=cls, object_hook=object_hook,\n298 parse_float=parse_float, parse_int=parse_int,\n--&gt; 299 parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n300\n301\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n352 parse_int is None and parse_float is None and\n353 parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354 return _default_decoder.decode(s)\n355 if cls is None:\n356 cls = JSONDecoder\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in decode(self, s, _w)\n337\n338 \"\"\"\n--&gt; 339 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n340 end = _w(s, end).end()\n341 if end != len(s):\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n355 obj, end = self.scan_once(s, idx)\n356 except StopIteration as err:\n--&gt; 357 raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n358 return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nalgo-1-iikpj_1 | 2020-06-17 05:29:48.047106: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-iikpj_1 | 2020-06-17 05:29:48.564452: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\n<\/code><\/pre>\n\n<p>I tried several ways to solve JSONDecodeError: Expecting value: line 1 column 1 (char 0) using json.loads, json.dumps etc but nothing helps.\nI also tried Rest API post to docker deployed model:<\/p>\n\n<pre><code>curl -v -X POST \\ -H 'content-type:application\/json' \\ -d '{\"data\": {\"inputs\": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \\ http:\/\/127.0.0.1:8080\/v1\/models\/Servo:predict\nbut still getting error:\n[![enter image description here][1]][1]\n<\/code><\/pre>\n\n<p>Please help me to resolve the issue. Initially, I was trying to use my tensorflow serving model and getting the same errors. Then I thought of following with the same model which was used in AWS example notebook (resnet_50_v2_fp32_NCHW.tar.gz'). So, the above experiment is using AWS example notebook with model provided by sagemaker-sample-data.<\/p>\n\n<p>Please help me out. Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592375129670,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":350.0,
        "Answer_body":"<p>Solved it. The error I was getting is due to roles\/permission of elastic inference attached to notebook. Once fixed these permissions by our devops team. It worked as expected.  See <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/142\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/142<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":-1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62422682",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1592876274340,
        "Question_original_content":"notebook instanc elast infer tensorflow model local deploy try replic http github com awslab amazon exampl blob master python sdk tensorflow serv elast infer model tensorflow serv pretrain model elast infer ipynb elast infer acceler attach notebook instanc conda amazonei tensorflow kernel accord document chang local time import boto region boto session region save model sampl data tensorflow model resnet resnet nchw tar format region import tensorflow serv import model role execut role tensorflow model model model data save model role role framework version predictor tensorflow model deploi initi instanc count instanc type local acceler type local notebook get follow log notebook attach tmpuqysel algo ynb algo ynb info main start servic algo ynb info main default model servo algo ynb info main tensorflow serv model config algo ynb model config list algo ynb config algo ynb servo algo ynb base path opt model export servo algo ynb model platform tensorflow algo ynb algo ynb algo ynb algo ynb algo ynb info main nginx config algo ynb load modul modul ngx http modul algo ynb algo ynb worker process auto algo ynb daemon algo ynb pid tmp nginx pid algo ynb error log dev stderr error algo ynb algo ynb worker rlimit nofil algo ynb algo ynb event algo ynb worker connect algo ynb algo ynb algo ynb http algo ynb includ nginx mime type algo ynb default type applic json algo ynb access log dev stdout combin algo ynb includ tensorflow serv algo ynb algo ynb upstream tf upstream algo ynb server localhost algo ynb algo ynb algo ynb upstream gunicorn upstream algo ynb server unix tmp gunicorn sock fail timeout algo ynb algo ynb algo ynb server algo ynb listen defer algo ynb client max bodi size algo ynb client bodi buffer size algo ynb subrequest output buffer size algo ynb algo ynb set tf version algo ynb set default tf model servo algo ynb algo ynb locat tf algo ynb rewrit tf break algo ynb proxi redirect algo ynb proxi pass request header algo ynb proxi set header content type applic json algo ynb proxi set header accept applic json algo ynb proxi pass http tf upstream algo ynb algo ynb algo ynb locat ping algo ynb content ping algo ynb algo ynb algo ynb locat invoc algo ynb content invoc algo ynb algo ynb algo ynb locat model invok algo ynb content invoc algo ynb algo ynb algo ynb locat model algo ynb proxi pass http gunicorn upstream model algo ynb algo ynb algo ynb locat algo ynb return error algo ynb algo ynb algo ynb keepal timeout algo ynb algo ynb algo ynb algo ynb algo ynb info main tensorflow version info algo ynb tensorflow modelserv dev sha algo ynb tensorflow librari algo ynb version algo ynb info main tensorflow serv command tensorflow model server port rest api port model config file model config cfg algo ynb info main start tensorflow serv pid algo ynb info main nginx version info algo ynb nginx version nginx algo ynb built gcc ubuntu ubuntu algo ynb built openssl mar algo ynb tl sni support enabl algo ynb configur argument prefix nginx sbin path usr sbin nginx modul path usr lib nginx modul conf path nginx nginx conf error log path var log nginx error log http log path var log nginx access log pid path var run nginx pid lock path var run nginx lock http client bodi temp path var cach nginx client temp http proxi temp path var cach nginx proxi temp http fastcgi temp path var cach nginx fastcgi temp http uwsgi temp path var cach nginx uwsgi temp http scgi temp path var cach nginx scgi temp user nginx group nginx compat file aio thread http addit modul http auth request modul http dav modul http flv modul http gunzip modul http gzip static modul http modul http random index modul http realip modul http secur link modul http slice modul http ssl modul http stub statu modul http sub modul http modul mail mail ssl modul stream stream realip modul stream ssl modul stream ssl preread modul opt fpie fstack protector strong wformat werror format secur fortifi sourc fpic opt bsymbol function fpie pie relro need pie algo ynb info main start nginx pid algo ynb tensorflow serv model server server core ad updat model algo ynb tensorflow serv model server server core ad model servo algo ynb tensorflow serv core basic manag successfulli reserv resourc load servabl servo version algo ynb tensorflow serv core loader har approv load servabl version servo version algo ynb tensorflow serv core loader har load servabl version servo version algo ynb extern org tensorflow tensorflow contrib session bundl bundl shim attempt load nativ savedmodelbundl bundl shim opt model export servo algo ynb extern org tensorflow tensorflow save model reader read savedmodel opt model export servo algo ynb extern org tensorflow tensorflow save model reader read meta graph tag serv algo ynb extern org tensorflow tensorflow save model loader restor savedmodel bundl algo ynb extern org tensorflow tensorflow save model loader run initi savedmodel bundl path opt model export servo algo ynb amazon elast infer client librari version algo ynb number elast infer acceler avail algo ynb elast infer acceler eia fcebcefb algo ynb elast infer acceler type eia medium algo ynb elast infer acceler ordin algo ynb algo ynb jun ping http algo ynb wed jun execut engin error get applic context tensorflow algo ynb wed jun execut engin tensorflow fail error algo ynb error code algo ynb error descript unabl authent acceler algo ynb request ac acceler eia fcebcefb algo ynb client version algo ynb extern org tensorflow tensorflow contrib session eia session non statu swapexstatewithei tmp input tmp output tmp freez statu intern fail initi oper whitelist server algo ynb warn main unexpect tensorflow serv exit statu restart algo ynb info main tensorflow version info algo ynb tensorflow modelserv dev sha algo ynb tensorflow librari algo ynb version algo ynb info main tensorflow serv command tensorflow model server port rest api port model config file model config cfg algo ynb info main start tensorflow serv pid enter code algo ynb tensorflow serv model server server core ad updat model algo ynb tensorflow serv model server server core ad model servo algo ynb tensorflow serv core basic manag successfulli reserv resourc load servabl servo version algo ynb tensorflow serv core loader har approv load servabl version servo version algo ynb tensorflow serv core loader har load servabl version servo version algo ynb extern org tensorflow tensorflow contrib session bundl bundl shim attempt load nativ savedmodelbundl bundl shim opt model export servo algo ynb extern org tensorflow tensorflow save model reader read savedmodel opt model export servo algo ynb extern org tensorflow tensorflow save model reader read meta graph tag serv algo ynb extern org tensorflow tensorflow save model loader restor savedmodel bundl algo ynb extern org tensorflow tensorflow save model loader run initi savedmodel bundl path opt model export servo algo ynb amazon elast infer client librari version algo ynb number elast infer acceler avail algo ynb elast infer acceler eia fcebcefb algo ynb elast infer acceler type eia medium algo ynb elast infer acceler ordin log stop notebook keep throw notebook cell sure model deploi correctli docker model run try infer predict model error algo iikpj wed jun execut engin error get applic context tensorflow algo iikpj wed jun execut engin tensorflow fail error algo iikpj error code algo iikpj error descript unabl authent acceler algo iikpj request adecdef adfdcdf acceler eia fcebcefb algo iikpj client version algo iikpj extern org tensorflow tensorflow contrib session eia session non statu swapexstatewithei tmp input tmp output tmp freez statu intern fail initi oper whitelist server algo iikpj warn main unexpect tensorflow serv exit statu restart algo iikpj info main tensorflow version info algo iikpj tensorflow modelserv dev sha algo iikpj tensorflow librari algo iikpj version algo iikpj info main tensorflow serv command tensorflow model server port rest api port model config file model config cfg algo iikpj info main start tensorflow serv pid algo iikpj tensorflow serv model server server core ad updat model algo iikpj tensorflow serv model server server core ad model servo algo iikpj error connect fail connect refus connect upstream client server request post invoc http subrequest model servo predict upstream http model servo predict host localhost algo iikpj error connect fail connect refus connect upstream client server request post invoc http subrequest model servo predict upstream http model servo predict host localhost algo iikpj jun post invoc http algo iikpj tensorflow serv core basic manag successfulli reserv resourc load servabl servo version algo iikpj tensorflow serv core loader har approv load servabl version servo version algo iikpj tensorflow serv core loader har load servabl version servo version algo iikpj extern org tensorflow tensorflow contrib session bundl bundl shim attempt load nativ savedmodelbundl bundl shim opt model export servo algo iikpj extern org tensorflow tensorflow save model reader read savedmodel opt model export servo algo iikpj extern org tensorflow tensorflow save model reader read meta graph tag serv jsondecodeerror traceback recent anaconda env amazonei tensorflow lib python site packag tensorflow serv predict self data initi arg arg customattribut self model attribut return super predictor self predict data arg anaconda env amazonei tensorflow lib python site packag predictor predict self data initi arg target model request arg self creat request arg data initi arg target model respons self session runtim client invok endpoint request arg return self handl respons respons def handl respons self respons anaconda env amazonei tensorflow lib python site packag predictor handl respons self respons self deseri deseri respons close stream return self deseri respons bodi respons contenttyp data respons bodi read respons bodi close anaconda env amazonei tensorflow lib python site packag predictor self stream content type try return json load codec getread utf stream final stream close anaconda env amazonei tensorflow lib python json init load cl object hook pars float pars int pars constant object pair hook cl cl object hook object hook pars float pars float pars int pars int pars constant pars constant object pair hook object pair hook anaconda env amazonei tensorflow lib python json init load encod cl object hook pars float pars int pars constant object pair hook pars int pars float pars constant object pair hook return default decod decod cl cl jsondecod anaconda env amazonei tensorflow lib python json decod decod self obj end self raw decod idx end end end end end len anaconda env amazonei tensorflow lib python json decod raw decod self idx obj end self scan idx stopiter err rais jsondecodeerror expect valu err valu return obj end jsondecodeerror expect valu line column char algo iikpj extern org tensorflow tensorflow save model loader restor savedmodel bundl algo iikpj extern org tensorflow tensorflow save model loader run initi savedmodel bundl path opt model export servo algo iikpj amazon elast infer client librari version tri wai solv jsondecodeerror expect valu line column char json load json dump help tri rest api post docker deploi model curl post content type applic json data input http model servo predict get error enter imag descript help resolv issu initi try us tensorflow serv model get error thought follow model aw exampl notebook resnet nchw tar experi aw exampl notebook model provid sampl data help thank",
        "Question_preprocessed_content":"notebook instanc elast infer tensorflow model local deploy try replic elast infer acceler attach notebook instanc kernel accord document chang local get follow log notebook log stop notebook keep throw notebook cell sure model deploi correctli docker model run try model error tri wai solv jsondecodeerror expect valu line column help tri rest api post docker deploi model help resolv issu initi try us tensorflow serv model get error thought follow model aw exampl notebook experi aw exampl notebook model provid help thank",
        "Question_gpt_summary_original":"the user is encountering challenges with deploying a tensorflow model with elastic inference on a notebook instance, resulting in errors such as \"jsondecodeerror: expecting value: line 1 column 1 (char 0)\" and \"curl -v -x post \\ -h 'content-type:application\/json' \\ -d '{\"data\": {\"inputs\": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \\ http:\/\/127.0.0.1:8080\/v1\/models\/servo",
        "Question_gpt_summary":"user encount challeng deploi tensorflow model elast infer notebook instanc result error jsondecodeerror expect valu line column char curl post content type applic json data input http model servo",
        "Answer_original_content":"solv error get role permiss elast infer attach notebook fix permiss devop team work expect http github com aw tensorflow serv contain issu",
        "Answer_preprocessed_content":"solv error get elast infer attach notebook fix permiss devop team work expect",
        "Answer_gpt_summary_original":"the solution to the error encountered while deploying a tensorflow model with elastic inference on a notebook instance is to fix the roles\/permissions of elastic inference attached to the notebook. the user's devops team fixed these permissions, and the deployment worked as expected.",
        "Answer_gpt_summary":"solut error encount deploi tensorflow model elast infer notebook instanc fix role permiss elast infer attach notebook user devop team fix permiss deploy work expect"
    },
    {
        "Question_id":73127303.0,
        "Question_title":"Get Experiment that Created Model in MLflow",
        "Question_body":"<p>I want to get the name of the experiment that contains the run that created a registered MLflow model. How can I do this using MLflow, if I just have the name of the model and the version?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1658855429667,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":44.0,
        "Answer_body":"<p>As @Andre has said, I had to write my own function to achieve this,<\/p>\n<pre><code>def get_model_experiment(model_name, model_version):\n    # get run_id of the model version\n    run_id = mlflow_client.get_model_version(model_name, model_version).run_id\n\n    # get the experiment_id from the run_id\n    experiment_id = mlflow_client.get_run(run_id).info.experiment_id\n\n    # get the experiment name from the experiment_id\n    return mlflow_client.get_experiment(experiment_id).name\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73127303",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1659073607990,
        "Question_original_content":"experi creat model want experi contain run creat regist model model version",
        "Question_preprocessed_content":"experi creat model want experi contain run creat regist model model version",
        "Question_gpt_summary_original":"The user is facing a challenge in identifying the name of the experiment that contains the run responsible for creating a registered MLflow model. The user has only the name and version of the model and is seeking guidance on how to retrieve this information using MLflow.",
        "Question_gpt_summary":"user face challeng identifi experi contain run respons creat regist model user version model seek guidanc retriev inform",
        "Answer_original_content":"andr said write function achiev def model experi model model version run model version run client model version model model version run experi run experi client run run info experi experi experi return client experi experi",
        "Answer_preprocessed_content":"said write function achiev",
        "Answer_gpt_summary_original":"the answer suggests writing a custom function called \"get_model_experiment\" to find the name of the experiment that created a registered model using its name and version. the function retrieves the run id of the model version, then gets the experiment id from the run id, and finally returns the name of the experiment using the experiment id.",
        "Answer_gpt_summary":"answer suggest write custom function call model experi experi creat regist model version function retriev run model version get experi run final return experi experi"
    },
    {
        "Question_id":null,
        "Question_title":"Machine Learning",
        "Question_body":"My company provides analytics and machine learning services to enterprises. Now that we are moving over to Azure (from AWS and on-prem), I would like to understand how our workflow might change. The main aspects I am uncertain about is how we ingest data from our customers and how we deploy models for real-time and batch inference with minimal technical overheads.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1595618344213,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/52106\/machine-learning-1.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-28T02:53:58.29Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nThanks for reaching out to us. For real-time machine learning deployment, we now have Azure Machine Learning Designer which is very easy to use and friendly for new user.\n\nFirst I want to share the workflow of Azure Machine Learning Service:\n\n\nWorkflow\nThe machine learning model workflow generally follows this sequence:\n\nTrain\n\nDevelop machine learning training scripts in Python, R, or with the visual designer.\nCreate and configure a compute target.\nSubmit the scripts to a configured compute target to run in that environment. During training, the scripts can read from or write to datastores. The logs and output produced during training are saved as runs in the workspace and grouped under experiments.\nPackage - After a satisfactory run is found, register the persisted model in the model registry.\n\nValidate - Query the experiment for logged metrics from the current and past runs. If the metrics don't indicate a desired outcome, loop back to step 1 and iterate on your scripts.\n\nDeploy - Develop a scoring script that uses the model and Deploy the model as a web service in Azure, or to an IoT Edge device.\n\nMonitor - Monitor for data drift between the training dataset and inference data of a deployed model. When necessary, loop back to step 1 to retrain the model with new training data.\n\nAnd Azure Machine Learning pipelines can help you make everything automatically: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-azure-machine-learning-architecture#ml-pipelines\n\nI have shared a basic workflow and quick start here for your reference: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy\n\n\n\n\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":0.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"machin learn compani provid analyt machin learn servic enterpris move azur aw prem like understand workflow chang main aspect uncertain ingest data custom deploi model real time batch infer minim technic overhead",
        "Question_preprocessed_content":"machin learn compani provid analyt machin learn servic enterpris move azur like understand workflow chang main aspect uncertain ingest data custom deploi model batch infer minim technic overhead",
        "Question_gpt_summary_original":"The user is facing challenges in understanding how their workflow might change while moving from AWS and on-prem to Azure. They are uncertain about how to ingest data from their customers and how to deploy models for real-time and batch inference with minimal technical overheads.",
        "Question_gpt_summary":"user face challeng understand workflow chang move aw prem azur uncertain ingest data custom deploi model real time batch infer minim technic overhead",
        "Answer_original_content":"thank reach real time machin learn deploy design easi us friendli new user want share workflow servic workflow machin learn model workflow gener follow sequenc train develop machin learn train script python visual design creat configur comput target submit script configur comput target run environ train script read write datastor log output produc train save run workspac group experi packag satisfactori run regist persist model model registri valid queri experi log metric current past run metric indic desir outcom loop step iter script deploi develop score script us model deploi model web servic azur iot edg devic monitor monitor data drift train dataset infer data deploi model necessari loop step retrain model new train data pipelin help automat http doc microsoft com azur machin learn concept azur machin learn architectur pipelin share basic workflow quick start refer http doc microsoft com azur machin learn tutori design automobil price deploi regard yutong",
        "Answer_preprocessed_content":"thank reach machin learn deploy design easi us friendli new user want share workflow servic workflow machin learn model workflow gener follow sequenc train develop machin learn train script python visual design creat configur comput target submit script configur comput target run environ train script read write datastor log output produc train save run workspac group experi packag satisfactori run regist persist model model registri valid queri experi log metric current past run metric indic desir outcom loop step iter script deploi develop score script us model deploi model web servic azur iot edg devic monitor monitor data drift train dataset infer data deploi model necessari loop step retrain model new train data pipelin help automat share basic workflow quick start refer regard yutong",
        "Answer_gpt_summary_original":"possible solutions for ingesting data from customers and deploying models for real-time and batch inference with minimal technical overheads when transitioning from aws and on-prem to azure are:\n\n- use azure designer for real-time machine learning deployment, which is easy to use and friendly for new users.\n- follow the machine learning model workflow, which involves developing machine learning training scripts in python, r, or with the visual designer, creating and configuring a compute target, submitting the scripts to a configured compute target to run in that environment, registering the persisted model in the model registry, developing a scoring script that uses the model, and deploying the model as a web service in azure or to an iot edge device.\n- use pipelines to automate the workflow.\n- refer to the basic workflow and quick start provided in the answer for reference.",
        "Answer_gpt_summary":"possibl solut ingest data custom deploi model real time batch infer minim technic overhead transit aw prem azur us azur design real time machin learn deploy easi us friendli new user follow machin learn model workflow involv develop machin learn train script python visual design creat configur comput target submit script configur comput target run environ regist persist model model registri develop score script us model deploi model web servic azur iot edg devic us pipelin autom workflow refer basic workflow quick start provid answer refer"
    }
]