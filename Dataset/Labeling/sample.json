[
    {
        "Question_title":"Error while defining sampling algorithm in hyper parameter tuning using random sampling - Version V1",
        "Question_body":"<p>I am trying to perform the random sampling to accomplish the hyper parameter tuning and tuning parameter version 1 (v1). I would like to get the chance to define the algorithm as sampling algorithm explicitly.<\/p>\n<p>Currently using the below code block and is there any chance of implementing explicitly defining sampling in V1? If not, any specific procedure to solve the issue is helpful.<\/p>\n<pre><code>from azureml.train.hyperdrive import RandomParameterSampling\nfrom azureml.train.hyperdrive import normal, uniform, choice\nparam_sampling = RandomParameterSampling( {\n        &quot;learning_rate&quot;: normal(10, 3),\n        &quot;keep_probability&quot;: uniform(0.05, 0.1),\n        &quot;batch_size&quot;: choice(16, 32, 64, 128)\n    }\n)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1659096794043,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":34.0,
        "Answer_body":"<p>There is an explicit procedure called a <strong>sweep job<\/strong>. This sweep job in <strong>hyperparameter value<\/strong>. We can mention the random sampling using the sweep job explicitly.<\/p>\n<p>From azure.ai.ml.sweep import Normal, Uniform, RandomParameterSampling<\/p>\n<pre><code>Command_job_for_sweep = command_job(\n    learning_rate = Normal(mu=value, sigma=value),\n    keep_probability=Uniform(min_value= your value, max_value= value),\n    batch_size = Choice(value=[.your values in list]),\n)\n\nSweep_job = command_job_sweep.sweep(\n    Computer =\u201dcluster\u201d,\n    sampling_algorithm=\u201drandom\u201d,\n    ....\n)\n<\/code><\/pre>\n<p>This will be available in <strong>version 2 (v2)<\/strong> of hyperparameter tuning in random sampling.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73166561",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1659100027590,
        "Question_original_content":"error defin sampl algorithm hyper paramet tune random sampl version try perform random sampl accomplish hyper paramet tune tune paramet version like chanc defin algorithm sampl algorithm explicitli current code block chanc implement explicitli defin sampl specif procedur solv issu help train hyperdr import randomparametersampl train hyperdr import normal uniform choic param sampl randomparametersampl learn rate normal probabl uniform batch size choic",
        "Question_preprocessed_content":"error defin sampl algorithm hyper paramet tune random sampl version try perform random sampl accomplish hyper paramet tune tune paramet version like chanc defin algorithm sampl algorithm explicitli current code block chanc implement explicitli defin sampl specif procedur solv issu help",
        "Question_gpt_summary_original":"The user is encountering challenges while defining a sampling algorithm in hyper parameter tuning using random sampling. They are trying to explicitly define the algorithm as a sampling algorithm in version V1 but are facing errors. They have provided a code block and are seeking help to solve the issue.",
        "Question_gpt_summary":"user encount challeng defin sampl algorithm hyper paramet tune random sampl try explicitli defin algorithm sampl algorithm version face error provid code block seek help solv issu",
        "Answer_original_content":"explicit procedur call sweep job sweep job hyperparamet valu mention random sampl sweep job explicitli azur sweep import normal uniform randomparametersampl command job sweep command job learn rate normal valu sigma valu probabl uniform min valu valu max valu valu batch size choic valu valu list sweep job command job sweep sweep cluster sampl algorithm random avail version hyperparamet tune random sampl",
        "Answer_preprocessed_content":"explicit procedur call sweep job sweep job hyperparamet valu mention random sampl sweep job explicitli import normal uniform randomparametersampl avail version hyperparamet tune random sampl",
        "Answer_gpt_summary_original":"Solution: The discussion suggests using a sweep job in hyperparameter tuning to explicitly define the random sampling algorithm. The code block provided includes importing the necessary modules and defining the sampling algorithm using Normal, Uniform, and Choice functions. However, it is mentioned that this solution will only be available in version 2 (v2) of hyperparameter tuning in random sampling.",
        "Answer_gpt_summary":"solut discuss suggest sweep job hyperparamet tune explicitli defin random sampl algorithm code block provid includ import necessari modul defin sampl algorithm normal uniform choic function mention solut avail version hyperparamet tune random sampl"
    },
    {
        "Question_title":"Import error when using remote Azure Automated Machine Learning model in Azure notebook",
        "Question_body":"<p>I have trained an automated machine learning model on an Azure ML compute cluster.<\/p>\n\n<p>I am trying to use that remote model in my Azure hosted Jupyter notebook. <\/p>\n\n<p>This is the code in the workbook that tries to load the remote model:<\/p>\n\n<pre><code>remote_run = AutoMLRun(experiment = experiment, run_id = '... Experiment id ...')\nremote_best_run, remote_fitted_model = remote_run.get_output()\n<\/code><\/pre>\n\n<p>This code fails with the following error:<\/p>\n\n<blockquote>\n  <p>ModuleNotFoundError                       Traceback (most recent call\n  last)  in \n        2 # remote_run.wait_for_completion(show_output = True)\n        3 import pandas as pd\n  ----> 4 remote_best_run, remote_fitted_model = remote_run.get_output()\n        5 #!pip list<\/p>\n  \n  <p>~\/anaconda3_501\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/run.py\n  in get_output(self, iteration, metric)\n      406 \n      407         with open(model_local, \"rb\") as model_file:\n  --> 408             fitted_model = pickle.load(model_file)\n      409         return curr_run, fitted_model\n      410 <\/p>\n  \n  <p>ModuleNotFoundError: No module named 'pandas._libs.tslibs.timestamps'<\/p>\n<\/blockquote>\n\n<p>Presumably there is a version difference between what is installed on the Azure ML compute cluster vs what is installed in the kernel of the Jupyter notebook, or I have a package missing. <\/p>\n\n<p>How can I make this remote model work?<\/p>\n\n<p>For additional reference, I am following this tutorial: <a href=\"https:\/\/notebooks.azure.com\/NileshA\/projects\/GlobalAI\" rel=\"nofollow noreferrer\">https:\/\/notebooks.azure.com\/NileshA\/projects\/GlobalAI<\/a><\/p>\n\n<p><strong>Note 1<\/strong> I can also reproduce this error by running the following code in my jupyter notebook: <\/p>\n\n<pre><code>import pickle\n\nwith open('model.pkl', 'rb') as p_f:\n    data = pickle.load(p_f)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1544869470737,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1544870483248,
        "Question_score":0.0,
        "Question_view_count":353.0,
        "Answer_body":"<p>I emailed the Auto ML helpdesk and they solved the problem.<\/p>\n\n<p>Quote from them: <\/p>\n\n<blockquote>\n  <p>We have a bug where the AutoML inferencing fails because the pandas\n  version is 0.22.0 which doesn\u2019t have some API support.<\/p>\n<\/blockquote>\n\n<p>I upgraded pandas on my hosted notebook to version 0.23.4, and after this the model unpickles and works successfully<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53791461",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1545041889310,
        "Question_original_content":"import error remot azur autom machin learn model azur notebook train autom machin learn model comput cluster try us remot model azur host jupyt notebook code workbook tri load remot model remot run automlrun experi experi run experi remot best run remot fit model remot run output code fail follow error modulenotfounderror traceback recent remot run wait complet output true import panda remot best run remot fit model remot run output pip list anaconda lib python site packag train automl run output self iter metric open model local model file fit model pickl load model file return curr run fit model modulenotfounderror modul name panda lib tslib timestamp presum version differ instal comput cluster instal kernel jupyt notebook packag miss remot model work addit refer follow tutori http notebook azur com nilesha project globalai note reproduc error run follow code jupyt notebook import pickl open model pkl data pickl load",
        "Question_preprocessed_content":"import error remot azur autom machin learn model azur notebook train autom machin learn model comput cluster try us remot model azur host jupyt notebook code workbook tri load remot model code fail follow error modulenotfounderror traceback true import panda pip list iter metric return modulenotfounderror modul name presum version differ instal comput cluster instal kernel jupyt notebook packag miss remot model work addit refer follow tutori note reproduc error run follow code jupyt notebook",
        "Question_gpt_summary_original":"The user is encountering an import error when trying to use a remote Azure Automated Machine Learning model in their Azure hosted Jupyter notebook. The error message suggests a version difference or missing package between the Azure ML compute cluster and the Jupyter notebook kernel. The user is seeking a solution to make the remote model work.",
        "Question_gpt_summary":"user encount import error try us remot azur autom machin learn model azur host jupyt notebook error messag suggest version differ miss packag comput cluster jupyt notebook kernel user seek solut remot model work",
        "Answer_original_content":"email auto helpdesk solv problem quot bug automl inferenc fail panda version doesnt api support upgrad panda host notebook version model unpickl work successfulli",
        "Answer_preprocessed_content":"email auto helpdesk solv problem quot bug automl inferenc fail panda version doesnt api support upgrad panda host notebook version model unpickl work successfulli",
        "Answer_gpt_summary_original":"Solution: The user emailed the Auto ML helpdesk and they suggested upgrading pandas on the hosted notebook to version 0.23.4, which solved the problem.",
        "Answer_gpt_summary":"solut user email auto helpdesk suggest upgrad panda host notebook version solv problem"
    },
    {
        "Question_title":"Clear local cache completely and rely on remote",
        "Question_body":"<p>Hi!<\/p>\n<p>First off thanks for a great tool! While we don\u2019t use the pipelines very much, we do use DVC to store our data in my team, several sets of 100s of GBs each.<\/p>\n<p>I often work with one data set at the time, usually for several weeks. It would be nice to be able to clear the data sets not currently in use completely from my local machine. We use a monorepo for everything, and if I understand gc correctly, the reason my cache doesn\u2019t get cleared is because there are .dvc files of all sets in the branch head.<\/p>\n<p>Since there are some datasets I almost never use, it would be nice to be able to clear them completely from the local cache and then once i need it I\u2019ll take my punishment and wait for it to download from the remote using dvc pull. My question, I guess, is if there is a nice way of doing this that I\u2019m missing? Currently I\u2019ve resorted to manually deleting everything i the cache folder every few months to start fresh and pull what I need. It works but doesn\u2019t feel like the correct way to go about it.<\/p>\n<p>All the best!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1608292066842,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":2932.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/davlid\">@davlid<\/a> !<\/p>\n<p>Deleting the cache dir and then pulling what you want is a pretty good workaround <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"> But I suppose you want to be able to tell <code>gc<\/code> that you want to keep only some specific dvcfiles? Kinda like you <code>dvc pull data<\/code> right now.<\/p>. <p>Hi!<\/p>\n<p>Yeah hehe, it seems to work fine, it just didn\u2019t seem like the proper way to go about it so I mainly wondered if I had missed something but I\u2019m happy to stick with this then.<\/p>\n<p>An additional option to gc would be nice, yes! Telling gc to keep only the listed targets in cache, regardless of whether there are .dvc files pointing to them or not. Maybe also check to only clear cache that have been pushed to the remote, but I don\u2019t know if that is expensive to check.<\/p>\n<p>Many thanks!<\/p>. <p>Makes sense! We have a ticket <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2036\">https:\/\/github.com\/iterative\/dvc\/issues\/2036<\/a> for cleaning up local cache if it is already present on remote. And we have a more generic ticket about revisiting gc <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2325\">https:\/\/github.com\/iterative\/dvc\/issues\/2325<\/a> where we also discuss ways to cleanup particular data. Glad that the current workaround works fine for you, we\u2019ll be adding an option for that in the future, so stay tuned <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"> Thanks for the feedback!<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/clear-local-cache-completely-and-rely-on-remote\/596",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-18T15:08:48.547Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/davlid\">@davlid<\/a> !<\/p>\n<p>Deleting the cache dir and then pulling what you want is a pretty good workaround <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"> But I suppose you want to be able to tell <code>gc<\/code> that you want to keep only some specific dvcfiles? Kinda like you <code>dvc pull data<\/code> right now.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-18T18:27:50.531Z",
                "Answer_body":"<p>Hi!<\/p>\n<p>Yeah hehe, it seems to work fine, it just didn\u2019t seem like the proper way to go about it so I mainly wondered if I had missed something but I\u2019m happy to stick with this then.<\/p>\n<p>An additional option to gc would be nice, yes! Telling gc to keep only the listed targets in cache, regardless of whether there are .dvc files pointing to them or not. Maybe also check to only clear cache that have been pushed to the remote, but I don\u2019t know if that is expensive to check.<\/p>\n<p>Many thanks!<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-18T20:10:04.769Z",
                "Answer_body":"<p>Makes sense! We have a ticket <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2036\">https:\/\/github.com\/iterative\/dvc\/issues\/2036<\/a> for cleaning up local cache if it is already present on remote. And we have a more generic ticket about revisiting gc <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2325\">https:\/\/github.com\/iterative\/dvc\/issues\/2325<\/a> where we also discuss ways to cleanup particular data. Glad that the current workaround works fine for you, we\u2019ll be adding an option for that in the future, so stay tuned <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"> Thanks for the feedback!<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"clear local cach complet reli remot thank great tool dont us pipelin us store data team set gb work data set time usual week nice abl clear data set current us complet local machin us monorepo understand correctli reason cach doesnt clear file set branch head dataset us nice abl clear complet local cach need ill punish wait download remot pull question guess nice wai miss current iv resort manual delet cach folder month start fresh pull need work doesnt feel like correct wai best",
        "Question_preprocessed_content":"clear local cach complet reli remot thank great tool dont us pipelin us store data team set gb work data set time usual week nice abl clear data set current us complet local machin us monorepo understand correctli reason cach doesnt clear file set branch head dataset us nice abl clear complet local cach need ill punish wait download remot pull question guess nice wai miss current iv resort manual delet cach folder month start fresh pull need work doesnt feel like correct wai best",
        "Question_gpt_summary_original":"The user is facing challenges with clearing data sets not currently in use completely from their local machine while using DVC to store their data. They are unable to clear the cache because there are .dvc files of all sets in the branch head. The user is looking for a better way to clear the cache and is currently manually deleting everything in the cache folder every few months to start fresh and pull what they need.",
        "Question_gpt_summary":"user face challeng clear data set current us complet local machin store data unabl clear cach file set branch head user look better wai clear cach current manual delet cach folder month start fresh pull need",
        "Answer_original_content":"davlid delet cach dir pull want pretti good workaround suppos want abl tell want specif file kinda like pull data right yeah hehe work fine didnt like proper wai mainli wonder miss happi stick addit option nice ye tell list target cach regardless file point mayb check clear cach push remot dont know expens check thank make sens ticket http github com iter issu clean local cach present remot gener ticket revisit http github com iter issu discuss wai cleanup particular data glad current workaround work fine ad option futur stai tune thank feedback",
        "Answer_preprocessed_content":"delet cach dir pull want pretti good workaround suppos want abl tell want specif file kinda like right yeah hehe work fine didnt like proper wai mainli wonder miss happi stick addit option nice ye tell list target cach regardless file point mayb check clear cach push remot dont know expens check thank make sens ticket clean local cach present remot gener ticket revisit discuss wai cleanup particular data glad current workaround work fine ad option futur stai tune thank feedback",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion:\n- Manually deleting everything in the cache folder every few months to start fresh and pull what is needed.\n- Adding an option to gc to keep only the listed targets in cache, regardless of whether there are .dvc files pointing to them or not.\n- Checking to only clear cache that have been pushed to the remote.\n\nNote: The discussion mentions that there is a ticket for cleaning up local cache if it is already present on remote and a more generic ticket about revisiting gc, but no specific solutions are provided for the user's challenge.",
        "Answer_gpt_summary":"possibl solut mention discuss manual delet cach folder month start fresh pull need ad option list target cach regardless file point check clear cach push remot note discuss mention ticket clean local cach present remot gener ticket revisit specif solut provid user challeng"
    },
    {
        "Question_title":"DVC and MLFlow - reproduce experiments using git commit ids",
        "Question_body":"<p>Hello! I\u2019ve started using DVC and I love it, thank you for your work!<\/p>\n<p>I have a question regarding DVC and MLFlow combination. I hope someone can help me with that.<br>\nI am using DVC to build\/run pipelines and version data and models. And I am using MLFlow to have a nice overview of all the experiments and to visualize\/store metrics and plots of the experiments.<br>\nDuring the training and evaluation stages, I am logging stuff to MLFlow (including current git commit id for reproducibility).<\/p>\n<p>Let\u2019s say I want to experiment with a different learning rate.<br>\nMy actions are:<\/p>\n<ul>\n<li>I am updating the learning rate in params.yaml<\/li>\n<li><code>git commit -m 'starting a new run with updated learning rate'<\/code><\/li>\n<li>\n<code>dvc repro<\/code> (new MLFlow run with metrics and current git commit id is created)<\/li>\n<li>I check the results and I like them. I want to save the model.<\/li>\n<li><code>git add 'dvc.lock'<\/code><\/li>\n<li><code>git commit -m 'awesome learning rate'<\/code><\/li>\n<li><code>dvc push<\/code><\/li>\n<li><code>git push<\/code><\/li>\n<\/ul>\n<p>So I have 2 commits here. Commit A before the run and commit B after.<\/p>\n<p>Let\u2019s say I checked my MLFlow dashboard and I want to get the trained model of the last run.<br>\nIt is linked to the commit A.<br>\nIf I <code>git checkout<\/code> commit A, I won\u2019t get the trained model until I <code>dvc repro<\/code> again.<br>\nAnd I can\u2019t log commit B to the MLFlow run during training\/evaluation because I haven\u2019t created commit B yet.<\/p>\n<p>Can\u2019t really see the whole picture\u2026 How to do it properly?<br>\nAny ideas would be very helpful!<\/p>\n<p>Thanks in advance<\/p>",
        "Question_answer_count":14,
        "Question_comment_count":0,
        "Question_creation_time":1606334637517,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":10.0,
        "Question_view_count":2994.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/al.ponomar\">@al.ponomar<\/a>, that\u2019s a great question.<br>\nDVC + MLflow (or other experiment loggers) is a quite common use case.<\/p>\n<aside class=\"quote no-group\" data-username=\"al.ponomar\" data-post=\"1\" data-topic=\"561\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/a\/6de8d8\/40.png\" class=\"avatar\"> al.ponomar:<\/div>\n<blockquote>\n<p>So I have 2 commits here. Commit A before the run and commit B after.<\/p>\n<\/blockquote>\n<\/aside>\n<p>The first commit is not required by DVC. You can change params and\/or code, do <code>repro<\/code> and then commit all the code and models together if you like the result. However, the 1st commit is required by MlFlow if you\u2019d like to attach a proper commit to the metrics. You can mitigate the issue by reporting params in mlflow but it might explode the number of entities you report - not everyone likes this idea.<\/p>\n<aside class=\"quote no-group\" data-username=\"al.ponomar\" data-post=\"1\" data-topic=\"561\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/a\/6de8d8\/40.png\" class=\"avatar\"> al.ponomar:<\/div>\n<blockquote>\n<p>If I <code>git checkout<\/code> commit A, I won\u2019t get the trained model until I <code>dvc repro<\/code> again.<\/p>\n<\/blockquote>\n<\/aside>\n<p><code>dvc checkout<\/code> will get you the model. You can set up Git hooks to do that automatically <a href=\"https:\/\/dvc.org\/doc\/command-reference\/install#installed-git-hooks\">https:\/\/dvc.org\/doc\/command-reference\/install#installed-git-hooks<\/a><\/p>\n<aside class=\"quote no-group\" data-username=\"al.ponomar\" data-post=\"1\" data-topic=\"561\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/a\/6de8d8\/40.png\" class=\"avatar\"> al.ponomar:<\/div>\n<blockquote>\n<p>Can\u2019t really see the whole picture\u2026 How to do it properly?<\/p>\n<\/blockquote>\n<\/aside>\n<p>Your workflow makes sense except the 1st commit that is required by mlflow - people tend to avoid doing that.<\/p>\n<p>PS: We are working on the experimentation experience improvements. Two big changes are expected in DVC 2.0 (in a month or two):<\/p>\n<ol>\n<li>DVC experiments without explicit commit - <a href=\"https:\/\/github.com\/iterative\/dvc\/wiki\/Experiments\">https:\/\/github.com\/iterative\/dvc\/wiki\/Experiments<\/a>. Plus model checkpoints - mostly for deep learning.<\/li>\n<li>Integrations with metrics loggers (it is in a closed repository for now).<\/li>\n<\/ol>. <p>Wow, what a fast and elaborate response!<br>\nThank you a lot, very helpful.<\/p>\n<p>Looking forward to DVC 2.0!<\/p>\n<p>Meanwhile, I was also thinking to add commit id B to the MLFow run after the experiment.<br>\nI can log to the right experient using <strong>mlflow_run_id<\/strong>.<br>\nThe file with <strong>mlflow_run_id<\/strong> is an output of the training pipeline stage and tracked by dvc.<br>\nSo after <code>dvc repro<\/code> is done and I commit my results, I can run a short script that will add the current commit id (commit B) to the right MLFlow run (<strong>mlflow_run_id<\/strong>).<\/p>. <aside class=\"quote no-group\" data-username=\"al.ponomar\" data-post=\"3\" data-topic=\"561\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/a\/6de8d8\/40.png\" class=\"avatar\"> al.ponomar:<\/div>\n<blockquote>\n<p>Looking forward to DVC 2.0!<\/p>\n<\/blockquote>\n<\/aside>\n<p><img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> We will be doing pre-release pretty soon. I can include you in the beta users program if you are up to it. I\u2019ll just need a contact - email or your name in our Discord chat.<\/p>\n<aside class=\"quote no-group\" data-username=\"al.ponomar\" data-post=\"3\" data-topic=\"561\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/a\/6de8d8\/40.png\" class=\"avatar\"> al.ponomar:<\/div>\n<blockquote>\n<p>Meanwhile, I was also thinking to add commit id B to the MLFow run after the experiment.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Good idea. It might work well.<\/p>. <p>Amazing! I\u2019ll let you know when it\u2019s ready for beta testing.<\/p>. <p>Integrations with metrics loggers sounds very useful, esp. if this would enable a way to plugin any ML logging\/tracking system such as mlflow, guildai, aim, wandb, mlmd etc. Is this code going to be moved to a git branch in the dvc open source tree, or will this remain closed source? It is nice to have the bits in an experimental branch when the feature is being developed.<\/p>. <p>Hi <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> Logger project will likely live in a separate git repo(we are not 100% sure yet), but it will be open-source, same as dvc.<\/p>. <p>Glad to hear that. The open source \u201cby the community, for the community\u201d aspect is one of the things that I find so exciting about the way DVC is evolving <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><br>\nIntegration with metrics loggers would make it so much easier to enable DVC naturally in those settings.  Look forward to it and curious to learn more about the design, which loggers are covered etc.  I can see why a separate repo might make sense if not all users of DVC need it. I guess there may be some dependencies to sort out.<\/p>. <p>Is this available now and open for contributions? From a quick look at the DVC 2.0 release plan I don\u2019t see this project mentioned, so I am guessing this is still in another repository.<\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/suparna\">@suparna<\/a>, <code>live<\/code> is still not released, though it shouldn\u2019t be much longer before we release it. DVC 2.0 will support it. It is not mentioned in release plan, because it will be standalone project, though DVC will have proper integration with it.<\/p>. <p><a class=\"mention\" href=\"\/u\/al.ponomar\">@al.ponomar<\/a> <a class=\"mention\" href=\"\/u\/suparna\">@suparna<\/a> <a class=\"mention\" href=\"\/u\/suparna.bhattacharya\">@suparna.bhattacharya<\/a> we have released the logger.<br>\nPrerelease version is avialable under:<\/p><aside class=\"onebox allowlistedgeneric\">\n  <header class=\"source\">\n      <img src=\"https:\/\/pypi.org\/static\/images\/favicon.6a76275d.ico\" class=\"site-icon\" width=\"16\" height=\"16\">\n      <a href=\"https:\/\/pypi.org\/project\/dvclive\/\" target=\"_blank\" rel=\"noopener\">PyPI<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/pypi.org\/static\/images\/twitter.90915068.jpg\" class=\"thumbnail onebox-avatar\" width=\"60\" height=\"60\">\n\n<h3><a href=\"https:\/\/pypi.org\/project\/dvclive\/\" target=\"_blank\" rel=\"noopener\">dvclive<\/a><\/h3>\n\n<p>Metric logger for ML projects.<\/p>\n\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>Stay tuned, for release, which is planned to go together with <code>DVC 2.0<\/code> release.<\/p>. <p>Thanks! Is the code available on github? How does one integrate this with MLflow and other tools which have their own logging APIs and UIs for analysis and visualization? Or lets say we are using Kubeflow with MLMD?<\/p>. <p><a class=\"mention\" href=\"\/u\/suparna\">@suparna<\/a><\/p>\n<blockquote>\n<p>Is the code available on github<\/p>\n<\/blockquote>\n<p>Not yet, though we will probably release it soon.<\/p>\n<p>So, <code>dvclive<\/code> is intending to replace MLFlow, rather than integrate with it. Its a metric logger producing outputs understandable by <code>dvc<\/code>. So, the metrics logs produced by <code>dvclive<\/code> can be used for example in <code>dvc plots<\/code> command.<\/p>\n<p>Currently neither <code>dvc<\/code> nor <code>dvclive<\/code> integrate with MLFlow or MLMD.<\/p>. <p>Hmm \u2026 I am afraid that wasn\u2019t what I was expecting. There are way too many open source metrics logging tools\/APIs in use as it is mlflow, guildai, mlmd, aim, not mention other proprietary tools like wandb. I was looking forward to a design that would integrate with these ecosystems not more parallel universes to choose from.<\/p>. <p><a class=\"mention\" href=\"\/u\/suparna\">@suparna<\/a> I understand your concerns. The thing is that the tools that you mentioned (<code>MLFlow<\/code>, <code>wandb<\/code>) are tools that, in a way, aim to solve similar problem DVC tries to solve: versioning your ML project and training pipelines. <code>dvclive<\/code> is library created to provide monitoring for the training loop (similar to mlflow and wandb), while not requiring user to run some kind of a server on training machine (contrary to both of them). Integrating DVC with wandb or mlflow would be hard if not impossible due to different approach those tools use for versioning - DVC is a CLI tool, while <code>wandb<\/code> and <code>mlflow<\/code> are server apps.<\/p>\n<p>What is your use case? How would you see the integration between DVC and, for example, <code>wandb<\/code>?<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-mlflow-reproduce-experiments-using-git-commit-ids\/561",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-26T00:46:13.989Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/al.ponomar\">@al.ponomar<\/a>, that\u2019s a great question.<br>\nDVC + MLflow (or other experiment loggers) is a quite common use case.<\/p>\n<aside class=\"quote no-group\" data-username=\"al.ponomar\" data-post=\"1\" data-topic=\"561\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/a\/6de8d8\/40.png\" class=\"avatar\"> al.ponomar:<\/div>\n<blockquote>\n<p>So I have 2 commits here. Commit A before the run and commit B after.<\/p>\n<\/blockquote>\n<\/aside>\n<p>The first commit is not required by DVC. You can change params and\/or code, do <code>repro<\/code> and then commit all the code and models together if you like the result. However, the 1st commit is required by MlFlow if you\u2019d like to attach a proper commit to the metrics. You can mitigate the issue by reporting params in mlflow but it might explode the number of entities you report - not everyone likes this idea.<\/p>\n<aside class=\"quote no-group\" data-username=\"al.ponomar\" data-post=\"1\" data-topic=\"561\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/a\/6de8d8\/40.png\" class=\"avatar\"> al.ponomar:<\/div>\n<blockquote>\n<p>If I <code>git checkout<\/code> commit A, I won\u2019t get the trained model until I <code>dvc repro<\/code> again.<\/p>\n<\/blockquote>\n<\/aside>\n<p><code>dvc checkout<\/code> will get you the model. You can set up Git hooks to do that automatically <a href=\"https:\/\/dvc.org\/doc\/command-reference\/install#installed-git-hooks\">https:\/\/dvc.org\/doc\/command-reference\/install#installed-git-hooks<\/a><\/p>\n<aside class=\"quote no-group\" data-username=\"al.ponomar\" data-post=\"1\" data-topic=\"561\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/a\/6de8d8\/40.png\" class=\"avatar\"> al.ponomar:<\/div>\n<blockquote>\n<p>Can\u2019t really see the whole picture\u2026 How to do it properly?<\/p>\n<\/blockquote>\n<\/aside>\n<p>Your workflow makes sense except the 1st commit that is required by mlflow - people tend to avoid doing that.<\/p>\n<p>PS: We are working on the experimentation experience improvements. Two big changes are expected in DVC 2.0 (in a month or two):<\/p>\n<ol>\n<li>DVC experiments without explicit commit - <a href=\"https:\/\/github.com\/iterative\/dvc\/wiki\/Experiments\">https:\/\/github.com\/iterative\/dvc\/wiki\/Experiments<\/a>. Plus model checkpoints - mostly for deep learning.<\/li>\n<li>Integrations with metrics loggers (it is in a closed repository for now).<\/li>\n<\/ol>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-26T11:59:52.935Z",
                "Answer_body":"<p>Wow, what a fast and elaborate response!<br>\nThank you a lot, very helpful.<\/p>\n<p>Looking forward to DVC 2.0!<\/p>\n<p>Meanwhile, I was also thinking to add commit id B to the MLFow run after the experiment.<br>\nI can log to the right experient using <strong>mlflow_run_id<\/strong>.<br>\nThe file with <strong>mlflow_run_id<\/strong> is an output of the training pipeline stage and tracked by dvc.<br>\nSo after <code>dvc repro<\/code> is done and I commit my results, I can run a short script that will add the current commit id (commit B) to the right MLFlow run (<strong>mlflow_run_id<\/strong>).<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-26T19:57:17.889Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"al.ponomar\" data-post=\"3\" data-topic=\"561\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/a\/6de8d8\/40.png\" class=\"avatar\"> al.ponomar:<\/div>\n<blockquote>\n<p>Looking forward to DVC 2.0!<\/p>\n<\/blockquote>\n<\/aside>\n<p><img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> We will be doing pre-release pretty soon. I can include you in the beta users program if you are up to it. I\u2019ll just need a contact - email or your name in our Discord chat.<\/p>\n<aside class=\"quote no-group\" data-username=\"al.ponomar\" data-post=\"3\" data-topic=\"561\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/a\/6de8d8\/40.png\" class=\"avatar\"> al.ponomar:<\/div>\n<blockquote>\n<p>Meanwhile, I was also thinking to add commit id B to the MLFow run after the experiment.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Good idea. It might work well.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-26T20:57:37.079Z",
                "Answer_body":"<p>Amazing! I\u2019ll let you know when it\u2019s ready for beta testing.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-16T16:35:40.044Z",
                "Answer_body":"<p>Integrations with metrics loggers sounds very useful, esp. if this would enable a way to plugin any ML logging\/tracking system such as mlflow, guildai, aim, wandb, mlmd etc. Is this code going to be moved to a git branch in the dvc open source tree, or will this remain closed source? It is nice to have the bits in an experimental branch when the feature is being developed.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-16T22:37:01.384Z",
                "Answer_body":"<p>Hi <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> Logger project will likely live in a separate git repo(we are not 100% sure yet), but it will be open-source, same as dvc.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-17T13:07:21.735Z",
                "Answer_body":"<p>Glad to hear that. The open source \u201cby the community, for the community\u201d aspect is one of the things that I find so exciting about the way DVC is evolving <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><br>\nIntegration with metrics loggers would make it so much easier to enable DVC naturally in those settings.  Look forward to it and curious to learn more about the design, which loggers are covered etc.  I can see why a separate repo might make sense if not all users of DVC need it. I guess there may be some dependencies to sort out.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-15T15:43:55.525Z",
                "Answer_body":"<p>Is this available now and open for contributions? From a quick look at the DVC 2.0 release plan I don\u2019t see this project mentioned, so I am guessing this is still in another repository.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-15T15:48:43.694Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/suparna\">@suparna<\/a>, <code>live<\/code> is still not released, though it shouldn\u2019t be much longer before we release it. DVC 2.0 will support it. It is not mentioned in release plan, because it will be standalone project, though DVC will have proper integration with it.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T14:27:12.742Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/al.ponomar\">@al.ponomar<\/a> <a class=\"mention\" href=\"\/u\/suparna\">@suparna<\/a> <a class=\"mention\" href=\"\/u\/suparna.bhattacharya\">@suparna.bhattacharya<\/a> we have released the logger.<br>\nPrerelease version is avialable under:<\/p><aside class=\"onebox allowlistedgeneric\">\n  <header class=\"source\">\n      <img src=\"https:\/\/pypi.org\/static\/images\/favicon.6a76275d.ico\" class=\"site-icon\" width=\"16\" height=\"16\">\n      <a href=\"https:\/\/pypi.org\/project\/dvclive\/\" target=\"_blank\" rel=\"noopener\">PyPI<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/pypi.org\/static\/images\/twitter.90915068.jpg\" class=\"thumbnail onebox-avatar\" width=\"60\" height=\"60\">\n\n<h3><a href=\"https:\/\/pypi.org\/project\/dvclive\/\" target=\"_blank\" rel=\"noopener\">dvclive<\/a><\/h3>\n\n<p>Metric logger for ML projects.<\/p>\n\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>Stay tuned, for release, which is planned to go together with <code>DVC 2.0<\/code> release.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T14:58:38.222Z",
                "Answer_body":"<p>Thanks! Is the code available on github? How does one integrate this with MLflow and other tools which have their own logging APIs and UIs for analysis and visualization? Or lets say we are using Kubeflow with MLMD?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T15:06:01.581Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/suparna\">@suparna<\/a><\/p>\n<blockquote>\n<p>Is the code available on github<\/p>\n<\/blockquote>\n<p>Not yet, though we will probably release it soon.<\/p>\n<p>So, <code>dvclive<\/code> is intending to replace MLFlow, rather than integrate with it. Its a metric logger producing outputs understandable by <code>dvc<\/code>. So, the metrics logs produced by <code>dvclive<\/code> can be used for example in <code>dvc plots<\/code> command.<\/p>\n<p>Currently neither <code>dvc<\/code> nor <code>dvclive<\/code> integrate with MLFlow or MLMD.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T18:24:42.515Z",
                "Answer_body":"<p>Hmm \u2026 I am afraid that wasn\u2019t what I was expecting. There are way too many open source metrics logging tools\/APIs in use as it is mlflow, guildai, mlmd, aim, not mention other proprietary tools like wandb. I was looking forward to a design that would integrate with these ecosystems not more parallel universes to choose from.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-18T08:54:08.171Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/suparna\">@suparna<\/a> I understand your concerns. The thing is that the tools that you mentioned (<code>MLFlow<\/code>, <code>wandb<\/code>) are tools that, in a way, aim to solve similar problem DVC tries to solve: versioning your ML project and training pipelines. <code>dvclive<\/code> is library created to provide monitoring for the training loop (similar to mlflow and wandb), while not requiring user to run some kind of a server on training machine (contrary to both of them). Integrating DVC with wandb or mlflow would be hard if not impossible due to different approach those tools use for versioning - DVC is a CLI tool, while <code>wandb<\/code> and <code>mlflow<\/code> are server apps.<\/p>\n<p>What is your use case? How would you see the integration between DVC and, for example, <code>wandb<\/code>?<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"reproduc experi git commit id hello iv start love thank work question combin hope help build run pipelin version data model nice overview experi visual store metric plot experi train evalu stage log stuff includ current git commit reproduc let want experi differ learn rate action updat learn rate param yaml git commit start new run updat learn rate repro new run metric current git commit creat check result like want save model git add lock git commit awesom learn rate push git push commit commit run commit let check dashboard want train model run link commit git checkout commit wont train model repro log commit run train evalu havent creat commit pictur properli idea help thank advanc",
        "Question_preprocessed_content":"reproduc experi git commit id hello iv start love thank work question combin hope help pipelin version data model nice overview experi metric plot experi train evalu stage log stuff let want experi differ learn rate action updat learn rate check result like want save model commit commit run commit let check dashboard want train model run link commit commit wont train model log commit run havent creat commit pictur properli idea help thank advanc",
        "Question_gpt_summary_original":"The user is facing a challenge while using DVC and MLFlow together to reproduce experiments using git commit ids. The issue arises when the user wants to get the trained model of the last run, which is linked to commit A, but they have already created commit B. The user cannot log commit B to the MLFlow run during training\/evaluation because it hasn't been created yet. The user is seeking advice on how to handle this situation properly.",
        "Question_gpt_summary":"user face challeng reproduc experi git commit id issu aris user want train model run link commit creat commit user log commit run train evalu hasn creat user seek advic handl situat properli",
        "Answer_original_content":"ponomar that great question experi logger common us case ponomar commit commit run commit commit requir chang param code repro commit code model like result commit requir youd like attach proper commit metric mitig issu report param explod number entiti report like idea ponomar git checkout commit wont train model repro checkout model set git hook automat http org doc command refer instal instal git hook ponomar pictur properli workflow make sens commit requir peopl tend avoid work experiment experi improv big chang expect month experi explicit commit http github com iter wiki experi plu model checkpoint deep learn integr metric logger close repositori wow fast elabor respons thank lot help look forward think add commit mlfow run experi log right experi run file run output train pipelin stage track repro commit result run short script add current commit commit right run run ponomar look forward pre releas pretti soon includ beta user program ill need contact email discord chat ponomar think add commit mlfow run experi good idea work amaz ill let know readi beta test integr metric logger sound us esp enabl wai plugin log track aim mlmd code go move git branch open sourc tree remain close sourc nice bit experiment branch featur develop logger project like live separ git repo sure open sourc glad hear open sourc commun commun aspect thing excit wai evolv integr metric logger easier enabl natur set look forward curiou learn design logger cover separ repo sens user need guess depend sort avail open contribut quick look releas plan dont project mention guess repositori suparna live releas shouldnt longer releas support mention releas plan standalon project proper integr ponomar suparna suparna bhattacharya releas logger prereleas version avial pypi live metric logger project stai tune releas plan releas thank code avail github integr tool log api ui analysi visual let kubeflow mlmd suparna code avail github probabl releas soon live intend replac integr metric logger produc output understand metric log produc live exampl plot command current live integr mlmd hmm afraid wasnt expect wai open sourc metric log tool api us mlmd aim mention proprietari tool like look forward design integr ecosystem parallel univers choos suparna understand concern thing tool mention tool wai aim solv similar problem tri solv version project train pipelin live librari creat provid monitor train loop similar requir user run kind server train machin contrari integr hard imposs differ approach tool us version cli tool server app us case integr exampl",
        "Answer_preprocessed_content":"that great question common us case commit commit run commit commit requir chang param code commit code model like result commit requir youd like attach proper commit metric mitig issu report param explod number entiti report like idea commit wont train model model set git hook automat pictur properli workflow make sens commit requir peopl tend avoid work experiment experi improv big chang expect experi explicit commit plu model checkpoint deep learn integr metric logger wow fast elabor respons thank lot help look forward think add commit mlfow run experi log right experi file output train pipelin stage track commit result run short script add current commit right run look forward pretti soon includ beta user program ill need contact email discord chat think add commit mlfow run experi good idea work amaz ill let know readi beta test integr metric logger sound us esp enabl wai plugin aim mlmd code go move git branch open sourc tree remain close sourc nice bit experiment branch featur develop logger project like live separ git repo glad hear open sourc commun commun aspect thing excit wai evolv integr metric logger easier enabl natur set look forward curiou learn design logger cover separ repo sens user need guess depend sort avail open contribut quick look releas plan dont project mention guess repositori releas shouldnt longer releas support mention releas plan standalon project proper integr releas logger prereleas version avial pypi live metric logger project stai tune releas plan releas thank code avail github integr tool log api ui analysi visual let kubeflow mlmd code avail github probabl releas soon intend replac integr metric logger produc output understand metric log produc exampl command current integr mlmd hmm afraid wasnt expect wai open sourc metric log us mlmd aim mention proprietari tool like look forward design integr ecosystem parallel univers choos understand concern thing tool mention tool wai aim solv similar problem tri solv version project train pipelin librari creat provid monitor train loop requir user run kind server train machin integr hard imposs differ approach tool us version cli tool server app us case integr exampl",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. The user can use `dvc checkout` to get the model and set up Git hooks to do it automatically.\n2. The user can add commit id B to the MLFlow run after the experiment by logging to the right experiment using `mlflow_run_id`.\n3. DVC 2.0 will support integrations with metrics loggers, which will be a standalone project, though DVC will have proper integration with it.\n4. `dvclive` is a library created to provide monitoring for the training loop, which can be used for versioning the ML project and training pipelines. It is not intended to integrate with MLFlow or other tools, but its metrics logs can be used",
        "Answer_gpt_summary":"possibl solut mention discuss user us checkout model set git hook automat user add commit run experi log right experi run support integr metric logger standalon project proper integr live librari creat provid monitor train loop version project train pipelin intend integr tool metric log"
    },
    {
        "Question_title":"--exclude-globs doesn't work in sync",
        "Question_body":"<p>I use the command to sync offline-run:<\/p>\n<pre><code class=\"lang-shell\">wandb sync --exclude-globs \"**\/*.npy\" --sync-all\n<\/code><\/pre>\n<p>However, the *.npy files are still uploaded. The path of these files are in wandb\/run_id\/files\/array . Is the way I use --exclude-globs incorrect?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1676597406406,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":92.0,
        "Answer_body":"<p>Find the same question here<\/p><aside class=\"onebox githubissue\" data-onebox-src=\"https:\/\/github.com\/wandb\/wandb\/issues\/3454\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/3454\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com\/wandb\/wandb<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"><\/path><\/svg>\n  <\/div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/3454\" target=\"_blank\" rel=\"noopener nofollow ugc\">[Q] How to use `wandb sync --exclude-globs` properly?<\/a>\n    <\/h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2022-04-01\" data-time=\"05:52:27\" data-timezone=\"UTC\">05:52AM - 01 Apr 22 UTC<\/span>\n      <\/div>\n\n\n      <div class=\"user\">\n        <a href=\"https:\/\/github.com\/JinchaoLove\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <img alt=\"JinchaoLove\" src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/1\/1c4fbc24ed8493cf5d20cb695405d36a1ad9781f.png\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          JinchaoLove\n        <\/a>\n      <\/div>\n    <\/div>\n\n    <div class=\"labels\">\n    <\/div>\n  <\/div>\n<\/div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">Hi, I want to exclude some large files, such as `*.pt`, when uploading an offlin<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026<\/a><\/span><span class=\"excerpt hidden\">e training directory to W&amp;B. I've tried the command lines `wandb sync --exclude-globs \\*.pt offline-run-xxx` and `wandb sync --exclude-globs \"*.pt\" offline-run-xxx`,  but they doesn't seem to work. May I ask how to use the `--exclude-globs` option properly?<\/span><\/p>\n  <\/div>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/exclude-globs-doesnt-work-in-sync\/3899",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-17T02:17:51.431Z",
                "Answer_body":"<p>Find the same question here<\/p><aside class=\"onebox githubissue\" data-onebox-src=\"https:\/\/github.com\/wandb\/wandb\/issues\/3454\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/3454\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com\/wandb\/wandb<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"><\/path><\/svg>\n  <\/div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/3454\" target=\"_blank\" rel=\"noopener nofollow ugc\">[Q] How to use `wandb sync --exclude-globs` properly?<\/a>\n    <\/h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2022-04-01\" data-time=\"05:52:27\" data-timezone=\"UTC\">05:52AM - 01 Apr 22 UTC<\/span>\n      <\/div>\n\n\n      <div class=\"user\">\n        <a href=\"https:\/\/github.com\/JinchaoLove\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <img alt=\"JinchaoLove\" src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/1\/1c4fbc24ed8493cf5d20cb695405d36a1ad9781f.png\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          JinchaoLove\n        <\/a>\n      <\/div>\n    <\/div>\n\n    <div class=\"labels\">\n    <\/div>\n  <\/div>\n<\/div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">Hi, I want to exclude some large files, such as `*.pt`, when uploading an offlin<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026<\/a><\/span><span class=\"excerpt hidden\">e training directory to W&amp;B. I've tried the command lines `wandb sync --exclude-globs \\*.pt offline-run-xxx` and `wandb sync --exclude-globs \"*.pt\" offline-run-xxx`,  but they doesn't seem to work. May I ask how to use the `--exclude-globs` option properly?<\/span><\/p>\n  <\/div>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2023-02-17T20:29:09.007Z",
                "Answer_body":"<p>Hello Yao!<\/p>\n<p>Looks like you got the solution for your question, which is great! Is there anything else that you need?<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1676600271431,
        "Question_original_content":"exclud glob work sync us command sync offlin run sync exclud glob npy sync npy file upload path file run file arrai wai us exclud glob incorrect",
        "Question_preprocessed_content":"work sync us command sync file upload path file wai us incorrect",
        "Question_gpt_summary_original":"The user is facing a challenge with the \"exclude-globs\" command while using \"wandb sync\" to sync offline-run. Despite using the command to exclude \"*.npy\" files, these files are still being uploaded. The user is unsure if they are using the command incorrectly.",
        "Question_gpt_summary":"user face challeng exclud glob command sync sync offlin run despit command exclud npy file file upload user unsur command incorrectli",
        "Answer_original_content":"question github com us sync exclud glob properli open apr utc jinchaolov want exclud larg file upload offlin train directori tri command line sync exclud glob offlin run sync exclud glob offlin run work ask us exclud glob option properli",
        "Answer_preprocessed_content":"question us properli open apr utc jinchaolov want exclud larg file upload offlin train directori tri command line work ask us option properli",
        "Answer_gpt_summary_original":"Solutions: No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut solut provid discuss"
    },
    {
        "Question_title":"How to stop all runs attached to a specific queue",
        "Question_body":"From slack\n\nHi team, I am trying to reconfigure queues for one of our agents, and one queue (used by several projects) is full and has several runs queued. Is there a feature to drain a queue, like a button or an API to stop all runs for a specific queue in all projects? (edited)",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650719473000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Under all runs, you can just filter the table by that queue and then stop the runs using the selection and multi-run action:\n\nIf you use the same name for your queues in all agents you can additionally select the specific agent using the filter:",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1500",
        "Tool":"Polyaxon",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-23T13:12:57Z",
                "Answer_score":1,
                "Answer_body":"Under all runs, you can just filter the table by that queue and then stop the runs using the selection and multi-run action:\n\nIf you use the same name for your queues in all agents you can additionally select the specific agent using the filter:"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0,
        "Question_closed_time":null,
        "Question_original_content":"stop run attach specif queue slack team try reconfigur queue agent queue project run queu featur drain queue like button api stop run specif queue project edit",
        "Question_preprocessed_content":"stop run attach specif queue slack team try reconfigur queue agent queue run queu featur drain queue like button api stop run specif queue project",
        "Question_gpt_summary_original":"The user is facing a challenge in reconfiguring queues for an agent as one queue used by several projects is full and has several runs queued. The user is seeking a feature to stop all runs for a specific queue in all projects.",
        "Question_gpt_summary":"user face challeng reconfigur queue agent queue project run queu user seek featur stop run specif queue project",
        "Answer_original_content":"run filter tabl queue stop run select multi run action us queue agent addition select specif agent filter",
        "Answer_preprocessed_content":"run filter tabl queue stop run select action us queue agent addition select specif agent filter",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that the user can filter the table by the specific queue and then stop all runs using the selection and multi-run action. Additionally, if the same queue name is used in all agents, the user can filter by the queue and select the specific agent to stop the runs.",
        "Answer_gpt_summary":"solut discuss suggest user filter tabl specif queue stop run select multi run action addition queue agent user filter queue select specif agent stop run"
    },
    {
        "Question_title":"Aws Sagemaker - ModuleNotFoundError: No module named 'cv2'",
        "Question_body":"<p>I am trying to run a object detection code in Aws. Although opencv is listed in the requirement file, i have the error &quot;no module named cv2&quot;. I am not sure how to fix this error. could someone help me please.<\/p>\n<p>My requirement.txt file has<\/p>\n<ul>\n<li>opencv-python<\/li>\n<li>numpy&gt;=1.18.2<\/li>\n<li>scipy&gt;=1.4.1<\/li>\n<li>wget&gt;=3.2<\/li>\n<li>tensorflow==2.3.1<\/li>\n<li>tensorflow-gpu==2.3.1<\/li>\n<li>tqdm==4.43.0<\/li>\n<li>pandas<\/li>\n<li>boto3<\/li>\n<li>awscli<\/li>\n<li>urllib3<\/li>\n<li>mss<\/li>\n<\/ul>\n<p>I tried installing &quot;imgaug&quot; and &quot;opencv-python headless&quot; as well.. but still not able to get rid of this error.<\/p>\n<pre><code>sh-4.2$ python train_launch.py \n[INFO-ROLE] arn:aws:iam::021945294007:role\/service-role\/AmazonSageMaker-ExecutionRole-20200225T145269\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_count has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\n2021-04-14 13:29:58 Starting - Starting the training job...\n2021-04-14 13:30:03 Starting - Launching requested ML instances......\n2021-04-14 13:31:11 Starting - Preparing the instances for training......\n2021-04-14 13:32:17 Downloading - Downloading input data...\n2021-04-14 13:32:41 Training - Downloading the training image..WARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n2021-04-14 13:33:03,970 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n2021-04-14 13:33:05,030 sagemaker-containers INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {\n        &quot;training&quot;: &quot;\/opt\/ml\/input\/data\/training&quot;\n    },\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;unfreezed_epochs&quot;: 2,\n        &quot;freezed_batch_size&quot;: 8,\n        &quot;freezed_epochs&quot;: 1,\n        &quot;unfreezed_batch_size&quot;: 8,\n        &quot;model_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {\n        &quot;training&quot;: {\n            &quot;TrainingInputMode&quot;: &quot;File&quot;,\n            &quot;S3DistributionType&quot;: &quot;FullyReplicated&quot;,\n            &quot;RecordWrapperType&quot;: &quot;None&quot;\n        }\n    },\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;yolov4-2021-04-14-15-29&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;train_indu&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 8,\n    &quot;num_gpus&quot;: 1,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;train_indu.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2}\nSM_USER_ENTRY_POINT=train_indu.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[&quot;training&quot;]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=train_indu\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=8\nSM_NUM_GPUS=1\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{&quot;training&quot;:&quot;\/opt\/ml\/input\/data\/training&quot;},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;yolov4-2021-04-14-15-29&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;train_indu&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:8,&quot;num_gpus&quot;:1,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;train_indu.py&quot;}\nSM_USER_ARGS=[&quot;--freezed_batch_size&quot;,&quot;8&quot;,&quot;--freezed_epochs&quot;,&quot;1&quot;,&quot;--model_dir&quot;,&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;--unfreezed_batch_size&quot;,&quot;8&quot;,&quot;--unfreezed_epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_CHANNEL_TRAINING=\/opt\/ml\/input\/data\/training\nSM_HP_UNFREEZED_EPOCHS=2\nSM_HP_FREEZED_BATCH_SIZE=8\nSM_HP_FREEZED_EPOCHS=1\nSM_HP_UNFREEZED_BATCH_SIZE=8\nSM_HP_MODEL_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\n\nInvoking script with the following command:\n\n\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2\n\n\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n[name: &quot;\/device:CPU:0&quot;\ndevice_type: &quot;CPU&quot;\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 4667030854237447206\n, name: &quot;\/device:XLA_CPU:0&quot;\ndevice_type: &quot;XLA_CPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 3059419181456814147\nphysical_device_desc: &quot;device: XLA_CPU device&quot;\n, name: &quot;\/device:XLA_GPU:0&quot;\ndevice_type: &quot;XLA_GPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 6024475084695919958\nphysical_device_desc: &quot;device: XLA_GPU device&quot;\n, name: &quot;\/device:GPU:0&quot;\ndevice_type: &quot;GPU&quot;\nmemory_limit: 14949928141\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 13034103301168381073\nphysical_device_desc: &quot;device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5&quot;\n]\nTraceback (most recent call last):\n  File &quot;train_indu.py&quot;, line 12, in &lt;module&gt;\n    from yolov3.dataset import Dataset\n  File &quot;\/opt\/ml\/code\/yolov3\/dataset.py&quot;, line 3, in &lt;module&gt;\n    import cv2\nModuleNotFoundError: No module named 'cv2'\n2021-04-14 13:33:08,453 sagemaker-containers ERROR    ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n\n2021-04-14 13:33:11 Uploading - Uploading generated training model\n2021-04-14 13:33:54 Failed - Training job failed\nTraceback (most recent call last):\n  File &quot;train_launch.py&quot;, line 41, in &lt;module&gt;\n    estimator.fit(s3_data_path, logs=True, job_name=job_name) #the argument logs is crucial if you want to see what happends\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 535, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 1210, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 3365, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 2957, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job yolov4-2021-04-14-15-29: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1618407986697,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1218.0,
        "Answer_body":"<p>Make sure your estimator has<\/p>\n<ul>\n<li>framework_version = '2.3',<\/li>\n<li>py_version = 'py37',<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67093041",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1618410091900,
        "Question_original_content":"modulenotfounderror modul name try run object detect code aw opencv list requir file error modul name sure fix error help requir txt file opencv python numpi scipi wget tensorflow tensorflow gpu tqdm panda boto awscli urllib mss tri instal imgaug opencv python headless abl rid error python train launch info role arn aw iam role servic role amazon executionrol train instanc type renam http readthedoc stabl html detail train instanc count renam http readthedoc stabl html detail train instanc type renam http readthedoc stabl html detail start start train job start launch request instanc start prepar instanc train download download input data train download train imag warn tensorflow usr local lib python dist packag tensorflow core init estim input deprec us compat estim input instead contain info import framework tensorflow contain train contain info invok user script train env addit framework paramet channel input dir train opt input data train current host algo framework modul tensorflow contain train main host algo hyperparamet unfreez epoch freez batch size freez epoch unfreez batch size model dir dataset dataset yolo result yolov small yolov model input config dir opt input config input data config train traininginputmod file sdistributiontyp fullyrepl recordwrappertyp input dir opt input master true job yolov log level master hostnam algo model dir opt model modul dir dataset dataset yolo result yolov smal yolov sourc sourcedir tar modul train indu network interfac eth num cpu num gpu output data dir opt output data output dir opt output output intermedi dir opt output intermedi resourc config current host algo host algo network interfac eth user entri point train indu environ variabl host algo network interfac eth hp freez batch size freez epoch model dir dataset dataset yolo result yolov small yolov model unfreez batch size unfreez epoch user entri point train indu framework param resourc config current host algo host algo network interfac eth input data config train recordwrappertyp sdistributiontyp fullyrepl traininginputmod file output data dir opt output data channel train current host algo modul train indu log level framework modul tensorflow contain train main input dir opt input input config dir opt input config output dir opt output num cpu num gpu model dir opt model modul dir dataset dataset yolo result yolov smal yolov sourc sourcedir tar train env addit framework paramet channel input dir train opt input data train current host algo framework modul tensorflow contain train main host algo hyperparamet freez batch size freez epoch model dir dataset dataset yolo result yolov small yolov model unfreez batch size unfreez epoch input config dir opt input config input data config train recordwrappertyp sdistributiontyp fullyrepl traininginputmod file input dir opt input master true job yolov log level master hostnam algo model dir opt model modul dir dataset dataset yolo result yolov smal yolov sourc sourcedir tar modul train indu network interfac eth num cpu num gpu output data dir opt output data output dir opt output output intermedi dir opt output intermedi resourc config current host algo host algo network interfac eth user entri point train indu user arg freez batch size freez epoch model dir dataset dataset yolo result yolov small yolov model unfreez batch size unfreez epoch output intermedi dir opt output intermedi channel train opt input data train unfreez epoch freez batch size freez epoch unfreez batch size model dir dataset dataset yolo result yolov small yolov model pythonpath opt code usr local bin usr lib python zip usr lib python usr lib python lib dynload usr local lib python dist packag usr lib python dist packag invok script follow command usr bin python train indu freez batch size freez epoch model dir dataset dataset yolo result yolov small yolov model unfreez batch size unfreez epoch warn tensorflow usr local lib python dist packag tensorflow core init estim input deprec us compat estim input instead devic cpu devic type cpu memori limit local incarn devic xla cpu devic type xla cpu memori limit local incarn physic devic desc devic xla cpu devic devic xla gpu devic type xla gpu memori limit local incarn physic devic desc devic xla gpu devic devic gpu devic type gpu memori limit local bu link incarn physic devic desc devic tesla pci bu comput capabl traceback recent file train indu line yolov dataset import dataset file opt code yolov dataset line import modulenotfounderror modul name contain error executeuserscripterror command usr bin python train indu freez batch size freez epoch model dir dataset dataset yolo result yolov small yolov model unfreez batch size unfreez epoch upload upload gener train model fail train job fail traceback recent file train launch line estim fit data path log true job job argument log crucial want happend file home user anaconda env jupytersystemenv lib python site packag estim line fit self latest train job wait log log file home user anaconda env jupytersystemenv lib python site packag estim line wait self session log job self job wait true log type log file home user anaconda env jupytersystemenv lib python site packag session line log job self check job statu job descript trainingjobstatu file home user anaconda env jupytersystemenv lib python site packag session line check job statu actual statu statu except unexpectedstatusexcept error train job yolov fail reason algorithmerror executeuserscripterror command usr bin python train indu freez batch size freez epoch model dir dataset dataset yolo result yolov small yolov model unfreez batch size unfreez epoch",
        "Question_preprocessed_content":"modulenotfounderror modul name try run object detect code aw opencv list requir file error modul name sure fix error help file panda boto awscli urllib mss tri instal imgaug headless abl rid error",
        "Question_gpt_summary_original":"The user is encountering a \"ModuleNotFoundError\" error while trying to run an object detection code in AWS SageMaker. The error message indicates that the \"cv2\" module is missing, even though it is listed in the requirement file. The user has tried installing \"imgaug\" and \"opencv-python headless\" but the error persists. The error occurs during the training job and causes it to fail.",
        "Question_gpt_summary":"user encount modulenotfounderror error try run object detect code error messag indic modul miss list requir file user tri instal imgaug opencv python headless error persist error occur train job caus fail",
        "Answer_original_content":"sure estim framework version version",
        "Answer_preprocessed_content":"sure estim",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the \"ModuleNotFoundError\" error encountered while running an object detection code in AWS SageMaker. The solution suggests ensuring that the estimator has the following parameters: framework_version = '2.3' and py_version = 'py37'.",
        "Answer_gpt_summary":"solut discuss provid solut modulenotfounderror error encount run object detect code solut suggest ensur estim follow paramet framework version version"
    },
    {
        "Question_title":"What are SageMaker pipelines actually?",
        "Question_body":"<p>Sagemaker pipelines are rather unclear to me, I'm not experienced in the field of ML but I'm working on figuring out the pipeline definitions.<\/p>\n<p>I have a few questions:<\/p>\n<ul>\n<li><p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/li>\n<li><p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/li>\n<li><p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/li>\n<\/ul>\n<p>I can't seem to find any examples besides the Python SDK usage, how come?<\/p>\n<p>The docs and workshops seem only to properly describe the Python SDK usage,it would be really helpful if someone could clear this up for me!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638395443060,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":716.0,
        "Answer_body":"<p>SageMaker has two things called Pipelines: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pipelines.html\" rel=\"nofollow noreferrer\">Model Building Pipelines<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>. I believe you're referring to the former<\/p>\n<p>A model building pipeline defines steps in a machine learning workflow, such as pre-processing, hyperparameter tuning, batch transformations, and setting up endpoints<\/p>\n<p>A serial inference pipeline is two or more SageMaker models run one after the other<\/p>\n<p>A model building pipeline is defined in JSON, and is hosted\/run in some sort of proprietary, serverless fashion by SageMaker<\/p>\n<blockquote>\n<p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/blockquote>\n<p>You can create\/modify them using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreatePipeline.html\" rel=\"nofollow noreferrer\">API<\/a>, which can also be called via the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-pipeline.html\" rel=\"nofollow noreferrer\">CLI<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#sagemaker.workflow.pipeline.Pipeline.create\" rel=\"nofollow noreferrer\">Python SDK<\/a>, or <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html\" rel=\"nofollow noreferrer\">CloudFormation<\/a>. These all use the AWS API under the hood<\/p>\n<p>You can start\/stop\/view them in SageMaker Studio:<\/p>\n<pre><code>Left-side Navigation bar &gt; SageMaker resources &gt; Drop-down menu &gt; Pipelines\n<\/code><\/pre>\n<blockquote>\n<p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/blockquote>\n<p>Unlikely. CodePipeline is more for building and deploying code, not specific to SageMaker. There is no direct integration as far as I can tell, other than that you can start a SM pipeline with CP<\/p>\n<blockquote>\n<p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/blockquote>\n<p>The Python SDK is a stand-alone library to interact with SageMaker in a developer-friendly fashion. It's more dynamic than CloudFormation. Let's you build pipelines using code. Whereas CloudFormation takes a static JSON string<\/p>\n<p>A very simple example of Python SageMaker SDK usage:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>processor = SKLearnProcessor(\n    framework_version=&quot;0.23-1&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    role=&quot;role-arn&quot;,\n)\n\nprocessing_step = ProcessingStep(\n    name=&quot;processing&quot;,\n    processor=processor,\n    code=&quot;preprocessor.py&quot;\n)\n\npipeline = Pipeline(name=&quot;foo&quot;, steps=[processing_step])\npipeline.upsert(role_arn = ...)\npipeline.start()\n<\/code><\/pre>\n<p><code>pipeline.definition()<\/code> produces rather verbose JSON like this:<\/p>\n\n<pre class=\"lang-json prettyprint-override\"><code>{\n&quot;Version&quot;: &quot;2020-12-01&quot;,\n&quot;Metadata&quot;: {},\n&quot;Parameters&quot;: [],\n&quot;PipelineExperimentConfig&quot;: {\n    &quot;ExperimentName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineName&quot;\n    },\n    &quot;TrialName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineExecutionId&quot;\n    }\n},\n&quot;Steps&quot;: [\n    {\n        &quot;Name&quot;: &quot;processing&quot;,\n        &quot;Type&quot;: &quot;Processing&quot;,\n        &quot;Arguments&quot;: {\n            &quot;ProcessingResources&quot;: {\n                &quot;ClusterConfig&quot;: {\n                    &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n                    &quot;InstanceCount&quot;: 1,\n                    &quot;VolumeSizeInGB&quot;: 30\n                }\n            },\n            &quot;AppSpecification&quot;: {\n                &quot;ImageUri&quot;: &quot;246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3&quot;,\n                &quot;ContainerEntrypoint&quot;: [\n                    &quot;python3&quot;,\n                    &quot;\/opt\/ml\/processing\/input\/code\/preprocessor.py&quot;\n                ]\n            },\n            &quot;RoleArn&quot;: &quot;arn:aws:iam::123456789012:role\/foo&quot;,\n            &quot;ProcessingInputs&quot;: [\n                {\n                    &quot;InputName&quot;: &quot;code&quot;,\n                    &quot;AppManaged&quot;: false,\n                    &quot;S3Input&quot;: {\n                        &quot;S3Uri&quot;: &quot;s3:\/\/bucket\/preprocessor.py&quot;,\n                        &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/input\/code&quot;,\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3InputMode&quot;: &quot;File&quot;,\n                        &quot;S3DataDistributionType&quot;: &quot;FullyReplicated&quot;,\n                        &quot;S3CompressionType&quot;: &quot;None&quot;\n                    }\n                }\n            ]\n        }\n    }\n  ]\n}\n<\/code><\/pre>\n<p>You could <em>use<\/em> the above JSON with CloudFormation\/CDK, but you <em>build<\/em> the JSON with the SageMaker SDK<\/p>\n<p>You can also define model building workflows using Step Function State Machines, using the <a href=\"https:\/\/aws-step-functions-data-science-sdk.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">Data Science SDK<\/a>, or <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/airflow\/index.html\" rel=\"nofollow noreferrer\">Airflow<\/a><\/p>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1654197850167,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70191668",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1638396070903,
        "Question_original_content":"pipelin actual pipelin unclear experienc field work figur pipelin definit question pipelin stand servic featur option creat consol cloudform cdk resourc pipelin essenti codepipelin integr differ python sdk differ cdk cloudform exampl python sdk usag come doc workshop properli python sdk usag help clear",
        "Question_preprocessed_content":"pipelin actual pipelin unclear experienc field work figur pipelin definit question pipelin option creat consol cloudform cdk resourc pipelin essenti codepipelin integr differ python sdk differ cdk cloudform exampl python sdk usag come doc workshop properli python sdk usag help clear",
        "Question_gpt_summary_original":"The user is facing challenges in understanding SageMaker pipelines and has several questions regarding its features and integration with other services such as CloudFormation and CDK. They are also confused about the differences between SageMaker pipelines and codepipeline, as well as the Python SDK and CloudFormation. The user is having difficulty finding examples of SageMaker pipelines and is seeking clarification on its usage.",
        "Question_gpt_summary":"user face challeng understand pipelin question featur integr servic cloudform cdk confus differ pipelin codepipelin python sdk cloudform user have difficulti find exampl pipelin seek clarif usag",
        "Answer_original_content":"thing call pipelin model build pipelin serial infer pipelin believ refer model build pipelin defin step machin learn workflow pre process hyperparamet tune batch transform set endpoint serial infer pipelin model run model build pipelin defin json host run sort proprietari serverless fashion pipelin stand servic featur option creat consol cloudform cdk resourc creat modifi api call cli python sdk cloudform us aw api hood start stop view studio left navig bar resourc drop menu pipelin pipelin essenti codepipelin integr differ unlik codepipelin build deploi code specif direct integr far tell start pipelin python sdk differ cdk cloudform python sdk stand librari interact develop friendli fashion dynam cloudform let build pipelin code cloudform take static json string simpl exampl python sdk usag processor sklearnprocessor framework version instanc count instanc type larg role role arn process step processingstep process processor processor code preprocessor pipelin pipelin foo step process step pipelin upsert role arn pipelin start pipelin definit produc verbos json like version metadata paramet pipelineexperimentconfig experimentnam execut pipelinenam trialnam execut pipelineexecutionid step process type process argument processingresourc clusterconfig instancetyp larg instancecount volumesizeingb appspecif imageuri dkr ecr west amazonaw com scikit learn cpu containerentrypoint python opt process input code preprocessor rolearn arn aw iam role foo processinginput inputnam code appmanag fals sinput suri bucket preprocessor localpath opt process input code sdatatyp sprefix sinputmod file sdatadistributiontyp fullyrepl scompressiontyp us json cloudform cdk build json sdk defin model build workflow step function state machin data scienc sdk airflow",
        "Answer_preprocessed_content":"thing call pipelin model build pipelin serial infer pipelin believ refer model build pipelin defin step machin learn workflow hyperparamet tune batch transform set endpoint serial infer pipelin model run model build pipelin defin json sort proprietari serverless fashion pipelin option creat consol cloudform cdk resourc api call cli python sdk cloudform us aw api hood studio pipelin essenti codepipelin integr differ unlik codepipelin build deploi code specif direct integr far tell start pipelin python sdk differ cdk cloudform python sdk librari interact fashion dynam cloudform let build pipelin code cloudform take static json string simpl exampl python sdk usag produc verbos json like us json build json sdk defin model build workflow step function state machin data scienc sdk airflow",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- SageMaker has two types of pipelines: Model Building Pipelines and Serial Inference Pipelines.\n- A model building pipeline defines steps in a machine learning workflow, such as pre-processing, hyperparameter tuning, batch transformations, and setting up endpoints.\n- A model building pipeline is defined in JSON and is hosted\/run in some sort of proprietary, serverless fashion by SageMaker.\n- You can create\/modify SageMaker pipelines using the API, which can also be called via the CLI, Python SDK, or CloudFormation.\n- You can start\/stop\/view SageMaker pipelines in SageMaker Studio.\n- CodePipeline is more for building and deploying code, not specific to SageMaker. There is no direct integration",
        "Answer_gpt_summary":"possibl solut mention discuss type pipelin model build pipelin serial infer pipelin model build pipelin defin step machin learn workflow pre process hyperparamet tune batch transform set endpoint model build pipelin defin json host run sort proprietari serverless fashion creat modifi pipelin api call cli python sdk cloudform start stop view pipelin studio codepipelin build deploi code specif direct integr"
    },
    {
        "Question_title":"Add stage without running its command",
        "Question_body":"<p>Hi, I have a script that takes hours to complete. I already run it outside dvc and collected its output. Is there a way to add a stage to the dag with that script avoiding to run it again, pointing the right input\/output\/dependencies? In other words, since computation takes too  much time, i would like to skip it and let dvc do the hashes and complete the run without actually executing the script. Is it possible?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1611854828167,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":378.0,
        "Answer_body":"<p>You could possibly try out <code>--no-exec<\/code> parameter to create dvc run (this would not execute the stages, but rather create\/modify the dvc.yaml) and use <code>dvc commit<\/code> to lock your files in.<\/p>. <p>It seems the solution, but I am using <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files?_ga=2.112395713.418137152.1611829605-1008788008.1610618428#generating-multiple-stages\" rel=\"noopener nofollow ugc\">multiple stages generation feature<\/a> writing directly the dvc.yaml file. Is it possible to skip execution with dvc repro?<\/p>. <p><a class=\"mention\" href=\"\/u\/mauro\">@mauro<\/a>, you can just do <code>dvc commit<\/code>, which will generate the <code>dvc.lock<\/code> file and store the outputs to <code>dvc<\/code>'s cache. This way, it should not run again on <code>repro<\/code>, do verify it with <code>dvc status<\/code> first though.<\/p>. <p>it worked, thanks <a class=\"mention\" href=\"\/u\/skshetry\">@skshetry<\/a>!<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/add-stage-without-running-its-command\/644",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-28T20:18:08.482Z",
                "Answer_body":"<p>You could possibly try out <code>--no-exec<\/code> parameter to create dvc run (this would not execute the stages, but rather create\/modify the dvc.yaml) and use <code>dvc commit<\/code> to lock your files in.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-29T16:14:56.410Z",
                "Answer_body":"<p>It seems the solution, but I am using <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files?_ga=2.112395713.418137152.1611829605-1008788008.1610618428#generating-multiple-stages\" rel=\"noopener nofollow ugc\">multiple stages generation feature<\/a> writing directly the dvc.yaml file. Is it possible to skip execution with dvc repro?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-30T04:42:59.353Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/mauro\">@mauro<\/a>, you can just do <code>dvc commit<\/code>, which will generate the <code>dvc.lock<\/code> file and store the outputs to <code>dvc<\/code>'s cache. This way, it should not run again on <code>repro<\/code>, do verify it with <code>dvc status<\/code> first though.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-02T18:22:28.078Z",
                "Answer_body":"<p>it worked, thanks <a class=\"mention\" href=\"\/u\/skshetry\">@skshetry<\/a>!<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"add stage run command script take hour complet run outsid collect output wai add stage dag script avoid run point right input output depend word comput take time like skip let hash complet run actual execut script possibl thank",
        "Question_preprocessed_content":"add stage run command script take hour complet run outsid collect output wai add stage dag script avoid run point right word comput take time like skip let hash complet run actual execut script possibl thank",
        "Question_gpt_summary_original":"The user is facing a challenge of adding a stage to the dag in DVC without running its command, as the script takes hours to complete. They have already run the script outside of DVC and collected its output, and want to avoid running it again while still being able to add the stage with the correct input\/output\/dependencies. The user wants to skip the computation and let DVC do the hashes and complete the run without executing the script.",
        "Question_gpt_summary":"user face challeng ad stage dag run command script take hour complet run script outsid collect output want avoid run abl add stage correct input output depend user want skip comput let hash complet run execut script",
        "Answer_original_content":"possibl try exec paramet creat run execut stage creat modifi yaml us commit lock file solut multipl stage gener featur write directli yaml file possibl skip execut repro mauro commit gener lock file store output cach wai run repro verifi statu work thank skshetri",
        "Answer_preprocessed_content":"possibl try paramet creat run us lock file solut multipl stage gener featur write directli yaml file possibl skip execut repro gener file store output cach wai run verifi work thank",
        "Answer_gpt_summary_original":"Solution:\n- Use the `--no-exec` parameter to create a DVC run without executing the stages, and then use `dvc commit` to lock the files in.\n- Alternatively, use `dvc commit` to generate the `dvc.lock` file and store the outputs to DVC's cache, which should prevent the script from running again on `repro`. Verify with `dvc status` first.",
        "Answer_gpt_summary":"solut us exec paramet creat run execut stage us commit lock file altern us commit gener lock file store output cach prevent script run repro verifi statu"
    },
    {
        "Question_title":"how to write to Azure PipelineData properly?",
        "Question_body":"<p>Im trying to learn Azure, with little luck (yet). All the tutorials show using PipelineData just as a file, when configured in &quot;upload&quot; mode. However, im getting &quot;FileNotFoundError: [Errno 2] No such file or directory: ''&quot; error. I would love to ask a more specific question, but i just can't see what im doing wrong.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore,Dataset,Environment\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.pipeline.steps import PythonScriptStep\nfrom azureml.pipeline.core import Pipeline, PipelineData\nimport os\n\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n\ncompute_name = &quot;cpucluster&quot;\ncompute_target = ComputeTarget(workspace=ws, name=compute_name)\naml_run_config = RunConfiguration()\naml_run_config.target = compute_target\naml_run_config.environment.python.user_managed_dependencies = False\naml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['pandas','scikit-learn'], \n    pip_packages=['azureml-sdk', 'azureml-dataprep[fuse,pandas]'], \n    pin_sdk_version=False)\n\noutput1 = PipelineData(&quot;processed_data1&quot;,datastore=datastore, output_mode=&quot;upload&quot;)\nprep_step = PythonScriptStep(\n    name=&quot;dataprep&quot;,\n    script_name=&quot;dataprep.py&quot;,\n    source_directory=os.path.join(os.getcwd(),'dataprep'),\n    arguments=[&quot;--output&quot;, output1],\n    outputs = [output1],\n    compute_target=compute_target,\n    runconfig=aml_run_config,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>In the dataprep.py i hve the following:<\/p>\n<pre><code>import numpy, argparse, pandas\nfrom azureml.core import Run\nrun = Run.get_context()\nparser = argparse.ArgumentParser()\nparser.add_argument('--output', dest='output', required=True)\nargs = parser.parse_args()\ndf = pandas.DataFrame(numpy.random.rand(100,3))\ndf.iloc[:, 2] = df.iloc[:,0] + df.iloc[:,1]\nprint(df.iloc[:5,:])\ndf.to_csv(args.output)\n\n<\/code><\/pre>\n<p>So, yeah. pd is supposed to write to the output, but my compute cluster says the following:<\/p>\n<pre><code>&quot;User program failed with FileNotFoundError: [Errno 2] No such file or directory: ''\\&quot;.\n<\/code><\/pre>\n<p>When i dont include the to_csv() function, the cluster does not complain<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1626541888290,
        "Question_favorite_count":null,
        "Question_last_edit_time":1626559748576,
        "Question_score":4.0,
        "Question_view_count":404.0,
        "Answer_body":"<p>Here is an <a href=\"https:\/\/github.com\/james-tn\/highperformance_python_in_azure\/blob\/master\/parallel_python_processing\/pipeline_definition.ipynb\" rel=\"nofollow noreferrer\">example<\/a> for PRS.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipelinedata?view=azure-ml-py\" rel=\"nofollow noreferrer\">PipelineData<\/a> was intended to represent &quot;transient&quot; data from one step to the next one, while OutputDatasetConfig was intended for capturing the final state of a dataset (and hence why you see features like lineage, ADLS support, etc). PipelineData always outputs data in a folder structure like {run_id}{output_name}. OutputDatasetConfig allows to decouple the data from the run and hence it allows you to control where to land the data (although by default it will produce similar folder structure). The OutputDatasetConfig allows even to register the output as a Dataset, where getting rid of such folder structure makes sense. From the docs itself: &quot;Represent how to copy the output of a run and be promoted as a FileDataset. The OutputFileDatasetConfig allows you to specify how you want a particular local path on the compute target to be uploaded to the specified destination&quot;.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-batch-scoring-classification#create-dataset-objects\" rel=\"nofollow noreferrer\">OutFileDatasetConfig<\/a> is a control plane concept to pass data between pipeline steps.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1626668067887,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68422680",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1626667519372,
        "Question_original_content":"write azur pipelinedata properli try learn azur littl luck tutori pipelinedata file configur upload mode get filenotfounderror errno file directori error love ask specif question wrong core import workspac datastor dataset environ core comput import computetarget amlcomput core comput target import computetargetexcept core runconfig import runconfigur core conda depend import condadepend pipelin step import pythonscriptstep pipelin core import pipelin pipelinedata import workspac config datastor default datastor comput cpuclust comput target computetarget workspac comput aml run config runconfigur aml run config target comput target aml run config environ python user manag depend fals aml run config environ python conda depend condadepend creat conda packag panda scikit learn pip packag sdk dataprep fuse panda pin sdk version fals output pipelinedata process data datastor datastor output mode upload prep step pythonscriptstep dataprep script dataprep sourc directori path join getcwd dataprep argument output output output output comput target comput target runconfig aml run config allow reus true dataprep hve follow import numpi argpars panda core import run run run context parser argpars argumentpars parser add argument output dest output requir true arg parser pars arg panda datafram numpi random rand iloc iloc iloc print iloc csv arg output yeah suppos write output comput cluster sai follow user program fail filenotfounderror errno file directori dont includ csv function cluster complain",
        "Question_preprocessed_content":"write azur pipelinedata properli try learn azur littl luck tutori pipelinedata file configur upload mode get filenotfounderror file directori error love ask specif question wrong hve follow yeah suppos write output comput cluster sai follow dont includ function cluster complain",
        "Question_gpt_summary_original":"The user is encountering challenges while trying to learn Azure and use PipelineData. They are getting a \"FileNotFoundError\" error when trying to write to the output using pd.to_csv() function. The tutorials they have followed show using PipelineData as a file in \"upload\" mode, but they are unable to identify what they are doing wrong.",
        "Question_gpt_summary":"user encount challeng try learn azur us pipelinedata get filenotfounderror error try write output csv function tutori follow pipelinedata file upload mode unabl identifi wrong",
        "Answer_original_content":"exampl pr pipelinedata intend repres transient data step outputdatasetconfig intend captur final state dataset featur like lineag adl support pipelinedata output data folder structur like run output outputdatasetconfig allow decoupl data run allow control land data default produc similar folder structur outputdatasetconfig allow regist output dataset get rid folder structur make sens doc repres copi output run promot filedataset outputfiledatasetconfig allow specifi want particular local path comput target upload specifi destin outfiledatasetconfig control plane concept pass data pipelin step",
        "Answer_preprocessed_content":"exampl pr pipelinedata intend repres transient data step outputdatasetconfig intend captur final state dataset pipelinedata output data folder structur like outputdatasetconfig allow decoupl data run allow control land data outputdatasetconfig allow regist output dataset get rid folder structur make sens doc repres copi output run promot filedataset outputfiledatasetconfig allow specifi want particular local path comput target upload specifi destin outfiledatasetconfig control plane concept pass data pipelin step",
        "Answer_gpt_summary_original":"Solution: The discussion provides some possible solutions to the challenge. One solution is to use OutputDatasetConfig instead of PipelineData to capture the final state of a dataset. OutputDatasetConfig allows the user to control where to land the data and even register the output as a Dataset. Another solution is to use OutFileDatasetConfig, which is a control plane concept to pass data between pipeline steps.",
        "Answer_gpt_summary":"solut discuss provid possibl solut challeng solut us outputdatasetconfig instead pipelinedata captur final state dataset outputdatasetconfig allow user control land data regist output dataset solut us outfiledatasetconfig control plane concept pass data pipelin step"
    },
    {
        "Question_title":"Why does the Optuna CSV file only display 1 item per parameter when I have multiple?",
        "Question_body":"<p>I have created the following piece of code in Python in order to optimize my network using Optuna.<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu'])\n\nmodel = Sequential([\n            layers.Conv2D(filters=dict_params['num_filters_1'],\n                          kernel_size=dict_params['kernel_size_1'],\n                          activation=dict_params['activations_1'],\n                          strides=dict_params['stride_num_1'],\n                          input_shape=self.input_shape),\n            layers.BatchNormalization(),\n            layers.MaxPooling2D(2, 2),\n\n            layers.Conv2D(filters=dict_params['num_filters_2'],\n                          kernel_size=dict_params['kernel_size_2'],\n                          activation=dict_params['activations_2'],\n                          strides=dict_params['stride_num_2']),\n<\/code><\/pre>\n<p>As you can see, I made multiple activation trials instead of one because I wanted to see if the model produced better results when each layer had a different activation function. I did the same with other parameters as you can see. My confusion begins when I return the study.bestparams object:<\/p>\n<pre><code>{&quot;num_filters&quot;: 32, &quot;kernel_size&quot;: 4, &quot;strides&quot;: 1, &quot;activation&quot;: &quot;selu&quot;, &quot;num_dense_nodes&quot;: 64, &quot;batch_size&quot;: 64}\n<\/code><\/pre>\n<p>The best parameters from the trials produced only one parameter. It does not tell me where the parameter was used and also doesn't display the other 3 activation functions I used (or the other parameters for that matter). Is there a way to precisely display the best settings my model used and at which layers? (I am aware of saving the best model and model summary but this does not help me too much)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630949196040,
        "Question_favorite_count":null,
        "Question_last_edit_time":1631055883776,
        "Question_score":0.0,
        "Question_view_count":82.0,
        "Answer_body":"<p>The problem  is you used the same parameter name  for all activations. Instead of :<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu'])\n<\/code><\/pre>\n<p>Try:<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation1', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation2', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation3', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation4', ['relu', 'sigmoid', 'tanh', 'selu'])\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69078338",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1631093676416,
        "Question_original_content":"csv file displai item paramet multipl creat follow piec code python order optim network activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu model sequenti layer convd filter dict param num filter kernel size dict param kernel size activ dict param activ stride dict param stride num input shape self input shape layer batchnorm layer maxpoolingd layer convd filter dict param num filter kernel size dict param kernel size activ dict param activ stride dict param stride num multipl activ trial instead want model produc better result layer differ activ function paramet confus begin return studi bestparam object num filter kernel size stride activ selu num dens node batch size best paramet trial produc paramet tell paramet displai activ function paramet matter wai precis displai best set model layer awar save best model model summari help",
        "Question_preprocessed_content":"csv file displai item paramet multipl creat follow piec code python order optim network multipl activ trial instead want model produc better result layer differ activ function paramet confus begin return object best paramet trial produc paramet tell paramet displai activ function wai precis displai best set model layer",
        "Question_gpt_summary_original":"The user encountered a challenge with the Optuna CSV file as it only displayed one item per parameter despite having multiple activation trials. The best parameters from the trials produced only one parameter, which did not tell the user where the parameter was used and also did not display the other activation functions used or the other parameters. The user is seeking a way to precisely display the best settings the model used and at which layers.",
        "Question_gpt_summary":"user encount challeng csv file displai item paramet despit have multipl activ trial best paramet trial produc paramet tell user paramet displai activ function paramet user seek wai precis displai best set model layer",
        "Answer_original_content":"problem paramet activ instead activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu try activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu",
        "Answer_preprocessed_content":"problem paramet activ instead try",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to use different parameter names for each activation function instead of using the same parameter name for all activations. By doing so, the user can precisely display the best settings the model used and at which layers.",
        "Answer_gpt_summary":"solut solut suggest discuss us differ paramet name activ function instead paramet activ user precis displai best set model layer"
    },
    {
        "Question_title":"Access Azure blob storage from within an Azure ML experiment",
        "Question_body":"<p>Azure ML Experiments provide ways to read and write CSV files to Azure blob storage through the <code>Reader<\/code> and <code>Writer<\/code> modules. However, I need to write a JSON file to blob storage. Since there is no module to do so, I'm trying to do so from within an <code>Execute Python Script<\/code> module.<\/p>\n\n<pre><code># Import the necessary items\nfrom azure.storage.blob import BlobService\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    account_name = 'mystorageaccount'\n    account_key='mykeyhere=='\n    json_string='{jsonstring here}'\n\n    blob_service = BlobService(account_name, account_key)\n\n    blob_service.put_block_blob_from_text(\"upload\",\"out.json\",json_string)\n\n    # Return value must be of a sequence of pandas.DataFrame\n    return dataframe1,\n<\/code><\/pre>\n\n<p>However, this results in an error: <code>ImportError: No module named azure.storage.blob<\/code><\/p>\n\n<p>This implies that the <code>azure-storage<\/code> Python package is not installed on Azure ML. <\/p>\n\n<p><em>How can I write to Azure blob storage from inside an Azure ML Experiment?<\/em><\/p>\n\n<p>Here's the fill error message:<\/p>\n\n<pre><code>Error 0085: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\ndata:text\/plain,Caught exception while executing function: Traceback (most recent call last):\n  File \"C:\\server\\invokepy.py\", line 162, in batch\n    mod = import_module(moduleName)\n  File \"C:\\pyhome\\lib\\importlib\\__init__.py\", line 37, in import_module\n    __import__(name)\n  File \"C:\\temp\\azuremod.py\", line 19, in &lt;module&gt;\n    from azure.storage.blob import BlobService\nImportError: No module named azure.storage.blob\n\n---------- End of error message from Python  interpreter  ----------\nStart time: UTC 02\/06\/2016 17:59:47\nEnd time: UTC 02\/06\/2016 18:00:00`\n<\/code><\/pre>\n\n<p>Thanks, everyone!<\/p>\n\n<p>UPDATE: Thanks to Dan and Peter for the ideas below. This is the progress I've made using those recommendations.  I created a clean Python 2.7 virtual environment (in VS 2005), and did a <code>pip install azure-storage<\/code> to get the dependencies into my site-packages directory.  I then zipped the site-packages folder and uploaded as the Zip file, as per Dan's note below.  I then included the reference to the site-packages directory and successfully imported the required items.  This resulted in a time out error when writing to blog storage.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wsrLn.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wsrLn.png\" alt=\"Failure to write to Blob storage\"><\/a><\/p>\n\n<p>Here is my code:<\/p>\n\n<pre><code># Get access to the uploaded Python packages    \nimport sys\npackages = \".\\Script Bundle\\site-packages\"\nsys.path.append(packages)\n\n# Import the necessary items from packages referenced above\nfrom azure.storage.blob import BlobService\nfrom azure.storage.queue import QueueService\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    account_name = 'mystorageaccount'\n    account_key='p8kSy3F...elided...3plQ=='\n\n    blob_service = BlobService(account_name, account_key)\n    blob_service.put_block_blob_from_text(\"upload\",\"out.txt\",\"Test to write\")\n\n    # All of the following also fail\n    #blob_service.create_container('images')\n    #blob_service.put_blob(\"upload\",\"testme.txt\",\"foo\",\"BlockBlob\")\n\n    #queue_service = QueueService(account_name, account_key)\n    #queue_service.create_queue('taskqueue')\n\n    # Return value must be of a sequence of pandas.DataFrame\n    return dataframe1,\n<\/code><\/pre>\n\n<p>And here is the new error log:<\/p>\n\n<pre><code>Error 0085: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\ndata:text\/plain,C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\util\\ssl_.py:79: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https:\/\/urllib3.readthedocs.org\/en\/latest\/security.html#insecureplatformwarning.\n  InsecurePlatformWarning\nCaught exception while executing function: Traceback (most recent call last):   \n  File \"C:\\server\\invokepy.py\", line 169, in batch\n    odfs = mod.azureml_main(*idfs)\n  File \"C:\\temp\\azuremod.py\", line 44, in azureml_main\n    blob_service.put_blob(\"upload\",\"testme.txt\",\"foo\",\"BlockBlob\")\n  File \".\\Script Bundle\\site-packages\\azure\\storage\\blob\\blobservice.py\", line 883, in put_blob\n    self._perform_request(request)\n  File \".\\Script Bundle\\site-packages\\azure\\storage\\storageclient.py\", line 171, in _perform_request\n    resp = self._filter(request)\n  File \".\\Script Bundle\\site-packages\\azure\\storage\\storageclient.py\", line 160, in _perform_request_worker\n    return self._httpclient.perform_request(request)\n  File \".\\Script Bundle\\site-packages\\azure\\storage\\_http\\httpclient.py\", line 181, in perform_request\n    self.send_request_body(connection, request.body)\n  File \".\\Script Bundle\\site-packages\\azure\\storage\\_http\\httpclient.py\", line 143, in send_request_body\n    connection.send(request_body)\n  File \".\\Script Bundle\\site-packages\\azure\\storage\\_http\\requestsclient.py\", line 81, in send\n    self.response = self.session.request(self.method, self.uri, data=request_body, headers=self.headers, timeout=self.timeout)\n  File \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 464, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"C:\\pyhome\\lib\\site-packages\\requests\\adapters.py\", line 431, in send\n    raise SSLError(e, request=request)\nSSLError: The write operation timed out\n\n---------- End of error message from Python  interpreter  ----------\nStart time: UTC 02\/10\/2016 15:33:00\nEnd time: UTC 02\/10\/2016 15:34:18\n<\/code><\/pre>\n\n<p>Where my current exploration is leading is that there is a dependency on the <code>requests<\/code> Python package in <code>azure-storage<\/code>. <code>requests<\/code> has a known bug in Python 2.7 for calling newer SSL protocols. Not sure, but I'm digging around in that area now.  <\/p>\n\n<p>UPDATE 2: This code runs perfectly fine inside of a Python 3 Jupyter notebook.  Additionally, if I make the Blob Container open to public access, I can directly READ from the Container through a URL.  For instance: <code>df = pd.read_csv(\"https:\/\/mystorageaccount.blob.core.windows.net\/upload\/test.csv\")<\/code> easily loads the file from blob storage.  However, I cannot use the <code>azure.storage.blob.BlobService<\/code> to read from the same file.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/X4vpX.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/X4vpX.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>UPDATE 3: Dan, in a comment below, suggested I try from the Jupyter notebooks <em>hosted on Azure ML<\/em>.  I had been running it from a local Jupyter notebook (see update 2 above).  <strong>However<\/strong>, it fails when run from an Azure ML Notebook, and the errors point to the <code>requires<\/code> package again. I'll need to find the known issues with that package, but from my reading, the known issue is with urllib3 and only impacts Python 2.7 and NOT any Python 3.x versions.  And this was run in a Python 3.x notebook.  Grrr.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/jjbp2.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jjbp2.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>UPDATE 4: As Dan notes below, this may be an issue with Azure ML networking, as <code>Execute Python Script<\/code> is relatively new and just got networking support.  However, I have also tested this on an Azure App Service webjob, which is on an entirely different Azure platform. (It is also on an entirely different Python distribution and supports both Python 2.7 and 3.4\/5, but only at 32 bit - even on 64 bit machines.)  The code there also fails, with an <code>InsecurePlatformWarning<\/code> message.<\/p>\n\n<pre><code>[02\/08\/2016 15:53:54 &gt; b40783: SYS INFO] Run script 'ListenToQueue.py' with script host - 'PythonScriptHost'\n[02\/08\/2016 15:53:54 &gt; b40783: SYS INFO] Status changed to Running\n[02\/08\/2016 15:54:09 &gt; b40783: INFO] test.csv\n[02\/08\/2016 15:54:09 &gt; b40783: ERR ] D:\\home\\site\\wwwroot\\env\\Lib\\site-packages\\requests\\packages\\urllib3\\util\\ssl_.py:315: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. For more information, see https:\/\/urllib3.readthedocs.org\/en\/latest\/security.html#snimissingwarning.\n[02\/08\/2016 15:54:09 &gt; b40783: ERR ]   SNIMissingWarning\n[02\/08\/2016 15:54:09 &gt; b40783: ERR ] D:\\home\\site\\wwwroot\\env\\Lib\\site-packages\\requests\\packages\\urllib3\\util\\ssl_.py:120: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https:\/\/urllib3.readthedocs.org\/en\/latest\/security.html#insecureplatformwarning.\n[02\/08\/2016 15:54:09 &gt; b40783: ERR ]   InsecurePlatformWarning\n[02\/08\/2016 15:54:09 &gt; b40783: ERR ] D:\\home\\site\\wwwroot\\env\\Lib\\site-packages\\requests\\packages\\urllib3\\util\\ssl_.py:120: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https:\/\/urllib3.readthedocs.org\/en\/latest\/security.html#insecureplatformwarning.\n[02\/08\/2016 15:54:09 &gt; b40783: ERR ]   InsecurePlatformWarning\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1454794898753,
        "Question_favorite_count":6.0,
        "Question_last_edit_time":1634165631492,
        "Question_score":16.0,
        "Question_view_count":6083.0,
        "Answer_body":"<p><strong>Bottom Line Up Front:<\/strong> Use HTTP instead of HTTPS for accessing Azure storage.<\/p>\n\n<p>When declaring BlobService pass in <code>protocol='http'<\/code> to force the service to communicate over HTTP. Note that you must have your container configured to allow requests over HTTP (which it does by default).<\/p>\n\n<p><code>client = BlobService(STORAGE_ACCOUNT, STORAGE_KEY, protocol=\"http\")<\/code><\/p>\n\n<p>History and credit:<\/p>\n\n<p>I posted a query on this topic to @AzureHelps and they opened a ticket on the MSDN forums: <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/46166b22-47ae-4808-ab87-402388dd7a5c\/trouble-writing-blob-storage-file-in-azure-ml-experiment?forum=MachineLearning&amp;prof=required\">https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/46166b22-47ae-4808-ab87-402388dd7a5c\/trouble-writing-blob-storage-file-in-azure-ml-experiment?forum=MachineLearning&amp;prof=required<\/a> <\/p>\n\n<p>Sudarshan Raghunathan replied with the magic.  Here are the steps to make it easy for everyone to duplicate my fix:<\/p>\n\n<ol>\n<li>Download azure.zip which provides the required libraries: <a href=\"https:\/\/azuremlpackagesupport.blob.core.windows.net\/python\/azure.zip\">https:\/\/azuremlpackagesupport.blob.core.windows.net\/python\/azure.zip<\/a><\/li>\n<li>Upload them as a DataSet to the Azure ML Studio<\/li>\n<li>Connect them to the Zip input on an <code>Execute Python Script<\/code> module<\/li>\n<li>Write your script as you would normally, being sure to create your <code>BlobService<\/code> object with <code>protocol='http'<\/code><\/li>\n<li>Run the Experiment - you should now be able to write to blob storage.<\/li>\n<\/ol>\n\n<p>Some example code can be found here: <a href=\"https:\/\/gist.github.com\/drdarshan\/92fff2a12ad9946892df\">https:\/\/gist.github.com\/drdarshan\/92fff2a12ad9946892df<\/a><\/p>\n\n<p>The code I used was the following, which doesn't first write the CSV to the file system, but sends as a text stream.<\/p>\n\n<pre><code>from azure.storage.blob import BlobService\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    account_name = 'mystorageaccount'\n    account_key='p8kSy3FACx...redacted...ebz3plQ=='\n    container_name = \"upload\"\n    json_output_file_name = 'testfromml.json'\n    json_orient = 'records' # Can be index, records, split, columns, values\n    json_force_ascii=False;\n\n    blob_service = BlobService(account_name, account_key, protocol='http')\n\n    blob_service.put_block_blob_from_text(container_name,json_output_file_name,dataframe1.to_json(orient=json_orient, force_ascii=json_force_ascii))\n\n    # Return value must be of a sequence of pandas.DataFrame\n    return dataframe1,\n<\/code><\/pre>\n\n<p>Some thoughts:<\/p>\n\n<ol>\n<li>I would prefer if the azure Python libraries were imported by default. Microsoft imports hundreds of 3rd party libraries into Azure ML as part of the Anaconda distribution. They should also include those necessary to work with Azure. We're in Azure, we've committed to Azure. Embrace it.<\/li>\n<li>I don't like that I have to use HTTP, instead of HTTPS. Granted, this is internal Azure communication, so it's likely no big deal. However, most of the documentation suggests the use of SSL \/ HTTPS when working with blob storage, so I'd prefer to be able to do that. <\/li>\n<li>I still get random timeout errors in the Experiment. Sometimes the Python code will execute in milliseconds, other times it runs for several 60 or seconds and then times out. This makes running it in an experiment very frustrating at times. However, when published as a Web Service I do not seem to have this problem.<\/li>\n<li>I would prefer that the experience from my local code matched more closely Azure ML. Locally, I can use HTTPS and never time out. It's blazing fast, and easy to write. But moving to an Azure ML experiment means some debugging, nearly every time. <\/li>\n<\/ol>\n\n<p>Huge props to Dan, Peter and Sudarshan, all from Microsoft, for their help in resolving this. I very much appreciate it!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35246826",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1455143484176,
        "Question_original_content":"access azur blob storag experi experi provid wai read write csv file azur blob storag reader writer modul need write json file blob storag modul try execut python script modul import necessari item azur storag blob import blobservic def main datafram datafram account mystorageaccount account kei mykeyher json string jsonstr blob servic blobservic account account kei blob servic block blob text upload json json string return valu sequenc panda datafram return datafram result error importerror modul name azur storag blob impli azur storag python packag instal write azur blob storag insid experi error messag error follow error occur script evalu view output log inform start error messag python interpret data text plain caught except execut function traceback recent file server invokepi line batch mod import modul modulenam file pyhom lib importlib init line import modul import file temp azuremod line azur storag blob import blobservic importerror modul name azur storag blob end error messag python interpret start time utc end time utc thank updat thank dan peter idea progress recommend creat clean python virtual environ pip instal azur storag depend site packag directori zip site packag folder upload zip file dan note includ refer site packag directori successfulli import requir item result time error write blog storag code access upload python packag import sy packag script bundl site packag sy path append packag import necessari item packag referenc azur storag blob import blobservic azur storag queue import queueservic def main datafram datafram account mystorageaccount account kei pksyf elid plq blob servic blobservic account account kei blob servic block blob text upload txt test write follow fail blob servic creat contain imag blob servic blob upload testm txt foo blockblob queue servic queueservic account account kei queue servic creat queue taskqueu return valu sequenc panda datafram return datafram new error log error follow error occur script evalu view output log inform start error messag python interpret data text plain pyhom lib site packag request packag urllib util ssl insecureplatformwarn true sslcontext object avail prevent urllib configur ssl appropri caus certain ssl connect fail inform http urllib readthedoc org latest secur html insecureplatformwarn insecureplatformwarn caught except execut function traceback recent file server invokepi line batch odf mod main idf file temp azuremod line main blob servic blob upload testm txt foo blockblob file script bundl site packag azur storag blob blobservic line blob self perform request request file script bundl site packag azur storag storagecli line perform request resp self filter request file script bundl site packag azur storag storagecli line perform request worker return self httpclient perform request request file script bundl site packag azur storag http httpclient line perform request self send request bodi connect request bodi file script bundl site packag azur storag http httpclient line send request bodi connect send request bodi file script bundl site packag azur storag http requestscli line send self respons self session request self method self uri data request bodi header self header timeout self timeout file pyhom lib site packag request session line request resp self send prep send kwarg file pyhom lib site packag request session line send adapt send request kwarg file pyhom lib site packag request adapt line send rais sslerror request request sslerror write oper time end error messag python interpret start time utc end time utc current explor lead depend request python packag azur storag request known bug python call newer ssl protocol sure dig area updat code run perfectli fine insid python jupyt notebook addition blob contain open public access directli read contain url instanc read csv http mystorageaccount blob core window net upload test csv easili load file blob storag us azur storag blob blobservic read file updat dan comment suggest try jupyt notebook host run local jupyt notebook updat fail run notebook error point requir packag need known issu packag read known issu urllib impact python python version run python notebook updat dan note issu network execut python script rel new got network support test azur app servic webjob entir differ azur platform entir differ python distribut support python bit bit machin code fail insecureplatformwarn messag sy info run script listentoqueu script host pythonscripthost sy info statu chang run info test csv err home site root env lib site packag request packag urllib util ssl snimissingwarn http request sni subject indic extens tl avail platform caus server present incorrect tl certif caus valid failur inform http urllib readthedoc org latest secur html snimissingwarn err snimissingwarn err home site root env lib site packag request packag urllib util ssl insecureplatformwarn true sslcontext object avail prevent urllib configur ssl appropri caus certain ssl connect fail inform http urllib readthedoc org latest secur html insecureplatformwarn err insecureplatformwarn err home site root env lib site packag request packag urllib util ssl insecureplatformwarn true sslcontext object avail prevent urllib configur ssl appropri caus certain ssl connect fail inform http urllib readthedoc org latest secur html insecureplatformwarn err insecureplatformwarn",
        "Question_preprocessed_content":"access azur blob storag experi experi provid wai read write csv file azur blob storag modul need write json file blob storag modul try modul result error impli python packag instal write azur blob storag insid experi error messag thank updat thank dan peter idea progress recommend creat clean python virtual environ depend directori zip folder upload zip file dan note includ refer directori successfulli import requir item result time error write blog storag code new error log current explor lead depend python packag known bug python call newer ssl protocol sure dig area updat code run perfectli fine insid python jupyt notebook addition blob contain open public access directli read contain url instanc easili load file blob storag us read file updat dan comment suggest try jupyt notebook host run local jupyt notebook fail run notebook error point packag need known issu packag read known issu urllib impact python python version run python notebook updat dan note issu network rel new got network support test azur app servic webjob entir differ azur platform code fail messag",
        "Question_gpt_summary_original":"The user is trying to write a JSON file to Azure blob storage from within an Azure ML experiment using an Execute Python Script module. However, there is no module to do so, resulting in an error: \"ImportError: No module named azure.storage.blob\". The user has tried to install the necessary dependencies into the site-packages directory and import them, but this resulted in a time-out error when writing to blob storage. The user has also tried running the code from a Jupyter notebook hosted on Azure ML, but it failed with an error related to the \"requires\" package. The user suspects that this may be an issue with Azure ML networking.",
        "Question_gpt_summary":"user try write json file azur blob storag experi execut python script modul modul result error importerror modul name azur storag blob user tri instal necessari depend site packag directori import result time error write blob storag user tri run code jupyt notebook host fail error relat requir packag user suspect issu network",
        "Answer_original_content":"line us http instead http access azur storag declar blobservic pass protocol http forc servic commun http note contain configur allow request http default client blobservic storag account storag kei protocol http histori credit post queri topic azurehelp open ticket msdn forum http social msdn microsoft com forum azur ddac troubl write blob storag file azur experi forum machinelearn prof requir sudarshan raghunathan repli magic step easi duplic fix download azur zip provid requir librari http packagesupport blob core window net python azur zip upload dataset studio connect zip input execut python script modul write script normal sure creat blobservic object protocol http run experi abl write blob storag exampl code http gist github com drdarshan aaddf code follow write csv file send text stream azur storag blob import blobservic def main datafram datafram account mystorageaccount account kei pksyfacx redact ebzplq contain upload json output file testfromml json json orient record index record split column valu json forc ascii fals blob servic blobservic account account kei protocol http blob servic block blob text contain json output file datafram json orient json orient forc ascii json forc ascii return valu sequenc panda datafram return datafram thought prefer azur python librari import default microsoft import hundr parti librari anaconda distribut includ necessari work azur azur commit azur embrac like us http instead http grant intern azur commun like big deal document suggest us ssl http work blob storag prefer abl random timeout error experi python code execut millisecond time run second time make run experi frustrat time publish web servic problem prefer experi local code match close local us http time blaze fast easi write move experi mean debug nearli time huge prop dan peter sudarshan microsoft help resolv appreci",
        "Answer_preprocessed_content":"line us http instead http access azur storag declar blobservic pass forc servic commun http note contain configur allow request http histori credit post queri topic open ticket msdn forum sudarshan raghunathan repli magic step easi duplic fix download provid requir librari upload dataset studio connect zip input modul write script normal sure creat object run experi abl write blob storag exampl code code follow write csv file send text stream thought prefer azur python librari import default microsoft import hundr parti librari anaconda distribut includ necessari work azur azur commit azur embrac like us http instead http grant intern azur commun like big deal document suggest us ssl http work blob storag prefer abl random timeout error experi python code execut millisecond time run second time make run experi frustrat time publish web servic problem prefer experi local code match close local us http time blaze fast easi write move experi mean debug nearli time huge prop dan peter sudarshan microsoft help resolv appreci",
        "Answer_gpt_summary_original":"The solution provided in the discussion is to use HTTP instead of HTTPS for accessing Azure storage. When declaring BlobService, pass in \"protocol='http'\" to force the service to communicate over HTTP. The user must have their container configured to allow requests over HTTP (which it does by default). The user can download the required libraries from the provided link, upload them as a DataSet to the Azure ML Studio, connect them to the Zip input on an Execute Python Script module, and write their script as they would normally, being sure to create their BlobService object with \"protocol='http'\". The user can find some example code in the provided link.",
        "Answer_gpt_summary":"solut provid discuss us http instead http access azur storag declar blobservic pass protocol http forc servic commun http user contain configur allow request http default user download requir librari provid link upload dataset studio connect zip input execut python script modul write script normal sure creat blobservic object protocol http user exampl code provid link"
    },
    {
        "Question_title":"split dataframe column header and values into multiple columns",
        "Question_body":"<p>I've uploaded my <code>csv<\/code> file on Azure, but for some reason it became like this<\/p>\n\n<pre><code> nominal;data;curs;cdx         Column 1\n0          1;21.06.2000;28  2300;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n1          1;22.06.2000;28  2200;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n2          1;23.06.2000;28  1900;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n3          1;24.06.2000;28  1700;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n4          1;27.06.2000;28  1300;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n5          1;28.06.2000;28  1100;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n<\/code><\/pre>\n\n<p>Basically instead of four columns <code>nominal<\/code>, <code>data<\/code>, <code>curs<\/code>, <code>cdx<\/code> I got two columns with one having all the values and the last one (it is empty or something because the last column has encoding issue) - no idea what.<\/p>\n\n<p>I have deleted the column <code>Column 1<\/code> like this<\/p>\n\n<pre><code>import pandas as pd\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    dataframe1.drop(['Column 1'], axis = 1, inplace = True)\n    print('Input pandas.DataFrame #1:\\r\\n\\r\\n{0}'.format(dataframe1))\n    return dataframe1,\n<\/code><\/pre>\n\n<p>How to split the first column into multiple now? To get 4 separate columns<\/p>\n\n<p>I am using pandas 0.18<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_time":1532029428013,
        "Question_favorite_count":null,
        "Question_last_edit_time":1532029712492,
        "Question_score":0.0,
        "Question_view_count":1086.0,
        "Answer_body":"<p>You need to split the column with:<\/p>\n\n<pre><code>dataframe1['nominal;data;curs;cdx'].str.split(';',expand=True)\n<\/code><\/pre>\n\n<p>Then change the headers with:<\/p>\n\n<pre><code>dataframe1.columns = 'nominal;data;curs;cdx'.split(';')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51430645",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1532031297043,
        "Question_original_content":"split datafram column header valu multipl column upload csv file azur reason like nomin data cur cdx column basic instead column nomin data cur cdx got column have valu column encod issu idea delet column column like import panda def main datafram datafram datafram drop column axi inplac true print input panda datafram format datafram return datafram split column multipl separ column panda",
        "Question_preprocessed_content":"split datafram column header valu multipl column upload file azur reason like basic instead column got column have valu idea delet column like split column multipl separ column panda",
        "Question_gpt_summary_original":"The user uploaded a CSV file on Azure, but the column headers and values got merged into two columns instead of four. The user deleted the extra column but now needs to split the first column into four separate columns using pandas 0.18.",
        "Question_gpt_summary":"user upload csv file azur column header valu got merg column instead user delet extra column need split column separ column panda",
        "Answer_original_content":"need split column datafram nomin data cur cdx str split expand true chang header datafram column nomin data cur cdx split",
        "Answer_preprocessed_content":"need split column chang header",
        "Answer_gpt_summary_original":"Solution: The user can split the first column into four separate columns using pandas 0.18 by using the following code: \n\ndataframe1['nominal;data;curs;cdx'].str.split(';',expand=True)\n\nThen, the user can change the headers using the following code:\n\ndataframe1.columns = 'nominal;data;curs;cdx'.split(';')",
        "Answer_gpt_summary":"solut user split column separ column panda follow code datafram nomin data cur cdx str split expand true user chang header follow code datafram column nomin data cur cdx split"
    },
    {
        "Question_title":"git-ignore dvc.lock in repositories where only the DVC pipelines are used",
        "Question_body":"<p>I want to use the pipeline functionality of dvc in a git repository. The data is managed otherwise and should not be versioned by dvc. The only functionality which is needed is that dvc reproduces the needed steps of the pipeline when <code>dvc repro<\/code> is called. Checking out the repository on a new system should lead to an 'empty' repository, where none of the pipeline steps are stored.<\/p>\n<p>Thus, - if I understand correctly - there is no need to track the dvc.lock file in the repository. However, adding dvc.lock to the .gitginore file leads to an error message:<\/p>\n<pre><code>ERROR: 'dvc.lock' is git-ignored.\n<\/code><\/pre>\n<p>Is there any way to disable the dvc.lock in .gitignore check for this usecase?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624362030233,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":493.0,
        "Answer_body":"<p>This is definitely possible, as DVC features are loosely coupled to one another. You can do pipelining by writing your dvc.yaml file(s), but avoid data management\/versioning by using <code>cache: false<\/code> in the stage outputs (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#output-subfields\" rel=\"nofollow noreferrer\"><code>outs<\/code> field<\/a>). See also helper <code>dvc stage add -O<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/stage\/add#options\" rel=\"nofollow noreferrer\">big O<\/a>, alias of <code>--outs-no-cache<\/code>).<\/p>\n<p>And the same for initial data dependencies, you can <code>dvc add --no-commit<\/code> them (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#options\" rel=\"nofollow noreferrer\">ref<\/a>).<\/p>\n<p>You do want to track <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#dvclock-file\" rel=\"nofollow noreferrer\">dvc.lock<\/a> in Git though, so that DVC can determine the latest stage of the pipeline associated with the Git commit in every repo copy or branch.<\/p>\n<p>You'll be responsible for placing the right data files\/dirs (matching .dvc files and dvc.lock) in the workspace for <code>dvc repro<\/code> or <code>dvc exp run<\/code> to behave as expected. <code>dvc checkout<\/code> won't be able to help you.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68082912",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1624393487732,
        "Question_original_content":"git ignor lock repositori pipelin want us pipelin function git repositori data manag version function need reproduc need step pipelin repro call check repositori new lead repositori pipelin step store understand correctli need track lock file repositori ad lock gitginor file lead error messag error lock git ignor wai disabl lock gitignor check usecas",
        "Question_preprocessed_content":"lock repositori pipelin want us pipelin function git repositori data manag version function need reproduc need step pipelin call check repositori new lead repositori pipelin step store understand correctli need track lock file repositori ad lock gitginor file lead error messag wai disabl lock gitignor check usecas",
        "Question_gpt_summary_original":"The user wants to use the pipeline functionality of dvc in a git repository without versioning the data by dvc. However, adding dvc.lock to the .gitignore file leads to an error message, and the user is looking for a way to disable the dvc.lock in .gitignore check for this use case.",
        "Question_gpt_summary":"user want us pipelin function git repositori version data ad lock gitignor file lead error messag user look wai disabl lock gitignor check us case",
        "Answer_original_content":"definit possibl featur loos coupl pipelin write yaml file avoid data manag version cach fals stage output out field helper stage add big alia out cach initi data depend add commit ref want track lock git determin latest stage pipelin associ git commit repo copi branch respons place right data file dir match file lock workspac repro exp run behav expect checkout won abl help",
        "Answer_preprocessed_content":"definit possibl featur loos coupl pipelin write yaml file avoid data stage output helper initi data depend want track lock git determin latest stage pipelin associ git commit repo copi branch respons place right data workspac behav expect won abl help",
        "Answer_gpt_summary_original":"Solution: The user can use the pipeline functionality of dvc in a git repository without versioning the data by dvc. They can write their dvc.yaml file(s) and avoid data management\/versioning by using <code>cache: false<\/code> in the stage outputs (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#output-subfields\" rel=\"nofollow noreferrer\"><code>outs<\/code> field<\/a>). They can also use <code>dvc add --no-commit<\/code> for initial data dependencies. However, they should track <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#dvclock-file\" rel=\"nofollow noreferrer\">dvc",
        "Answer_gpt_summary":"solut user us pipelin function git repositori version data write yaml file avoid data manag version cach fals stage output out field us add commit initi data depend track"
    },
    {
        "Question_title":"Error with wandb on win10",
        "Question_body":"<p>I got this on win10,it\u2019s stucked<br>\nthe enviornment is<\/p>\n<ul>\n<li>python3.7.10<\/li>\n<li>wandb 0.12.9<\/li>\n<\/ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b2ce90f5e63f7db9be89aa163ee55f0ab468a430.png\" data-download-href=\"\/uploads\/short-url\/pvNysR6Ps6qxY0fl0Y5gjzfovBK.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b2ce90f5e63f7db9be89aa163ee55f0ab468a430.png\" alt=\"image\" data-base62-sha1=\"pvNysR6Ps6qxY0fl0Y5gjzfovBK\" width=\"547\" height=\"500\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b2ce90f5e63f7db9be89aa163ee55f0ab468a430_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">686\u00d7626 26.9 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1641055368878,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":214.0,
        "Answer_body":"<p>I\u2019ve deleted wandb in docker and pip ,then I reinstalled them.<br>\nAnd I got the right page after waiting about 5 or 6 minutes.<br>\nBut I don\u2019t know whtether the reason is the versions are different or something.<br>\nThis time I didn\u2019t set the LOCAL_RESOTRE var, I don\u2019t know whether the time will decrease.<br>\nAnd I notice that once I get the right page, the next time I can get in immediately.<br>\nThanks a lot.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/error-with-wandb-on-win10\/1656",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-04T04:34:18.999Z",
                "Answer_body":"<p>Hey there, have you tried setting the LOCAL_RESTORE variable?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-06T07:05:27.148Z",
                "Answer_body":"<p>Hi there, I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-07T01:54:32.752Z",
                "Answer_body":"<p>is this the right way to set the variable<br>\nseems also stucked<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/35267e52d2fb1f88e49e10136936a1b8ce87161e.png\" data-download-href=\"\/uploads\/short-url\/7AbMdL2TPUcAqwnkiktDhrmbmdU.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/35267e52d2fb1f88e49e10136936a1b8ce87161e_2_690x420.png\" alt=\"image\" data-base62-sha1=\"7AbMdL2TPUcAqwnkiktDhrmbmdU\" width=\"690\" height=\"420\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/35267e52d2fb1f88e49e10136936a1b8ce87161e_2_690x420.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/35267e52d2fb1f88e49e10136936a1b8ce87161e_2_1035x630.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/35267e52d2fb1f88e49e10136936a1b8ce87161e.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/35267e52d2fb1f88e49e10136936a1b8ce87161e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1303\u00d7794 53.5 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-09T01:03:19.763Z",
                "Answer_body":"<p>I\u2019ve deleted wandb in docker and pip ,then I reinstalled them.<br>\nAnd I got the right page after waiting about 5 or 6 minutes.<br>\nBut I don\u2019t know whtether the reason is the versions are different or something.<br>\nThis time I didn\u2019t set the LOCAL_RESOTRE var, I don\u2019t know whether the time will decrease.<br>\nAnd I notice that once I get the right page, the next time I can get in immediately.<br>\nThanks a lot.<\/p>",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-03-10T01:04:06.382Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1641690199763,
        "Question_original_content":"error win got win stuck enviorn python imag",
        "Question_preprocessed_content":"error win got win stuck enviorn imag",
        "Question_gpt_summary_original":"The user is encountering an error with wandb on Windows 10 while using Python 3.7.10 and wandb version 0.12.9. The error is causing the program to get stuck.",
        "Question_gpt_summary":"user encount error window python version error caus program stuck",
        "Answer_original_content":"iv delet docker pip reinstal got right page wait minut dont know whtether reason version differ time didnt set local resotr var dont know time decreas notic right page time immedi thank lot",
        "Answer_preprocessed_content":"iv delet docker pip reinstal got right page wait minut dont know whtether reason version differ time didnt set var dont know time decreas notic right page time immedi thank lot",
        "Answer_gpt_summary_original":"Solutions provided: \n- The user deleted wandb in docker and pip, then reinstalled them which resolved the issue after waiting for 5-6 minutes. However, it is unclear whether the issue was caused by different versions or something else.\n- The user did not set the LOCAL_RESOTRE var this time, and it is unknown whether this would decrease the waiting time.\n- Once the user was able to access the page successfully, they were able to access it immediately the next time.",
        "Answer_gpt_summary":"solut provid user delet docker pip reinstal resolv issu wait minut unclear issu caus differ version user set local resotr var time unknown decreas wait time user abl access page successfulli abl access immedi time"
    },
    {
        "Question_title":"How to select a target column in a Vertex AI AutoML time series model",
        "Question_body":"<p>I am testing out Google Cloud Vertex AI with a time series AutoML model.<\/p>\n<p>I have created a dataset, from a Biguery table, with 2 columns, one of a timestamp and another of a numeric value I want to predict:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/t5R7S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t5R7S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><code>salesorderdate<\/code> is my <code>TIMESTAMP<\/code> column and <code>orders<\/code> is the value I want to predict.<\/p>\n<p>When I proceed to the next step I cannot select <code>orders<\/code> as my value to predict, there are no available options for this field:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/HOed3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/HOed3.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What am I missing here? Surely the time series value <em>is<\/em> the target value in this case? Is there an expectation of more fields here, and can one actually add additional features as columns to a time series model in this way?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624627510520,
        "Question_favorite_count":null,
        "Question_last_edit_time":1624971874476,
        "Question_score":1.0,
        "Question_view_count":606.0,
        "Answer_body":"<p>I guess from your question that you are using &quot;forecasting models&quot;. Please note that it is in &quot;Preview&quot; <a href=\"https:\/\/cloud.google.com\/products#product-launch-stages\" rel=\"nofollow noreferrer\">Product launch stage<\/a> with all consequences of that fact.<\/p>\n<p>In the documentation you may find <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-tabular#data-structure\" rel=\"nofollow noreferrer\">Training data structure<\/a> following information:<\/p>\n<blockquote>\n<ul>\n<li>There must be at least two and no more than 1,000 columns.<\/li>\n<\/ul>\n<p>For datasets that train AutoML models, one column must be the target,\nand there must be at least one feature available to train the model.\nIf the training data does not include the target column, Vertex AI\ncannot associate the training data with the desired result.<\/p>\n<\/blockquote>\n<p>I suppose you are using AutoML models so in this situation you need to have 3 columns in the data set:<\/p>\n<ul>\n<li>Time column - used to place the observation represented by that row in time<\/li>\n<li>time series identifier column as &quot;Forecasting training data usually includes multiple time series&quot;<\/li>\n<li>and target column which is value that model should learn to predict.<\/li>\n<\/ul>\n<p>If you want to predict <code>orders<\/code> this should be target column. But before you are choosing this target this &quot;time series identifier column&quot; is already chosen in previous step, so you do not have available column to choose.<\/p>\n<p>So you need to add to your BigQuery table at least one additional column with will be used as time series column. You can add to your data set column with the same value in each row. This concept is presented in <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/bp-tabular#data_preparation_best_practices\" rel=\"nofollow noreferrer\">Forecasting data preparation best practices<\/a>:<\/p>\n<blockquote>\n<p>You can train a forecasting model on a single time series (in other\nwords, the time series identifier column contains the same value for\nall rows). However, Vertex AI is a better fit for training data that\ncontains two or more time series. For best results, you should have at\nleast 10 time series for every column used to train the model.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1624960598496,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68131742",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1624889338667,
        "Question_original_content":"select target column automl time seri model test googl cloud time seri automl model creat dataset bigueri tabl column timestamp numer valu want predict salesorderd timestamp column order valu want predict proce step select order valu predict avail option field miss sure time seri valu target valu case expect field actual add addit featur column time seri model wai",
        "Question_preprocessed_content":"select target column automl time seri model test googl cloud time seri automl model creat dataset bigueri tabl column timestamp numer valu want predict column valu want predict proce step select valu predict avail option field miss sure time seri valu target valu case expect field actual add addit featur column time seri model wai",
        "Question_gpt_summary_original":"The user is facing challenges in selecting a target column in a Vertex AI AutoML time series model. They have created a dataset with a timestamp and a numeric value they want to predict, but when they proceed to the next step, they cannot select the value they want to predict. The user is unsure if they are missing something and if they can add additional features as columns to a time series model.",
        "Question_gpt_summary":"user face challeng select target column automl time seri model creat dataset timestamp numer valu want predict proce step select valu want predict user unsur miss add addit featur column time seri model",
        "Answer_original_content":"guess question forecast model note preview product launch stage consequ fact document train data structur follow inform column dataset train automl model column target featur avail train model train data includ target column associ train data desir result suppos automl model situat need column data set time column place observ repres row time time seri identifi column forecast train data usual includ multipl time seri target column valu model learn predict want predict order target column choos target time seri identifi column chosen previou step avail column choos need add bigqueri tabl addit column time seri column add data set column valu row concept present forecast data prepar best practic train forecast model singl time seri word time seri identifi column contain valu row better fit train data contain time seri best result time seri column train model",
        "Answer_preprocessed_content":"guess question forecast model note preview product launch stage consequ fact document train data structur follow inform column dataset train automl model column target featur avail train model train data includ target column associ train data desir result suppos automl model situat need column data set time column place observ repres row time time seri identifi column forecast train data usual includ multipl time seri target column valu model learn predict want predict target column choos target time seri identifi column chosen previou step avail column choos need add bigqueri tabl addit column time seri column add data set column valu row concept present forecast data prepar best practic train forecast model singl time seri better fit train data contain time seri best result time seri column train model",
        "Answer_gpt_summary_original":"Solution:\n- The user needs to have at least three columns in the dataset for AutoML time series models: a time column, a time series identifier column, and a target column.\n- If the user wants to predict a specific value, that value should be the target column.\n- The time series identifier column is already chosen in the previous step, so the user needs to add at least one additional column to the BigQuery table to use as the time series column.\n- The user can add a column with the same value in each row to use as the time series column.\n- It is recommended to have at least 10 time series for every column used to train the model for best results.",
        "Answer_gpt_summary":"solut user need column dataset automl time seri model time column time seri identifi column target column user want predict specif valu valu target column time seri identifi column chosen previou step user need add addit column bigqueri tabl us time seri column user add column valu row us time seri column recommend time seri column train model best result"
    },
    {
        "Question_title":"Unable to connect to MLFLOW_TRACKING_URI when running MLflow run in Docker container",
        "Question_body":"<p>I have setup a mlflow server locally at http:\/\/localhost:5000<\/p>\n<p>I followed the instructions at <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker<\/a> and tried to run the example docker with<\/p>\n<pre><code>\/mlflow\/examples\/docker$ mlflow run . -P alpha=0.5\n<\/code><\/pre>\n<p>but I encountered the following error.<\/p>\n<pre><code>2021\/05\/09 17:11:20 INFO mlflow.projects.docker: === Building docker image docker-example:7530274 ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.utils: === Created directory \/tmp\/tmp9wpxyzd_ for downloading remote URIs passed to arguments of type 'path' ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.backend.local: === Running command 'docker run --rm -v \/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts:\/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts -e MLFLOW_RUN_ID=ae69145133bf49efac22b1d390c354f1 -e MLFLOW_TRACKING_URI=http:\/\/localhost:5000 -e MLFLOW_EXPERIMENT_ID=0 docker-example:7530274 python train.py --alpha 0.5 --l1-ratio 0.1' in run with ID 'ae69145133bf49efac22b1d390c354f1' === \n\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/__init__.py:55: DeprecationWarning: MLflow support for Python 2 is deprecated and will be dropped in a future release. At that point, existing Python 2 workflows that use MLflow will continue to work without modification, but Python 2 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3 - see https:\/\/docs.python.org\/3\/howto\/pyporting.html for a migration guide.\n  &quot;for a migration guide.&quot;, DeprecationWarning)\nTraceback (most recent call last):\n  File &quot;train.py&quot;, line 56, in &lt;module&gt;\n    with mlflow.start_run():\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/client.py&quot;, line 96, in get_run\n    return self._tracking_client.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 49, in get_run\n    return self.store.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 133, in call_endpoint\n    host_creds=host_creds, endpoint=endpoint, method=method, params=json_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 70, in http_request\n    url=url, headers=headers, verify=verify, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 51, in request_with_ratelimit_retries\n    response = requests.request(**kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/api.py&quot;, line 58, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 508, in request\n    resp = self.send(prep, **send_kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 618, in send\n    r = adapter.send(request, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/adapters.py&quot;, line 508, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get?run_uuid=ae69145133bf49efac22b1d390c354f1&amp;run_id=ae69145133bf49efac22b1d390c354f1 (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f5cbd80d690&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))\n2021\/05\/09 17:11:22 ERROR mlflow.cli: === Run (ID 'ae69145133bf49efac22b1d390c354f1') failed ===\n<\/code><\/pre>\n<p>Any ideas how to fix this? I tried adding the following in MLproject file but it doesn't help<\/p>\n<pre><code>environment: [[&quot;network&quot;, &quot;host&quot;], [&quot;add-host&quot;, &quot;host.docker.internal:host-gateway&quot;]]\n<\/code><\/pre>\n<p>Thanks for your help! =)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620552530280,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1620554070856,
        "Question_score":0.0,
        "Question_view_count":1151.0,
        "Answer_body":"<p>Run MLflow server such was that it will use your machine IP instead of <code>localhost<\/code>.  Then point the <code>mlflow run<\/code> to that IP instead of <code>http:\/\/localhost:5000<\/code>.   The main reason is that <code>localhost<\/code> of Docker process is its own, not your machine.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67456172",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1620627546968,
        "Question_original_content":"unabl connect track uri run run docker contain setup server local http localhost follow instruct http github com tree master exampl docker tri run exampl docker exampl docker run alpha encount follow error info project docker build docker imag docker exampl info project util creat directori tmp tmpwpxyzd download remot uri pass argument type path info project backend local run command docker run home mlf mlf aebfefacbdcf artifact home mlf mlf aebfefacbdcf artifact run aebfefacbdcf track uri http localhost experi docker exampl python train alpha ratio run aebfefacbdcf opt conda lib python site packag init deprecationwarn support python deprec drop futur releas point exist python workflow us continu work modif python user longer access latest featur bugfix recommend upgrad python http doc python org howto pyport html migrat guid migrat guid deprecationwarn traceback recent file train line start run file opt conda lib python site packag track fluent line start run activ run obj client run exist run file opt conda lib python site packag track client line run return self track client run run file opt conda lib python site packag track track servic client line run return self store run run file opt conda lib python site packag store track rest store line run respons proto self endpoint getrun req bodi file opt conda lib python site packag store track rest store line endpoint return endpoint self host cred endpoint method json bodi respons proto file opt conda lib python site packag util rest util line endpoint host cred host cred endpoint endpoint method method param json bodi file opt conda lib python site packag util rest util line http request url url header header verifi verifi kwarg file opt conda lib python site packag util rest util line request ratelimit retri respons request request kwarg file opt conda lib python site packag request api line request return session request method method url url kwarg file opt conda lib python site packag request session line request resp self send prep send kwarg file opt conda lib python site packag request session line send adapt send request kwarg file opt conda lib python site packag request adapt line send rais connectionerror request request request except connectionerror httpconnectionpool host localhost port max retri exceed url api run run uuid aebfefacbdcf run aebfefacbdcf caus newconnectionerror fail establish new connect errno connect refus error cli run aebfefacbdcf fail idea fix tri ad follow mlproject file help environ network host add host host docker intern host gatewai thank help",
        "Question_preprocessed_content":"unabl connect run run docker contain setup server local follow instruct tri run exampl docker encount follow error idea fix tri ad follow mlproject file help thank help",
        "Question_gpt_summary_original":"The user encountered an error when trying to run an MLflow example docker with the MLflow server set up locally at http:\/\/localhost:5000. The error message suggests that the connection to the MLflow tracking URI failed, and the user is seeking help to fix the issue.",
        "Question_gpt_summary":"user encount error try run exampl docker server set local http localhost error messag suggest connect track uri fail user seek help fix issu",
        "Answer_original_content":"run server us machin instead localhost point run instead http localhost main reason localhost docker process machin",
        "Answer_preprocessed_content":"run server us machin instead point instead main reason docker process machin",
        "Answer_gpt_summary_original":"Solution: The user can run the MLflow server using their machine IP instead of localhost and then point the mlflow run to that IP instead of http:\/\/localhost:5000. This is because the localhost of the Docker process is its own, not the user's machine.",
        "Answer_gpt_summary":"solut user run server machin instead localhost point run instead http localhost localhost docker process user machin"
    },
    {
        "Question_title":"What is correct input for mxnet's linear learner in AWS SageMaker?",
        "Question_body":"<p>I am trying to create a simple linear learner in AWS SageMaker with MXNet. I have never worked with SageMaker or MXNet previously. Fitting the model gives runtime error as follows and shuts the instance:<\/p>\n\n<blockquote>\n  <p>UnexpectedStatusException: Error for Training job\n  linear-learner-2020-02-11-06-13-22-712: Failed. Reason: ClientError:\n  Unable to read data channel 'train'. Requested content-type is\n  'application\/x-recordio-protobuf'. Please verify the data matches the\n  requested content-type. (caused by MXNetError)<\/p>\n<\/blockquote>\n\n<p>I think that the data should be converted to protobuf format before passing as training data. Could someone please explain to me what is the correct format for MXNet models? What is the best way to convert a simple data frame into protobuf?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1581404906080,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":617.0,
        "Answer_body":"<p><a href=\"https:\/\/github.com\/awslabs\/fraud-detection-using-machine-learning\/blob\/master\/source\/notebooks\/sagemaker_fraud_detection.ipynb\" rel=\"nofollow noreferrer\">This end-to-end demo<\/a> shows usage of Linear Learner from input data pre-processed in <code>pandas<\/code> dataframes and then converted to protobuf using the SDK. But note that:<\/p>\n\n<ul>\n<li>There is no need to use protobuf, you can also pass csv data with the target variable on the first column of the files, as <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html#ll-input_output\" rel=\"nofollow noreferrer\">indicated here<\/a>.<\/li>\n<li>There is no need to know MXNet in order to use the SageMaker Linear Learner, just use the SDK of your choice, bring data to S3, and orchestrate training and inference :)<\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60163614",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1581411605430,
        "Question_original_content":"correct input mxnet linear learner try creat simpl linear learner mxnet work mxnet previous fit model give runtim error follow shut instanc unexpectedstatusexcept error train job linear learner fail reason clienterror unabl read data channel train request content type applic recordio protobuf verifi data match request content type caus mxneterror think data convert protobuf format pass train data explain correct format mxnet model best wai convert simpl data frame protobuf",
        "Question_preprocessed_content":"correct input mxnet linear learner try creat simpl linear learner mxnet work mxnet previous fit model give runtim error follow shut instanc unexpectedstatusexcept error train job fail reason clienterror unabl read data channel train request verifi data match request think data convert protobuf format pass train data explain correct format mxnet model best wai convert simpl data frame protobuf",
        "Question_gpt_summary_original":"The user is facing challenges while creating a linear learner in AWS SageMaker with MXNet. The model is giving a runtime error and shutting down the instance. The error message suggests that the data format is incorrect and needs to be converted to protobuf format. The user is seeking guidance on the correct format for MXNet models and the best way to convert a data frame into protobuf.",
        "Question_gpt_summary":"user face challeng creat linear learner mxnet model give runtim error shut instanc error messag suggest data format incorrect need convert protobuf format user seek guidanc correct format mxnet model best wai convert data frame protobuf",
        "Answer_original_content":"end end demo show usag linear learner input data pre process panda datafram convert protobuf sdk note need us protobuf pass csv data target variabl column file indic need know mxnet order us linear learner us sdk choic bring data orchestr train infer",
        "Answer_preprocessed_content":"demo show usag linear learner input data datafram convert protobuf sdk note need us protobuf pass csv data target variabl column file indic need know mxnet order us linear learner us sdk choic bring data orchestr train infer",
        "Answer_gpt_summary_original":"Solutions provided:\n- The user can pass csv data with the target variable on the first column of the files instead of using protobuf format.\n- The user does not need to know MXNet to use the SageMaker Linear Learner, they can just use the SDK of their choice, bring data to S3, and orchestrate training and inference. \n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"solut provid user pass csv data target variabl column file instead protobuf format user need know mxnet us linear learner us sdk choic bring data orchestr train infer person opinion bias includ respons"
    },
    {
        "Question_title":"Does the primary key of Web Service API in ML Studio expire?",
        "Question_body":"<p>I deployed a web-service from an experiment in ML studio. I tested the API, and everything was working fine. I tested it in Postman. After 2 hours, I got an authentication error when I sent a request using the same API. So to resolve this, I republished my Web Service and got new authentication code, so the API is working fine for now. I have two questions:<\/p>\n\n<p>1) Does the primary key automatically expire after a while or by signing out from ML studio? \n2) What is the application of the second key in ML Studio APIs? Where do we need the second key? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1535343803617,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":228.0,
        "Answer_body":"<blockquote>\n  <p>1) Does the primary key automatically expire after a while or by signing out from ML studio?<\/p>\n<\/blockquote>\n\n<p>I could not find any limit of the primary key in the office docs. Per my test, my primary key does not expire more than two hours or sign out from ML studio.<\/p>\n\n<blockquote>\n  <p>2) What is the application of the second key in ML Studio APIs? Where do we need the second key?<\/p>\n<\/blockquote>\n\n<p>The second key is the same usage of the primary key, like a backup of the primary key. Also, the primary key equals the API key in the ML studio.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52032535",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1535450343787,
        "Question_original_content":"primari kei web servic api studio expir deploi web servic experi studio test api work fine test postman hour got authent error sent request api resolv republish web servic got new authent code api work fine question primari kei automat expir sign studio applic second kei studio api need second kei",
        "Question_preprocessed_content":"primari kei web servic api studio expir deploi experi studio test api work fine test postman hour got authent error sent request api resolv republish web servic got new authent code api work fine question primari kei automat expir sign studio applic second kei studio api need second kei",
        "Question_gpt_summary_original":"The user encountered an authentication error when using a web-service API deployed from an experiment in ML Studio. They republished the service to obtain a new authentication code and resolved the issue. The user has questions about whether the primary key expires and the application of the second key in ML Studio APIs.",
        "Question_gpt_summary":"user encount authent error web servic api deploi experi studio republish servic obtain new authent code resolv issu user question primari kei expir applic second kei studio api",
        "Answer_original_content":"primari kei automat expir sign studio limit primari kei offic doc test primari kei expir hour sign studio applic second kei studio api need second kei second kei usag primari kei like backup primari kei primari kei equal api kei studio",
        "Answer_preprocessed_content":"primari kei automat expir sign studio limit primari kei offic doc test primari kei expir hour sign studio applic second kei studio api need second kei second kei usag primari kei like backup primari kei primari kei equal api kei studio",
        "Answer_gpt_summary_original":"Solution:\n- The primary key does not have a specific expiration time or affected by signing out from ML Studio.\n- The second key is a backup of the primary key and can be used in the same way as the primary key.",
        "Answer_gpt_summary":"solut primari kei specif expir time affect sign studio second kei backup primari kei wai primari kei"
    },
    {
        "Question_title":"Vertex AI - Viewing Pipeline Output",
        "Question_body":"<p>I have followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/schedule-cloud-scheduler\" rel=\"nofollow noreferrer\">this<\/a> tutorial to create my first scheduled Vertex AI Pipeline to run every minute. The only thing it does is prints <code>&quot;Hello, &lt;any-greet-string&gt;&quot;<\/code> and also returns this same string. I can see that it is running because the last run time updates and the last run result is &quot;Success&quot; every time.<\/p>\n<p>My question is very simple: Where can I see this string printed and the output of my pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1639659363547,
        "Question_favorite_count":null,
        "Question_last_edit_time":1639726542407,
        "Question_score":2.0,
        "Question_view_count":346.0,
        "Answer_body":"<p>The output of the <code>print()<\/code> statements in the pipeline can be found in &quot;Cloud Logging&quot; with the appropriate filters. To check logs for each component in the pipeline, click on the respective component in the console and click &quot;<strong>VIEW LOGS<\/strong>&quot; in the right pane. A new pane with the logs will open in the pipeline page which will allow us to see the output of the component. Refer to the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I ran a sample <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#4\" rel=\"nofollow noreferrer\">pipeline<\/a> from this codelab, <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#0\" rel=\"nofollow noreferrer\">Intro to Vertex Pipelines<\/a> and below is the output for one of the <code>print()<\/code> statements in the pipeline.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<br \/>\n<h4>UPDATE:<\/h4>\n<p>Every component in a pipeline run is deployed as an individual Vertex AI custom job. Corresponding to the sample pipeline consisting 3 components, there are 3 entries in the &quot;CUSTOM JOBS&quot; section as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Therefore, to view the logs on the run level, we would need to query the log entries with the respective <code>job_id<\/code>s of the pipeline components and the <code>job_id<\/code> of the Cloud Scheduler job. The query would look like this.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.labels.job_id=(&quot;JOB_ID_1&quot; OR &quot;JOB_ID_2&quot; [OR &quot;JOB_ID_N&quot;...])\nseverity&gt;=DEFAULT\n<\/code><\/pre>\n<p>If there are no simultaneous pipeline runs, a simpler query like below can be used.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.type=(&quot;cloud_scheduler_job&quot; OR &quot;ml_job&quot;)\nseverity&gt;=DEFAULT\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1639750456896,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70379395",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1639667359983,
        "Question_original_content":"view pipelin output follow tutori creat schedul pipelin run minut thing print hello return string run run time updat run result success time question simpl string print output pipelin",
        "Question_preprocessed_content":"view pipelin output follow tutori creat schedul pipelin run minut thing print return string run run time updat run result success time question simpl string print output pipelin",
        "Question_gpt_summary_original":"The user has successfully created a scheduled Vertex AI Pipeline that prints a greeting string and returns the same string. However, the user is facing a challenge in locating the output of the pipeline and the printed string.",
        "Question_gpt_summary":"user successfulli creat schedul pipelin print greet string return string user face challeng locat output pipelin print string",
        "Answer_original_content":"output print statement pipelin cloud log appropri filter check log compon pipelin click respect compon consol click view log right pane new pane log open pipelin page allow output compon refer screenshot ran sampl pipelin codelab intro vertex pipelin output print statement pipelin updat compon pipelin run deploi individu custom job correspond sampl pipelin consist compon entri custom job section shown view log run level need queri log entri respect job id pipelin compon job cloud schedul job queri look like resourc label job job job job sever default simultan pipelin run simpler queri like resourc type cloud schedul job job sever default",
        "Answer_preprocessed_content":"output statement pipelin cloud log appropri filter check log compon pipelin click respect compon consol click view log right pane new pane log open pipelin page allow output compon refer screenshot ran sampl pipelin codelab intro vertex pipelin output statement pipelin updat compon pipelin run deploi individu custom job correspond sampl pipelin consist compon entri custom job section shown view log run level need queri log entri respect pipelin compon cloud schedul job queri look like simultan pipelin run simpler queri like",
        "Answer_gpt_summary_original":"The solution to locating the output of the pipeline and the printed string is to check the Cloud Logging with the appropriate filters. To check logs for each component in the pipeline, click on the respective component in the console and click \"VIEW LOGS\" in the right pane. A new pane with the logs will open in the pipeline page which will allow us to see the output of the component. Additionally, every component in a pipeline run is deployed as an individual Vertex AI custom job, so to view the logs on the run level, we would need to query the log entries with the respective job_ids of the pipeline components and the job_id of the Cloud Scheduler job.",
        "Answer_gpt_summary":"solut locat output pipelin print string check cloud log appropri filter check log compon pipelin click respect compon consol click view log right pane new pane log open pipelin page allow output compon addition compon pipelin run deploi individu custom job view log run level need queri log entri respect job id pipelin compon job cloud schedul job"
    },
    {
        "Question_title":"How can we get the pipeline to read columns with special characters?",
        "Question_body":"<p>I am using the \"usecols\" parameter to get some columns of a .xlsx file (I am using the xls_local.py file from the Kedro tutorial) but the program says that \"usecols do not match columns, columns expected but not found:\" and it only shows the columns that have special characters. How can I fix this, please? Thank you very much for your attention.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573053746820,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":157.0,
        "Answer_body":"<p>As far as I can tell, this isn't a <code>kedro<\/code> issue, but a <code>pandas.read_excel<\/code> issue, which is what <code>kedro<\/code> uses under the hood. This seems to be broken in <code>pandas<\/code> itself, and a workaround is to reference the columns using letters instead, so something like <code>usecols='A:D'<\/code> and then you can rename the columns to what they should be by doing <code>df.columns = [\"colname with special characters\", \"b\", \"c\", \"d\"]<\/code> for example.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58733422",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1573057563320,
        "Question_original_content":"pipelin read column special charact usecol paramet column xlsx file xl local file tutori program sai usecol match column column expect show column special charact fix thank attent",
        "Question_preprocessed_content":"pipelin read column special charact usecol paramet column xlsx file program sai usecol match column column expect show column special charact fix thank attent",
        "Question_gpt_summary_original":"The user is facing a challenge with the \"usecols\" parameter while trying to read specific columns from a .xlsx file using the xls_local.py file from the Kedro tutorial. The program is not able to read columns with special characters and is displaying an error message stating that the expected columns are not found. The user is seeking help to fix this issue.",
        "Question_gpt_summary":"user face challeng usecol paramet try read specif column xlsx file xl local file tutori program abl read column special charact displai error messag state expect column user seek help fix issu",
        "Answer_original_content":"far tell isn issu panda read excel issu us hood broken panda workaround refer column letter instead like usecol renam column column colnam special charact exampl",
        "Answer_preprocessed_content":"far tell isn issu issu us hood broken workaround refer column letter instead like renam column exampl",
        "Answer_gpt_summary_original":"Solution: The user can reference the columns using letters instead of column names with special characters while using the \"usecols\" parameter to read specific columns from a .xlsx file. They can then rename the columns to their original names using the \"df.columns\" attribute.",
        "Answer_gpt_summary":"solut user refer column letter instead column name special charact usecol paramet read specif column xlsx file renam column origin name column attribut"
    },
    {
        "Question_title":"How can I export a model (Predictor) trained on Amazon Forecast?",
        "Question_body":"<p>On Amazon Forecast, how can I export a model (a Predictor in Forecast lingo) that I already trained? For example, export an ARIMA or Prophet model weights to a file t be downloaded or stored on S3.<\/p>\n<p>Running forecasts on new data is just too slow and I would like to use Forecast to train models and eventually deploy them somewhere else.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610981455530,
        "Question_favorite_count":null,
        "Question_last_edit_time":1611076562296,
        "Question_score":1.0,
        "Question_view_count":97.0,
        "Answer_body":"<p>Short answer: you can't export the models from Forecast<\/p>\n<p>Ref: <a href=\"https:\/\/github.com\/aws-samples\/amazon-forecast-samples\/issues\/104#issuecomment-763119541\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-forecast-samples\/issues\/104#issuecomment-763119541<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65776789",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1611089773083,
        "Question_original_content":"export model predictor train amazon forecast amazon forecast export model predictor forecast lingo train exampl export arima prophet model weight file download store run forecast new data slow like us forecast train model eventu deploi",
        "Question_preprocessed_content":"export model train amazon forecast amazon forecast export model train exampl export arima prophet model weight file download store run forecast new data slow like us forecast train model eventu deploi",
        "Question_gpt_summary_original":"The user is facing a challenge of exporting a model (Predictor) trained on Amazon Forecast, such as an ARIMA or Prophet model, to a file that can be downloaded or stored on S3. The user wants to use Forecast to train models and deploy them elsewhere, as running forecasts on new data is too slow.",
        "Question_gpt_summary":"user face challeng export model predictor train amazon forecast arima prophet model file download store user want us forecast train model deploi run forecast new data slow",
        "Answer_original_content":"short answer export model forecast ref http github com aw sampl amazon forecast sampl issu issuecom",
        "Answer_preprocessed_content":"short answer export model forecast ref",
        "Answer_gpt_summary_original":"Solution: No solution provided.",
        "Answer_gpt_summary":"solut solut provid"
    },
    {
        "Question_title":"Why can Random Cut Forest's `record_set()` method for data conversion\/upload not be used with the \"test\" channel?",
        "Question_body":"<h1>Original Question<\/h1>\n<p>I want to use RCF's &quot;test&quot; channel, to get performance-metrics of the model.<\/p>\n<p>I have previously used the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/algorithms\/randomcutforest.html?highlight=record_set#sagemaker.RandomCutForest.record_set\" rel=\"nofollow noreferrer\"><code>record_set()<\/code><\/a> method without specifying a channel and training worked fine.<\/p>\n<p>However if I upload my feature matrix and label vector using <code>record_set()<\/code> and set <code>channel='test'<\/code> like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker import RandomCutForest\n\nrcf = RandomCutForest(\n    role=execution_role,\n    instance_count=1,\n    instance_type='ml.m5.large',\n    data_location=f's3:\/\/{bucket}\/{prefix}\/',\n    output_path=f's3:\/\/{bucket}\/{prefix}\/output',\n    base_job_name=base_job_name,\n    eval_metrics=['accuracy', 'precision_recall_fscore']\n)\n\ntest_set = rcf.record_set(\n    features,\n    labels=labels,\n    channel='test'\n)\n\nrcf.fit(test_set)\n<\/code><\/pre>\n<p>... I get the following error during training:<\/p>\n<blockquote>\n<p>Failure reason<br \/>\nClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError) Caused by: 'ShardedByS3Key' is not one of ['FullyReplicated'] Failed validating 'enum' in schema['properties']['test']['properties']['S3DistributionType']: {'enum': ['FullyReplicated'], 'type': 'string'} On instance['test']['S3DistributionType']: 'ShardedByS3Key'<\/p>\n<\/blockquote>\n<hr \/>\n<h2>Appendix<\/h2>\n<p>The same is true for HPO-jobs that use data on the &quot;test&quot; channel:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tuner import HyperparameterTuner, IntegerParameter\n\nhpo = HyperparameterTuner(\n    estimator=rcf,\n    objective_metric_name='test:f1',\n    hyperparameter_ranges={\n        'num_samples_per_tree': IntegerParameter(32, 512),\n        'num_trees': IntegerParameter(50, 1000)\n    },\n    max_jobs=10,\n    max_parallel_jobs=2\n)\n\ntrain_set = rcf.record_set(\n    features,\n    channel='train'\n)\n\ntest_set = rcf.record_set(\n    features,\n    labels=labels,\n    channel='test'\n)\n\nhpo.fit([train_set, test_set])\n<\/code><\/pre>\n<p>Again the initial upload and &quot;train&quot; channel work fine, the &quot;test&quot; channel fails:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/p0xa0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/p0xa0.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<hr \/>\n<h3>EDIT 1<\/h3>\n<p>I tried to instantiate my own <code>RecordSet<\/code> objects like this (after the data was already uploaded to S3 in the correct format):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>train_data = RecordSet(\n    s3_data='s3:\/\/path-to-train-data\/.amazon.manifest',\n    feature_dim=features.shape[1],\n    num_records=features.shape[0],\n    s3_data_type='ManifestFile',\n    channel='train'\n)\n\ntest_data = RecordSet(\n    s3_data='s3:\/\/path-to-test-data\/.amazon.manifest',\n    feature_dim=features.shape[1],\n    num_records=features.shape[0],\n    s3_data_type='ManifestFile',\n    channel='test'\n)\n<\/code><\/pre>\n<p>But the same error occurs. The issue is, that the constructor of <code>RecordSet<\/code> does not permit to pass a different <code>distribution<\/code> argument. It is hardcoded to be <code>ShardedByS3Key<\/code>, as can be seen in the source code <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/2ebba8a454de03a2bc49267c91dbacddd6183585\/src\/sagemaker\/amazon\/amazon_estimator.py#L340\" rel=\"nofollow noreferrer\">HERE<\/a>.<\/p>\n<p>I've also opened an <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/2925\" rel=\"nofollow noreferrer\">issue on GitHub<\/a>, since this bug is still present in the latest version of SageMaker.<\/p>\n<h3>EDIT 2<\/h3>\n<p>There is another post on a similar issue on StackOverflow, where an older version of the Python API is used: <a href=\"https:\/\/stackoverflow.com\/questions\/68161725\/sagemaker-random-cut-forest-training-with-validation\">Sagemaker Random Cut Forest Training with Validation<\/a>.<\/p>\n<p>After some refactoring in newer versions, users don't have to create <code>TrainingInput<\/code> objects anymore. This is now supposed to be done via <code>record_set()<\/code> (which returns the class <code>RecordSet<\/code>). As a result, we lose control over the <code>distribution<\/code> property of the underlying <code>TrainingInput<\/code> object and can't set it to <code>FullyReplicated<\/code> anymore.<\/p>\n<p>This is essentially what breaks the &quot;test&quot; channel for RCF in current versions of SageMaker and also prevents hyperparameter optimization via <code>HyperparameterTuner<\/code>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644425382927,
        "Question_favorite_count":null,
        "Question_last_edit_time":1644496542936,
        "Question_score":0.0,
        "Question_view_count":95.0,
        "Answer_body":"<p>Thanks for opening the issue, I added a +1.\nIn the meantime, you can use alternative SDKs to train Random Cut Forest and set test channel distribution to FullyReplicated.<\/p>\n<p>For example, those SDKs should give you this control:<\/p>\n<ul>\n<li>AWS CLI <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-training-job.html\" rel=\"nofollow noreferrer\">create_training_job<\/a><\/li>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_training_job\" rel=\"nofollow noreferrer\">boto3 create_training_job<\/a><\/li>\n<li>SageMaker Python SDK <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#\" rel=\"nofollow noreferrer\">Estimator<\/a> to which you pass the RCF docker image in <code>image_uri<\/code> parameter<\/li>\n<\/ul>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71053554",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1645791604667,
        "Question_original_content":"random cut forest record set method data convers upload test channel origin question want us rcf test channel perform metric model previous record set method specifi channel train work fine upload featur matrix label vector record set set channel test like import randomcutforest rcf randomcutforest role execut role instanc count instanc type larg data locat bucket prefix output path bucket prefix output base job base job eval metric accuraci precis recal fscore test set rcf record set featur label label channel test rcf fit test set follow error train failur reason clienterror unabl initi algorithm fail valid input data configur caus validationerror caus shardedbyskei fullyrepl fail valid enum schema properti test properti sdistributiontyp enum fullyrepl type string instanc test sdistributiontyp shardedbyskei appendix true hpo job us data test channel tuner import hyperparametertun integerparamet hpo hyperparametertun estim rcf object metric test hyperparamet rang num sampl tree integerparamet num tree integerparamet max job max parallel job train set rcf record set featur channel train test set rcf record set featur label label channel test hpo fit train set test set initi upload train channel work fine test channel fail edit tri instanti recordset object like data upload correct format train data recordset data path train data amazon manifest featur dim featur shape num record featur shape data type manifestfil channel train test data recordset data path test data amazon manifest featur dim featur shape num record featur shape data type manifestfil channel test error occur issu constructor recordset permit pass differ distribut argument hardcod shardedbyskei seen sourc code open issu github bug present latest version edit post similar issu stackoverflow older version python api random cut forest train valid refactor newer version user creat traininginput object anymor suppos record set return class recordset result lose control distribut properti underli traininginput object set fullyrepl anymor essenti break test channel rcf current version prevent hyperparamet optim hyperparametertun",
        "Question_preprocessed_content":"random cut forest method data test channel origin question want us rcf test channel model previous method specifi channel train work fine upload featur matrix label vector set like follow error train failur reason clienterror unabl initi algorithm fail valid input data configur caus shardedbi kei fail valid enum schema instanc shardedbi kei appendix true us data test channel initi upload train channel work fine test channel fail edit tri instanti object like error occur issu constructor permit pass differ argument hardcod seen sourc code open issu github bug present latest version edit post similar issu stackoverflow older version python api random cut forest train valid refactor newer version user creat object anymor suppos result lose control properti underli object set anymor essenti break test channel rcf current version prevent hyperparamet optim",
        "Question_gpt_summary_original":"The user is encountering an error when attempting to use the Random Cut Forest's `record_set()` method with the \"test\" channel. The error message indicates that the algorithm is unable to validate the input data configuration due to an incorrect S3 distribution type. The user has attempted to create their own `RecordSet` objects, but the constructor does not allow for a different distribution argument. This issue prevents the user from using the \"test\" channel for performance metrics and hyperparameter optimization. The user has opened an issue on GitHub regarding this bug.",
        "Question_gpt_summary":"user encount error attempt us random cut forest record set method test channel error messag indic algorithm unabl valid input data configur incorrect distribut type user attempt creat recordset object constructor allow differ distribut argument issu prevent user test channel perform metric hyperparamet optim user open issu github bug",
        "Answer_original_content":"thank open issu ad meantim us altern sdk train random cut forest set test channel distribut fullyrepl exampl sdk control aw cli creat train job boto creat train job python sdk estim pass rcf docker imag imag uri paramet",
        "Answer_preprocessed_content":"thank open issu ad meantim us altern sdk train random cut forest set test channel distribut fullyrepl exampl sdk control aw cli boto python sdk estim pass rcf docker imag paramet",
        "Answer_gpt_summary_original":"Solution: The user can use alternative SDKs to train Random Cut Forest and set the test channel distribution to FullyReplicated until the bug is fixed. The AWS CLI, boto3 create_training_job, and SageMaker Python SDK Estimator can give the user control over the distribution.",
        "Answer_gpt_summary":"solut user us altern sdk train random cut forest set test channel distribut fullyrepl bug fix aw cli boto creat train job python sdk estim user control distribut"
    },
    {
        "Question_title":"How to export a MLFlow Model from Azure Databricks as an Azure DevOps Artifacts for CD phase?",
        "Question_body":"<p>I am trying to create an MLOps Pipeline using Azure DevOps and Azure Databricks. From Azure DevOps, I am submitting a Databricks job to a cluster, which trains a Machine Learning Model and saves it into MLFlow Model Registry with a custom flavour (using PyFunc Custom Model).<\/p>\n<p>Now after the job gets over, I want to export this MLFlow Object (with all dependencies - Conda dependencies, two model files - one <code>.pkl<\/code> and one <code>.h5<\/code>, the Python Class with <code>load_context()<\/code> and <code>predict()<\/code> functions defined so that after exporting I can import it and call predict as we do with MLFlow Models).<\/p>\n<p>How do I export this entire MLFlow Model and save it as an AzureDevOps Artifact to be used in the CD phase (where I will deploy it to an AKS cluster with a custom base image)?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1629179063687,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":575.0,
        "Answer_body":"<p>There is no official way to export a Databricks MLflow run from one workspace to another. However, there is an &quot;unofficial&quot; tool that does most of the job with the main limitation being that notebook revisions linked to a run cannot be exported due to lack of a REST API endpoint for this.<\/p>\n<p><a href=\"https:\/\/github.com\/amesar\/mlflow-export-import\" rel=\"nofollow noreferrer\">https:\/\/github.com\/amesar\/mlflow-export-import<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68812238",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1629787454223,
        "Question_original_content":"export model azur databrick azur devop artifact phase try creat mlop pipelin azur devop azur databrick azur devop submit databrick job cluster train machin learn model save model registri custom flavour pyfunc custom model job get want export object depend conda depend model file pkl python class load context predict function defin export import predict model export entir model save azuredevop artifact phase deploi ak cluster custom base imag",
        "Question_preprocessed_content":"export model azur databrick azur devop artifact phase try creat mlop pipelin azur devop azur databrick azur devop submit databrick job cluster train machin learn model save model registri custom flavour job get want export object export entir model save azuredevop artifact phase",
        "Question_gpt_summary_original":"The user is facing challenges in exporting an MLFlow Model from Azure Databricks as an Azure DevOps Artifacts for the CD phase. They have successfully trained a Machine Learning Model and saved it into the MLFlow Model Registry with a custom flavour, but are struggling to export the entire MLFlow Model with all dependencies, including Conda dependencies, two model files, and the Python Class with load_context() and predict() functions defined. The user wants to save the exported MLFlow Model as an Azure DevOps Artifact to be used in the CD phase for deployment to an AKS cluster with a custom base image.",
        "Question_gpt_summary":"user face challeng export model azur databrick azur devop artifact phase successfulli train machin learn model save model registri custom flavour struggl export entir model depend includ conda depend model file python class load context predict function defin user want save export model azur devop artifact phase deploy ak cluster custom base imag",
        "Answer_original_content":"offici wai export databrick run workspac unoffici tool job main limit notebook revis link run export lack rest api endpoint http github com amesar export import",
        "Answer_preprocessed_content":"offici wai export databrick run workspac unoffici tool job main limit notebook revis link run export lack rest api endpoint",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use an \"unofficial\" tool called mlflow-export-import, which can export most of the MLFlow Model with all dependencies, including Conda dependencies, two model files, and the Python Class with load_context() and predict() functions defined. However, it has a limitation that notebook revisions linked to a run cannot be exported due to lack of a REST API endpoint for this.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us unoffici tool call export import export model depend includ conda depend model file python class load context predict function defin limit notebook revis link run export lack rest api endpoint"
    },
    {
        "Question_title":"No module named 'automl' when unpickle auto-trained model",
        "Question_body":"<p>I'm trying to reproduce 2 tutorials below using my own dataset instead of MNIST dataset.\n<a href=\"https:\/\/docs.microsoft.com\/ja-jp\/azure\/machine-learning\/service\/tutorial-auto-train-models\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/ja-jp\/azure\/machine-learning\/service\/tutorial-auto-train-models<\/a>\n<a href=\"https:\/\/docs.microsoft.com\/ja-jp\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/ja-jp\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml<\/a><\/p>\n\n<p>About\n'\/notebooks\/tutorials\/03.auto-train-models.ipynb'\nthere's no problem. I've got 'model.pkl'.<\/p>\n\n<p>However, \n'\/notebooks\/tutorials\/02.deploy-models.ipynb'\nhas an error below in 'Predict test data' cell.\nI guess it's a matter of 'pickle' and 'import'.<\/p>\n\n<p>Tell me solutions, please.<\/p>\n\n<pre><code>ModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-6-11cf888b622f&gt; in &lt;module&gt;\n      2 from sklearn.externals import joblib\n      3 \n----&gt; 4 clf = joblib.load('.\/model.pkl')\n      5 # clf = joblib.load('.\/sklearn_mnist_model.pkl')\n      6 y_hat = clf.predict(X_test)\n\n~\/anaconda3_501\/lib\/python3.6\/site-packages\/sklearn\/externals\/joblib\/numpy_pickle.py in load(filename, mmap_mode)\n    576                     return load_compatibility(fobj)\n    577 \n--&gt; 578                 obj = _unpickle(fobj, filename, mmap_mode)\n    579 \n    580     return obj\n\n~\/anaconda3_501\/lib\/python3.6\/site-packages\/sklearn\/externals\/joblib\/numpy_pickle.py in _unpickle(fobj, filename, mmap_mode)\n    506     obj = None\n    507     try:\n--&gt; 508         obj = unpickler.load()\n    509         if unpickler.compat_mode:\n    510             warnings.warn(\"The file '%s' has been generated with a \"\n\n~\/anaconda3_501\/lib\/python3.6\/pickle.py in load(self)\n   1048                     raise EOFError\n   1049                 assert isinstance(key, bytes_types)\n-&gt; 1050                 dispatch[key[0]](self)\n   1051         except _Stop as stopinst:\n   1052             return stopinst.value\n\n~\/anaconda3_501\/lib\/python3.6\/pickle.py in load_global(self)\n   1336         module = self.readline()[:-1].decode(\"utf-8\")\n   1337         name = self.readline()[:-1].decode(\"utf-8\")\n-&gt; 1338         klass = self.find_class(module, name)\n   1339         self.append(klass)\n   1340     dispatch[GLOBAL[0]] = load_global\n\n~\/anaconda3_501\/lib\/python3.6\/pickle.py in find_class(self, module, name)\n   1386             elif module in _compat_pickle.IMPORT_MAPPING:\n   1387                 module = _compat_pickle.IMPORT_MAPPING[module]\n-&gt; 1388         __import__(module, level=0)\n   1389         if self.proto &gt;= 4:\n   1390             return _getattribute(sys.modules[module], name)[0]\n\nModuleNotFoundError: No module named 'automl'\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1543815453630,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":4788.0,
        "Answer_body":"<p>you have to include azureml-train-automl package. and you have to do this:<\/p>\n\n<p>import azureml.train.automl<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53588040",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1543816325648,
        "Question_original_content":"modul name automl unpickl auto train model try reproduc tutori dataset instead mnist dataset http doc microsoft com azur machin learn servic tutori auto train model http doc microsoft com azur machin learn servic tutori deploi model aml notebook tutori auto train model ipynb problem got model pkl notebook tutori deploi model ipynb error predict test data cell guess matter pickl import tell solut modulenotfounderror traceback recent sklearn extern import joblib clf joblib load model pkl clf joblib load sklearn mnist model pkl hat clf predict test anaconda lib python site packag sklearn extern joblib numpi pickl load filenam mmap mode return load compat fobj obj unpickl fobj filenam mmap mode return obj anaconda lib python site packag sklearn extern joblib numpi pickl unpickl fobj filenam mmap mode obj try obj unpickl load unpickl compat mode warn warn file gener anaconda lib python pickl load self rais eoferror assert isinst kei byte type dispatch kei self stop stopinst return stopinst valu anaconda lib python pickl load global self modul self readlin decod utf self readlin decod utf klass self class modul self append klass dispatch global load global anaconda lib python pickl class self modul elif modul compat pickl import map modul compat pickl import map modul import modul level self proto return getattribut sy modul modul modulenotfounderror modul name automl",
        "Question_preprocessed_content":"modul name automl unpickl model try reproduc tutori dataset instead mnist dataset problem got error predict test data cell guess matter pickl import tell solut",
        "Question_gpt_summary_original":"The user is encountering an error when trying to unpickle an auto-trained model in a tutorial. The error message suggests that the issue may be related to the 'pickle' and 'import' modules, specifically the 'automl' module. The user is seeking solutions to this problem.",
        "Question_gpt_summary":"user encount error try unpickl auto train model tutori error messag suggest issu relat pickl import modul specif automl modul user seek solut problem",
        "Answer_original_content":"includ train automl packag import train automl",
        "Answer_preprocessed_content":"includ packag import",
        "Answer_gpt_summary_original":"Solution: The solution mentioned in the discussion is to include the 'azureml-train-automl' package and import the 'azureml.train.automl' module.",
        "Answer_gpt_summary":"solut solut mention discuss includ train automl packag import train automl modul"
    },
    {
        "Question_title":"Remote s3 cache storage with minio",
        "Question_body":"<p>Hello,<br>\nFirst of all thank you for your contribution.<br>\nI would like to ask if there is a way to keep all my cache of a dataset into a remote minio bucket and not appearing into my local storage.<\/p>\n<p>I have added my dataset into a remote minio bucket<\/p>\n<pre><code class=\"lang-auto\">dvc remote add myminio -d s3:\/\/abucket\/DVC\ndvc remote modify myminio endpointurl 'http:\/\/.....\/'\ndvc remote modify myminio access_key_id 'user'\ndvc remote modify myminio secret_access_key 'pass'\n\ndvc add data\ngit add .gitignore data.dvc .dvc\/config\ngit commit -m '-Added remote storage,-Added data'\ndvc push\nrm -r .dvc\/cache\n<\/code><\/pre>\n<p>But when I try to do a dvc pull, the cache directory always return.<\/p>\n<p>Is there a way to define that my cache folder is my remote?<br>\nAlso can I use dvc pull on another PC without data.dvc?<\/p>\n<p>Thank you in advance<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1674558901821,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":81.0,
        "Answer_body":"<p>That\u2019s intended behavior. Pull is downloading files to local cache so that you could use them locally.<\/p>\n<blockquote>\n<p>Also can I use dvc pull on another PC without data.dvc?<\/p>\n<\/blockquote>\n<p>you need it in one shape or form to provide the information about what you want to download. Maybe you are looking for <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\" class=\"inline-onebox\">get<\/a> ?<\/p>. <p>Hello,<\/p>\n<p>I manage to do something convenient for me by using<\/p>\n<pre><code class=\"lang-auto\">dvc import-url data_temp data --to-remote\n<\/code><\/pre>\n<p>I uploaded the dvc files on a github and now I can share my dataset with other users.<\/p>\n<p>I would like to know I little bit more about working with dvc pull<\/p>\n<p>First this is my tree of data_temp directory.<\/p>\n<pre><code class=\"lang-auto\">folder1\nfolder2\nfolder3\nfile\n<\/code><\/pre>\n<p>where folders are like this<\/p>\n<pre><code class=\"lang-auto\">folder1\n    subfolder\n        subfolder_a\n            file1\n            ....\n            file20\n        subfolder_b\n            file1\n<\/code><\/pre>\n<p>I tried to do pull data on another computer and it works<br>\nI would like to understand why \u2018dvc pull\u2019 works as intended (downloads everything)<br>\nbut in the scenacio<\/p>\n<ol>\n<li>I want to pull only selected files, I should first create a directory named \u2018data_temp\u2019 to proceed with dvc pull data\/file<\/li>\n<li>Why I cannot pull selected directory with dvc pull data\/folder1<\/li>\n<\/ol>\n<p>Thank you in advance!<\/p>. <p>I\u2019m not 100% sure I understand your usecase, but if you need to pull individual subdirectories, I\u2019d recommend first creating <code>data_temp<\/code>, then importing subfolders. That will make it possible to pull subfolders:<\/p>\n<pre><code class=\"lang-bash\">mkdir  data_temp\ndvc import-url &lt;url\/to\/folder1&gt; data_temp\/folder1\n<\/code><\/pre>\n<p>with this you will be able to pull: <code>dvc pull data_temp\/folder1<\/code><\/p>\n<p>Fore more info, please have a look at the <a href=\"https:\/\/dvc.org\/doc\/start\/data-management\/data-versioning\" rel=\"noopener nofollow ugc\">docs<\/a>.<\/p>. <p>Thank you for your reply!<br>\nActually this is what I try to achieve, <a href=\"https:\/\/dvc.org\/doc\/user-guide\/data-management\/managing-external-data\" rel=\"noopener nofollow ugc\">managing-external-data<\/a>.<\/p>\n<p>My usecase is that I have a large dataset with limited storage, I want to share the dataset without creating unnecessary cache to me or my colleagues. Also, I need to select part of the data (e.g. with a certain tag) or dropping out garbage, this is why I try to pull certain folders from the external storage. I believe the Examples section is what I want to achieve. However, I haven\u2019t managed to do it in minio <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/frowning.png?v=12\" title=\":frowning:\" class=\"emoji\" alt=\":frowning:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>I tried <strong>import-url<\/strong> before and even though the configuration exist and dvc pull works, dvc import-url does not locate the credentials<\/p>\n<blockquote>\n<p>ERROR: unexpected error - Unable to locate credentials<br>\nI also tried<\/p>\n<\/blockquote>\n<pre><code class=\"lang-auto\">export AWS_ACCESS_KEY_ID=&lt;user_name&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;password&gt;\nexport AWS_S3_ENDPOINT_URL =http:\/\/...:port\/\n\n<\/code><\/pre>\n<p>Which returns :<\/p>\n<pre><code class=\"lang-auto\">Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>. <p>Unfortunately there\u2019s currently no way to provide <code>import-url<\/code> a custom endpoint URL via environment variables (access key\/secret key env variables do work though). You could open a feature request on <a href=\"http:\/\/github.com\/iterative\/dvc\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - iterative\/dvc: \ud83e\udd89Data Version Control | Git for Data &amp; Models | ML Experiments Management<\/a>.<\/p>\n<p>You can use the following workaround to run <code>import-url<\/code> with minio:<\/p>\n<pre><code class=\"lang-bash\">dvc remote add minio s3:\/\/&lt;bucket&gt;\ndvc remote modify minio endpoint-url http:\/\/&lt;minio host&gt;:&lt;minio port&gt;\ndvc remote modify --local minio access_key_id &lt;access key id&gt;\ndvc remote modify --local minio secret_access_key &lt;access key&gt;\n<\/code><\/pre>\n<p>Then you will be able to use use the remote:\/\/ syntax for import url with the previously defined remote:<\/p>\n<pre><code class=\"lang-bash\">dvc import-url remote:\/\/minio\/path\/on\/bucket\n<\/code><\/pre>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/remote-s3-cache-storage-with-minio\/1472",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-24T12:40:11.898Z",
                "Answer_body":"<p>That\u2019s intended behavior. Pull is downloading files to local cache so that you could use them locally.<\/p>\n<blockquote>\n<p>Also can I use dvc pull on another PC without data.dvc?<\/p>\n<\/blockquote>\n<p>you need it in one shape or form to provide the information about what you want to download. Maybe you are looking for <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\" class=\"inline-onebox\">get<\/a> ?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-26T08:20:01.134Z",
                "Answer_body":"<p>Hello,<\/p>\n<p>I manage to do something convenient for me by using<\/p>\n<pre><code class=\"lang-auto\">dvc import-url data_temp data --to-remote\n<\/code><\/pre>\n<p>I uploaded the dvc files on a github and now I can share my dataset with other users.<\/p>\n<p>I would like to know I little bit more about working with dvc pull<\/p>\n<p>First this is my tree of data_temp directory.<\/p>\n<pre><code class=\"lang-auto\">folder1\nfolder2\nfolder3\nfile\n<\/code><\/pre>\n<p>where folders are like this<\/p>\n<pre><code class=\"lang-auto\">folder1\n    subfolder\n        subfolder_a\n            file1\n            ....\n            file20\n        subfolder_b\n            file1\n<\/code><\/pre>\n<p>I tried to do pull data on another computer and it works<br>\nI would like to understand why \u2018dvc pull\u2019 works as intended (downloads everything)<br>\nbut in the scenacio<\/p>\n<ol>\n<li>I want to pull only selected files, I should first create a directory named \u2018data_temp\u2019 to proceed with dvc pull data\/file<\/li>\n<li>Why I cannot pull selected directory with dvc pull data\/folder1<\/li>\n<\/ol>\n<p>Thank you in advance!<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-28T18:09:07.609Z",
                "Answer_body":"<p>I\u2019m not 100% sure I understand your usecase, but if you need to pull individual subdirectories, I\u2019d recommend first creating <code>data_temp<\/code>, then importing subfolders. That will make it possible to pull subfolders:<\/p>\n<pre><code class=\"lang-bash\">mkdir  data_temp\ndvc import-url &lt;url\/to\/folder1&gt; data_temp\/folder1\n<\/code><\/pre>\n<p>with this you will be able to pull: <code>dvc pull data_temp\/folder1<\/code><\/p>\n<p>Fore more info, please have a look at the <a href=\"https:\/\/dvc.org\/doc\/start\/data-management\/data-versioning\" rel=\"noopener nofollow ugc\">docs<\/a>.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-30T08:12:45.130Z",
                "Answer_body":"<p>Thank you for your reply!<br>\nActually this is what I try to achieve, <a href=\"https:\/\/dvc.org\/doc\/user-guide\/data-management\/managing-external-data\" rel=\"noopener nofollow ugc\">managing-external-data<\/a>.<\/p>\n<p>My usecase is that I have a large dataset with limited storage, I want to share the dataset without creating unnecessary cache to me or my colleagues. Also, I need to select part of the data (e.g. with a certain tag) or dropping out garbage, this is why I try to pull certain folders from the external storage. I believe the Examples section is what I want to achieve. However, I haven\u2019t managed to do it in minio <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/frowning.png?v=12\" title=\":frowning:\" class=\"emoji\" alt=\":frowning:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>I tried <strong>import-url<\/strong> before and even though the configuration exist and dvc pull works, dvc import-url does not locate the credentials<\/p>\n<blockquote>\n<p>ERROR: unexpected error - Unable to locate credentials<br>\nI also tried<\/p>\n<\/blockquote>\n<pre><code class=\"lang-auto\">export AWS_ACCESS_KEY_ID=&lt;user_name&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;password&gt;\nexport AWS_S3_ENDPOINT_URL =http:\/\/...:port\/\n\n<\/code><\/pre>\n<p>Which returns :<\/p>\n<pre><code class=\"lang-auto\">Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-30T08:50:37.149Z",
                "Answer_body":"<p>Unfortunately there\u2019s currently no way to provide <code>import-url<\/code> a custom endpoint URL via environment variables (access key\/secret key env variables do work though). You could open a feature request on <a href=\"http:\/\/github.com\/iterative\/dvc\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - iterative\/dvc: \ud83e\udd89Data Version Control | Git for Data &amp; Models | ML Experiments Management<\/a>.<\/p>\n<p>You can use the following workaround to run <code>import-url<\/code> with minio:<\/p>\n<pre><code class=\"lang-bash\">dvc remote add minio s3:\/\/&lt;bucket&gt;\ndvc remote modify minio endpoint-url http:\/\/&lt;minio host&gt;:&lt;minio port&gt;\ndvc remote modify --local minio access_key_id &lt;access key id&gt;\ndvc remote modify --local minio secret_access_key &lt;access key&gt;\n<\/code><\/pre>\n<p>Then you will be able to use use the remote:\/\/ syntax for import url with the previously defined remote:<\/p>\n<pre><code class=\"lang-bash\">dvc import-url remote:\/\/minio\/path\/on\/bucket\n<\/code><\/pre>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"remot cach storag minio hello thank contribut like ask wai cach dataset remot minio bucket appear local storag ad dataset remot minio bucket remot add myminio abucket remot modifi myminio endpointurl http remot modifi myminio access kei user remot modifi myminio secret access kei pass add data git add gitignor data config git commit ad remot storag ad data push cach try pull cach directori return wai defin cach folder remot us pull data thank advanc",
        "Question_preprocessed_content":"remot cach storag minio hello thank contribut like ask wai cach dataset remot minio bucket appear local storag ad dataset remot minio bucket try pull cach directori return wai defin cach folder remot us pull thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges in keeping all their cache of a dataset into a remote minio bucket and not appearing into their local storage. They have added their dataset into a remote minio bucket, but when they try to do a dvc pull, the cache directory always returns. The user is also seeking a way to define that their cache folder is their remote and if they can use dvc pull on another PC without data.dvc.",
        "Question_gpt_summary":"user face challeng keep cach dataset remot minio bucket appear local storag ad dataset remot minio bucket try pull cach directori return user seek wai defin cach folder remot us pull data",
        "Answer_original_content":"that intend behavior pull download file local cach us local us pull data need shape form provid inform want download mayb look hello manag conveni import url data temp data remot upload file github share dataset user like know littl bit work pull tree data temp directori folder folder folder file folder like folder subfold subfold file file subfold file tri pull data work like understand pull work intend download scenacio want pull select file creat directori name data temp proce pull data file pull select directori pull data folder thank advanc sure understand usecas need pull individu subdirectori recommend creat data temp import subfold possibl pull subfold mkdir data temp import url data temp folder abl pull pull data temp folder fore info look doc thank repli actual try achiev manag extern data usecas larg dataset limit storag want share dataset creat unnecessari cach colleagu need select data certain tag drop garbag try pull certain folder extern storag believ exampl section want achiev havent manag minio tri import url configur exist pull work import url locat credenti error unexpect error unabl locat credenti tri export aw access kei export aw secret access kei export aw endpoint url http port return forbidden error occur call headobject oper forbidden unfortun there current wai provid import url custom endpoint url environ variabl access kei secret kei env variabl work open featur request github iter data version control git data model experi manag us follow workaround run import url minio remot add minio remot modifi minio endpoint url http remot modifi local minio access kei remot modifi local minio secret access kei abl us us remot syntax import url previous defin remot import url remot minio path bucket",
        "Answer_preprocessed_content":"that intend behavior pull download file local cach us local us pull need shape form provid inform want download mayb look hello manag conveni upload file github share dataset user like know littl bit work pull tree directori folder like tri pull data work like understand pull work intend scenacio want pull select file creat directori name proce pull pull select directori pull thank advanc sure understand usecas need pull individu subdirectori recommend creat import subfold possibl pull subfold abl pull fore info look doc thank repli actual try achiev usecas larg dataset limit storag want share dataset creat unnecessari cach colleagu need select data drop garbag try pull certain folder extern storag believ exampl section want achiev havent manag minio tri configur exist pull work locat credenti error unexpect error unabl locat credenti tri return unfortun there current wai provid custom endpoint url environ variabl open featur request github data version control git data model experi manag us follow workaround run minio abl us us syntax import url previous defin remot",
        "Answer_gpt_summary_original":"Solutions provided:\n- The user can use `dvc import-url` to upload the dvc files on a Github and share the dataset with other users.\n- To pull individual subdirectories, the user can first create `data_temp`, then import subfolders. That will make it possible to pull subfolders.\n- To use `import-url` with Minio, the user can add a remote for Minio and modify it with the endpoint URL, access key ID, and secret access key. Then, they can use the `remote:\/\/` syntax for import URL with the previously defined remote. \n\nNo solution provided:\n- The user's question about why they cannot pull selected directory with `dvc pull data\/folder1` was not answered",
        "Answer_gpt_summary":"solut provid user us import url upload file github share dataset user pull individu subdirectori user creat data temp import subfold possibl pull subfold us import url minio user add remot minio modifi endpoint url access kei secret access kei us remot syntax import url previous defin remot solut provid user question pull select directori pull data folder answer"
    },
    {
        "Question_title":"can i take best parameters and best model of optuna function and apply this model directly in my notebook?",
        "Question_body":"<p>i esttablished a function of optuna to find out best model of gbm and xgboost for my data but i was wondering if i can take the best model and apply it directly into my notebook(extracting best model as an object to reuse it later)\nhere is my objective function:<\/p>\n<pre><code>import lightgbm as lgb \nimport optuna\nimport sklearn.metrics\nfrom xgboost import XGBRegressor\nfrom optuna.integration import XGBoostPruningCallback\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nbest_booster = None\ngbm = None\ndef objective(trial,random_state=22,n_jobs=1,early_stopping_rounds=50):\n    \n    regrosser_name = trial.suggest_categorical(&quot;regressor&quot;, [&quot;XGBoost&quot;, &quot;lightgbm&quot;])\n    train_x, valid_x, train_y, valid_y = train_test_split(X_train, y_train, test_size=0.25)\n    dtrain = lgb.Dataset(train_x, label=train_y)\n    # Step 2. Setup values for the hyperparameters:\n    if regrosser_name == 'XGBoost':\n        params = {\n        &quot;verbosity&quot;: 0,  # 0 (silent) - 3 (debug)\n        &quot;objective&quot;: &quot;reg:squarederror&quot;,\n        &quot;n_estimators&quot;: 10000,\n        &quot;max_depth&quot;: trial.suggest_int(&quot;max_depth&quot;, 4, 12),\n        &quot;learning_rate&quot;: trial.suggest_loguniform(&quot;learning_rate&quot;, 0.005, 0.05),\n        &quot;colsample_bytree&quot;: trial.suggest_loguniform(&quot;colsample_bytree&quot;, 0.2, 0.6),\n        &quot;subsample&quot;: trial.suggest_loguniform(&quot;subsample&quot;, 0.4, 0.8),\n        &quot;alpha&quot;: trial.suggest_loguniform(&quot;alpha&quot;, 0.01, 10.0),\n        &quot;lambda&quot;: trial.suggest_loguniform(&quot;lambda&quot;, 1e-8, 10.0),\n        &quot;gamma&quot;: trial.suggest_loguniform(&quot;lambda&quot;, 1e-8, 10.0),\n        &quot;min_child_weight&quot;: trial.suggest_loguniform(&quot;min_child_weight&quot;, 10, 1000),\n        &quot;seed&quot;: random_state,\n        &quot;n_jobs&quot;: n_jobs,\n        }\n        model = XGBRegressor(**params)\n        model.fit(train_x, train_y)\n        y_pred = model.predict(X_val)\n        accuracy_rf = sklearn.metrics.mean_absolute_error(valid_y, y_pred)\n        return accuracy_rf\n    \n        print(rf_max_depth)\n        print(rf_n_estimators)\n        \n    else:\n        param = {\n        &quot;objective&quot;: &quot;binary&quot;,\n        &quot;metric&quot;: &quot;binary_logloss&quot;,\n        &quot;verbosity&quot;: -1,\n        &quot;boosting_type&quot;: &quot;gbdt&quot;,\n        &quot;lambda_l1&quot;: trial.suggest_float(&quot;lambda_l1&quot;, 1e-8, 10.0, log=True),\n        &quot;lambda_l2&quot;: trial.suggest_float(&quot;lambda_l2&quot;, 1e-8, 10.0, log=True),\n        &quot;num_leaves&quot;: trial.suggest_int(&quot;num_leaves&quot;, 2, 256),\n        &quot;feature_fraction&quot;: trial.suggest_float(&quot;feature_fraction&quot;, 0.4, 1.0),\n        &quot;bagging_fraction&quot;: trial.suggest_float(&quot;bagging_fraction&quot;, 0.4, 1.0),\n        &quot;bagging_freq&quot;: trial.suggest_int(&quot;bagging_freq&quot;, 1, 7),\n        &quot;min_child_samples&quot;: trial.suggest_int(&quot;min_child_samples&quot;, 5, 100),\n        }\n        gbm = lgb.train(param, dtrain)\n        preds_gbm = gbm.predict(valid_x)\n        pred_labels_gbm = np.rint(preds_gbm)\n        accuracy_gbm = sklearn.metrics.mean_absolute_error(valid_y, pred_labels_gbm)\n        return accuracy_gbm\n<\/code><\/pre>\n<p>and here is how i tried to solve this issue:<\/p>\n<pre><code>def callback(study, trial):\n    global best_booster\n    if study.best_trial == trial:\n        best_booster = gbm\nif __name__ == &quot;__main__&quot;:\n    study = optuna.create_study(direction=&quot;maximize&quot;)\n    study.optimize(objective, n_trials=100, callbacks=[callback])\n<\/code><\/pre>\n<p>i think its about importing somthing, and if there is any tips on my optuna function please state it<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648541981733,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":585.0,
        "Answer_body":"<p>If I understood your question correctly, then yes, that's what models are for.<\/p>\n<p>Like bring your saved model to your notebook, feed it data that has the same structure  as what you used to train it, and it should serve its purpose.  Or use it in a pipeline.<\/p>\n<p>Even 1 line of the same structure as an np array can be used.  For example, my model predicts whether a loan should be approved or not.<\/p>\n<p>For example, a bank customer wants a loan and submits his information.  The bank officer inputs this info in the system.  The system transforms this information into a single np array with the same structure as the dataset used to train the model.<\/p>\n<p>The model is then used by the system to predict whether the loan should be approved or not.<\/p>\n<p>I save my optuna xgb models as json, e.g.<\/p>\n<p>my_model.get_booster().save_model(f'{savepath}my_model.json')<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71658687",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1649608540120,
        "Question_original_content":"best paramet best model function appli model directli notebook esttablish function best model gbm xgboost data wonder best model appli directli notebook extract best model object reus later object function import lightgbm lgb import import sklearn metric xgboost import xgbregressor integr import xgboostpruningcallback sklearn ensembl import histgradientboostingregressor sklearn model select import train test split sklearn import dataset sklearn ensembl import randomforestregressor sklearn metric import mean absolut error best booster gbm def object trial random state job earli stop round regross trial suggest categor regressor xgboost lightgbm train valid train valid train test split train train test size dtrain lgb dataset train label train step setup valu hyperparamet regross xgboost param verbos silent debug object reg squarederror estim max depth trial suggest int max depth learn rate trial suggest loguniform learn rate colsampl bytre trial suggest loguniform colsampl bytre subsampl trial suggest loguniform subsampl alpha trial suggest loguniform alpha lambda trial suggest loguniform lambda gamma trial suggest loguniform lambda min child weight trial suggest loguniform min child weight seed random state job job model xgbregressor param model fit train train pred model predict val accuraci sklearn metric mean absolut error valid pred return accuraci print max depth print estim param object binari metric binari logloss verbos boost type gbdt lambda trial suggest float lambda log true lambda trial suggest float lambda log true num leav trial suggest int num leav featur fraction trial suggest float featur fraction bag fraction trial suggest float bag fraction bag freq trial suggest int bag freq min child sampl trial suggest int min child sampl gbm lgb train param dtrain pred gbm gbm predict valid pred label gbm rint pred gbm accuraci gbm sklearn metric mean absolut error valid pred label gbm return accuraci gbm tri solv issu def callback studi trial global best booster studi best trial trial best booster gbm main studi creat studi direct maxim studi optim object trial callback callback think import somth tip function state",
        "Question_preprocessed_content":"best paramet best model function appli model directli notebook esttablish function best model gbm xgboost data wonder best model appli directli notebook object function tri solv issu think import somth tip function state",
        "Question_gpt_summary_original":"The user has established an Optuna function to find the best model of GBM and XGBoost for their data. They are wondering if they can take the best model and apply it directly into their notebook as an object to reuse it later. The user has tried to solve this issue by creating a callback function, but they are unsure if they need to import something and are open to any tips on their Optuna function.",
        "Question_gpt_summary":"user establish function best model gbm xgboost data wonder best model appli directli notebook object reus later user tri solv issu creat callback function unsur need import open tip function",
        "Answer_original_content":"understood question correctli ye model like bring save model notebook feed data structur train serv purpos us pipelin line structur arrai exampl model predict loan approv exampl bank custom want loan submit inform bank offic input info transform inform singl arrai structur dataset train model model predict loan approv save xgb model json model booster save model savepath model json",
        "Answer_preprocessed_content":"understood question correctli ye model like bring save model notebook feed data structur train serv purpos us pipelin line structur arrai exampl model predict loan approv exampl bank custom want loan submit inform bank offic input info transform inform singl arrai structur dataset train model model predict loan approv save xgb model json",
        "Answer_gpt_summary_original":"Solution: The user can save their Optuna XGBoost model as a JSON file using the `save_model()` function and then load it into their notebook as an object to reuse it later. They can feed the saved model with data that has the same structure as what they used to train it, and it should serve its purpose. Alternatively, they can use the saved model in a pipeline.",
        "Answer_gpt_summary":"solut user save xgboost model json file save model function load notebook object reus later feed save model data structur train serv purpos altern us save model pipelin"
    },
    {
        "Question_title":"Can I add OpenAPI specification to a webservice deployed with AzureML in AKS?",
        "Question_body":"I'd like to deploy a machine learning service using AzureML on AKS. I also need to add some OpenAPI specification for it.\n\nFeatures in https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python are neat, but that of having API docs\/swagger for the webservice seems missing.\n\nHaving some documentation is useful especially if the model takes in input several features of different type.\n\nTo overcome this, I currently get models trained in AzureML and include them in Docker containers that use the python FastAPI library to build the API and OpenAPI\/Swagger specs, and those are deployed on some host.\n\nCan I do something equivalent to this with AKS in AzureML instead? If so, how?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1600897231890,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@DavideFiocco-7346 The deployments of Azure ML provide a swagger specification URI that can be used directly. The documentation of this is available here. You can print your swagger_uri of the web service and check if it confirms with the specifications you are creating currently.\n\nIf the above response helps, please accept the response as answer. Thanks!!",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/105437\/can-i-add-openapi-specification-to-a-webservice-de.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-09-24T06:54:05.547Z",
                "Answer_score":2,
                "Answer_body":"@DavideFiocco-7346 The deployments of Azure ML provide a swagger specification URI that can be used directly. The documentation of this is available here. You can print your swagger_uri of the web service and check if it confirms with the specifications you are creating currently.\n\nIf the above response helps, please accept the response as answer. Thanks!!",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1600930445547,
        "Question_original_content":"add openapi specif webservic deploi ak like deploi machin learn servic ak need add openapi specif featur http doc microsoft com azur machin learn deploi azur kubernet servic tab python neat have api doc swagger webservic miss have document us especi model take input featur differ type overcom current model train includ docker contain us python fastapi librari build api openapi swagger spec deploi host equival ak instead",
        "Question_preprocessed_content":"add openapi specif webservic deploi ak like deploi machin learn servic ak need add openapi specif featur neat have api webservic miss have document us especi model take input featur differ type overcom current model train includ docker contain us python fastapi librari build api spec deploi host equival ak instead",
        "Question_gpt_summary_original":"The user is facing a challenge in adding OpenAPI specification to a machine learning service deployed using AzureML on AKS. The user finds the API documentation missing in the features provided by AzureML and is currently using Docker containers with FastAPI library to build the API and OpenAPI\/Swagger specs. The user is seeking guidance on how to add OpenAPI specification to the webservice deployed with AzureML in AKS.",
        "Question_gpt_summary":"user face challeng ad openapi specif machin learn servic deploi ak user find api document miss featur provid current docker contain fastapi librari build api openapi swagger spec user seek guidanc add openapi specif webservic deploi ak",
        "Answer_original_content":"davidefiocco deploy provid swagger specif uri directli document avail print swagger uri web servic check confirm specif creat current respons help accept respons answer thank",
        "Answer_preprocessed_content":"deploy provid swagger specif uri directli document avail print web servic check confirm specif creat current respons help accept respons answer thank",
        "Answer_gpt_summary_original":"Solution: The user can use the swagger specification URI provided by Azure ML deployments to add OpenAPI specification to the webservice deployed with AzureML in AKS. The documentation for this is available on the Azure ML website. The user can print the swagger_uri of the web service and check if it confirms with the specifications they are creating currently.",
        "Answer_gpt_summary":"solut user us swagger specif uri provid deploy add openapi specif webservic deploi ak document avail websit user print swagger uri web servic check confirm specif creat current"
    },
    {
        "Question_title":"How to mlflow-autolog a sklearn ConfusionMatrixDisplay?",
        "Question_body":"<p>I'm trying to log the plot of a <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator\" rel=\"nofollow noreferrer\">confusion matrix generated with scikit-learn<\/a> for a <em>test<\/em> set using <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.sklearn.html\" rel=\"nofollow noreferrer\">mlflow's support for scikit-learn<\/a>.<\/p>\n<p>For this, I tried something that resemble the code below (I'm using mlflow hosted on Databricks, and <code>sklearn==1.0.1<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sklearn.datasets\nimport pandas as pd\nimport numpy as np\nimport mlflow\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nmlflow.set_tracking_uri(&quot;databricks&quot;)\nmlflow.set_experiment(&quot;\/Users\/name.surname\/plotcm&quot;)\n\ndata = sklearn.datasets.fetch_20newsgroups(categories=['alt.atheism', 'sci.space'])\n\ndf = pd.DataFrame(data = np.c_[data['data'], data['target']])\\\n       .rename({0:'text', 1:'class'}, axis = 'columns')\n\ntrain, test = train_test_split(df)\n\nmy_pipeline = Pipeline([\n    ('vectorizer', TfidfVectorizer()),\n    ('classifier', SGDClassifier(loss='modified_huber')),\n])\n\nmlflow.sklearn.autolog()\n\nfrom sklearn.metrics import ConfusionMatrixDisplay # should I import this after the call to `.autolog()`?\n\nmy_pipeline.fit(train['text'].values, train['class'].values)\n\ncm = ConfusionMatrixDisplay.from_predictions(\n      y_true=test[&quot;class&quot;], y_pred=my_pipeline.predict(test[&quot;text&quot;])\n  )\n<\/code><\/pre>\n<p>while the confusion matrix for the training set is saved in my mlflow run, no png file is created in the mlflow frontend for the <code>test<\/code> set.<\/p>\n<p>If I try to add<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>cm.figure_.savefig('test_confusion_matrix.png')\nmlflow.log_artifact('test_confusion_matrix.png')\n<\/code><\/pre>\n<p>that does the job, but requires explicitly logging the artifact.<\/p>\n<p>Is there an idiomatic\/proper way to autolog the confusion matrix computed using a test set after <code>my_pipeline.fit()<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657892681357,
        "Question_favorite_count":null,
        "Question_last_edit_time":1658083880967,
        "Question_score":0.0,
        "Question_view_count":157.0,
        "Answer_body":"<p>The proper way to do this is to use <code>mlflow.log_figure<\/code> as a fluent API announced in <code>MLflow 1.13.0<\/code>. You can read the documentation <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_figure\" rel=\"nofollow noreferrer\">here<\/a>. This code will do the job.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.log_figure(cm.figure_, 'test_confusion_matrix.png')\n<\/code><\/pre>\n<p>This function implicitly store the image, and then calls <code>log_artifact<\/code> against that path, something like you did.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72994988",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1658304934100,
        "Question_original_content":"autolog sklearn confusionmatrixdisplai try log plot confus matrix gener scikit learn test set support scikit learn tri resembl code host databrick sklearn import sklearn dataset import panda import numpi import sklearn pipelin import pipelin sklearn linear model import sgdclassifi sklearn featur extract text import tfidfvector sklearn model select import train test split set track uri databrick set experi user surnam plotcm data sklearn dataset fetch newsgroup categori alt atheism sci space datafram data data data data target renam text class axi column train test train test split pipelin pipelin vector tfidfvector classifi sgdclassifi loss modifi huber sklearn autolog sklearn metric import confusionmatrixdisplai import autolog pipelin fit train text valu train class valu confusionmatrixdisplai predict true test class pred pipelin predict test text confus matrix train set save run png file creat frontend test set try add figur savefig test confus matrix png log artifact test confus matrix png job requir explicitli log artifact idiomat proper wai autolog confus matrix comput test set pipelin fit",
        "Question_preprocessed_content":"autolog sklearn confusionmatrixdisplai try log plot confus matrix gener test set support tri resembl code confus matrix train set save run png file creat frontend set try add job requir explicitli log artifact wai autolog confus matrix comput test set",
        "Question_gpt_summary_original":"The user is trying to log the plot of a confusion matrix generated with scikit-learn for a test set using mlflow's support for scikit-learn. However, while the confusion matrix for the training set is saved in the mlflow run, no png file is created in the mlflow frontend for the test set. The user is looking for an idiomatic\/proper way to autolog the confusion matrix computed using a test set after my_pipeline.fit().",
        "Question_gpt_summary":"user try log plot confus matrix gener scikit learn test set support scikit learn confus matrix train set save run png file creat frontend test set user look idiomat proper wai autolog confus matrix comput test set pipelin fit",
        "Answer_original_content":"proper wai us log figur fluent api announc read document code job log figur figur test confus matrix png function implicitli store imag call log artifact path like",
        "Answer_preprocessed_content":"proper wai us fluent api announc read document code job function implicitli store imag call path like",
        "Answer_gpt_summary_original":"Solution: The proper way to autolog the confusion matrix computed using a test set after my_pipeline.fit() is to use mlflow.log_figure as a fluent API announced in MLflow 1.13.0. The function mlflow.log_figure(cm.figure_, 'test_confusion_matrix.png') implicitly stores the image and then calls log_artifact against that path.",
        "Answer_gpt_summary":"solut proper wai autolog confus matrix comput test set pipelin fit us log figur fluent api announc function log figur figur test confus matrix png implicitli store imag call log artifact path"
    },
    {
        "Question_title":"Assume Sagemaker Notebook instance role from Docker container with default network mode",
        "Question_body":"<p>I have an interesting use case and a problem.<\/p>\n<p>We are leveraging <strong>Sagemaker Notebooks<\/strong> as a development environment for our data science teams. These notebooks are essentially EC2 instances with a (relatively) nice IDE (not as good as Cloud9, though).<\/p>\n<p>In addition, we are running some docker containers on these instances. However, we are forced to use <code>--network=host<\/code> mode, otherwise, the role assigned to the Notebook Instance is not assumed inside the docker container.<\/p>\n<p>On the host (here <code>1234567890<\/code> is our account number, and <code>DataScientist<\/code> is the role attached to the Sagemaker Notebook instance):<\/p>\n<pre><code>$ aws sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>Running the same command inside a Docker container with <code>--network=host<\/code> produces the same result:<\/p>\n<pre><code>$ docker run --network host amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>However, it doesn't work with Docker <code>--network=bridge<\/code>:<\/p>\n<pre><code>$ docker run amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAIMGPPFPT5T6N7BYX6:i-0b2a9080d5ed1cb98&quot;,\n    &quot;Account&quot;: &quot;366152344081&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::366152344081:assumed-role\/BaseNotebookInstanceEc2InstanceRole\/i-0b2a9080d5ed1cb98&quot;\n}\n<\/code><\/pre>\n<p>As you can see, it's a completely different role being assumed. Notice the account number 366152344081 and the role ARN - it's sth internal to AWS.<\/p>\n<p>We would like to keep the default networking option for Docker (bridge) and at the same time be able to assume the correct role (the one attached to SageMaker Notebook instance e.g. <code>DataScientist<\/code> in our case) attached to the host system (Sagemaker Notebook). Are there any hacks (e.g. iptable rules, etc.) to achieve that?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628255185970,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1639139946700,
        "Question_score":0.0,
        "Question_view_count":195.0,
        "Answer_body":"<p>If we look at the networks that were created on a clean Sagemaker Notebook instance, we can notice a user-defined bridge network named <code>sagemaker-local<\/code>:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nf1d5a59a8c9e        bridge              bridge              local\n6142e6764495        host                host                local\n194adfb00f0a        none                null                local\n99de6c086aa8        sagemaker-local     bridge              local\n<\/code><\/pre>\n<p>If we then attach to this custom bridge, we will be able to assume the correct role (the one attached to the Sagemaker Notebook instance itself):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run --network sagemaker-local amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<hr \/>\n<p><strong>UPDATE<\/strong><\/p>\n<p>As of this writing (10 Dec 2021) you don't need to attach to <code>sagemaker-local<\/code> bridge network anymore, the default <code>bridge<\/code> will work as well (note <code>--network bridge<\/code> is implicit in this call):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>Make sure you restart your SageMaker Notebook instance.<\/p>\n<p>Also, <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/setup.sh\" rel=\"nofollow noreferrer\">here<\/a> I found some manual patching (iptables etc.), but with the update it's already patched.<\/p>\n<p>Thanks to AWS who fixed this :)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1639131288447,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68682085",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1638951877412,
        "Question_original_content":"assum notebook instanc role docker contain default network mode interest us case problem leverag notebook develop environ data scienc team notebook essenti instanc rel nice id good cloud addit run docker contain instanc forc us network host mode role assign notebook instanc assum insid docker contain host account number datascientist role attach notebook instanc aw st caller ident userid aroaupvgymmvxxj account arn arn aw st assum role datascientist run command insid docker contain network host produc result docker run network host amazon aw cli st caller ident userid aroaupvgymmvxxj account arn arn aw st assum role datascientist work docker network bridg docker run amazon aw cli st caller ident userid aroaimgppfpttnbyx badedcb account arn arn aw st assum role basenotebookinstanceecinstancerol badedcb complet differ role assum notic account number role arn sth intern aw like default network option docker bridg time abl assum correct role attach notebook instanc datascientist case attach host notebook hack iptabl rule achiev",
        "Question_preprocessed_content":"assum notebook instanc role docker contain default network mode interest us case problem leverag notebook develop environ data scienc team notebook essenti instanc nice id addit run docker contain instanc forc us mode role assign notebook instanc assum insid docker contain host run command insid docker contain produc result work docker complet differ role assum notic account number role arn sth intern aw like default network option docker time abl assum correct role attach host hack achiev",
        "Question_gpt_summary_original":"The user is facing a challenge in assuming the correct role attached to the Sagemaker Notebook instance inside a Docker container. They are forced to use the \"--network=host\" mode to assume the role, but they would like to use the default networking option for Docker (bridge) while still being able to assume the correct role. When running the same command inside a Docker container with \"--network=host\", the correct role is assumed, but it doesn't work with \"--network=bridge\". The user is seeking a solution to this problem.",
        "Question_gpt_summary":"user face challeng assum correct role attach notebook instanc insid docker contain forc us network host mode assum role like us default network option docker bridg abl assum correct role run command insid docker contain network host correct role assum work network bridg user seek solut problem",
        "Answer_original_content":"look network creat clean notebook instanc notic user defin bridg network name local docker network network driver scope fdaac bridg bridg local host host local adfbfa null local decaa local bridg local attach custom bridg abl assum correct role attach notebook instanc docker run network local amazon aw cli st caller ident userid aroaupvgymmvxxj account arn arn aw st assum role datascientist updat write dec need attach local bridg network anymor default bridg work note network bridg implicit docker run amazon aw cli st caller ident userid aroaupvgymmvxxj account arn arn aw st assum role datascientist sure restart notebook instanc manual patch iptabl updat patch thank aw fix",
        "Answer_preprocessed_content":"look network creat clean notebook instanc notic bridg network name attach custom bridg abl assum correct role updat write need attach bridg network anymor default work sure restart notebook instanc manual patch updat patch thank aw fix",
        "Answer_gpt_summary_original":"Solution:\n- The user can attach to the custom bridge network named \"sagemaker-local\" to assume the correct role attached to the Sagemaker Notebook instance inside a Docker container.\n- As of December 10, 2021, the default \"bridge\" network will work as well, and the user doesn't need to attach to \"sagemaker-local\" anymore.",
        "Answer_gpt_summary":"solut user attach custom bridg network name local assum correct role attach notebook instanc insid docker contain decemb default bridg network work user need attach local anymor"
    },
    {
        "Question_title":"Deepar Prediction Quantiles Explained",
        "Question_body":"<p>I am working with Deepar and trying to get a better understanding of the quantile values returned. From the documentation, the likelihood hyperparameter explains that: <code>...provide quantiles of the distribution and return samples<\/code>. <\/p>\n\n<p>If I look at a single data point the quantiles returned are linear. E.g. the 0.1 quantile has the lowest predicted value and 0.9 quantile has the highest predicted value. I am having trouble understanding this. If these are samples from the distribution, shouldn't they look similar to the distribution selected with the likelihood hyperparameter (negative-binomial in my case)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570719704473,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":568.0,
        "Answer_body":"<p>DeepAR returns probabilistic forecasts in terms of quantiles: by default, the 0.1, 0.2, 0.3, ..., 0.9 quantiles are returned. This means that, according to the model, in each future time step you have 10% chance of observing something lower than the 0.1 quantile, 20% chance of observing something lower than the 0.2 quantile, and so on. Quantiles are in fact in order, and they must be by definition of quantile. Hope this clarifies is a bit!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58325923",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1573131591227,
        "Question_original_content":"deepar predict quantil explain work deepar try better understand quantil valu return document likelihood hyperparamet explain provid quantil distribut return sampl look singl data point quantil return linear quantil lowest predict valu quantil highest predict valu have troubl understand sampl distribut shouldn look similar distribut select likelihood hyperparamet neg binomi case",
        "Question_preprocessed_content":"deepar predict quantil explain work deepar try better understand quantil valu return document likelihood hyperparamet explain look singl data point quantil return linear quantil lowest predict valu quantil highest predict valu have troubl understand sampl distribut shouldn look similar distribut select likelihood hyperparamet",
        "Question_gpt_summary_original":"The user is having trouble understanding the quantile values returned by Deepar, specifically how they are linear for a single data point and do not resemble the selected distribution (negative-binomial).",
        "Question_gpt_summary":"user have troubl understand quantil valu return deepar specif linear singl data point resembl select distribut neg binomi",
        "Answer_original_content":"deepar return probabilist forecast term quantil default quantil return mean accord model futur time step chanc observ lower quantil chanc observ lower quantil quantil fact order definit quantil hope clarifi bit",
        "Answer_preprocessed_content":"deepar return probabilist forecast term quantil default quantil return mean accord model futur time step chanc observ lower quantil chanc observ lower quantil quantil fact order definit quantil hope clarifi bit",
        "Answer_gpt_summary_original":"Solution: The discussion provides an explanation of how DeepAR returns probabilistic forecasts in terms of quantiles. The quantiles returned are in order and represent the probability of observing a value lower than that quantile. No specific solution is provided to address the user's trouble in understanding the quantile values returned by DeepAR.",
        "Answer_gpt_summary":"solut discuss provid explan deepar return probabilist forecast term quantil quantil return order repres probabl observ valu lower quantil specif solut provid address user troubl understand quantil valu return deepar"
    },
    {
        "Question_title":"how to login wandb with another acount using colab?",
        "Question_body":"<p>For example, I have A, B acounts.<\/p>\n<p>First, I log in Google Colab with A account.\nand I want to log in wandb with B acounts. ( using !wandb login )\nis it possible??<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644556957917,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":316.0,
        "Answer_body":"<p>You can you the following commands to force a relogin:<\/p>\n<ul>\n<li>from terminal<\/li>\n<\/ul>\n<pre><code>wandb login --relogin\n<\/code><\/pre>\n<ul>\n<li>Using the API:<\/li>\n<\/ul>\n<pre><code>import wandb\nwandb.login(relogin=True)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71075704",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1644559750243,
        "Question_original_content":"login acount colab exampl acount log googl colab account want log acount login possibl",
        "Question_preprocessed_content":"login acount colab exampl acount log googl colab account want log acount possibl",
        "Question_gpt_summary_original":"The user is trying to log in to Wandb with a different account while using Google Colab. They are currently logged in with account A and want to log in with account B using the command \"!wandb login\". The user is seeking clarification on whether this is possible.",
        "Question_gpt_summary":"user try log differ account googl colab current log account want log account command login user seek clarif possibl",
        "Answer_original_content":"follow command forc relogin termin login relogin api import login relogin true",
        "Answer_preprocessed_content":"follow command forc relogin termin api",
        "Answer_gpt_summary_original":"Solution: The user can force a relogin to Wandb with a different account using the command \"!wandb login --relogin\" from the terminal or \"wandb.login(relogin=True)\" using the API.",
        "Answer_gpt_summary":"solut user forc relogin differ account command login relogin termin login relogin true api"
    },
    {
        "Question_title":"Sagemaker Serverless Inference & custom container: Model archiver subprocess fails",
        "Question_body":"<p>I would like to host a model on Sagemaker using the new <a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/12\/amazon-sagemaker-serverless-inference\/?nc1=h_ls\" rel=\"nofollow noreferrer\">Serverless Inference<\/a>.<\/p>\n<p>I wrote my own container for inference and handler following several guides. These are the requirements:<\/p>\n<pre><code>mxnet\nmulti-model-server\nsagemaker-inference\nretrying\nnltk\ntransformers==4.12.4\ntorch==1.10.0\n<\/code><\/pre>\n<p>On non-serverless endpoints, this container works perfectly well. However, with the serverless version I get the following error message when loading the model:<\/p>\n<pre><code>ERROR - \/.sagemaker\/mms\/models\/model already exists.\n<\/code><\/pre>\n<p>The error is thrown by the following subprocess<\/p>\n<pre><code>['model-archiver', '--model-name', 'model', '--handler', '\/home\/model-server\/handler_service.py:handle', '--model-path', '\/opt\/ml\/model', '--export-path', '\/.sagemaker\/mms\/models', '--archive-format', 'no-archive']\n<\/code><\/pre>\n<p>So something that has to do with the <code>model-archiver<\/code> (which I guess is a process from the MMS package?).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1639400254370,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":373.0,
        "Answer_body":"<p>So the issue really was related to hosting the model using the sagemaker inference toolkit and MMS which always uses the multi-model scenario which is not supported by serverless inference.<\/p>\n<p>I ended up writing my own Flask API which actually is nearly as easy and more customizable. Ping me for details if you're interested.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70335049",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1639671069487,
        "Question_original_content":"serverless infer custom contain model archiv subprocess fail like host model new serverless infer wrote contain infer handler follow guid requir mxnet multi model server infer retri nltk transform torch non serverless endpoint contain work perfectli serverless version follow error messag load model error mm model model exist error thrown follow subprocess model archiv model model handler home model server handler servic handl model path opt model export path mm model archiv format archiv model archiv guess process mm packag",
        "Question_preprocessed_content":"serverless infer custom contain model archiv subprocess fail like host model new serverless infer wrote contain infer handler follow guid requir endpoint contain work perfectli serverless version follow error messag load model error thrown follow subprocess",
        "Question_gpt_summary_original":"The user is encountering an error when trying to host a model on Sagemaker using the new Serverless Inference. They have written their own container for inference and handler, but when loading the model, they receive an error message stating that the model already exists. The error is thrown by the subprocess 'model-archiver', which is part of the MMS package.",
        "Question_gpt_summary":"user encount error try host model new serverless infer written contain infer handler load model receiv error messag state model exist error thrown subprocess model archiv mm packag",
        "Answer_original_content":"issu relat host model infer toolkit mm us multi model scenario support serverless infer end write flask api actual nearli easi customiz ping detail interest",
        "Answer_preprocessed_content":"issu relat host model infer toolkit mm us scenario support serverless infer end write flask api actual nearli easi customiz ping detail interest",
        "Answer_gpt_summary_original":"Solution: The user encountered an error when trying to host a model on Sagemaker using the new Serverless Inference. They have written their own container for inference and handler, but when loading the model, they receive an error message stating that the model already exists. One solution mentioned in the discussion is to write a custom Flask API, which is nearly as easy and more customizable than using the Sagemaker inference toolkit and MMS. However, no details were provided on how to write the custom Flask API.",
        "Answer_gpt_summary":"solut user encount error try host model new serverless infer written contain infer handler load model receiv error messag state model exist solut mention discuss write custom flask api nearli easi customiz infer toolkit mm detail provid write custom flask api"
    },
    {
        "Question_title":"Different run sets within a panel grid",
        "Question_body":"<p>Hello, I want to create a grid panel containing several linecharts but the selected runs are different.<br>\nTo elaborate on my need: I have several algorithms, evaluated across timesteps on several environments. I want one line chart per environment. To illustrate, my final requirement is to get something that looks like this:<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/f\/f2059dafd12562e424e2b66a3e2aabb2b4e1b204.png\" alt=\"Screenshot from 2023-01-19 13-12-26\" data-base62-sha1=\"yx1zXviDcru4wcBIHNfwKEhCyFK\" width=\"435\" height=\"213\"><\/p>\n<p>How would you do that?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1674130582264,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":126.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/qgallouedec\">@qgallouedec<\/a>, thanks for writing in! As you\u2019d like to have a figure with independent charts inside, one option would be to use <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/custom-charts\">custom charts<\/a>. I let you <a href=\"https:\/\/vega.github.io\/vega\/examples\/barley-trellis-plot\/\" rel=\"noopener nofollow ugc\">here<\/a> and <a href=\"https:\/\/vega.github.io\/vega\/examples\/brushing-scatter-plots\/\" rel=\"noopener nofollow ugc\">here<\/a> two Vega examples that may be useful in order to build your figure. Other way I can think of for this is rendering the chart through <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log\/plots#matplotlib-and-plotly-plots\">Plotly\/Matplotlib<\/a> and then log it. Please let me know if any of these would be useful!<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/different-run-sets-within-a-panel-grid\/3721",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-20T18:42:33.197Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/qgallouedec\">@qgallouedec<\/a>, thanks for writing in! As you\u2019d like to have a figure with independent charts inside, one option would be to use <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/custom-charts\">custom charts<\/a>. I let you <a href=\"https:\/\/vega.github.io\/vega\/examples\/barley-trellis-plot\/\" rel=\"noopener nofollow ugc\">here<\/a> and <a href=\"https:\/\/vega.github.io\/vega\/examples\/brushing-scatter-plots\/\" rel=\"noopener nofollow ugc\">here<\/a> two Vega examples that may be useful in order to build your figure. Other way I can think of for this is rendering the chart through <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log\/plots#matplotlib-and-plotly-plots\">Plotly\/Matplotlib<\/a> and then log it. Please let me know if any of these would be useful!<\/p>",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2023-01-26T11:04:05.664Z",
                "Answer_body":"<p>Hi Quentin,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-26T17:13:06.886Z",
                "Answer_body":"<p>Thanks very much for your answer <a class=\"mention\" href=\"\/u\/luis_bergua1\">@luis_bergua1<\/a>! Indeed, that\u2019s what I was looking for. With a reasonable effort, the custom chat can do what I want. However, I\u2019m not a big fan of the integration with Vega, the rendering is not the same as with Wandb charts, and we don\u2019t have access to the same options (or at least, not in the same way). I think it\u2019s a pity that we can\u2019t dissociate the runset within a panel grid , it would allow me to do exactly that, with lower effort and taking advantage of all the nice wandb options.<br>\nBest,<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-31T13:23:07.648Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/qgallouedec\">@qgallouedec<\/a>, thanks for your answer! Happy to hear that the custom chart would work for you. I\u2019ll also share your feedback about having this available without using Vega with our Team. Thanks!<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1674240153196,
        "Question_original_content":"differ run set panel grid hello want creat grid panel contain linechart select run differ elabor need algorithm evalu timestep environ want line chart environ illustr final requir look like",
        "Question_preprocessed_content":"differ run set panel grid hello want creat grid panel contain linechart select run differ elabor need algorithm evalu timestep environ want line chart environ illustr final requir look like",
        "Question_gpt_summary_original":"The user wants to create a grid panel with several line charts, but the selected runs are different. The user has several algorithms evaluated across timesteps on several environments and wants one line chart per environment. The user needs help to achieve this requirement.",
        "Question_gpt_summary":"user want creat grid panel line chart select run differ user algorithm evalu timestep environ want line chart environ user need help achiev requir",
        "Answer_original_content":"qgallouedec thank write youd like figur independ chart insid option us custom chart let vega exampl us order build figur wai think render chart plotli matplotlib log let know us",
        "Answer_preprocessed_content":"thank write youd like figur independ chart insid option us custom chart let vega exampl us order build figur wai think render chart log let know us",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Using custom charts to create a figure with independent charts inside.\n2. Rendering the chart through Plotly\/Matplotlib and then logging it.\n\nBoth solutions are suggested to help the user achieve the requirement of having one line chart per environment.",
        "Answer_gpt_summary":"possibl solut mention discuss custom chart creat figur independ chart insid render chart plotli matplotlib log solut suggest help user achiev requir have line chart environ"
    },
    {
        "Question_title":"Vertex AI - how to monitor training progress?",
        "Question_body":"<h1>Question<\/h1>\n<p>Is there a way to monitor the console output of model training progress during the Vertex AI training?<\/p>\n<h2>Background<\/h2>\n<p>Suppose we have a Tensorflow\/Keras model training code:<\/p>\n<pre><code>model = keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n])\n\noptimizer = tf.keras.optimizers.RMSprop(0.001)\nmodel.compile(\n    loss='mse',\n    optimizer=optimizer,\n    metrics=['mae', 'mse']\n)\n\nEPOCHS = 1000\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\nearly_history = model.fit(normed_train_data, train_labels, \n                    epochs=EPOCHS, validation_split = 0.2, \n                    callbacks=[early_stop])\n<\/code><\/pre>\n<p>When run the model training from the command line, we can see the progress in the console.<\/p>\n<pre><code>Epoch 1\/1000\nOMP: Info #211: KMP_AFFINITY: decoding x2APIC ids.\nOMP: Info #209: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\nOMP: Info #156: KMP_AFFINITY: 4 available OS procs\nOMP: Info #157: KMP_AFFINITY: Uniform topology\nOMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores\/pkg x 2 threads\/core (2 total cores)\nOMP: Info #213: KMP_AFFINITY: OS proc to physical thread map:\nOMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \nOMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \nOMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \nOMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \nOMP: Info #249: KMP_AFFINITY: pid 1 tid 17 thread 0 bound to OS proc set 0\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 17 thread 1 bound to OS proc set 1\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 28 thread 2 bound to OS proc set 2\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 29 thread 3 bound to OS proc set 3\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 30 thread 4 bound to OS proc set 0\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 18 thread 5 bound to OS proc set 1\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 31 thread 6 bound to OS proc set 2\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 32 thread 7 bound to OS proc set 3\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 33 thread 8 bound to OS proc set 0\n8\/8 [==============================] - 2s 31ms\/step - loss: 579.6393 - mae: 22.7661 - mse: 579.6393 - val_loss: 571.7239 - val_mae: 22.5494 - val_mse: 571.7239\nEpoch 2\/1000\n8\/8 [==============================] - 0s 7ms\/step - loss: 527.9056 - mae: 21.6268 - mse: 527.9056 - val_loss: 520.5531 - val_mae: 21.3917 - val_mse: 520.5531\n...\n<\/code><\/pre>\n<p>However, if we run the training in the Vertex AI training, there looks to be no menu\/option to see the console output. Not sure if it is logged in Log Explorer. Please help understand how to monitor the training progress realtime.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/tbaBD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tbaBD.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651193148017,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":339.0,
        "Answer_body":"<p>You may view training logs in the <strong>GCP Logs Explorer<\/strong> by using below query.<\/p>\n<pre><code>resource.type=&quot;ml_job&quot;\nresource.labels.job_id=&quot;your-training-custom-job-ID&quot;\n<\/code><\/pre>\n<p>The <em><strong>your-training-custom-job-ID<\/strong><\/em> can be found on the ongoing Vertex AI Training in GCP console as seen on the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ks4Yf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ks4Yf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Below is the screenshot of the logs for the Vertex AI training in GCP logs explorer using the above query.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/waHu6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/waHu6.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You may click on <strong>Jump to now<\/strong> to immediately view the latest logs. Also, you may use <strong>Stream Logs<\/strong> option to view <strong>REAL TIME<\/strong> log data which you can also adjust the buffer window in which has certain trade offs. You may refer to this <a href=\"https:\/\/cloud.google.com\/logging\/docs\/view\/streaming-live-tailing\" rel=\"nofollow noreferrer\">documentation<\/a> for more information on streaming logs in GCP logs explorer.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1651563481836,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72051655",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1651562908648,
        "Question_original_content":"monitor train progress question wai monitor consol output model train progress train background suppos tensorflow kera model train code model kera sequenti layer dens activ relu input shape len train dataset kei layer dens activ relu layer dens optim kera optim rmsprop model compil loss mse optim optim metric mae mse epoch earli stop kera callback earlystop monitor val loss patienc earli histori model fit norm train data train label epoch epoch valid split callback earli stop run model train command line progress consol epoch omp info kmp affin decod xapic id omp info kmp affin affin capabl global cpuid leaf info omp info kmp affin initi proc set respect omp info kmp affin avail proc omp info kmp affin uniform topolog omp info kmp affin packag core pkg thread core total core omp info kmp affin proc physic thread map omp info kmp affin proc map packag core thread omp info kmp affin proc map packag core thread omp info kmp affin proc map packag core thread omp info kmp affin proc map packag core thread omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set step loss mae mse val loss val mae val mse epoch step loss mae mse val loss val mae val mse run train train look menu option consol output sure log log explor help understand monitor train progress realtim",
        "Question_preprocessed_content":"monitor train progress question wai monitor consol output model train progress train background suppos model train code run model train command line progress consol run train train look consol output sure log log explor help understand monitor train progress realtim",
        "Question_gpt_summary_original":"The user is facing a challenge in monitoring the console output of model training progress during the Vertex AI training. They are unsure if the output is logged in Log Explorer and are seeking help to understand how to monitor the training progress in real-time.",
        "Question_gpt_summary":"user face challeng monitor consol output model train progress train unsur output log log explor seek help understand monitor train progress real time",
        "Answer_original_content":"view train log gcp log explor queri resourc type job resourc label job train custom job train custom job ongo train gcp consol seen screenshot screenshot log train gcp log explor queri click jump immedi view latest log us stream log option view real time log data adjust buffer window certain trade off refer document inform stream log gcp log explor",
        "Answer_preprocessed_content":"view train log gcp log explor queri ongo train gcp consol seen screenshot screenshot log train gcp log explor queri click jump immedi view latest log us stream log option view real time log data adjust buffer window certain trade off refer document inform stream log gcp log explor",
        "Answer_gpt_summary_original":"Solution: The user can monitor the console output of model training progress during the Vertex AI training by viewing the training logs in the GCP Logs Explorer using a specific query. The query is \"resource.type=ml_job resource.labels.job_id=your-training-custom-job-ID\" where \"your-training-custom-job-ID\" can be found on the ongoing Vertex AI Training in GCP console. The user can click on \"Jump to now\" to immediately view the latest logs or use the \"Stream Logs\" option to view real-time log data. The user can adjust the buffer window, but it has certain trade-offs. The documentation on streaming logs in GCP logs explorer can be referred to for more information.",
        "Answer_gpt_summary":"solut user monitor consol output model train progress train view train log gcp log explor specif queri queri resourc type job resourc label job train custom job train custom job ongo train gcp consol user click jump immedi view latest log us stream log option view real time log data user adjust buffer window certain trade off document stream log gcp log explor refer inform"
    },
    {
        "Question_title":"AWS Sagemaker - ClientError: Data download failed:Could not download",
        "Question_body":"<p>I encountered and error when I deploy my training job in my notebook instance.\nThis what it says:\n<code>&quot;UnexpectedStatusException: Error for Training job tensorflow-training-2021-01-26-09-55-05-768: Failed. Reason: ClientError: Data download failed:Could not download s3:\/\/forex-model-data\/data\/train2001_2020.npz: insufficient disk space&quot;<\/code><\/p>\n<p>I deploy training job to try running it to different instances in 3 epoch. I use ml.c5.4xlarge, ml.c5.18xlarge, ml.m5.24xlarge, also I have two sets of training data, train2001_2020.npz and train2016_2020.npz.<\/p>\n<p>First, I run train2001_2020 to ml.c5.18xlarge and ml.c5.18xlarge and the training job completed, then I switch to train2016_2020 and run it to ml.c5.4xlarge and ml.c5.18xlarge and it goes well. Then when I tried to run it using ml.m5.24xlarge I got an error (quoted above), but my dataset is train2016_2020 not train2001_2020 then when I rerun it again with all other instances it has the same error. What happen?<\/p>\n<p>I stopped the instances and refresh everything, but I encountered same issue.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611668079743,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":774.0,
        "Answer_body":"<p>It's not really clear to all the test are you doing, but that error usually means that there is not enough disk space on the instance you are using for the training job. You can try to increase the additional storage for the instance (you can do in the estimator parameters if you are using the sagemaker SDK in a notebook).<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65902366",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1611831268823,
        "Question_original_content":"clienterror data download fail download encount error deploi train job notebook instanc sai unexpectedstatusexcept error train job tensorflow train fail reason clienterror data download fail download forex model data data train npz insuffici disk space deploi train job try run differ instanc epoch us xlarg xlarg xlarg set train data train npz train npz run train xlarg xlarg train job complet switch train run xlarg xlarg goe tri run xlarg got error quot dataset train train rerun instanc error happen stop instanc refresh encount issu",
        "Question_preprocessed_content":"clienterror data download fail download encount error deploi train job notebook instanc sai deploi train job try run differ instanc epoch us set train data run train job complet switch run goe tri run got error dataset rerun instanc error happen stop instanc refresh encount issu",
        "Question_gpt_summary_original":"The user encountered an error when deploying a training job in their notebook instance on AWS Sagemaker. The error message indicated that there was insufficient disk space to download the training data from an S3 bucket. The user attempted to run the training job on different instances with different sets of training data, and while some instances completed the job successfully, others encountered the same error. The user tried refreshing everything and restarting the instances, but the issue persisted.",
        "Question_gpt_summary":"user encount error deploi train job notebook instanc error messag indic insuffici disk space download train data bucket user attempt run train job differ instanc differ set train data instanc complet job successfulli encount error user tri refresh restart instanc issu persist",
        "Answer_original_content":"clear test error usual mean disk space instanc train job try increas addit storag instanc estim paramet sdk notebook",
        "Answer_preprocessed_content":"clear test error usual mean disk space instanc train job try increas addit storag instanc",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to increase the additional storage for the instance being used for the training job. This can be done in the estimator parameters if the user is using the Sagemaker SDK in a notebook.",
        "Answer_gpt_summary":"solut possibl solut mention discuss increas addit storag instanc train job estim paramet user sdk notebook"
    },
    {
        "Question_title":"Get Experiment that Created Model in MLflow",
        "Question_body":"<p>I want to get the name of the experiment that contains the run that created a registered MLflow model. How can I do this using MLflow, if I just have the name of the model and the version?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1658855429667,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":44.0,
        "Answer_body":"<p>As @Andre has said, I had to write my own function to achieve this,<\/p>\n<pre><code>def get_model_experiment(model_name, model_version):\n    # get run_id of the model version\n    run_id = mlflow_client.get_model_version(model_name, model_version).run_id\n\n    # get the experiment_id from the run_id\n    experiment_id = mlflow_client.get_run(run_id).info.experiment_id\n\n    # get the experiment name from the experiment_id\n    return mlflow_client.get_experiment(experiment_id).name\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73127303",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1659073607990,
        "Question_original_content":"experi creat model want experi contain run creat regist model model version",
        "Question_preprocessed_content":"experi creat model want experi contain run creat regist model model version",
        "Question_gpt_summary_original":"The user is facing a challenge in identifying the name of the experiment that contains the run responsible for creating a registered MLflow model. The user has only the name and version of the model and is seeking guidance on how to retrieve this information using MLflow.",
        "Question_gpt_summary":"user face challeng identifi experi contain run respons creat regist model user version model seek guidanc retriev inform",
        "Answer_original_content":"andr said write function achiev def model experi model model version run model version run client model version model model version run experi run experi client run run info experi experi experi return client experi experi",
        "Answer_preprocessed_content":"said write function achiev",
        "Answer_gpt_summary_original":"Solution: One solution mentioned in the discussion is to write a function that retrieves the experiment name containing the run responsible for creating a registered MLflow model. The function uses the MLflow client to get the run ID of the model version, then gets the experiment ID from the run ID, and finally retrieves the experiment name from the experiment ID.",
        "Answer_gpt_summary":"solut solut mention discuss write function retriev experi contain run respons creat regist model function us client run model version get experi run final retriev experi experi"
    },
    {
        "Question_title":"Is there any equivalent of hyperopts lognormal in Optuna?",
        "Question_body":"<p>I am trying to use Optuna for hyperparameter tuning of my model.<\/p>\n<p>I am stuck in a place where I want to define a search space having lognormal\/normal distribution. It is possible in <code>hyperopt<\/code> using <code>hp.lognormal<\/code>. Is it possible to define such a space using a combination of the existing <code>suggest_<\/code> api of <code>Optuna<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1610971789293,
        "Question_favorite_count":null,
        "Question_last_edit_time":1610981620636,
        "Question_score":0.0,
        "Question_view_count":140.0,
        "Answer_body":"<p>You could perhaps make use of inverse transforms from <code>suggest_float(..., 0, 1)<\/code> (i.e. U(0, 1)) since Optuna currently doesn't provide <code>suggest_<\/code> variants for those two distributions directly. This example might be a starting point <a href=\"https:\/\/gist.github.com\/hvy\/4ef02ee2945fe50718c71953e1d6381d\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/hvy\/4ef02ee2945fe50718c71953e1d6381d<\/a>\nPlease find the code below<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import erfcinv\n\nimport optuna\n\n\ndef objective(trial):\n    # Suggest from U(0, 1) with Optuna.\n    x = trial.suggest_float(&quot;x&quot;, 0, 1)\n\n    # Inverse transform into normal.\n    y0 = norm.ppf(x, loc=0, scale=1)\n\n    # Inverse transform into lognormal.\n    y1 = np.exp(-np.sqrt(2) * erfcinv(2 * x))\n\n    return y0, y1\n\n\nif __name__ == &quot;__main__&quot;:\n    n_objectives = 2  # Normal and lognormal.\n\n    study = optuna.create_study(\n        sampler=optuna.samplers.RandomSampler(),\n        # Could be &quot;maximize&quot;. Does not matter for this demonstration.\n        directions=[&quot;minimize&quot;] * n_objectives,\n    )\n    study.optimize(objective, n_trials=10000)\n\n    fig, axs = plt.subplots(n_objectives)\n    for i in range(n_objectives):\n        axs[i].hist(list(t.values[i] for t in study.trials), bins=100)\n    plt.show()\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1611565726960,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65774253",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1611382397500,
        "Question_original_content":"equival hyperopt lognorm try us hyperparamet tune model stuck place want defin search space have lognorm normal distribut possibl hyperopt lognorm possibl defin space combin exist suggest api",
        "Question_preprocessed_content":"equival hyperopt lognorm try us hyperparamet tune model stuck place want defin search space have distribut possibl possibl defin space combin exist api",
        "Question_gpt_summary_original":"The user is facing a challenge in defining a search space with lognormal\/normal distribution while using Optuna for hyperparameter tuning of their model. They are seeking to know if it is possible to define such a space using a combination of the existing suggest_ api of Optuna.",
        "Question_gpt_summary":"user face challeng defin search space lognorm normal distribut hyperparamet tune model seek know possibl defin space combin exist suggest api",
        "Answer_original_content":"us invers transform suggest float current provid suggest variant distribut directli exampl start point http gist github com hvy efeefecedd code import matplotlib pyplot plt import numpi scipi stat import norm scipi special import erfcinv import def object trial suggest trial suggest float invers transform normal norm ppf loc scale invers transform lognorm exp sqrt erfcinv return main object normal lognorm studi creat studi sampler sampler randomsampl maxim matter demonstr direct minim object studi optim object trial fig ax plt subplot object rang object ax hist list valu studi trial bin plt",
        "Answer_preprocessed_content":"us invers transform current provid variant distribut directli exampl start point code",
        "Answer_gpt_summary_original":"Solution:\nThe user can make use of inverse transforms from `suggest_float(..., 0, 1)` to define a search space with lognormal\/normal distribution since Optuna currently doesn't provide `suggest_` variants for those two distributions directly. The code example provided in the discussion can be used as a starting point.",
        "Answer_gpt_summary":"solut user us invers transform suggest float defin search space lognorm normal distribut current provid suggest variant distribut directli code exampl provid discuss start point"
    },
    {
        "Question_title":"Failure of experiment when using base_dockerfile instead of base_image",
        "Question_body":"<p>I am attempting to submit an experiment to the Azure Machine Learning Service using a custom docker image.  Everything works ok when I provide the docker image, but fails if I choose to provide a dockerfile.<\/p>\n\n<p>The use of a base_dockerfile in the DockerSection object is documented <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment.dockersection?view=azure-ml-py\" rel=\"nofollow noreferrer\">here<\/a> and was added in v1.0.53 of the sdk (as noted <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/service\/azure-machine-learning-release-notes.md\" rel=\"nofollow noreferrer\">here<\/a>)<\/p>\n\n<p>Example code:<\/p>\n\n<pre><code>ds = DockerSection()\nds.enabled = True\nds.base_dockerfile = \"FROM ubuntu:latest RUN echo 'Hello world!'\"\nds.base_image = None\n<\/code><\/pre>\n\n<p>The rest of the code is the same as when running with a predefined image from the registry (e.g. setting base_image in the above code).<\/p>\n\n<p>Example error from ML service is:<\/p>\n\n<blockquote>\n  <p>raise ActivityFailedException(error_details=json.dumps(error,\n  indent=4))\n  azureml.exceptions._azureml_exception.ActivityFailedException:\n  ActivityFailedException:\n          Message: Activity Failed: {\n      \"error\": {\n          \"code\": \"ServiceError\",\n          \"message\": \"InternalServerError\",\n          \"details\": []\n      },\n      \"correlation\": {\n          \"operation\": null,\n          \"request\": \"K\/C4FSnEz74=\"\n      },\n      \"environment\": \"southcentralus\",\n      \"location\": \"southcentralus\",\n      \"time\": \"2019-08-20T16:33:17.130928Z\" }\n          InnerException None\n          ErrorResponse {\"error\": {\"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"ServiceError\\\",\\n<br>\n  \\\"message\\\": \\\"InternalServerError\\\",\\n        \\\"details\\\": []\\n<br>\n  },\\n    \\\"correlation\\\": {\\n        \\\"operation\\\": null,\\n<br>\n  \\\"request\\\": \\\"K\/C4FSnEz74=\\\"\\n  },\\n    \\\"environment\\\":\n  \\\"southcentralus\\\",\\n    \\\"location\\\": \\\"southcentralus\\\",\\n<br>\n  \\\"time\\\": \\\"2019-08-20T16:33:17.130928Z\\\"\\n}\"}}<\/p>\n<\/blockquote>\n\n<p>I've used an example dockerfile in the code above (taken from the SDK documentation) but get the same error if I use the dockerfile that created the base image that works ok from the registry.<\/p>\n\n<p>Any ideas - or pointers to samples where this actually works - appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1566320034643,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":307.0,
        "Answer_body":"<p>Thanks for reporting this issue! This appears to be a bug that our team is investigating.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57578332",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1566323058620,
        "Question_original_content":"failur experi base dockerfil instead base imag attempt submit experi servic custom docker imag work provid docker imag fail choos provid dockerfil us base dockerfil dockersect object document ad sdk note exampl code dockersect enabl true base dockerfil ubuntu latest run echo hello world base imag rest code run predefin imag registri set base imag code exampl error servic rais activityfailedexcept error detail json dump error indent except except activityfailedexcept activityfailedexcept messag activ fail error code serviceerror messag internalservererror detail correl oper null request cfsnez environ southcentralu locat southcentralu time innerexcept errorrespons error messag activ fail error code serviceerror messag internalservererror detail correl oper null request cfsnez environ southcentralu locat southcentralu time exampl dockerfil code taken sdk document error us dockerfil creat base imag work registri idea pointer sampl actual work appreci",
        "Question_preprocessed_content":"failur experi instead attempt submit experi servic custom docker imag work provid docker imag fail choos provid dockerfil us dockersect object document ad sdk exampl code rest code run predefin imag registri exampl error servic rais indent activityfailedexcept messag activ fail correl environ southcentralu locat southcentralu time innerexcept errorrespons exampl dockerfil code error us dockerfil creat base imag work registri idea pointer sampl actual work appreci",
        "Question_gpt_summary_original":"The user is encountering challenges when attempting to submit an experiment to the Azure Machine Learning Service using a custom docker image. The experiment fails when using a base_dockerfile instead of a base_image. The error message received is an ActivityFailedException with an InternalServerError. The user has tried using an example dockerfile from the SDK documentation but still encounters the same error. The user is seeking help or pointers to samples where this issue has been resolved.",
        "Question_gpt_summary":"user encount challeng attempt submit experi servic custom docker imag experi fail base dockerfil instead base imag error messag receiv activityfailedexcept internalservererror user tri exampl dockerfil sdk document encount error user seek help pointer sampl issu resolv",
        "Answer_original_content":"thank report issu appear bug team investig",
        "Answer_preprocessed_content":"thank report issu appear bug team investig",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Endpoints for getting metadata about published models?",
        "Question_body":"My question centers around working with AML models that have been published as web services, as described here:\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service?tabs=azure-portal\n\nAre there any endpoints or ways of obtaining more detailed information about a published model? For example, the documentation states that inputs to the model are passed in via a \"data\" property, and obviously, this will vary my the model:\n\n{\n\"data\":\n[\n<model-specific-data-structure>\n]\n}\n\nIs there a way to programatically find out what the model expects as input?\n\nThe full 'wish-list' of metadata info we'd like is listed here:\n\nWhat models are available for serving\n\n\nWhat is the model prediction endpoint\n\n\nWhat are the required inputs and their data types\n\n\nWhat are the model outputs and data types\n\nAre there any endpoints or any way at getting to this information?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649425228733,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @MKRP-6344\n\nThanks for reaching out to us, I will answer your question below, at the meantime, if you feel like I am not getting your point well, please point it out and correct me.\n\nI think you are mentioning how to monitor published model and collect data, there are several choice depends on the data you want to collect:\n\nCollect data from models in production - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-data-collection\n\nThe following data can be collected:\n\nModel input data from web services deployed in an AKS cluster. Voice audio, images, and video are not collected.\nModel predictions using production input data.\n\nOnce collection is enabled, the data you collect helps you:\n\nMonitor data drifts on the production data you collect.\nAnalyze collected data using Power BI or Azure Databricks\nMake better decisions about when to retrain or optimize your model.\nRetrain your model with the collected data.\n\n2 . Monitor and collect data from ML web service endpoints - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-app-insights\n\nYou can use Azure Application Insights to collect the following data from an endpoint:\n\nOutput data\nResponses\nRequest rates, response times, and failure rates\nDependency rates, response times, and failure rates\nExceptions\n\n3 . More details from Data Drift - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-monitor-datasets?tabs=python\n\nWith Azure Machine Learning dataset monitors (preview), you can:\n\nAnalyze drift in your data to understand how it changes over time.\nMonitor model data for differences between training and serving datasets. Start by collecting model data from deployed models.\nMonitor new data for differences between any baseline and target dataset.\nProfile features in data to track how statistical properties change over time.\nSet up alerts on data drift for early warnings to potential issues.\nCreate a new dataset version when you determine the data has drifted too much.\n\nHope above information helps, please let us know if you need further helps!\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful, thanks a lot.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/805976\/endpoints-for-getting-metadata-about-published-mod.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-10T22:33:36.347Z",
                "Answer_score":0,
                "Answer_body":"Hello @MKRP-6344\n\nThanks for reaching out to us, I will answer your question below, at the meantime, if you feel like I am not getting your point well, please point it out and correct me.\n\nI think you are mentioning how to monitor published model and collect data, there are several choice depends on the data you want to collect:\n\nCollect data from models in production - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-data-collection\n\nThe following data can be collected:\n\nModel input data from web services deployed in an AKS cluster. Voice audio, images, and video are not collected.\nModel predictions using production input data.\n\nOnce collection is enabled, the data you collect helps you:\n\nMonitor data drifts on the production data you collect.\nAnalyze collected data using Power BI or Azure Databricks\nMake better decisions about when to retrain or optimize your model.\nRetrain your model with the collected data.\n\n2 . Monitor and collect data from ML web service endpoints - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-app-insights\n\nYou can use Azure Application Insights to collect the following data from an endpoint:\n\nOutput data\nResponses\nRequest rates, response times, and failure rates\nDependency rates, response times, and failure rates\nExceptions\n\n3 . More details from Data Drift - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-monitor-datasets?tabs=python\n\nWith Azure Machine Learning dataset monitors (preview), you can:\n\nAnalyze drift in your data to understand how it changes over time.\nMonitor model data for differences between training and serving datasets. Start by collecting model data from deployed models.\nMonitor new data for differences between any baseline and target dataset.\nProfile features in data to track how statistical properties change over time.\nSet up alerts on data drift for early warnings to potential issues.\nCreate a new dataset version when you determine the data has drifted too much.\n\nHope above information helps, please let us know if you need further helps!\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful, thanks a lot.",
                "Answer_comment_count":6,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1649630016347,
        "Question_original_content":"endpoint get metadata publish model question center work aml model publish web servic describ http doc microsoft com azur machin learn consum web servic tab azur portal endpoint wai obtain detail inform publish model exampl document state input model pass data properti obvious vari model data wai programat model expect input wish list metadata info like list model avail serv model predict endpoint requir input data type model output data type endpoint wai get inform",
        "Question_preprocessed_content":"endpoint get metadata publish model question center work aml model publish web servic describ endpoint wai obtain detail inform publish model exampl document state input model pass data properti obvious vari model data wai programat model expect input metadata info like list model avail serv model predict endpoint requir input data type model output data type endpoint wai get inform",
        "Question_gpt_summary_original":"The user is seeking information on how to obtain more detailed metadata about published AML models, including the model's available inputs and outputs, required data types, and prediction endpoint. They are looking for a way to programmatically find out what the model expects as input.",
        "Question_gpt_summary":"user seek inform obtain detail metadata publish aml model includ model avail input output requir data type predict endpoint look wai programmat model expect input",
        "Answer_original_content":"hello mkrp thank reach answer question meantim feel like get point point correct think mention monitor publish model collect data choic depend data want collect collect data model product http doc microsoft com azur machin learn enabl data collect follow data collect model input data web servic deploi ak cluster voic audio imag video collect model predict product input data collect enabl data collect help monitor data drift product data collect analyz collect data power azur databrick better decis retrain optim model retrain model collect data monitor collect data web servic endpoint http doc microsoft com azur machin learn enabl app insight us azur applic insight collect follow data endpoint output data respons request rate respons time failur rate depend rate respons time failur rate except detail data drift http doc microsoft com azur machin learn monitor dataset tab python dataset monitor preview analyz drift data understand chang time monitor model data differ train serv dataset start collect model data deploi model monitor new data differ baselin target dataset profil featur data track statist properti chang time set alert data drift earli warn potenti issu creat new dataset version determin data drift hope inform help let know need help regard yutong kindli accept answer feel help thank lot",
        "Answer_preprocessed_content":"hello thank reach answer question meantim feel like get point point correct think mention monitor publish model collect data choic depend data want collect collect data model product follow data collect model input data web servic deploi ak cluster voic audio imag video collect model predict product input data collect enabl data collect help monitor data drift product data collect analyz collect data power azur databrick better decis retrain optim model retrain model collect data monitor collect data web servic endpoint us azur applic insight collect follow data endpoint output data respons request rate respons time failur rate depend rate respons time failur rate except detail data drift dataset monitor analyz drift data understand chang time monitor model data differ train serv dataset start collect model data deploi model monitor new data differ baselin target dataset profil featur data track statist properti chang time set alert data drift earli warn potenti issu creat new dataset version determin data drift hope inform help let know need help regard yutong kindli accept answer feel help thank lot",
        "Answer_gpt_summary_original":"The discussion provides three possible solutions to obtain more detailed metadata about published AML models. The first solution is to collect data from models in production, which includes model input data and predictions. The second solution is to monitor and collect data from ML web service endpoints, which includes output data, responses, request rates, response times, and failure rates. The third solution is to use Azure Machine Learning dataset monitors to analyze drift in data, monitor model data for differences between training and serving datasets, profile features in data, set up alerts on data drift, and create a new dataset version when necessary.",
        "Answer_gpt_summary":"discuss provid possibl solut obtain detail metadata publish aml model solut collect data model product includ model input data predict second solut monitor collect data web servic endpoint includ output data respons request rate respons time failur rate solut us dataset monitor analyz drift data monitor model data differ train serv dataset profil featur data set alert data drift creat new dataset version necessari"
    },
    {
        "Question_title":"Parameterized SQL query in Azure ML",
        "Question_body":"<p>Background: There seems to be a way to parameterize <code>DataPath<\/code> with <code>PipelineParameter<\/code>\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb<\/a><\/p>\n<p>But I'd like to parameterize my SQL query with PipelineParameter, for example, with this query<\/p>\n<pre><code>sql_query = &quot;&quot;&quot;\nSELECT id, foo, bar FROM baz\nWHERE baz.id BETWEEN 10 AND 20\n&quot;&quot;&quot;\ndataset = Dataset.Tabular.from_sql_query((sql_datastore, sql_query))\n<\/code><\/pre>\n<p>I'd like to use PipelineParameter to parameterize <code>10<\/code> and <code>20<\/code> as <code>param_1<\/code> and <code>param_2<\/code>. Is this possible?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603497171040,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":188.0,
        "Answer_body":"<p>Found a way to do this:<\/p>\n<p>Pass your params to PythonScriptStep<\/p>\n<pre><code>param_1 = PipelineParameter(name='min_id', default_value=5)\nparam_2 = PipelineParameter(name='max_id', default_value=10)\nsql_datastore = &quot;sql_datastore&quot;\nstep = PythonScriptStep(script_name='script.py', arguments=[param_1, param_2, \nsql_datastore])\n<\/code><\/pre>\n<p>In script.py<\/p>\n<pre><code>min_id_param = sys.argv[1]\nmax_id_param = sys.argv[2]\nsql_datastore_name = sys.argv[3]\nquery = &quot;&quot;&quot;\nSELECT id, foo, bar FROM baz\nWHERE baz.id BETWEEN {} AND {}\n&quot;&quot;&quot;.format(min_id_param, max_id_param)\nrun = Run.get_context()\nsql_datastore = Datastore.get(run.experiment.workspace, sql_datastore_name)\ndataset = Dataset.Tabular.from_sql_query((sql_datastore, query))\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1603755560267,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64508625",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1603727871847,
        "Question_original_content":"parameter sql queri background wai parameter datapath pipelineparamet http github com azur machinelearningnotebook blob master us machin learn pipelin intro pipelin aml pipelin showcas datapath pipelineparamet ipynb like parameter sql queri pipelineparamet exampl queri sql queri select foo bar baz baz dataset dataset tabular sql queri sql datastor sql queri like us pipelineparamet parameter param param possibl",
        "Question_preprocessed_content":"parameter sql queri background wai parameter like parameter sql queri pipelineparamet exampl queri like us pipelineparamet parameter possibl",
        "Question_gpt_summary_original":"The user is trying to parameterize a SQL query in Azure ML using PipelineParameter, specifically to parameterize the values of \"10\" and \"20\" in the query. The user is seeking clarification on whether this is possible.",
        "Question_gpt_summary":"user try parameter sql queri pipelineparamet specif parameter valu queri user seek clarif possibl",
        "Answer_original_content":"wai pass param pythonscriptstep param pipelineparamet min default valu param pipelineparamet max default valu sql datastor sql datastor step pythonscriptstep script script argument param param sql datastor script min param sy argv max param sy argv sql datastor sy argv queri select foo bar baz baz format min param max param run run context sql datastor datastor run experi workspac sql datastor dataset dataset tabular sql queri sql datastor queri",
        "Answer_preprocessed_content":"wai pass param pythonscriptstep",
        "Answer_gpt_summary_original":"Solution: The user can pass the parameters to PythonScriptStep and then use them in the script to parameterize the SQL query. The solution involves defining PipelineParameter for the parameters, passing them to PythonScriptStep, and then using them in the script to parameterize the SQL query.",
        "Answer_gpt_summary":"solut user pass paramet pythonscriptstep us script parameter sql queri solut involv defin pipelineparamet paramet pass pythonscriptstep script parameter sql queri"
    },
    {
        "Question_title":"Uploading a Dataframe to AWS S3 Bucket from SageMaker",
        "Question_body":"After successfully uploading CSV files from S3 to SageMaker notebook instance, I am stuck on doing the reverse.\n\nI have a dataframe and want to upload that to S3 Bucket as CSV or JSON. The code that I have is below:\n\nbucket='bucketname'\ndata_key = 'test.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\ndf.to_csv(data_location)\nI assumed since I successfully used pd.read_csv() while loading, using df.to_csv() would also work but it didn't. Probably it is generating error because this way I cannot pick the privacy options while uploading a file manually to S3. Is there a way to upload the data to S3 from SageMaker?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562042043000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":807.0,
        "Answer_body":"One way to solve this would be to save the CSV to the local storage on the SageMaker notebook instance, and then use the S3 API's via boto3 to upload the file as an s3 object. S3 docs for upload_file() available here.\n\nNote, you'll need to ensure that your SageMaker hosted notebook instance has proper ReadWrite permissions in its IAM role, otherwise you'll receive a permissions error.\n\ncode you already have, saving the file locally to whatever directory you wish\n\nfile_name = \"mydata.csv\"\ndf.to_csv(file_name)\n\ninstantiate S3 client and upload to s3\n\nimport boto3\n\ns3 = boto3.resource('s3')\ns3.meta.client.upload_file(file_name, 'YOUR_S3_BUCKET_NAME', 'DESIRED_S3_OBJECT_NAME')\nAlternatively, upload_fileobj() may help for parallelizing as a multi-part upload.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfoMiB7A8SFOpr5uklZZuNg\/uploading-a-dataframe-to-aws-s-3-bucket-from-sage-maker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-07-02T04:34:23.000Z",
                "Answer_score":0,
                "Answer_body":"One way to solve this would be to save the CSV to the local storage on the SageMaker notebook instance, and then use the S3 API's via boto3 to upload the file as an s3 object. S3 docs for upload_file() available here.\n\nNote, you'll need to ensure that your SageMaker hosted notebook instance has proper ReadWrite permissions in its IAM role, otherwise you'll receive a permissions error.\n\ncode you already have, saving the file locally to whatever directory you wish\n\nfile_name = \"mydata.csv\"\ndf.to_csv(file_name)\n\ninstantiate S3 client and upload to s3\n\nimport boto3\n\ns3 = boto3.resource('s3')\ns3.meta.client.upload_file(file_name, 'YOUR_S3_BUCKET_NAME', 'DESIRED_S3_OBJECT_NAME')\nAlternatively, upload_fileobj() may help for parallelizing as a multi-part upload.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1562042063000,
        "Question_original_content":"upload datafram aw bucket successfulli upload csv file notebook instanc stuck revers datafram want upload bucket csv json code bucket bucketnam data kei test csv data locat format bucket data kei csv data locat assum successfulli read csv load csv work probabl gener error wai pick privaci option upload file manual wai upload data",
        "Question_preprocessed_content":"upload datafram aw bucket successfulli upload csv file notebook instanc stuck revers datafram want upload bucket csv json code bucket bucketnam assum successfulli load work probabl gener error wai pick privaci option upload file manual wai upload data",
        "Question_gpt_summary_original":"The user is facing challenges in uploading a dataframe to an AWS S3 bucket from SageMaker. They have tried using the df.to_csv() method but it did not work. The user suspects that the error may be due to the inability to select privacy options while uploading the file manually to S3. The user is seeking a solution to upload the data to S3 from SageMaker.",
        "Question_gpt_summary":"user face challeng upload datafram aw bucket tri csv method work user suspect error inabl select privaci option upload file manual user seek solut upload data",
        "Answer_original_content":"wai solv save csv local storag notebook instanc us api boto upload file object doc upload file avail note need ensur host notebook instanc proper readwrit permiss iam role receiv permiss error code save file local directori wish file mydata csv csv file instanti client upload import boto boto resourc meta client upload file file bucket desir object altern upload fileobj help parallel multi upload",
        "Answer_preprocessed_content":"wai solv save csv local storag notebook instanc us api boto upload file object doc avail note need ensur host notebook instanc proper readwrit permiss iam role receiv permiss error code save file local directori wish instanti client upload import boto altern help parallel upload",
        "Answer_gpt_summary_original":"The solution suggested in the discussion is to save the CSV to the local storage on the SageMaker notebook instance and then use the S3 API's via boto3 to upload the file as an s3 object. The user needs to ensure that the SageMaker hosted notebook instance has proper ReadWrite permissions in its IAM role, otherwise, they will receive a permissions error. The code to save the file locally and upload it to S3 using boto3 is also provided.",
        "Answer_gpt_summary":"solut suggest discuss save csv local storag notebook instanc us api boto upload file object user need ensur host notebook instanc proper readwrit permiss iam role receiv permiss error code save file local upload boto provid"
    },
    {
        "Question_title":"Which IAM roles and policies should I delete to not being charged by AWS?",
        "Question_body":"<p>Are these roles to be deleted?<\/p>\n<ol>\n<li>AmazonSageMakerServiceCatalogProductsLaunchRole<\/li>\n<li>AmazonSageMakerServiceCatalogProductsUseRole<\/li>\n<li>AWSServiceRoleForAmazonSageMakerNotebooks<\/li>\n<\/ol>\n<p>Are these roles to be deleted?<\/p>\n<ol>\n<li>AmazonSageMakerServiceCatalogProductsUseRole<\/li>\n<li>Plus some execution policies<\/li>\n<\/ol>\n<p>Is Jupyter server within sagemaker studio also be stopped for not being charged?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633708664963,
        "Question_favorite_count":null,
        "Question_last_edit_time":1634116759467,
        "Question_score":0.0,
        "Question_view_count":105.0,
        "Answer_body":"<p><strong>AWS IAM is a free service<\/strong> - you do not get charged for roles, policies or any other aspect of IAM.<\/p>\n<p>From <a href=\"https:\/\/aws.amazon.com\/iam\/#:%7E:text=IAM%20is%20a%20feature%20of,AWS%20services%20by%20your%20users.\" rel=\"nofollow noreferrer\">the documentation<\/a>:<\/p>\n<blockquote>\n<p>IAM is a feature of your AWS account offered at no additional charge. You will be charged only for use of other AWS services by your users.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69498670",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1633709785048,
        "Question_original_content":"iam role polici delet charg aw role delet amazonservicecatalogproductslaunchrol amazonservicecatalogproductsuserol awsserviceroleforamazonnotebook role delet amazonservicecatalogproductsuserol plu execut polici jupyt server studio stop charg",
        "Question_preprocessed_content":"iam role polici delet charg aw role delet amazonservicecatalogproductslaunchrol amazonservicecatalogproductsuserol awsserviceroleforamazonnotebook role delet amazonservicecatalogproductsuserol plu execut polici jupyt server studio stop charg",
        "Question_gpt_summary_original":"The user is seeking guidance on which IAM roles and policies to delete in order to avoid being charged by AWS. They are specifically asking if certain roles, such as AmazonSageMakerServiceCatalogProductsLaunchRole and AmazonSageMakerServiceCatalogProductsUseRole, should be deleted, as well as some execution policies. The user also inquires if stopping the Jupyter server within SageMaker Studio will prevent charges.",
        "Question_gpt_summary":"user seek guidanc iam role polici delet order avoid charg aw specif ask certain role amazonservicecatalogproductslaunchrol amazonservicecatalogproductsuserol delet execut polici user inquir stop jupyt server studio prevent charg",
        "Answer_original_content":"aw iam free servic charg role polici aspect iam document iam featur aw account offer addit charg charg us aw servic user",
        "Answer_preprocessed_content":"aw iam free servic charg role polici aspect iam document iam featur aw account offer addit charg charg us aw servic user",
        "Answer_gpt_summary_original":"Solution: There are no solutions provided as the user's concern about being charged for IAM roles and policies is unfounded. IAM is a free service and users will only be charged for the use of other AWS services.",
        "Answer_gpt_summary":"solut solut provid user concern charg iam role polici unfound iam free servic user charg us aw servic"
    },
    {
        "Question_title":"Azure ML ExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH",
        "Question_body":"<p>I am using LightGBM in Azure ML Jupyter notebooks, it works fine and I also installed graphviz.<\/p>\n<p>However this line:<\/p>\n<pre><code>lgb.plot_tree(clf, tree_index = 1, figsize=(20,12))\n<\/code><\/pre>\n<p>throws this error:<\/p>\n<pre><code>ExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660921902893,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":94.0,
        "Answer_body":"<p>Common problem (very common).  There are two systems named Graphviz, and you need both!\nsee <a href=\"https:\/\/stackoverflow.com\/questions\/73040021\/im-getting-this-issue-when-trying-to-run-the-code-i-found-on-github-pydot-and\/73041302#73041302\">I&#39;m getting this issue when trying to run the code I found on GitHub. Pydot and graphivz are installed but still getting this error<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73418843",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1660922530248,
        "Question_original_content":"executablenotfound fail execut posixpath dot sure graphviz execut system path lightgbm jupyt notebook work fine instal graphviz line lgb plot tree clf tree index figsiz throw error executablenotfound fail execut posixpath dot sure graphviz execut system path",
        "Question_preprocessed_content":"executablenotfound fail execut posixpath sure graphviz execut system path lightgbm jupyt notebook work fine instal graphviz line throw error",
        "Question_gpt_summary_original":"The user encountered an error while using LightGBM in Azure ML Jupyter notebooks. The error message \"ExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\" appeared when trying to plot a tree using the lgb.plot_tree() function. The user had already installed graphviz but was still unable to execute the function.",
        "Question_gpt_summary":"user encount error lightgbm jupyt notebook error messag executablenotfound fail execut posixpath dot sure graphviz execut system path appear try plot tree lgb plot tree function user instal graphviz unabl execut function",
        "Answer_original_content":"common problem common system name graphviz need get issu try run code github pydot graphivz instal get error",
        "Answer_preprocessed_content":"common problem system name graphviz need get issu try run code github pydot graphivz instal get error",
        "Answer_gpt_summary_original":"Solutions provided: The discussion suggests that the error is a common problem and that the user needs to install both systems named Graphviz to resolve the issue.",
        "Answer_gpt_summary":"solut provid discuss suggest error common problem user need instal system name graphviz resolv issu"
    },
    {
        "Question_title":"Train multiple models with various measures and accumulate predictions",
        "Question_body":"<p>So I have been playing around with Azure ML lately, and I got one dataset where I have multiple values I want to predict. All of them uses different algorithms and when I try to train multiple models within one experiment; it says the \u201ctrain model can only predict one value\u201d, and there are not enough input ports on the train-model to take in multiple values even if I was to use the same algorithm for each measure. I tried launching the column selector and making rules, but I get the same error as mentioned. How do I predict multiple values and later put the predicted columns together for the web service output so I don\u2019t have to have multiple API\u2019s?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1465894075710,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1763.0,
        "Answer_body":"<p>What you would want to do is to train each model and save them as already trained models.\nSo create a new experiment, train your models and save them by right clicking on each model and they will show up in the left nav bar in the Studio. Now you are able to drag your models into the canvas and have them score predictions where you eventually make them end up in the same output as I have done in my example through the \u201cAdd columns\u201d module. I made this example for Ronaldo (Real Madrid CF player) on how he will perform in match after training day. You can see my demo on <a href=\"http:\/\/ronaldoinform.azurewebsites.net\" rel=\"nofollow noreferrer\">http:\/\/ronaldoinform.azurewebsites.net<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZwzUy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZwzUy.png\" alt=\"Ronaldo InForm\"><\/a><\/p>\n\n<p>For more detailed explanation on how to save the models and train multiple values; you can check out Raymond Langaeian (MSFT) answer in the comment section on this link:\n<a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-convert-training-experiment-to-scoring-experiment\/\" rel=\"nofollow noreferrer\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-convert-training-experiment-to-scoring-experiment\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37807158",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1465904564903,
        "Question_original_content":"train multipl model measur accumul predict plai late got dataset multipl valu want predict us differ algorithm try train multipl model experi sai train model predict valu input port train model multipl valu us algorithm measur tri launch column selector make rule error mention predict multipl valu later predict column web servic output dont multipl api",
        "Question_preprocessed_content":"train multipl model measur accumul predict plai late got dataset multipl valu want predict us differ algorithm try train multipl model experi sai train model predict valu input port multipl valu us algorithm measur tri launch column selector make rule error mention predict multipl valu later predict column web servic output dont multipl api",
        "Question_gpt_summary_original":"The user is facing challenges in training multiple models with various measures in Azure ML. They are unable to predict multiple values within one experiment and are encountering errors when attempting to use the column selector and making rules. The user is seeking a solution to predict multiple values and combine them for the web service output to avoid having multiple APIs.",
        "Question_gpt_summary":"user face challeng train multipl model measur unabl predict multipl valu experi encount error attempt us column selector make rule user seek solut predict multipl valu combin web servic output avoid have multipl api",
        "Answer_original_content":"want train model save train model creat new experi train model save right click model left nav bar studio abl drag model canva score predict eventu end output exampl add column modul exampl ronaldo real madrid player perform match train dai demo http ronaldoinform azurewebsit net detail explan save model train multipl valu check raymond langaeian msft answer comment section link http azur microsoft com document articl machin learn convert train experi score experi",
        "Answer_preprocessed_content":"want train model save train model creat new experi train model save right click model left nav bar studio abl drag model canva score predict eventu end output exampl add column modul exampl ronaldo perform match train dai demo detail explan save model train multipl valu check raymond langaeian answer comment section link",
        "Answer_gpt_summary_original":"Solution: The user can train each model and save them as already trained models. Then, they can create a new experiment, drag the saved models into the canvas, and use the \"Add columns\" module to combine the predictions into the same output. A detailed explanation on how to save the models and train multiple values can be found in Raymond Langaeian's answer in the comment section of this link: https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-convert-training-experiment-to-scoring-experiment\/.",
        "Answer_gpt_summary":"solut user train model save train model creat new experi drag save model canva us add column modul combin predict output detail explan save model train multipl valu raymond langaeian answer comment section link http azur microsoft com document articl machin learn convert train experi score experi"
    },
    {
        "Question_title":"AWS Sagemaker inference endpoint doesn't scale in with autoscaling",
        "Question_body":"<p>I have an AWS Sagemaker inference endpoint with autoscaling enabled with SageMakerVariantInvocationsPerInstance target metric. When I send a lot of requests to the endpoint the number of instances correctly scales out to the maximum instance count. But after I stop sending the requests the number of instances doesn't scale in to 1, minimum instance count. I waited for many hours. Is there a reason for this behaviour?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608117787513,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":1028.0,
        "Answer_body":"<p>AutoScaling requires a cloudwatch alarm to trigger to scale in.  Sagemaker doesn't push 0 value metrics when there's no activity (it just doesn't push anything).  This leads to the alarm being put into insufficient data and not triggering the autoscaling scale in action when your workload suddenly ends.<\/p>\n<p>Workarounds are either:<\/p>\n<ol>\n<li>Have a step scaling policy using the cloudwatch metric math FILL() function for your scale in.  This way you can tell CloudWatch &quot;if there's no data, pretend this was the metric value when evaluating the alarm.  This is only possible with step scaling since target tracking creates the alarms for you (and AutoScaling will periodically recreate them, so if you make manual changes they'll get deleted)<\/li>\n<li>Have scheduled scaling set the size back down to 1 every evening<\/li>\n<li>Make sure traffic continues at a low level for some times<\/li>\n<\/ol>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65322286",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1608300540056,
        "Question_original_content":"infer endpoint scale autosc infer endpoint autosc enabl variantinvocationsperinst target metric send lot request endpoint number instanc correctli scale maximum instanc count stop send request number instanc scale minimum instanc count wait hour reason behaviour thank",
        "Question_preprocessed_content":"infer endpoint scale autosc infer endpoint autosc enabl variantinvocationsperinst target metric send lot request endpoint number instanc correctli scale maximum instanc count stop send request number instanc scale minimum instanc count wait hour reason behaviour thank",
        "Question_gpt_summary_original":"The user is facing a challenge with their AWS Sagemaker inference endpoint, which has autoscaling enabled with SageMakerVariantInvocationsPerInstance target metric. The endpoint scales out to the maximum instance count when a lot of requests are sent, but it does not scale in to the minimum instance count of 1 after the requests stop, even after waiting for many hours. The user is seeking a reason for this behavior.",
        "Question_gpt_summary":"user face challeng infer endpoint autosc enabl variantinvocationsperinst target metric endpoint scale maximum instanc count lot request sent scale minimum instanc count request stop wait hour user seek reason behavior",
        "Answer_original_content":"autosc requir cloudwatch alarm trigger scale push valu metric activ push lead alarm insuffici data trigger autosc scale action workload suddenli end workaround step scale polici cloudwatch metric math function scale wai tell cloudwatch data pretend metric valu evalu alarm possibl step scale target track creat alarm autosc period recreat manual chang delet schedul scale set size even sure traffic continu low level time",
        "Answer_preprocessed_content":"autosc requir cloudwatch alarm trigger scale push valu metric activ lead alarm insuffici data trigger autosc scale action workload suddenli end workaround step scale polici cloudwatch metric math function scale wai tell cloudwatch data pretend metric valu evalu alarm possibl step scale target track creat alarm schedul scale set size even sure traffic continu low level time",
        "Answer_gpt_summary_original":"Solutions provided in the discussion are:\n\n1. Use a step scaling policy with the CloudWatch metric math FILL() function for scale-in.\n2. Set scheduled scaling to reduce the size back down to 1 every evening.\n3. Ensure that traffic continues at a low level for some time.\n\nThe reason for the behavior is that Sagemaker does not push 0 value metrics when there is no activity, which leads to the alarm being put into insufficient data and not triggering the autoscaling scale-in action when the workload suddenly ends.",
        "Answer_gpt_summary":"solut provid discuss us step scale polici cloudwatch metric math function scale set schedul scale reduc size even ensur traffic continu low level time reason behavior push valu metric activ lead alarm insuffici data trigger autosc scale action workload suddenli end"
    },
    {
        "Question_title":"Create a Multi Model Endpoint using AWS Sagemaker Boto",
        "Question_body":"<p>I have created 2 models which are not too complex and renamed them and placed them into a same location in S3 bucket.<\/p>\n\n<p>I need to create a multi model endpoint such that the 2 models have a same end point. \nThe model i am using is AWS in built Linear-learner model type regressor. <\/p>\n\n<p>I am stuck as to how they should be deployed. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1574954456480,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":364.0,
        "Answer_body":"<p>SageMaker's Linear Learner algorithm container does not currently implement the requirements for <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">multi-model endpoints<\/a>. You could request support in the <a href=\"https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285&amp;start=0\" rel=\"nofollow noreferrer\">AWS Forums<\/a>.<\/p>\n\n<p>You could also build your own version of the Linear Learner algorithm. To deploy the models to a multi-model endpoint you would need to build your own container that meets the requirements for multi-model endpoints and implement your own version of the Linear Learner algorithm. This sample notebook gives an example of how you would create your multi-model compatible container that serves MxNet models, but you could adapt it to implement a Linear Learner algorithm:<\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1582591338968,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59091944",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1582590253856,
        "Question_original_content":"creat multi model endpoint boto creat model complex renam place locat bucket need creat multi model endpoint model end point model aw built linear learner model type regressor stuck deploi",
        "Question_preprocessed_content":"creat multi model endpoint boto creat model complex renam place locat bucket need creat multi model endpoint model end point model aw built model type regressor stuck deploi",
        "Question_gpt_summary_original":"The user has created two models and stored them in the same location in an S3 bucket. They need to create a multi-model endpoint for the two models using the AWS in-built Linear-learner model type regressor, but they are unsure how to deploy them.",
        "Question_gpt_summary":"user creat model store locat bucket need creat multi model endpoint model aw built linear learner model type regressor unsur deploi",
        "Answer_original_content":"linear learner algorithm contain current implement requir multi model endpoint request support aw forum build version linear learner algorithm deploi model multi model endpoint need build contain meet requir multi model endpoint implement version linear learner algorithm sampl notebook give exampl creat multi model compat contain serv mxnet model adapt implement linear learner algorithm http github com awslab amazon exampl blob master advanc function multi model bring multi model endpoint bring ipynb",
        "Answer_preprocessed_content":"linear learner algorithm contain current implement requir endpoint request support aw forum build version linear learner algorithm deploi model endpoint need build contain meet requir endpoint implement version linear learner algorithm sampl notebook give exampl creat compat contain serv mxnet model adapt implement linear learner algorithm",
        "Answer_gpt_summary_original":"Solutions provided: \n- Request support in the AWS forums to implement multi-model endpoints for the Linear Learner algorithm container.\n- Build a custom container that meets the requirements for multi-model endpoints and implement a version of the Linear Learner algorithm. A sample notebook is provided as an example. \n\nNo personal opinions or biases were included in the summary.",
        "Answer_gpt_summary":"solut provid request support aw forum implement multi model endpoint linear learner algorithm contain build custom contain meet requir multi model endpoint implement version linear learner algorithm sampl notebook provid exampl person opinion bias includ summari"
    },
    {
        "Question_title":"Wildcards in stage deps?",
        "Question_body":"<p>Is it possible to specify dependancies for a stage using wildcard characters (aka glob)? I would like to be able to write something like<\/p>\n<pre><code class=\"lang-auto\">stages:\n  compare:\n    cmd: python src\/compare_data.py\n    deps: \n    - src\/compare_data.py\n    - data\/parameter_*.csv\n<\/code><\/pre>\n<p>In this case, the <code>parameter_001.csv<\/code> etc files came from a previous stage that includes a <code>foreach<\/code> loop that created the <code>csv<\/code> files.<\/p>\n<p>When I tried the above, I get the error <code>dependency 'data\/parameter_*.csv' does not exist<\/code>, even though the file (with <code>*<\/code> replaced by <code>001<\/code>) does actually exist.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649271460010,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":197.0,
        "Answer_body":"<blockquote>\n<p>Is it possible to specify dependancies for a stage using wildcard characters (aka glob)?<\/p>\n<\/blockquote>\n<p>It is not currently possible, see comment here <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/5252#issuecomment-758628040\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Wildcards in pipeline dependencies \u00b7 Issue #5252 \u00b7 iterative\/dvc \u00b7 GitHub<\/a><\/p>\n<p>Could you set the full <code>data<\/code> folder as a dependency?<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/wildcards-in-stage-deps\/1151",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-07T11:29:47.036Z",
                "Answer_body":"<blockquote>\n<p>Is it possible to specify dependancies for a stage using wildcard characters (aka glob)?<\/p>\n<\/blockquote>\n<p>It is not currently possible, see comment here <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/5252#issuecomment-758628040\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Wildcards in pipeline dependencies \u00b7 Issue #5252 \u00b7 iterative\/dvc \u00b7 GitHub<\/a><\/p>\n<p>Could you set the full <code>data<\/code> folder as a dependency?<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"wildcard stage dep possibl specifi depend stage wildcard charact aka glob like abl write like stage compar cmd python src compar data dep src compar data data paramet csv case paramet csv file came previou stage includ foreach loop creat csv file tri error depend data paramet csv exist file replac actual exist",
        "Question_preprocessed_content":"wildcard stage dep possibl specifi depend stage wildcard charact like abl write like case file came previou stage includ loop creat file tri error file actual exist",
        "Question_gpt_summary_original":"The user is facing a challenge in specifying dependencies for a stage using wildcard characters. They want to use a wildcard character to specify dependencies for a stage, but they are getting an error message saying that the dependency does not exist, even though the file with the wildcard replaced by a specific value does exist.",
        "Question_gpt_summary":"user face challeng specifi depend stage wildcard charact want us wildcard charact specifi depend stage get error messag sai depend exist file wildcard replac specif valu exist",
        "Answer_original_content":"possibl specifi depend stage wildcard charact aka glob current possibl comment wildcard pipelin depend issu iter github set data folder depend",
        "Answer_preprocessed_content":"possibl specifi depend stage wildcard charact current possibl comment wildcard pipelin depend issu iter github set folder depend",
        "Answer_gpt_summary_original":"Solutions provided: None.",
        "Answer_gpt_summary":"solut provid"
    },
    {
        "Question_title":"Does AWS Sagemaker charges you per API request?",
        "Question_body":"<p>AWS pricing page describes how much it costs per hour to run AWS Sagemaker for online realtime inference.\n<a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/<\/a><\/p>\n<p>But AWS usually also charges for API requests.\nDo they charge extra per every API inference request to the Sagemaker model?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1604509294833,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":187.0,
        "Answer_body":"<p>I am on the AWS SageMaker team.  For &quot;Real-Time Inference&quot; you are only charged for:<\/p>\n<ol>\n<li>usage of the instance types you choose (instance hours)<\/li>\n<li>storage attached to those instance (GB storage hours)<\/li>\n<li>data in and out of your Endpoint (Bytes in\/out)<\/li>\n<\/ol>\n<p>See &quot;Pricing Example #6: Real-Time Inference&quot; as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64684503",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1608159311612,
        "Question_original_content":"charg api request aw price page describ cost hour run onlin realtim infer http aw amazon com price aw usual charg api request charg extra api infer request model",
        "Question_preprocessed_content":"charg api request aw price page describ cost hour run onlin realtim infer aw usual charg api request charg extra api infer request model",
        "Question_gpt_summary_original":"The user is seeking clarification on whether AWS charges extra for every API inference request made to the Sagemaker model, in addition to the hourly cost of running the service.",
        "Question_gpt_summary":"user seek clarif aw charg extra api infer request model addit hourli cost run servic",
        "Answer_original_content":"team real time infer charg usag instanc type choos instanc hour storag attach instanc storag hour data endpoint byte price exampl real time infer",
        "Answer_preprocessed_content":"team infer charg usag instanc type choos storag attach instanc data endpoint price exampl infer",
        "Answer_gpt_summary_original":"Solution: The AWS SageMaker team member clarified that for \"Real-Time Inference,\" users are only charged for the usage of the instance types chosen, storage attached to those instances, and data in and out of the Endpoint. There are no additional charges for every API inference request made to the Sagemaker model.",
        "Answer_gpt_summary":"solut team member clarifi real time infer user charg usag instanc type chosen storag attach instanc data endpoint addit charg api infer request model"
    },
    {
        "Question_title":"Sagemaker API to list Hyperparameters",
        "Question_body":"<p>I'm currently trying to implement MLFlow Tracking into my training pipeline and would like to log the hyperparameters of my hyperparameter Tuning of each training job.<\/p>\n\n<p>Does anyone know, how to pull the list of hyperparameters that can be seen on the sagemaker training job interface (on the AWS console)? Is there any other smarter way to list how models perform in comparison in Sagemaker (and displayed)?<\/p>\n\n<p>I would assume there must be an easy and Pythonic way to do this (either boto3 or the sagemaker api) to get this data. I wasn't able to find it in Cloudwatch.<\/p>\n\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1591864702897,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":484.0,
        "Answer_body":"<p>there is indeed a rather pythonic way in the SageMaker python SDK:<\/p>\n\n<pre><code>tuner = sagemaker.tuner.HyperparameterTuner.attach('&lt; your tuning jobname&gt;')\n\nresults = tuner.analytics().dataframe()  # all your tuning metadata, in pandas!\n<\/code><\/pre>\n\n<p>See full example here <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62320331",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1592173650467,
        "Question_original_content":"api list hyperparamet current try implement track train pipelin like log hyperparamet hyperparamet tune train job know pull list hyperparamet seen train job interfac aw consol smarter wai list model perform comparison displai assum easi python wai boto api data wasn abl cloudwatch thank advanc",
        "Question_preprocessed_content":"api list hyperparamet current try implement track train pipelin like log hyperparamet hyperparamet tune train job know pull list hyperparamet seen train job interfac smarter wai list model perform comparison assum easi python wai data wasn abl cloudwatch thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges in implementing MLFlow Tracking into their training pipeline and is seeking help to log the hyperparameters of their hyperparameter tuning for each training job. They are looking for a way to pull the list of hyperparameters seen on the Sagemaker training job interface and display how models perform in comparison in Sagemaker. The user has tried to find this data in Cloudwatch but was unsuccessful.",
        "Question_gpt_summary":"user face challeng implement track train pipelin seek help log hyperparamet hyperparamet tune train job look wai pull list hyperparamet seen train job interfac displai model perform comparison user tri data cloudwatch unsuccess",
        "Answer_original_content":"python wai python sdk tuner tuner hyperparametertun attach result tuner analyt datafram tune metadata panda exampl http github com aw sampl amazon tuneranalyt sampl blob master tune job analyt ipynb",
        "Answer_preprocessed_content":"python wai python sdk exampl",
        "Answer_gpt_summary_original":"Solution: The user can use the SageMaker python SDK to pull the list of hyperparameters seen on the Sagemaker training job interface and display how models perform in comparison in Sagemaker. They can attach the hyperparameter tuner to their tuning job name and use the analytics() function to get all the tuning metadata in pandas. A full example is provided in the given link.",
        "Answer_gpt_summary":"solut user us python sdk pull list hyperparamet seen train job interfac displai model perform comparison attach hyperparamet tuner tune job us analyt function tune metadata panda exampl provid given link"
    },
    {
        "Question_title":"Pulling data from Stream Analytics to Azure Machine Learning",
        "Question_body":"<p>Working on a IoT telemetry project that receives humidity and weather pollution data from different sites on the field. I will then apply Machine Learning on the collected data. I'm using Event Hubs and Stream Analytics. Is there a way of pulling the data to Azure Machine Learning without the hassle of writing an application to get it from Stream Analytics and push to AML web service?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1467278958850,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":627.0,
        "Answer_body":"<p>Stream Analytics has a functionality called the \u201c<a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/\" rel=\"nofollow\">Functions<\/a>\u201d. You can call any web service you\u2019ve published using AML from within Stream Analytics and apply it within your Stream Analytics query. Check this <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/stream-analytics-machine-learning-integration-tutorial\/\" rel=\"nofollow\">link for a tutorial<\/a>.\nExample workflow in your case would be like the following;<\/p>\n\n<ul>\n<li>Telemetry arrives and reaches Stream Analytics<\/li>\n<li>Streaming Analytics (SA) calls the Machine Learning function to apply it on the data<\/li>\n<li>SA redirects it to the output accordingly, here you can use the PowerBI to create a predictions dashboards.<\/li>\n<\/ul>\n\n<p>Another way would be using R, and here\u2019s a good tutorial showing that <a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/\" rel=\"nofollow\">https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/<\/a> . \nIt is more work of course but can give you more control as you control the code.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38119062",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1467284456456,
        "Question_original_content":"pull data stream analyt work iot telemetri project receiv humid weather pollut data differ site field appli machin learn collect data event hub stream analyt wai pull data hassl write applic stream analyt push aml web servic",
        "Question_preprocessed_content":"pull data stream analyt work iot telemetri project receiv humid weather pollut data differ site field appli machin learn collect data event hub stream analyt wai pull data hassl write applic stream analyt push aml web servic",
        "Question_gpt_summary_original":"The user is facing a challenge in pulling data from Stream Analytics to Azure Machine Learning for an IoT telemetry project without having to write an application to get the data from Stream Analytics and push it to AML web service.",
        "Question_gpt_summary":"user face challeng pull data stream analyt iot telemetri project have write applic data stream analyt push aml web servic",
        "Answer_original_content":"stream analyt function call function web servic youv publish aml stream analyt appli stream analyt queri check link tutori exampl workflow case like follow telemetri arriv reach stream analyt stream analyt call machin learn function appli data redirect output accordingli us powerbi creat predict dashboard wai here good tutori show http blog technet microsoft com machinelearn azur avail function azur stream analyt work cours control control code",
        "Answer_preprocessed_content":"stream analyt function call function web servic youv publish aml stream analyt appli stream analyt queri check link tutori exampl workflow case like follow telemetri arriv reach stream analyt stream analyt call machin learn function appli data redirect output accordingli us powerbi creat predict dashboard wai here good tutori show work cours control control code",
        "Answer_gpt_summary_original":"Solution: The user can use the \"Functions\" functionality in Stream Analytics to call any web service published using AML and apply it within the Stream Analytics query. Another solution is to use R, which requires more work but provides more control over the code.",
        "Answer_gpt_summary":"solut user us function function stream analyt web servic publish aml appli stream analyt queri solut us requir work provid control code"
    },
    {
        "Question_title":"Returning nearest neighbors from SKLearn model deployed in AWS SageMaker",
        "Question_body":"<p>I built an unsupervised NearestNeighbors model in AWS Sagemaker, and deployed this to an endpoint. Now, I am trying to use the model endpoint to generate the k-nearest neighbors for a given input vector. <\/p>\n\n<p>However, I am getting the following error:<\/p>\n\n<pre><code>AttributeError                            Traceback (most recent call last)\n&lt;ipython-input-31-f595a603f928&gt; in &lt;module&gt;()\n     12 # print(predictor.predict(sample_vector))\n     13 \n---&gt; 14 distance, indice = pred.kneighbors(sample_vector, n_neighbors=11)\n\nAttributeError: 'SKLearnPredictor' object has no attribute 'kneighbors'\n<\/code><\/pre>\n\n<p>The SKLearn NearestNeighbors learner does not have a predict method. Trying to use the 'predict' method instead of '.kneighbors' therefore also yields an error:<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"&lt;!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\"&gt;\n&lt;title&gt;500 Internal Server Error&lt;\/title&gt;\n&lt;h1&gt;Internal Server Error&lt;\/h1&gt;\n&lt;p&gt;The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.&lt;\/p&gt;\n\". See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-scikit-learn-2019-06-29-13-11-50-512 in account 820407560908 for more information.\n<\/code><\/pre>\n\n<p>Is there a way to call this endpoint within Sagemaker, or does the Sagemaker SKLearn SDK only allow for models with a 'predict' method?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1561818096170,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":694.0,
        "Answer_body":"<p>At inference, 3 functions are used one after the other: <code>input_fn<\/code>, <code>predict_fn<\/code>, <code>output_fn<\/code>. They take default values, but you can override them to do desired custom actions. In your case, you can for example override the <code>predict_fn<\/code> to run the desired command. See more details here <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#deploying-scikit-learn-models\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#deploying-scikit-learn-models<\/a> <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56818280",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1561838537720,
        "Question_original_content":"return nearest neighbor sklearn model deploi built unsupervis nearestneighbor model deploi endpoint try us model endpoint gener nearest neighbor given input vector get follow error attributeerror traceback recent print predictor predict sampl vector distanc indic pred kneighbor sampl vector neighbor attributeerror sklearnpredictor object attribut kneighbor sklearn nearestneighbor learner predict method try us predict method instead kneighbor yield error modelerror error occur modelerror call invokeendpoint oper receiv server error model messag intern server error intern server error server encount intern error unabl complet request server overload error applic http east consol aw amazon com cloudwatch home region east logeventview group aw endpoint scikit learn account inform wai endpoint sklearn sdk allow model predict method",
        "Question_preprocessed_content":"return nearest neighbor sklearn model deploi built unsupervis nearestneighbor model deploi endpoint try us model endpoint gener neighbor given input vector get follow error sklearn nearestneighbor learner predict method try us predict method instead yield error wai endpoint sklearn sdk allow model predict method",
        "Question_gpt_summary_original":"The user built an unsupervised NearestNeighbors model in AWS Sagemaker and deployed it to an endpoint. However, when trying to generate the k-nearest neighbors for a given input vector, the user encountered an error indicating that the SKLearn NearestNeighbors learner does not have a predict method. Attempting to use the predict method instead of '.kneighbors' also resulted in an error. The user is unsure if there is a way to call this endpoint within Sagemaker or if the Sagemaker SKLearn SDK only allows for models with a 'predict' method.",
        "Question_gpt_summary":"user built unsupervis nearestneighbor model deploi endpoint try gener nearest neighbor given input vector user encount error indic sklearn nearestneighbor learner predict method attempt us predict method instead kneighbor result error user unsur wai endpoint sklearn sdk allow model predict method",
        "Answer_original_content":"infer function input predict output default valu overrid desir custom action case exampl overrid predict run desir command detail http readthedoc stabl sklearn html deploi scikit learn model",
        "Answer_preprocessed_content":"infer function default valu overrid desir custom action case exampl overrid run desir command detail",
        "Answer_gpt_summary_original":"Solution: The user can override the predict_fn function to run the desired command. More details can be found in the Sagemaker documentation.",
        "Answer_gpt_summary":"solut user overrid predict function run desir command detail document"
    },
    {
        "Question_title":"Customer Error: imread read blank (None) image for file- Sagemaker AWS",
        "Question_body":"<p>I am following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_mscoco_multi_label\/Image-classification-multilabel-lst.ipynb\" rel=\"nofollow noreferrer\">tutorial<\/a> with my custom data and my custom S3 buckets where train and validation data are. I am getting the following error:<\/p>\n<pre><code>Customer Error: imread read blank (None) image for file: \/opt\/ml\/input\/data\/train\/s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n<\/code><\/pre>\n<p>I have all my training data are in one folder named '<code>train<\/code>' I have set up my <code>lst<\/code> file like this suggested by <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">doc<\/a>,<\/p>\n<pre><code>22  1   s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n86  0   s3:\/\/image-classification\/image_classification_model_data\/train\/img-002.png\n...\n<\/code><\/pre>\n<p>My other configurations:<\/p>\n<pre><code>s3_bucket = 'image-classification'\nprefix =  'image_classification_model_data'\n\n\ns3train = 's3:\/\/{}\/{}\/train\/'.format(s3_bucket, prefix)\ns3validation = 's3:\/\/{}\/{}\/validation\/'.format(s3_bucket, prefix)\n\ns3train_lst = 's3:\/\/{}\/{}\/train_lst\/'.format(s3_bucket, prefix)\ns3validation_lst = 's3:\/\/{}\/{}\/validation_lst\/'.format(s3_bucket, prefix)\n\n\n\ntrain_data = sagemaker.inputs.TrainingInput(s3train, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data = sagemaker.inputs.TrainingInput(s3validation, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\ntrain_data_lst = sagemaker.inputs.TrainingInput(s3train_lst, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data_lst = sagemaker.inputs.TrainingInput(s3validation_lst, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\n\ndata_channels = {'train': train_data, 'validation': validation_data, 'train_lst': train_data_lst, \n                 'validation_lst': validation_data_lst}\n<\/code><\/pre>\n<p>I checked the images downloaded and checked physically, I see the image. Now sure what this error gets thrown out as <code>blank<\/code>. Any suggestion would be great.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616651745117,
        "Question_favorite_count":null,
        "Question_last_edit_time":1616652513867,
        "Question_score":0.0,
        "Question_view_count":164.0,
        "Answer_body":"<p>Sagemaker copies the input data you specify in <code>s3train<\/code> into the instance in <code>\/opt\/ml\/input\/data\/train\/<\/code> and that's why you have an error, because as you can see from the error message is trying to concatenate the filename in the <code>lst<\/code> file with the path where it expect the image to be. So just put only the filenames in your <code>lst<\/code>and should be fine (remove the s3 path).<\/p>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66793845",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1616684272012,
        "Question_original_content":"custom error imread read blank imag file aw follow tutori custom data custom bucket train valid data get follow error custom error imread read blank imag file opt input data train imag classif imag classif model data train img png train data folder name train set lst file like suggest doc imag classif imag classif model data train img png imag classif imag classif model data train img png configur bucket imag classif prefix imag classif model data strain train format bucket prefix svalid valid format bucket prefix strain lst train lst format bucket prefix svalid lst valid lst format bucket prefix train data input traininginput strain distribut fullyrepl content type applic imag data type sprefix valid data input traininginput svalid distribut fullyrepl content type applic imag data type sprefix train data lst input traininginput strain lst distribut fullyrepl content type applic imag data type sprefix valid data lst input traininginput svalid lst distribut fullyrepl content type applic imag data type sprefix data channel train train data valid valid data train lst train data lst valid lst valid data lst check imag download check physic imag sure error get thrown blank suggest great",
        "Question_preprocessed_content":"custom error imread read blank imag file aw follow tutori custom data custom bucket train valid data get follow error train data folder name set file like suggest doc configur check imag download check physic imag sure error get thrown suggest great",
        "Question_gpt_summary_original":"The user is encountering an error while following an image classification tutorial using custom data and S3 buckets. The error message indicates that the image file is blank, even though the user has physically checked and confirmed that the image is present. The user has provided their lst file and other configurations, but is unsure why the error is occurring and is seeking suggestions to resolve the issue.",
        "Question_gpt_summary":"user encount error follow imag classif tutori custom data bucket error messag indic imag file blank user physic check confirm imag present user provid lst file configur unsur error occur seek suggest resolv issu",
        "Answer_original_content":"copi input data specifi strain instanc opt input data train error error messag try concaten filenam lst file path expect imag filenam lstand fine remov path",
        "Answer_preprocessed_content":"copi input data specifi instanc error error messag try concaten filenam file path expect imag filenam fine",
        "Answer_gpt_summary_original":"Solution: The user can resolve the error by removing the S3 path from the filenames in their lst file and only including the filenames. This will allow Sagemaker to copy the input data into the instance and concatenate the filename in the lst file with the path where it expects the image to be.",
        "Answer_gpt_summary":"solut user resolv error remov path filenam lst file includ filenam allow copi input data instanc concaten filenam lst file path expect imag"
    },
    {
        "Question_title":"Azure Machine Learning Request Response latency",
        "Question_body":"<p>I have made an Azure Machine Learning Experiment which takes a small dataset (12x3 array) and some parameters and does some calculations using a few Python modules (a linear regression calculation and some more). This all works fine.<\/p>\n\n<p>I have deployed the experiment and now want to throw data at it from the front-end of my application. The API-call goes in and comes back with correct results, but it takes up to 30 seconds to calculate a simple linear regression. Sometimes it is 20 seconds, sometimes only 1 second. I even got it down to 100 ms one time (which is what I'd like), but 90% of the time the request takes more than 20 seconds to complete, which is unacceptable.<\/p>\n\n<p>I guess it has something to do with it still being an experiment, or it is still in a development slot, but I can't find the settings to get it to run on a faster machine.<\/p>\n\n<p>Is there a way to speed up my execution?<\/p>\n\n<p>Edit: To clarify: The varying timings are obtained with the same test data, simply by sending the same request multiple times. This made me conclude it must have something to do with my request being put in a queue, there is some start-up latency or I'm throttled in some other way.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1453718439993,
        "Question_favorite_count":5.0,
        "Question_last_edit_time":1453911336527,
        "Question_score":8.0,
        "Question_view_count":1128.0,
        "Answer_body":"<p>First, I am assuming you are doing your timing test on the published AML endpoint.<\/p>\n\n<p>When a call is made to the AML the first call must warm up the container. By default a web service has 20 containers. Each container is cold, and a cold container can cause a large(30 sec) delay. In the string returned by the AML endpoint, only count requests that have the <code>isWarm<\/code> flag set to true. By smashing the service with MANY requests(relative to how many containers you have running) can get all your containers warmed.<\/p>\n\n<p>If you are sending out dozens of requests a instance, the endpoint might be getting throttled. You can adjust the number of calls your endpoint can accept by going to manage.windowsazure.com\/<\/p>\n\n<ol>\n<li>manage.windowsazure.com\/<\/li>\n<li>Azure ML Section from left bar<\/li>\n<li>select your workspace<\/li>\n<li>go to web services tab<\/li>\n<li>Select your web service from list<\/li>\n<li>adjust the number of calls with slider<\/li>\n<\/ol>\n\n<p>By enabling debugging onto your endpoint you can get logs about the execution time for each of your modules to complete. You can use this to determine if a module is not running as you intended which may add to the time.<\/p>\n\n<p>Overall, there is an overhead when using the Execute python module, but I'd expect this request to complete in under 3 secs. <\/p>",
        "Answer_comment_count":11.0,
        "Answer_last_edit_time":1453911048927,
        "Answer_score":8.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34990561",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1453832406127,
        "Question_original_content":"request respons latenc experi take small dataset arrai paramet calcul python modul linear regress calcul work fine deploi experi want throw data end applic api goe come correct result take second calcul simpl linear regress second second got time like time request take second complet unaccept guess experi develop slot set run faster machin wai speed execut edit clarifi vari time obtain test data simpli send request multipl time conclud request queue start latenc throttl wai",
        "Question_preprocessed_content":"request respons latenc experi take small dataset paramet calcul python modul work fine deploi experi want throw data applic goe come correct result take second calcul simpl linear regress second second got time time request take second complet unaccept guess experi develop slot set run faster machin wai speed execut edit clarifi vari time obtain test data simpli send request multipl time conclud request queue latenc throttl wai",
        "Question_gpt_summary_original":"The user has encountered challenges with the response latency of their Azure Machine Learning Experiment API. Although the API-call returns correct results, the time taken to calculate a simple linear regression varies from 1 second to 30 seconds, which is unacceptable. The user suspects that the issue may be due to the experiment still being in a development slot or being run on a slower machine. The user is seeking a way to speed up the execution.",
        "Question_gpt_summary":"user encount challeng respons latenc experi api api return correct result time taken calcul simpl linear regress vari second second unaccept user suspect issu experi develop slot run slower machin user seek wai speed execut",
        "Answer_original_content":"assum time test publish aml endpoint aml warm contain default web servic contain contain cold cold contain caus larg sec delai string return aml endpoint count request iswarm flag set true smash servic request rel contain run contain warm send dozen request instanc endpoint get throttl adjust number call endpoint accept go manag windowsazur com manag windowsazur com section left bar select workspac web servic tab select web servic list adjust number call slider enabl debug endpoint log execut time modul complet us determin modul run intend add time overal overhead execut python modul expect request complet sec",
        "Answer_preprocessed_content":"assum time test publish aml endpoint aml warm contain default web servic contain contain cold cold contain caus larg delai string return aml endpoint count request flag set true smash servic request contain warm send dozen request instanc endpoint get throttl adjust number call endpoint accept go section left bar select workspac web servic tab select web servic list adjust number call slider enabl debug endpoint log execut time modul complet us determin modul run intend add time overal overhead execut python modul expect request complet sec",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Warm up the container by sending many requests with the `isWarm` flag set to true to get all the containers warmed.\n- Adjust the number of calls the endpoint can accept by going to manage.windowsazure.com.\n- Enable debugging on the endpoint to get logs about the execution time for each module to determine if a module is not running as intended, which may add to the time.",
        "Answer_gpt_summary":"possibl solut mention discuss warm contain send request iswarm flag set true contain warm adjust number call endpoint accept go manag windowsazur com enabl debug endpoint log execut time modul determin modul run intend add time"
    },
    {
        "Question_title":"Trying to add the log_param() function",
        "Question_body":"with mlflow.start_run(nested =True):\n### instantiate the RNN model object\u00a0\n\u00a0 \u00a0 regr = Sequential()\u00a0\n\n\n### add the input and LSTM layers\u00a0\nregr.add(LSTM(units =4, activation ='sigmoid', input_shape =(None, 1)))\u00a0 \u00a0\u00a0\n\n\n### add the output layer\nregr.add(Dense(units =1))\n\n\n### compile the RNN\u00a0\noptimizer = 'adam'\nloss = 'mean_squared_error'\nmetrics = ['accuracy']\u00a0\nregr.compile(optimizer, loss , metrics)\u00a0\n\n\n### fit the model on the training set\u00a0\nbatch_size = 5\nepochs = 1\nregr.fit(X_train, y_train, batch_size, epochs)\u00a0\n\n\nlog_param(\"loss\", loss)\u00a0\nlog_param(\"epochs\", epochs)\nlog_param(\"optimizer\", optimizer)\nlog_param(\"batch_size\", batch_size)\nlog_param(\"metrics\", metrics)",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1539895945000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":11.0,
        "Answer_body":"\ue5d3\n\ue5d3\njust want to get the params in the mlflow ui",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/CxuBUiu1gys",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-10-18T20:53:53",
                "Answer_body":"\ue5d3\n\ue5d3\njust want to get the params in the mlflow ui"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"try add log param function start run nest true instanti rnn model object regr sequenti add input lstm layer regr add lstm unit activ sigmoid input shape add output layer regr add dens unit compil rnn optim adam loss mean squar error metric accuraci regr compil optim loss metric fit model train set batch size epoch regr fit train train batch size epoch log param loss loss log param epoch epoch log param optim optim log param batch size batch size log param metric metric",
        "Question_preprocessed_content":"try add function true instanti rnn model object regr sequenti add input lstm layer activ sigmoid add output layer compil rnn optim adam loss metric loss metric fit model train set epoch epoch loss epoch optim metric",
        "Question_gpt_summary_original":"The user is trying to add the log_param() function with mlflow.start_run(nested=True) to track the performance of an RNN model. However, there is no mention of any specific challenges faced by the user in the given text.",
        "Question_gpt_summary":"user try add log param function start run nest true track perform rnn model mention specif challeng face user given text",
        "Answer_original_content":"want param",
        "Answer_preprocessed_content":"want param",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut mention discuss"
    },
    {
        "Question_title":"How do I access an input parameter in Azure Machine Learning endpoints?",
        "Question_body":"I've created an Azure ML Endpoint Pipeline with a single 'Execute Python Script'. From the script, I am looking for a way to access the input 'ParameterAssignments' that I POST to the endpoint to trigger the pipeline. I expected to see them somewhere in Run.get_context(), but I haven't had any luck. I simply need a way to POST arbitrary values that my Python scripts can access. Thank you!",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1602485723003,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"I just confirmed with our engineer that you cannot set up a pipeline parameter and use it without tying it with any of the module parameter. So the workaround is - make the pipeline parameter as one of the inputs (i.e. dataset) to \"Execute Python Script\" module and set it as pipeline parameter. Then you can change it every time when calling the pipeline.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/123204\/how-do-i-access-an-input-parameter-in-azure-machin.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-19T01:08:06.24Z",
                "Answer_score":1,
                "Answer_body":"I just confirmed with our engineer that you cannot set up a pipeline parameter and use it without tying it with any of the module parameter. So the workaround is - make the pipeline parameter as one of the inputs (i.e. dataset) to \"Execute Python Script\" module and set it as pipeline parameter. Then you can change it every time when calling the pipeline.",
                "Answer_comment_count":3,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-05-27T06:21:17.273Z",
                "Answer_score":0,
                "Answer_body":"Hi @LuZhangAI-1027, I am finding a way to connect to Postgres in my pipeline. I don't think 'Import Data' supports it, so I am thinking to write my own 'Execute Python Script' to load the data from Postgres. As I don't use 'Import Data', is there another way to access the pipeline parameters inside 'Execute Python Script'?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1603069686240,
        "Question_original_content":"access input paramet endpoint creat endpoint pipelin singl execut python script script look wai access input parameterassign post endpoint trigger pipelin expect run context haven luck simpli need wai post arbitrari valu python script access thank",
        "Question_preprocessed_content":"access input paramet endpoint creat endpoint pipelin singl execut python script script look wai access input parameterassign post endpoint trigger pipelin expect haven luck simpli need wai post arbitrari valu python script access thank",
        "Question_gpt_summary_original":"The user is facing a challenge in accessing input parameter assignments in Azure Machine Learning endpoints. They have created an endpoint pipeline with a single 'Execute Python Script' and are looking for a way to access the input 'ParameterAssignments' that they POST to the endpoint to trigger the pipeline. They have tried using Run.get_context() but have not been successful and are seeking a way to POST arbitrary values that their Python scripts can access.",
        "Question_gpt_summary":"user face challeng access input paramet assign endpoint creat endpoint pipelin singl execut python script look wai access input parameterassign post endpoint trigger pipelin tri run context success seek wai post arbitrari valu python script access",
        "Answer_original_content":"confirm engin set pipelin paramet us ty modul paramet workaround pipelin paramet input dataset execut python script modul set pipelin paramet chang time call pipelin",
        "Answer_preprocessed_content":"confirm engin set pipelin paramet us ty modul paramet workaround pipelin paramet input execut python script modul set pipelin paramet chang time call pipelin",
        "Answer_gpt_summary_original":"Solution: The suggested workaround is to make the pipeline parameter as one of the inputs to the 'Execute Python Script' module and set it as a pipeline parameter. This way, the user can change it every time when calling the pipeline.",
        "Answer_gpt_summary":"solut suggest workaround pipelin paramet input execut python script modul set pipelin paramet wai user chang time call pipelin"
    },
    {
        "Question_title":"How to Deploy Amazon-SageMaker Locally in Python",
        "Question_body":"<p>I trained my model in Amazon-SageMaker and downloaded it to my local computer. Unfortunately, I don't have any idea how to run the model locally.<\/p>\n\n<p>The Model is in a directory with files like:<\/p>\n\n<pre><code>image-classification-0001.params\nimage-classification-0002.params\nimage-classification-0003.params\nimage-classification-0004.params\nimage-classification-0005.params\nimage-classification-symbol.json\nmodel-shapes.json\n<\/code><\/pre>\n\n<p>Would anyone know how to run this locally with Python, or be able to point me to a resource that could help? I am trying to avoid calling the model using the Amazon API.<\/p>\n\n<p>Edit: The model I used was created with code very similar to this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb\" rel=\"nofollow noreferrer\">example<\/a>.<\/p>\n\n<p>Any help is appreciated, I will award the bounty to whoever is most helpful, even if they don't completely solve the question. <\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_time":1520225638010,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1536012091088,
        "Question_score":3.0,
        "Question_view_count":3742.0,
        "Answer_body":"<p>Following SRC's advice, I was able to get it to work by following the instructions in this <a href=\"https:\/\/stackoverflow.com\/questions\/47190614\/how-to-load-a-trained-mxnet-model\">question<\/a> and this <a href=\"http:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html\" rel=\"nofollow noreferrer\">doc<\/a> which describe how to load a MXnet model.<\/p>\n\n<p>I loaded the model like so:<\/p>\n\n<pre><code>lenet_model = mx.mod.Module.load('model_directory\/image-classification',5)\nimage_l = 64\nimage_w = 64\nlenet_model.bind(for_training=False, data_shapes=[('data',(1,3,image_l,image_w))],label_shapes=lenet_model._label_shapes)\n<\/code><\/pre>\n\n<p>Then predicted using the slightly modified helper functions in the previously linked documentation:<\/p>\n\n<pre><code>import mxnet as mx\nimport matplotlib.pyplot as plot\nimport cv2\nimport numpy as np\nfrom mxnet.io import DataBatch\n\ndef get_image(url, show=False):\n    # download and show the image\n    fname = mx.test_utils.download(url)\n    img = cv2.cvtColor(cv2.imread(fname), cv2.COLOR_BGR2RGB)\n    if img is None:\n         return None\n    if show:\n         plt.imshow(img)\n         plt.axis('off')\n    # convert into format (batch, RGB, width, height)\n    img = cv2.resize(img, (64, 64))\n    img = np.swapaxes(img, 0, 2)\n    img = np.swapaxes(img, 1, 2)\n    img = img[np.newaxis, :]\n    return img\n\ndef predict(url, labels):\n    img = get_image(url, show=True)\n    # compute the predict probabilities\n    lenet_model.forward(DataBatch([mx.nd.array(img)]))\n    prob = lenet_model.get_outputs()[0].asnumpy()\n\n    # print the top-5\n    prob = np.squeeze(prob)\n    a = np.argsort(prob)[::-1]\n\n    for i in a[0:5]:\n       print('probability=%f, class=%s' %(prob[i], labels[i]))\n<\/code><\/pre>\n\n<p>Finally I called the prediction with this code:<\/p>\n\n<pre><code>labels = ['a','b','c', 'd','e', 'f']\npredict('https:\/\/eximagesite\/img_tst_a.jpg', labels )\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1582341918227,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49103679",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1520719061830,
        "Question_original_content":"deploi amazon local python train model amazon download local unfortun idea run model local model directori file like imag classif param imag classif param imag classif param imag classif param imag classif param imag classif symbol json model shape json know run local python abl point resourc help try avoid call model amazon api edit model creat code similar exampl help appreci award bounti help complet solv question",
        "Question_preprocessed_content":"deploi amazon local python train model amazon download local unfortun idea run model local model directori file like know run local python abl point resourc help try avoid call model amazon api edit model creat code similar exampl help appreci award bounti help complet solv question",
        "Question_gpt_summary_original":"The user has downloaded their model trained in Amazon-SageMaker to their local computer but does not know how to run it locally using Python. They are seeking help to avoid calling the model using the Amazon API. The model is in a directory with files such as image-classification-0001.params, image-classification-symbol.json, and model-shapes.json. The user has also provided a link to the code used to create the model.",
        "Question_gpt_summary":"user download model train amazon local know run local python seek help avoid call model amazon api model directori file imag classif param imag classif symbol json model shape json user provid link code creat model",
        "Answer_original_content":"follow src advic abl work follow instruct question doc load mxnet model load model like lenet model mod modul load model directori imag classif imag imag lenet model bind train fals data shape data imag imag label shape lenet model label shape predict slightli modifi helper function previous link document import mxnet import matplotlib pyplot plot import import numpi mxnet import databatch def imag url fals download imag fname test util download url img cvtcolor imread fname color bgrrgb img return plt imshow img plt axi convert format batch rgb width height img resiz img img swapax img img swapax img img img newaxi return img def predict url label img imag url true comput predict probabl lenet model forward databatch arrai img prob lenet model output asnumpi print prob squeez prob argsort prob print probabl class prob label final call predict code label predict http eximagesit img tst jpg label",
        "Answer_preprocessed_content":"follow src advic abl work follow instruct question doc load mxnet model load model like predict slightli modifi helper function previous link document final call predict code",
        "Answer_gpt_summary_original":"The solution provided in the discussion is to load the MXnet model using the instructions provided in the StackOverflow question and the MXnet documentation. The user loaded the model and predicted using the helper functions provided in the documentation. The user modified the helper functions slightly and called the prediction using the provided code.",
        "Answer_gpt_summary":"solut provid discuss load mxnet model instruct provid stackoverflow question mxnet document user load model predict helper function provid document user modifi helper function slightli call predict provid code"
    },
    {
        "Question_title":"Sagemaker to use processed pickled ndarray instead of csv files from S3",
        "Question_body":"<p>I understand that you can pass a CSV file from S3 into a Sagemaker XGBoost container using the following code<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>train_channel = sagemaker.session.s3_input(train_data, content_type='text\/csv')\nvalid_channel = sagemaker.session.s3_input(validation_data, content_type='text\/csv')\n\ndata_channels = {'train': train_channel, 'validation': valid_channel}\nxgb_model.fit(inputs=data_channels,  logs=True)\n<\/code><\/pre>\n\n<p>But I have an ndArray stored in S3 bucket. These are processed, label encoded, feature engineered arrays. I would want to pass this into the container instead of the csv. I do understand I can always convert my ndarray into csv files before saving it in S3. Just checking if there is an array option.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1566996073187,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":92.0,
        "Answer_body":"<p>There are multiple options for algorithms in SageMaker:<\/p>\n\n<ol>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">Built-in algorithms<\/a>, like the SageMaker XGBoost you mention<\/li>\n<li>Custom, user-created algorithm code, which can be:\n\n<ul>\n<li>Written for a pre-built docker image, available for Sklearn, TensorFlow, Pytorch, MXNet<\/li>\n<li>Written in your own container<\/li>\n<\/ul><\/li>\n<\/ol>\n\n<p>When you use built-ins (option 1), your choice of data format options is limited to what the built-ins support, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#InputOutput-XGBoost\" rel=\"nofollow noreferrer\">which is only csv and libsvm in the case of the built-in XGBoost<\/a>. If you want to use custom data formats and pre-processing logic before XGBoost, it is absolutely possible if you use your own script leveraging the open-source XGBoost. You can get inspiration from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">Random Forest demo<\/a> to see how to create custom models in pre-built containers<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57692681",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1568417822248,
        "Question_original_content":"us process pickl ndarrai instead csv file understand pass csv file xgboost contain follow code train channel session input train data content type text csv valid channel session input valid data content type text csv data channel train train channel valid valid channel xgb model fit input data channel log true ndarrai store bucket process label encod featur engin arrai want pass contain instead csv understand convert ndarrai csv file save check arrai option",
        "Question_preprocessed_content":"us process pickl ndarrai instead csv file understand pass csv file xgboost contain follow code ndarrai store bucket process label encod featur engin arrai want pass contain instead csv understand convert ndarrai csv file save check arrai option",
        "Question_gpt_summary_original":"The user wants to pass a processed pickled ndarray from an S3 bucket into a Sagemaker XGBoost container instead of using CSV files. They are looking for an array option and are aware that they can convert their ndarray into CSV files before saving it in S3.",
        "Question_gpt_summary":"user want pass process pickl ndarrai bucket xgboost contain instead csv file look arrai option awar convert ndarrai csv file save",
        "Answer_original_content":"multipl option algorithm built algorithm like xgboost mention custom user creat algorithm code written pre built docker imag avail sklearn tensorflow pytorch mxnet written contain us built in option choic data format option limit built in support csv libsvm case built xgboost want us custom data format pre process logic xgboost absolut possibl us script leverag open sourc xgboost inspir random forest demo creat custom model pre built contain",
        "Answer_preprocessed_content":"multipl option algorithm algorithm like xgboost mention custom algorithm code written docker imag avail sklearn tensorflow pytorch mxnet written contain us choic data format option limit support csv libsvm case xgboost want us custom data format logic xgboost absolut possibl us script leverag xgboost inspir random forest demo creat custom model contain",
        "Answer_gpt_summary_original":"There are two options for algorithms in SageMaker: built-in algorithms and custom, user-created algorithm code. When using built-in algorithms, the choice of data format options is limited to what the built-ins support, which is only csv and libsvm in the case of the built-in XGBoost. However, it is possible to use custom data formats and pre-processing logic before XGBoost by using a custom script leveraging the open-source XGBoost. The user can get inspiration from the Random Forest demo to see how to create custom models in pre-built containers. No solution is provided for passing a processed pickled ndarray from an S3 bucket into a Sagemaker XGBoost container instead of using CSV files.",
        "Answer_gpt_summary":"option algorithm built algorithm custom user creat algorithm code built algorithm choic data format option limit built in support csv libsvm case built xgboost possibl us custom data format pre process logic xgboost custom script leverag open sourc xgboost user inspir random forest demo creat custom model pre built contain solut provid pass process pickl ndarrai bucket xgboost contain instead csv file"
    },
    {
        "Question_title":"How to setup AWS sagemaker - Resource limit Error",
        "Question_body":"<p>I'm trying to set up my first SageMaker Studio so my team and myself can run some post processing scripts in a shared environment but I'm having issues.<\/p>\n<p>I've followed the steps in this video(<a href=\"https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices<\/a>) which are:<\/p>\n<ol>\n<li>Select Standard setup<\/li>\n<li>Select AWS Identity and Access Management (IAM)<\/li>\n<li>Under permissions - Create and select new execution role<\/li>\n<li>Under Network and storage - Select VPC, Subnet and Security group<\/li>\n<li>Hit the submit button at the bottom of the page.<\/li>\n<\/ol>\n<p>In the video, he clicks submit and is taken to the control panel where he starts the next phase of adding users, however I'm greeted with this error.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/g4k2g.png\" rel=\"nofollow noreferrer\"> Resource limit Error<\/a><\/p>\n<p>I've checked my Registered domains under route 53 and it says No domains to display, I've also checked my S2 and I have no instances so I have no idea where the 2 domains being utilized are.<\/p>\n<p>My dashboard, image and Notebooks are all empty so as far as I know there's nothing setup on this Sage Maker account.<\/p>\n<p>Could anyone tell me how to resolve this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615398273417,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":237.0,
        "Answer_body":"<p>You can have maximum 1 studio domain per region, by the default limits. Though, it seems like you have two domains already provisioned. Try to delete all the domains through the AWS cli and recreate with the AWS Management Console.<\/p>\n<p>Unfortunately, AWS Management Console cannot visualize more than one Studio domain.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66570138",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1626975605176,
        "Question_original_content":"setup resourc limit error try set studio team run post process script share environ have issu follow step video http youtub com watch widhcwvrjcu channel amazonwebservic select standard setup select aw ident access manag iam permiss creat select new execut role network storag select vpc subnet secur group hit submit button page video click submit taken control panel start phase ad user greet error resourc limit error check regist domain rout sai domain displai check instanc idea domain util dashboard imag notebook far know setup sage maker account tell resolv error",
        "Question_preprocessed_content":"setup resourc limit error try set studio team run post process script share environ have issu follow step select standard setup select aw ident access manag permiss creat select new execut role network storag select vpc subnet secur group hit submit button page video click submit taken control panel start phase ad user greet error resourc limit error check regist domain rout sai domain displai check instanc idea domain util dashboard imag notebook far know setup sage maker account tell resolv error",
        "Question_gpt_summary_original":"The user is encountering a \"Resource limit Error\" while setting up their first SageMaker Studio to run post-processing scripts in a shared environment. They followed the steps in a video tutorial but are unable to proceed due to the error. The user has checked their Registered domains under route 53 and S2 but found no instances. They are seeking help to resolve the error.",
        "Question_gpt_summary":"user encount resourc limit error set studio run post process script share environ follow step video tutori unabl proce error user check regist domain rout instanc seek help resolv error",
        "Answer_original_content":"maximum studio domain region default limit like domain provis try delet domain aw cli recreat aw manag consol unfortun aw manag consol visual studio domain",
        "Answer_preprocessed_content":"maximum studio domain region default limit like domain provis try delet domain aw cli recreat aw manag consol unfortun aw manag consol visual studio domain",
        "Answer_gpt_summary_original":"Solution: The user can try to delete all the domains through the AWS cli and recreate with the AWS Management Console. However, it should be noted that the AWS Management Console cannot visualize more than one Studio domain.",
        "Answer_gpt_summary":"solut user try delet domain aw cli recreat aw manag consol note aw manag consol visual studio domain"
    },
    {
        "Question_title":"Azure ML Enpoint deployment failed EAST US region",
        "Question_body":"I have an Azure ML Real-time inference endpoint deployed ran for a month till yesterday. Today it is in the state of \"Failed\".\n\nI did create a new compute and did a new deployment in the same region EAST US and it failed again.\n\nWhat's going? Is this just a problem for me or a general issue?\n\nThanks\n-Dali",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1594945530060,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, thanks for reaching out. I successfully deployed in the east us region. Please review the following troubleshooting guidelines. Also check for any service\/resource health issues that could be impacting your service. Let me know if you're still experiencing issues afterwards and please share the logs so we can investigate further. Thanks.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/48609\/azure-ml-enpoint-deployment-failed-east-us-region.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-17T02:30:29.753Z",
                "Answer_score":0,
                "Answer_body":"Hi, thanks for reaching out. I successfully deployed in the east us region. Please review the following troubleshooting guidelines. Also check for any service\/resource health issues that could be impacting your service. Let me know if you're still experiencing issues afterwards and please share the logs so we can investigate further. Thanks.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":44.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1594953029752,
        "Question_original_content":"enpoint deploy fail east region real time infer endpoint deploi ran month till yesterdai todai state fail creat new comput new deploy region east fail go problem gener issu thank dali",
        "Question_preprocessed_content":"enpoint deploy fail east region infer endpoint deploi ran month till yesterdai todai state fail creat new comput new deploy region east fail go problem gener issu thank dali",
        "Question_gpt_summary_original":"The user's Azure ML Real-time inference endpoint deployment in the EAST US region has failed after running successfully for a month. The user attempted to create a new compute and deploy again in the same region, but it failed again. The user is unsure if this is a general issue or specific to their account.",
        "Question_gpt_summary":"user real time infer endpoint deploy east region fail run successfulli month user attempt creat new comput deploi region fail user unsur gener issu specif account",
        "Answer_original_content":"thank reach successfulli deploi east region review follow troubleshoot guidelin check servic resourc health issu impact servic let know experienc issu share log investig thank",
        "Answer_preprocessed_content":"thank reach successfulli deploi east region review follow troubleshoot guidelin check health issu impact servic let know experienc issu share log investig thank",
        "Answer_gpt_summary_original":"Solutions provided:\n- Review troubleshooting guidelines\n- Check for any service\/resource health issues that could be impacting the service\n- Share logs for further investigation\n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"solut provid review troubleshoot guidelin check servic resourc health issu impact servic share log investig person opinion bias includ respons"
    },
    {
        "Question_title":"How do I retrieve a model in Vertex AI?",
        "Question_body":"<p>I defined a training job:<\/p>\n<pre><code>job = aiplatform.AutoMLTextTrainingJob(...\n<\/code><\/pre>\n<p>then I created a model by running the job:<\/p>\n<pre><code>model = job.run(...\n<\/code><\/pre>\n<p>It worked fine but it is now the next day and the variable <code>model<\/code> was in a Jupyter notebook and no longer exists. I have tried to get it back with:<\/p>\n<pre><code>from google.cloud import aiplatform_v1beta1\n\ndef sample_get_model():\n    client = aiplatform_v1beta1.ModelServiceClient()\n\n    model_id=id_of_training_pipeline\n    name= f'projects\/{PROJECT}\/locations\/{REGION}\/models\/{model_id}'\n    \n    request = aiplatform_v1beta1.GetModelRequest(name=name)\n    response = client.get_model(request=request)\n    print(response)\n\nsample_get_model()\n<\/code><\/pre>\n<p>I have also tried the id of v1 of the model created in place of <code>id_of_training_pipeline<\/code> and I have tried <code>\/pipelines\/pipeline_id<\/code><\/p>\n<p>but I get:\n<code>E0805 15:12:36.784008212   28406 hpack_parser.cc:1234]       Error parsing metadata: error=invalid value key=content-type value=text\/html; charset=UTF-8<\/code><\/p>\n<p>(<code>PROJECT<\/code> and <code>REGION<\/code> are set correctly).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659709186783,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":45.0,
        "Answer_body":"<p>Found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/samples\/aiplatform-get-model-sample#aiplatform_get_model_sample-python\" rel=\"nofollow noreferrer\">this<\/a> Google code which works.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73251212",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1659712728223,
        "Question_original_content":"retriev model defin train job job aiplatform automltexttrainingjob creat model run job model job run work fine dai variabl model jupyt notebook longer exist tri googl cloud import aiplatform vbeta def sampl model client aiplatform vbeta modelservicecli model train pipelin project project locat region model model request aiplatform vbeta getmodelrequest respons client model request request print respons sampl model tri model creat place train pipelin tri pipelin pipelin hpack parser error pars metadata error invalid valu kei content type valu text html charset utf project region set correctli",
        "Question_preprocessed_content":"retriev model defin train job creat model run job work fine dai variabl jupyt notebook longer exist tri tri model creat place tri set correctli",
        "Question_gpt_summary_original":"The user is facing a challenge in retrieving a model in Vertex AI that was created by running a training job in a Jupyter notebook. The user has tried to retrieve the model using the id of the training pipeline and the v1 id of the model, but is encountering an error related to parsing metadata.",
        "Question_gpt_summary":"user face challeng retriev model creat run train job jupyt notebook user tri retriev model train pipelin model encount error relat pars metadata",
        "Answer_original_content":"googl code work",
        "Answer_preprocessed_content":"googl code work",
        "Answer_gpt_summary_original":"Solution: A Google code was suggested which can be used to retrieve the model in Vertex AI that was created by running a training job in a Jupyter notebook.",
        "Answer_gpt_summary":"solut googl code suggest retriev model creat run train job jupyt notebook"
    },
    {
        "Question_title":"SageMaker create PyTorchModel without deploying",
        "Question_body":"<p>If I instantiate a SageMaker <code>PyTorchModel<\/code> object like this:<\/p>\n\n<pre><code>from sagemaker.pytorch import PyTorchModel\n\nmodel = PyTorchModel(name=name_from_base('model-name'),\n                     model_data=model_data,\n                     role=role,\n                     framework_version='1.0.0',\n                     entry_point='serve.py',\n                     source_dir='src',\n                     sagemaker_session=sagemaker_session,\n                     predictor_cls=ImagePredictor)\n\n#model.create_without_deploying??\n<\/code><\/pre>\n\n<p>Is there a way that I can create this model using the sagemaker python SDK so that the model shows up in the SageMaker console, but <em>without<\/em> actually deploying it to an endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559938343560,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1559938910380,
        "Question_score":2.0,
        "Question_view_count":434.0,
        "Answer_body":"<p>I don't think it is possible to do so using the high-level SageMaker Pyhton SDK. However, you should be able to do it by calling the CreateModel API using the low-level boto3 <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model<\/a>. For your reference, below is an example snippet code on how to do it.<\/p>\n\n<pre><code>%%time\nimport boto3\nimport time\n\nsage = boto3.Session().client(service_name='sagemaker')\n\nimage_uri = '520713654638.dkr.ecr.us-east-1.amazonaws.com\/sagemaker-pytorch:1.0.0-cpu-py3'\nmodel_data ='s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/output\/model.tar.gz'\nsource = 's3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/sourcedir.tar.gz'\nrole = 'arn:aws:iam::xxxxxxxx:role\/service-role\/AmazonSageMaker-ExecutionRole-xxxxxx'\n\ntimestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\nmodel_name = 'my-pytorch-model' + timestamp\n\nresponse = sage.create_model(\n    ModelName=model_name,\n    PrimaryContainer={\n        'Image': image_uri,\n        'ModelDataUrl': model_data,\n        'Environment': { 'SAGEMAKER_CONTAINER_LOG_LEVEL':'20', 'SAGEMAKER_ENABLE_CLOUDWATCH_METRICS': 'False', \n                   'SAGEMAKER_PROGRAM': 'generate.py','SAGEMAKER_REGION': 'us-east-1','SAGEMAKER_SUBMIT_DIRECTORY': source}\n         },\n         ExecutionRoleArn=role\n}\nprint(response)\n<\/code><\/pre>\n\n<p>If you get no error message, then the model will shows up in the SageMaker console<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56500704",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1561626837347,
        "Question_original_content":"creat pytorchmodel deploi instanti pytorchmodel object like pytorch import pytorchmodel model pytorchmodel base model model data model data role role framework version entri point serv sourc dir src session session predictor cl imagepredictor model creat deploi wai creat model python sdk model show consol actual deploi endpoint",
        "Question_preprocessed_content":"creat pytorchmodel deploi instanti object like wai creat model python sdk model show consol actual deploi endpoint",
        "Question_gpt_summary_original":"The user is looking for a way to create a SageMaker PyTorchModel object using the sagemaker python SDK so that the model appears in the SageMaker console without deploying it to an endpoint.",
        "Question_gpt_summary":"user look wai creat pytorchmodel object python sdk model appear consol deploi endpoint",
        "Answer_original_content":"think possibl high level pyhton sdk abl call createmodel api low level boto http boto amazonaw com document api latest refer servic html client creat model refer exampl snippet code time import boto import time sage boto session client servic imag uri dkr ecr east amazonaw com pytorch cpu model data output model tar sourc sourcedir tar role arn aw iam role servic role amazon executionrol timestamp time strftime time gmtime model pytorch model timestamp respons sage creat model modelnam model primarycontain imag imag uri modeldataurl model data environ contain log level enabl cloudwatch metric fals program gener region east submit directori sourc executionrolearn role print respons error messag model show consol",
        "Answer_preprocessed_content":"think possibl pyhton sdk abl call createmodel api boto refer exampl snippet code error messag model show consol",
        "Answer_gpt_summary_original":"Solution: It is not possible to create a SageMaker PyTorchModel object using the high-level SageMaker Python SDK without deploying it to an endpoint. However, it can be done by calling the CreateModel API using the low-level boto3. The discussion provides an example snippet code on how to do it using boto3.",
        "Answer_gpt_summary":"solut possibl creat pytorchmodel object high level python sdk deploi endpoint call createmodel api low level boto discuss provid exampl snippet code boto"
    },
    {
        "Question_title":"Shared development details",
        "Question_body":"<p>Hi,<\/p>\n<p>we are trying to use DVC very close to the described <a href=\"https:\/\/dvc.org\/doc\/use-cases\/shared-development-server\" rel=\"noopener nofollow ugc\">shared-development-server<\/a> use-case. This is great for starting, but the devil is in the details.<br>\nThe setup is that we use a shared storage in an HPC environment, where we try to reduce data duplication.<\/p>\n<p>We use a shared cache and because we later might want to connect this to a remote storage, we create a local \u201cremote\u201d storage for the moment.<\/p>\n<p>In the setup we have 2 different types of data. Smaller data files, that are ok if they get downloaded to the individual development folders and a folder containing millions of large files that we don\u2019t want to duplicate.<\/p>\n<p>We have 2 issues now.<\/p>\n<p>Firstly, the local \u201cremote\u201d causes error messages because of writing conflicts. We used the group setting \u201cdvc config cache.shared group\u201d, which seems to work in the cache, but if a folder on the remote 00 to FF seems to be reused, we get write conflicts during \u201cdvc push\u201d.<\/p>\n<p>Secondly, we are not sure how to fare with the large folder with the millions of large files. We probably should create a link to the folder from each individual directory. The data in this folder is generated elsewhere. We want to make sure if we ever have to re-run the data generation, we are informed if this lead to different files. Hence, we want to hash the folder or it\u2019s content, but we are not sure if it\u2019s viable to do this for this amount of files individually. What would be best solution here?<\/p>\n<p>Thanks a lot and best regards<br>\nMarius<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1607527227363,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":520.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/garmhausenmarius\">@garmhausenmarius<\/a>!<\/p>\n<aside class=\"quote no-group\" data-username=\"garmhausenmarius\" data-post=\"1\" data-topic=\"580\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/g\/a698b9\/40.png\" class=\"avatar\"> garmhausenmarius:<\/div>\n<blockquote>\n<p>Firstly, the local \u201cremote\u201d causes error messages because of writing conflicts. We used the group setting \u201cdvc config cache.shared group\u201d, which seems to work in the cache, but if a folder on the remote 00 to FF seems to be reused, we get write conflicts during \u201cdvc push\u201d.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Could you provide more info on how such conflict looks like? Do you, by any chance have some log showing those errors?<\/p>\n<p>As to your second question: I presume each dev\/scientist have hers\/his own folder on this HPC environment, right? I believe it would be possible to even work with the \u201cbig\u201d dataset, if you set cache type to <code>reflink<\/code>, <code>symlink<\/code>, or <code>hardlink<\/code>. Take a look here: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a>.<\/p>\n<p>Is your data versioning handled by <code>git\/dvc<\/code> repository? If so, you can always utilize <code>dvc import<\/code> and <code>dvc get<\/code> to obtain the smaller dataset, if <code>*link<\/code> optimizations are not what you are looking for.<\/p>\n<p>Best regards,<br>\nPawe\u0142<\/p>. <p>Thank you for your swift reply Pawel!<\/p>\n<p>Here are the commands that reproduce the error:<\/p>\n<p>507 dvc pull<br>\n508 dvc push<\/p>\n<p>510 git checkout -b dev\/obsolete<br>\n512 echo \u201ctest\u201d &gt; data\/test.txt<br>\n513 dvc add data\/test.txt<br>\n514 git add data\/.gitignore data\/test.txt.dvc<br>\n515 git commit -m \u201cX\u201d.<br>\n516 dvc push<br>\nERROR: failed to upload \u2018\u2026\/\u2026\/\u2026\/\u2026\/data\/mm\/shared\/dvc\/d8\/e8fca2dc0f896fd7cb4cb0031ba249\u2019 to \u2018\u2026\/\u2026\/\u2026\/\u2026\/data\/mm\/shared\/dvc-storage\/d8\/e8fca2dc0f896fd7cb4cb0031ba249\u2019 - [Errno 13] Permission denied: \u2018\/data\/mm\/shared\/dvc-storage\/d8\/e8fca2dc0f896fd7cb4cb0031ba249.hzdYodwh7UgSBGuVAD4iSE.tmp\u2019<br>\nERROR: failed to push data to the cloud - 1 files failed to upload<\/p>\n<p>The differences in the dvc (cache) and dvc-strorage (local remote) is that the files and folder get created with permissions on group level but not in the uplink case. So there is a new file, we can add it to the cache, but if the folder exists already in the \u201clocal remote\u201d that folder has user specific permission set, which prevents in upload. The target folder seems to iterate, in this case d8. Maybe we could force all of these folder to have the right permissions if not further folder are added after this point?<\/p>\n<p>I will have a look at the link that you provided, to see if this is already what we are looking for.<\/p>\n<p>Best regards<br>\nMarius<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/shared-development-details\/580",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-09T16:24:47.846Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/garmhausenmarius\">@garmhausenmarius<\/a>!<\/p>\n<aside class=\"quote no-group\" data-username=\"garmhausenmarius\" data-post=\"1\" data-topic=\"580\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/g\/a698b9\/40.png\" class=\"avatar\"> garmhausenmarius:<\/div>\n<blockquote>\n<p>Firstly, the local \u201cremote\u201d causes error messages because of writing conflicts. We used the group setting \u201cdvc config cache.shared group\u201d, which seems to work in the cache, but if a folder on the remote 00 to FF seems to be reused, we get write conflicts during \u201cdvc push\u201d.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Could you provide more info on how such conflict looks like? Do you, by any chance have some log showing those errors?<\/p>\n<p>As to your second question: I presume each dev\/scientist have hers\/his own folder on this HPC environment, right? I believe it would be possible to even work with the \u201cbig\u201d dataset, if you set cache type to <code>reflink<\/code>, <code>symlink<\/code>, or <code>hardlink<\/code>. Take a look here: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a>.<\/p>\n<p>Is your data versioning handled by <code>git\/dvc<\/code> repository? If so, you can always utilize <code>dvc import<\/code> and <code>dvc get<\/code> to obtain the smaller dataset, if <code>*link<\/code> optimizations are not what you are looking for.<\/p>\n<p>Best regards,<br>\nPawe\u0142<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-09T17:13:56.646Z",
                "Answer_body":"<p>Thank you for your swift reply Pawel!<\/p>\n<p>Here are the commands that reproduce the error:<\/p>\n<p>507 dvc pull<br>\n508 dvc push<\/p>\n<p>510 git checkout -b dev\/obsolete<br>\n512 echo \u201ctest\u201d &gt; data\/test.txt<br>\n513 dvc add data\/test.txt<br>\n514 git add data\/.gitignore data\/test.txt.dvc<br>\n515 git commit -m \u201cX\u201d.<br>\n516 dvc push<br>\nERROR: failed to upload \u2018\u2026\/\u2026\/\u2026\/\u2026\/data\/mm\/shared\/dvc\/d8\/e8fca2dc0f896fd7cb4cb0031ba249\u2019 to \u2018\u2026\/\u2026\/\u2026\/\u2026\/data\/mm\/shared\/dvc-storage\/d8\/e8fca2dc0f896fd7cb4cb0031ba249\u2019 - [Errno 13] Permission denied: \u2018\/data\/mm\/shared\/dvc-storage\/d8\/e8fca2dc0f896fd7cb4cb0031ba249.hzdYodwh7UgSBGuVAD4iSE.tmp\u2019<br>\nERROR: failed to push data to the cloud - 1 files failed to upload<\/p>\n<p>The differences in the dvc (cache) and dvc-strorage (local remote) is that the files and folder get created with permissions on group level but not in the uplink case. So there is a new file, we can add it to the cache, but if the folder exists already in the \u201clocal remote\u201d that folder has user specific permission set, which prevents in upload. The target folder seems to iterate, in this case d8. Maybe we could force all of these folder to have the right permissions if not further folder are added after this point?<\/p>\n<p>I will have a look at the link that you provided, to see if this is already what we are looking for.<\/p>\n<p>Best regards<br>\nMarius<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"share develop detail try us close describ share develop server us case great start devil detail setup us share storag hpc environ try reduc data duplic us share cach later want connect remot storag creat local remot storag moment setup differ type data smaller data file download individu develop folder folder contain million larg file dont want duplic issu firstli local remot caus error messag write conflict group set config cach share group work cach folder remot reus write conflict push secondli sure fare larg folder million larg file probabl creat link folder individu directori data folder gener want sure run data gener inform lead differ file want hash folder content sure viabl file individu best solut thank lot best regard mariu",
        "Question_preprocessed_content":"share develop detail try us close describ great start devil detail setup us share storag hpc environ try reduc data duplic us share cach later want connect remot storag creat local remot storag moment setup differ type data smaller data file download individu develop folder folder contain million larg file dont want duplic issu firstli local remot caus error messag write conflict group set config group work cach folder remot reus write conflict push secondli sure fare larg folder million larg file probabl creat link folder individu directori data folder gener want sure data gener inform lead differ file want hash folder content sure viabl file individu best solut thank lot best regard mariu",
        "Question_gpt_summary_original":"The user is encountering two issues while using DVC for shared development on a shared storage in an HPC environment. Firstly, the local \"remote\" is causing error messages due to writing conflicts during \"dvc push\". Secondly, the user is unsure how to handle a large folder with millions of files that they do not want to duplicate and want to hash for data generation. The user is seeking advice on the best solution for these challenges.",
        "Question_gpt_summary":"user encount issu share develop share storag hpc environ firstli local remot caus error messag write conflict push secondli user unsur handl larg folder million file want duplic want hash data gener user seek advic best solut challeng",
        "Answer_original_content":"garmhausenmariu garmhausenmariu firstli local remot caus error messag write conflict group set config cach share group work cach folder remot reus write conflict push provid info conflict look like chanc log show error second question presum dev scientist folder hpc environ right believ possibl work big dataset set cach type reflink symlink hardlink look http org doc user guid larg dataset optim data version handl git repositori util import obtain smaller dataset link optim look best regard paw thank swift repli pawel command reproduc error pull push git checkout dev obsolet echo test data test txt add data test txt git add data gitignor data test txt git commit push error fail upload data share efcadcffdcbcbba data share storag efcadcffdcbcbba errno permiss deni data share storag efcadcffdcbcbba hzdyodwhugsbguvadis tmp error fail push data cloud file fail upload differ cach strorag local remot file folder creat permiss group level uplink case new file add cach folder exist local remot folder user specif permiss set prevent upload target folder iter case mayb forc folder right permiss folder ad point look link provid look best regard mariu",
        "Answer_preprocessed_content":"garmhausenmariu firstli local remot caus error messag write conflict group set config group work cach folder remot reus write conflict push provid info conflict look like chanc log show error second question presum folder hpc environ right believ possibl work big dataset set cach type look data version handl repositori util obtain smaller dataset optim look best regard paw thank swift repli pawel command reproduc error pull push git checkout echo test add git add git commit push error fail upload permiss deni error fail push data cloud file fail upload differ strorag file folder creat permiss group level uplink case new file add cach folder exist local remot folder user specif permiss set prevent upload target folder iter case mayb forc folder right permiss folder ad point look link provid look best regard mariu",
        "Answer_gpt_summary_original":"Solution 1: The user can set cache type to reflink, symlink, or hardlink to work with the large dataset without duplicating it. They can also use dvc import and dvc get to obtain the smaller dataset if link optimizations are not what they are looking for.\n\nSolution 2: The user can force all of the folders to have the right permissions if no further folders are added after a certain point to prevent upload errors due to user-specific permissions. \n\nNote: No solution is provided for the issue of handling a large folder with millions of files that they do not want to duplicate and want to hash for data generation.",
        "Answer_gpt_summary":"solut user set cach type reflink symlink hardlink work larg dataset duplic us import obtain smaller dataset link optim look solut user forc folder right permiss folder ad certain point prevent upload error user specif permiss note solut provid issu handl larg folder million file want duplic want hash data gener"
    },
    {
        "Question_title":"AML run.log() and run.log_list() fail without error",
        "Question_body":"<p>I have a Pipeline with DatabricksSteps each containing:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\n#do stuff\nrun.log(name, val, desc)\nrun.log_list(name, vals, desc)\nrun.log_image(title, fig, desc)\n<\/code><\/pre>\n\n<p>Only <code>log_image()<\/code> seems to work.  The image appears in the \"images\" section of the AML experiment workspace as expected, but the \"tracked metrics\" and \"charts\" areas are blank.  In an interactive job, <code>run.log()<\/code> and <code>run.log_list()<\/code> work as expected.  I tested that there is no problem with the arguments by using <code>print()<\/code> instead of <code>run.log()<\/code>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1569018204223,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":121.0,
        "Answer_body":"<p>Add run.flush() at the end of the script.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58035744",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1569427861030,
        "Question_original_content":"aml run log run log list fail error pipelin databricksstep contain core run import run run run context stuff run log val desc run log list val desc run log imag titl fig desc log imag work imag appear imag section aml experi workspac expect track metric chart area blank interact job run log run log list work expect test problem argument print instead run log",
        "Question_preprocessed_content":"aml fail error pipelin databricksstep contain work imag appear imag section aml experi workspac expect track metric chart area blank interact job work expect test problem argument instead",
        "Question_gpt_summary_original":"The user is facing challenges with the AML run.log() and run.log_list() functions, which fail without error. The only function that seems to work is run.log_image(). The tracked metrics and charts areas are blank, and the user has tested that there is no problem with the arguments by using print() instead of run.log(). The issue occurs in a Pipeline with DatabricksSteps.",
        "Question_gpt_summary":"user face challeng aml run log run log list function fail error function work run log imag track metric chart area blank user test problem argument print instead run log issu occur pipelin databricksstep",
        "Answer_original_content":"add run flush end script",
        "Answer_preprocessed_content":"add end script",
        "Answer_gpt_summary_original":"Solution: The solution mentioned in the discussion is to add run.flush() at the end of the script.",
        "Answer_gpt_summary":"solut solut mention discuss add run flush end script"
    },
    {
        "Question_title":"Azure Machine Learning Services - AutoMl - Error running experiment.submit: \"\/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\"",
        "Question_body":"Hello,\n\nI created a notebook in the workspace and when I sent the experiment for training I received error message undefined symbol: XGDMatrixSetDenseInfo for algorithm Xgboost. Do you know how to fix the problem?\n\n\n\n\nAzure ML Version: 1.22.0\nCompute Instance: Standard_DS3_v2\n\nCode:\n\nimport logging\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.core.experiment import Experiment\n\nautoml_settings = {\n\"iteration_timeout_minutes\": 10,\n\"experiment_timeout_hours\": 0.3,\n\"enable_early_stopping\": True,\n\"primary_metric\": 'normalized_root_mean_squared_error',\n\"featurization\": 'auto',\n\"verbosity\": logging.INFO,\n\"n_cross_validations\": 5\n}\n\nautoml_config = AutoMLConfig(task='regression',\ndebug_log='automated_ml_errors.log',\ntraining_data=x_train,\nlabel_column_name=\"production_time\",\n**automl_settings)\n\nexperiment = Experiment(ws, \"train-model\")\nlocal_run = experiment.submit(automl_config, show_output=True)\n\nFull Error Message:\n\n\n\n\nERROR: FitException:\nMessage: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\nInnerException: AttributeError: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\nErrorResponse\n{\n\"error\": {\n\"code\": \"SystemError\",\n\"message\": \"Encountered an internal AutoML error. Error Message\/Code: FitException. Additional Info: FitException:\\n\\tMessage: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\\n\\tInnerException: None\\n\\tErrorResponse \\n{\\n \\\"error\\\": {\\n \\\"message\\\": \\\"\/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\\\",\\n \\\"target\\\": \\\"Xgboost\\\",\\n \\\"reference_code\\\": \\\"Xgboost\\\"\\n }\\n}\",\n\"details_uri\": \"https:\/\/docs.microsoft.com\/azure\/machine-learning\/resource-known-issues#automated-machine-learning\",\n\"target\": \"Xgboost\",\n\"inner_error\": {\n\"code\": \"ClientError\",\n\"inner_error\": {\n\"code\": \"AutoMLInternal\"\n}\n},\n\"reference_code\": \"Xgboost\"\n}\n}\n\n\n\n\n\nBest regards,\nCristina\n\n\n\n\n\n\n\n\n79658-packages.txt",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616170731780,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, can you try uninstalling and reinstalling Xgboost (try versions <= 0.90 if you continue to get errors).",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/322886\/azure-machine-learning-services-automl-error-runni.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-19T23:29:59.997Z",
                "Answer_score":0,
                "Answer_body":"Hi, can you try uninstalling and reinstalling Xgboost (try versions <= 0.90 if you continue to get errors).",
                "Answer_comment_count":3,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1616196599996,
        "Question_original_content":"servic automl error run experi submit anaconda env lib libxgboost undefin symbol xgdmatrixsetdenseinfo hello creat notebook workspac sent experi train receiv error messag undefin symbol xgdmatrixsetdenseinfo algorithm xgboost know fix problem version comput instanc standard code import log train automl import automlconfig core experi import experi automl set iter timeout minut experi timeout hour enabl earli stop true primari metric normal root mean squar error featur auto verbos log info cross valid automl config automlconfig task regress debug log autom error log train data train label column product time automl set experi experi train model local run experi submit automl config output true error messag error fitexcept messag anaconda env lib libxgboost undefin symbol xgdmatrixsetdenseinfo innerexcept attributeerror anaconda env lib libxgboost undefin symbol xgdmatrixsetdenseinfo errorrespons error code systemerror messag encount intern automl error error messag code fitexcept addit info fitexcept tmessag anaconda env lib libxgboost undefin symbol xgdmatrixsetdenseinfo tinnerexcept terrorrespons error messag anaconda env lib libxgboost undefin symbol xgdmatrixsetdenseinfo target xgboost refer code xgboost detail uri http doc microsoft com azur machin learn resourc known issu autom machin learn target xgboost inner error code clienterror inner error code automlintern refer code xgboost best regard cristina packag txt",
        "Question_preprocessed_content":"servic automl error run undefin symbol xgdmatrixsetdenseinfo hello creat notebook workspac sent experi train receiv error messag undefin symbol xgdmatrixsetdenseinfo algorithm xgboost know fix problem version comput instanc code import log import automlconfig import experi automlconfig experi experi error messag error fitexcept messag undefin symbol xgdmatrixsetdenseinfo innerexcept attributeerror undefin symbol xgdmatrixsetdenseinfo errorrespons error target xgboost xgboost best regard cristina",
        "Question_gpt_summary_original":"The user encountered an error message \"undefined symbol: XGDMatrixSetDenseInfo\" while running an experiment for training in Azure Machine Learning Services AutoML with Xgboost algorithm. The error occurred during the execution of the code and the user is seeking help to fix the problem.",
        "Question_gpt_summary":"user encount error messag undefin symbol xgdmatrixsetdenseinfo run experi train servic automl xgboost algorithm error occur execut code user seek help fix problem",
        "Answer_original_content":"try uninstal reinstal xgboost try version continu error",
        "Answer_preprocessed_content":"try uninstal reinstal xgboost",
        "Answer_gpt_summary_original":"Solution: The user can try uninstalling and reinstalling Xgboost, and if the error persists, they can try using versions of Xgboost that are less than or equal to 0.90.",
        "Answer_gpt_summary":"solut user try uninstal reinstal xgboost error persist try version xgboost equal"
    },
    {
        "Question_title":"Best way to run 1000s of training jobs on sagemaker",
        "Question_body":"<p>I have thousands of training jobs that I want to run on sagemaker. Basically I have a list of hyperparameters and I want to train the model for <em>all<\/em> of those hyperparmeters in parallel (not a standard hyperparameter tuning where we just want to optimize the hyperparameter, here we want to train for all of the hyperparameters). I have searched the docs quite extensively but it surprises me that I couldn't find any info about this, even though it seems like a pretty basic functionality.<\/p>\n<p>For example, let's say I have 10,000 training jobs, and my quota is 20 instances, what is the best way to run these jobs utilizing all my available instances? In particular,<\/p>\n<ul>\n<li>Is there a &quot;queue manager&quot; functionality that takes the list of hyperparameters and runs the training jobs in batches of 20 until they are all done (even better if it could keep track of failed\/completed jobs).<\/li>\n<li>Is it best practice to run a single training job per instance? If that's the case do I need to ask for a much higher quota on the number of instance?<\/li>\n<li>If this functionality does not exist in sagemaker, is it worth using EC2 instead since it's a bit cheaper?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1650506611153,
        "Question_favorite_count":null,
        "Question_last_edit_time":1650508898300,
        "Question_score":0.0,
        "Question_view_count":269.0,
        "Answer_body":"<p>Your question is very broad and the best way forward would depend on other details of your use-case, so we will have to make some assumptions.<\/p>\n<p>[Queue manager]\nSageMaker does <em>not<\/em> have a queue manager. If at the end you decide you need a queue manager, I would suggest looking towards AWS Batch.<\/p>\n<p>[Single vs multiple training jobs]\nSince you need to run 10s of thousands job I assume you are training fairly lightweight models, so to save on time, you would be better off reusing instances for multiple training jobs. (Otherwise, with 20 instances limit, you need 500 rounds of training, with a 3 min start time - depending on instance type - you need 25 hours just for the wait time. Depending on the complexity of each individual model, this 25hours might be significant or totally acceptable).<\/p>\n<p>[Instance limit increase]\nYou can always ask for a limit increase, but going from a limit of 20 to 10k at once is likely that will not be accepted by the AWS support team, unless you are part of an organisation with a track record of usage on AWS, in which case this might be fine.<\/p>\n<p>[One possible option] (Assuming multiple lightweight models)\nYou could create a single training job, with instance count, the number of instances available to you.\nInside the training job, your code can run a for loop and perform all the individual training jobs you need.<\/p>\n<p>In this case, you will need to know which which instance is which so you can make the split of the HPOs. SageMaker writes this information on the file: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-dist-training\" rel=\"nofollow noreferrer\">\/opt\/ml\/input\/config\/resourceconfig.json<\/a> so using that you can easily have each instance run a subset of the trainings required.<\/p>\n<p>Another thing to think of, is if you need to save the generated models (which you probably need). You can either save everything in the output model directory - standard SM approach- but this would zip all models in a model.tar.gz file.\nIf you don't want this, and prefer to have each model individually saved, I'd suggest using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">checkpoints<\/a> directory that will sync anything written there to your s3 location.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71948090",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1650614502720,
        "Question_original_content":"best wai run train job thousand train job want run basic list hyperparamet want train model hyperparmet parallel standard hyperparamet tune want optim hyperparamet want train hyperparamet search doc extens surpris couldn info like pretti basic function exampl let train job quota instanc best wai run job util avail instanc particular queue manag function take list hyperparamet run train job batch better track fail complet job best practic run singl train job instanc case need ask higher quota number instanc function exist worth instead bit cheaper",
        "Question_preprocessed_content":"best wai run train job thousand train job want run basic list hyperparamet want train model hyperparmet parallel search doc extens surpris couldn info like pretti basic function exampl let train job quota instanc best wai run job util avail instanc particular queue manag function take list hyperparamet run train job batch best practic run singl train job instanc case need ask higher quota number instanc function exist worth instead bit cheaper",
        "Question_gpt_summary_original":"The user is facing challenges in running thousands of training jobs on sagemaker. They are looking for a way to train the model for all hyperparameters in parallel and utilize all available instances. The user is seeking information on whether there is a queue manager functionality that can run training jobs in batches, whether it is best practice to run a single training job per instance, and whether it is worth using EC2 instead of sagemaker.",
        "Question_gpt_summary":"user face challeng run thousand train job look wai train model hyperparamet parallel util avail instanc user seek inform queue manag function run train job batch best practic run singl train job instanc worth instead",
        "Answer_original_content":"question broad best wai forward depend detail us case assumpt queue manag queue manag end decid need queue manag suggest look aw batch singl multipl train job need run thousand job assum train fairli lightweight model save time better reus instanc multipl train job instanc limit need round train min start time depend instanc type need hour wait time depend complex individu model hour signific total accept instanc limit increas ask limit increas go limit like accept aw support team organis track record usag aw case fine possibl option assum multipl lightweight model creat singl train job instanc count number instanc avail insid train job code run loop perform individu train job need case need know instanc split hpo write inform file opt input config resourceconfig json easili instanc run subset train requir thing think need save gener model probabl need save output model directori standard approach zip model model tar file want prefer model individu save suggest checkpoint directori sync written locat",
        "Answer_preprocessed_content":"question broad best wai forward depend detail assumpt queue manag queue manag end decid need queue manag suggest look aw batch singl multipl train job need run thousand job assum train fairli lightweight model save time better reus instanc multipl train job instanc limit increas ask limit increas go limit like accept aw support team organis track record usag aw case fine possibl option creat singl train job instanc count number instanc avail insid train job code run loop perform individu train job need case need know instanc split hpo write inform file easili instanc run subset train requir thing think need save gener model save output model directori standard approach zip model file want prefer model individu save suggest checkpoint directori sync written locat",
        "Answer_gpt_summary_original":"Solutions provided in the discussion are:\n\n- SageMaker does not have a queue manager. AWS Batch can be used if a queue manager is needed.\n- Reusing instances for multiple training jobs is better to save time.\n- Instance limit increase can be requested, but it may not be accepted.\n- A single training job can be created with the instance count equal to the number of instances available. Inside the training job, a for loop can be used to perform all the individual training jobs needed.\n- The information on which instance is which can be found in the file \/opt\/ml\/input\/config\/resourceconfig.json.\n- Checkpoints directory can be used to save each model individually instead of saving all models in a model.tar.gz file.",
        "Answer_gpt_summary":"solut provid discuss queue manag aw batch queue manag need reus instanc multipl train job better save time instanc limit increas request accept singl train job creat instanc count equal number instanc avail insid train job loop perform individu train job need inform instanc file opt input config resourceconfig json checkpoint directori save model individu instead save model model tar file"
    },
    {
        "Question_title":"reading hdf5 file from s3 to sagemaker, is the whole file transferred?",
        "Question_body":"<p>I'm reading a file from my S3 bucket in a notebook in sagemaker studio (same account) using the following code:<\/p>\n<pre><code>dataset_path_in_h5=&quot;\/Mode1\/SingleFault\/SimulationCompleted\/IDV2\/Mode1_IDVInfo_2_100\/Run1\/processdata&quot;\ns3 = s3fs.S3FileSystem()\nh5_file = h5py.File(s3.open(s3url,'rb'), 'r')\ndata = h5_file.get(dataset_path_in_h5)\n<\/code><\/pre>\n<p>But I don't know what actually append behind the scene, does the whole h5 file is being transferred  ? that's seems unlikely as the code is executed quite fast while the whole file is 20GB. Or is just the dataset in dataset_path_in_h5 is transferred ?\nI suppose that if the whole file is transferred at each call it could cost me a lot.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662025046283,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":18.0,
        "Answer_body":"<p>When you open the file, a file object is created. It has a tiny memory footprint. The dataset values aren't read into memory until you access them.<\/p>\n<p>You are returning <code>data<\/code> as a NumPy array. That loads the entire dataset into memory. (NOTE: the <code>.get()<\/code> method you are using is deprecated. Current syntax is provided in the example.)<\/p>\n<p>As an alternative to returning an array, you can create a dataset object (which also has a small memory foorprint). When you do, the data is read into memory as you need it. Dataset objects behave like NumPy arrays. (Use of a dataset object vs NumPy array depends on downstream usage. Frequently you don't need an array, but sometimes they are required.) Also, if chunked I\/O was enabled when the dataset was created, datasets are read in chunks.<\/p>\n<p>Differences shown below. Note, I used Python's file context manager to open the file. It avoids problems if the file isn't closed properly (you forget or the program exits prematurely).<\/p>\n<pre><code>dataset_path_in_h5=&quot;\/Mode1\/SingleFault\/SimulationCompleted\/IDV2\/Mode1_IDVInfo_2_100\/Run1\/processdata&quot;\ns3 = s3fs.S3FileSystem()\nwith h5py.File(s3.open(s3url,'rb'), 'r') as h5_file:\n     # your way to get a numpy array -- .get() is depreciated:\n     data = h5_file.get(dataset_path_in_h5)\n     # this is the preferred syntax to return an array:\n     data_arr = h5_file[dataset_path_in_h5][()]\n     # this returns a h5py dataset object:\n     data_ds = h5_file[dataset_path_in_h5]  # deleted [()] \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73567221",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1662041978820,
        "Question_original_content":"read hdf file file transfer read file bucket notebook studio account follow code dataset path mode singlefault simulationcomplet idv mode idvinfo run processdata sf sfilesystem file hpy file open surl data file dataset path know actual append scene file transfer unlik code execut fast file dataset dataset path transfer suppos file transfer cost lot",
        "Question_preprocessed_content":"read hdf file file transfer read file bucket notebook studio follow code know actual append scene file transfer unlik code execut fast file dataset transfer suppos file transfer cost lot",
        "Question_gpt_summary_original":"The user is facing a challenge in determining whether the entire 20GB HDF5 file is being transferred when reading a specific dataset from their S3 bucket in Sagemaker Studio using Python code. They are concerned about the potential cost implications if the entire file is being transferred each time the code is executed.",
        "Question_gpt_summary":"user face challeng determin entir hdf file transfer read specif dataset bucket studio python code concern potenti cost implic entir file transfer time code execut",
        "Answer_original_content":"open file file object creat tini memori footprint dataset valu aren read memori access return data numpi arrai load entir dataset memori note method deprec current syntax provid exampl altern return arrai creat dataset object small memori foorprint data read memori need dataset object behav like numpi arrai us dataset object numpi arrai depend downstream usag frequent need arrai requir chunk enabl dataset creat dataset read chunk differ shown note python file context manag open file avoid problem file isn close properli forget program exit prematur dataset path mode singlefault simulationcomplet idv mode idvinfo run processdata sf sfilesystem hpy file open surl file wai numpi arrai depreci data file dataset path prefer syntax return arrai data arr file dataset path return hpy dataset object data file dataset path delet",
        "Answer_preprocessed_content":"open file file object creat tini memori footprint dataset valu aren read memori access return numpi arrai load entir dataset memori altern return arrai creat dataset object data read memori need dataset object behav like numpi arrai chunk enabl dataset creat dataset read chunk differ shown note python file context manag open file avoid problem file isn close properli",
        "Answer_gpt_summary_original":"Solution:\n- Instead of returning a NumPy array, create a dataset object which has a small memory footprint and reads data into memory as needed.\n- Use of a dataset object vs NumPy array depends on downstream usage. Frequently you don't need an array, but sometimes they are required.\n- If chunked I\/O was enabled when the dataset was created, datasets are read in chunks.",
        "Answer_gpt_summary":"solut instead return numpi arrai creat dataset object small memori footprint read data memori need us dataset object numpi arrai depend downstream usag frequent need arrai requir chunk enabl dataset creat dataset read chunk"
    },
    {
        "Question_title":"Should I run forecast predictive model with AWS lambda or sagemaker?",
        "Question_body":"<p>I've been reading some articles regarding this topic and have preliminary thoughts as what I should do with it, but still want to see if anyone can share comments if you have more experience with running machine learning on AWS. I was doing a project for a professor at school, and we decided to use AWS. I need to find a cost-effective and efficient way to deploy a forecasting model on it. <\/p>\n\n<p>What we want to achieve is:<\/p>\n\n<ul>\n<li>read the data from S3 bucket monthly (there will be new data coming in every month), <\/li>\n<li>run a few python files (.py) for custom-built packages and install dependencies (including the files, no more than 30kb), <\/li>\n<li>produce predicted results into a file back in S3 (JSON or CSV works), or push to other endpoints (most likely to be some BI tools - tableau etc.) - but really this step can be flexible (not web for sure) <\/li>\n<\/ul>\n\n<p><strong>First thought I have is AWS sagemaker<\/strong>. However, we'll be using \"fb prophet\" model to predict the results, and we built a customized package to use in the model, therefore, I don't think the notebook instance is gonna help us. (Please correct me if I'm wrong) My understanding is that sagemaker is a environment to build and train the model, but we already built and trained the model. Plus, we won't be using AWS pre-built models anyways.<\/p>\n\n<p>Another thing is if we want to use custom-built package, we will need to create container image, and I've never done that before, not sure about the efforts to do that.<\/p>\n\n<p><strong>2nd option is to create multiple lambda functions<\/strong><\/p>\n\n<ul>\n<li><p>one that triggers to run the python scripts from S3 bucket (2-3 .py files) every time a new file is imported into S3 bucket, which will happen monthly.<\/p><\/li>\n<li><p>one that trigger after the python scripts are done running and produce results and save into S3 bucket.<\/p><\/li>\n<\/ul>\n\n<p>3rd option will combine both options:\n - Use lambda function to trigger the implementation on the python scripts in S3 bucket when the new file comes in.\n - Push the result using sagemaker endpoint, which means we host the model on sagemaker and deploy from there.<\/p>\n\n<p>I am still not entirely sure how to put pre-built model and python scripts onto sagemaker instance and host from there.<\/p>\n\n<p>I'm hoping whoever has more experience with AWS service can help give me some guidance, in terms of more cost-effective and efficient way to run model.<\/p>\n\n<p>Thank you!! <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1586306942937,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":2675.0,
        "Answer_body":"<p>I would say it all depends on how heavy your model is \/ how much data you're running through it. You're right to identify that Lambda will likely be less work. It's quite easy to get a lambda up and running to do the things that you need, and <a href=\"https:\/\/aws.amazon.com\/lambda\/pricing\/\" rel=\"nofollow noreferrer\">Lambda has a very generous free tier<\/a>. The problem is:<\/p>\n\n<ol>\n<li><p>Lambda functions are fundamentally limited in their processing capacity (they timeout after <em>max<\/em> 15 minutes).<\/p><\/li>\n<li><p>Your model might be expensive to load.<\/p><\/li>\n<\/ol>\n\n<p>If you have a lot of data to run through your model, you will need multiple lambdas. Multiple lambdas means you have to load your model multiple times, and that's wasted work. If you're working with \"big data\" this will get expensive once you get through the free tier.<\/p>\n\n<p>If you don't have much data, Lambda will work just fine. I would eyeball it as follows: assuming your data processing step is dominated by your model step, and if all your model interactions (loading the model + evaluating all your data) take less than 15min, you're definitely fine. If they take more, you'll need to do a back-of-the-envelope calculation to figure out whether you'd leave the Lambda free tier.<\/p>\n\n<p>Regarding Lambda: You can literally copy-paste code in to setup a prototype. If your execution takes more than 15min for all your data, you'll need a method of splitting your data up between multiple Lambdas. Consider <a href=\"https:\/\/aws.amazon.com\/step-functions\/\" rel=\"nofollow noreferrer\">Step Functions<\/a> for this.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61091659",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1586317398727,
        "Question_original_content":"run forecast predict model aw lambda read articl topic preliminari thought want share comment experi run machin learn aw project professor school decid us aw need cost effect effici wai deploi forecast model want achiev read data bucket monthli new data come month run python file custom built packag instal depend includ file produc predict result file json csv work push endpoint like tool tableau step flexibl web sure thought prophet model predict result built custom packag us model think notebook instanc gonna help correct wrong understand environ build train model built train model plu won aw pre built model anywai thing want us custom built packag need creat contain imag sure effort option creat multipl lambda function trigger run python script bucket file time new file import bucket happen monthli trigger python script run produc result save bucket option combin option us lambda function trigger implement python script bucket new file come push result endpoint mean host model deploi entir sure pre built model python script instanc host hope experi aw servic help guidanc term cost effect effici wai run model thank",
        "Question_preprocessed_content":"run forecast predict model aw lambda read articl topic preliminari thought want share comment experi run machin learn aw project professor school decid us aw need effici wai deploi forecast model want achiev read data bucket monthli run python file packag instal depend produc predict result file push endpoint step flexibl thought prophet model predict result built custom packag us model think notebook instanc gonna help understand environ build train model built train model plu won aw model anywai thing want us packag need creat contain imag sure effort option creat multipl lambda function trigger run python script bucket time new file import bucket happen monthli trigger python script run produc result save bucket option combin option us lambda function trigger implement python script bucket new file come push result endpoint mean host model deploi entir sure model python script instanc host hope experi aw servic help guidanc term effici wai run model thank",
        "Question_gpt_summary_original":"The user is trying to find a cost-effective and efficient way to deploy a forecasting model on AWS. They are considering using AWS Sagemaker, but they have a customized package and won't be using AWS pre-built models. They are also unsure about creating a container image. Another option they are considering is creating multiple lambda functions to trigger the implementation of python scripts in S3 bucket and push the result using Sagemaker endpoint. The user is seeking guidance on the most cost-effective and efficient way to run the model.",
        "Question_gpt_summary":"user try cost effect effici wai deploi forecast model aw consid custom packag won aw pre built model unsur creat contain imag option consid creat multipl lambda function trigger implement python script bucket push result endpoint user seek guidanc cost effect effici wai run model",
        "Answer_original_content":"depend heavi model data run right identifi lambda like work easi lambda run thing need lambda gener free tier problem lambda function fundament limit process capac timeout max minut model expens load lot data run model need multipl lambda multipl lambda mean load model multipl time wast work work big data expens free tier data lambda work fine eyebal follow assum data process step domin model step model interact load model evalu data min definit fine need envelop calcul figur leav lambda free tier lambda liter copi past code setup prototyp execut take min data need method split data multipl lambda consid step function",
        "Answer_preprocessed_content":"depend heavi model data run right identifi lambda like work easi lambda run thing need lambda gener free tier problem lambda function fundament limit process capac model expens load lot data run model need multipl lambda multipl lambda mean load model multipl time wast work work big data expens free tier data lambda work fine eyebal follow assum data process step domin model step model interact min definit fine need calcul figur leav lambda free tier lambda liter code setup prototyp execut take min data need method split data multipl lambda consid step function",
        "Answer_gpt_summary_original":"The discussion suggests that using AWS Lambda could be a cost-effective solution for deploying the forecasting model, but it has limitations in processing capacity and may require multiple lambdas if there is a lot of data to process. If the model interactions take less than 15 minutes, Lambda will work fine. However, if it takes more than 15 minutes, the user will need to split the data between multiple Lambdas using Step Functions. No solution is provided for creating a container image.",
        "Answer_gpt_summary":"discuss suggest aw lambda cost effect solut deploi forecast model limit process capac requir multipl lambda lot data process model interact minut lambda work fine take minut user need split data multipl lambda step function solut provid creat contain imag"
    },
    {
        "Question_title":"Azure ML - Managed Identity for Compute Instance",
        "Question_body":"We need to connect Azure Data Lake Storage Gen2 to Azure Machine Learning by means of a datastore. For security reasons we do not want to provide the credential-based authentication credentials (service principal or SAS token). Instead we want to connect with identity based access.\n\nThe problem we face is that we are not able to assign a managed identity to a compute instance, so we can connect from notebooks to the Data Lake. In the documentation is explained how to assign a managed identity to a cluster, but we need the same for the compute instance, as it is the only way to run commands directly from the notebook.\n\nIs there a way to assign managed identity to an Azure Machine Learning Compute Instance? Otherwise, we would like to know the best approach to overcome this issue, considering that we do not want to introduce the credentials in the code.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637143820660,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Answer_body":"@Nimbeo-7089 Thanks for the question. Currently It\u2019s not supported yet to assign managed identity to an Azure Machine Learning Compute Instance, you\u2019d need to use credential-based access. We have forwarded to the product team to support in the near future.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/630520\/azure-ml-managed-identity-for-compute-instance.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-17T13:54:33.153Z",
                "Answer_score":1,
                "Answer_body":"@Nimbeo-7089 Thanks for the question. Currently It\u2019s not supported yet to assign managed identity to an Azure Machine Learning Compute Instance, you\u2019d need to use credential-based access. We have forwarded to the product team to support in the near future.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1637157273152,
        "Question_original_content":"manag ident comput instanc need connect azur data lake storag gen mean datastor secur reason want provid credenti base authent credenti servic princip sa token instead want connect ident base access problem face abl assign manag ident comput instanc connect notebook data lake document explain assign manag ident cluster need comput instanc wai run command directli notebook wai assign manag ident comput instanc like know best approach overcom issu consid want introduc credenti code",
        "Question_preprocessed_content":"manag ident comput instanc need connect azur data lake storag gen mean datastor secur reason want provid authent credenti instead want connect ident base access problem face abl assign manag ident comput instanc connect notebook data lake document explain assign manag ident cluster need comput instanc wai run command directli notebook wai assign manag ident comput instanc like know best approach overcom issu consid want introduc credenti code",
        "Question_gpt_summary_original":"The user is facing a challenge in connecting Azure Data Lake Storage Gen2 to Azure Machine Learning through a datastore without providing credential-based authentication credentials for security reasons. They want to connect with identity-based access but are unable to assign a managed identity to a compute instance, which is necessary to run commands directly from the notebook. The user is seeking a solution to assign a managed identity to an Azure Machine Learning Compute Instance or the best approach to overcome this issue without introducing credentials in the code.",
        "Question_gpt_summary":"user face challeng connect azur data lake storag gen datastor provid credenti base authent credenti secur reason want connect ident base access unabl assign manag ident comput instanc necessari run command directli notebook user seek solut assign manag ident comput instanc best approach overcom issu introduc credenti code",
        "Answer_original_content":"nimbeo thank question current support assign manag ident comput instanc youd need us credenti base access forward product team support near futur",
        "Answer_preprocessed_content":"thank question current support assign manag ident comput instanc youd need us access forward product team support near futur",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion. The user is informed that it is currently not possible to assign a managed identity to an Azure Machine Learning Compute Instance and that credential-based access is required. The issue has been forwarded to the product team for future support.",
        "Answer_gpt_summary":"solut solut provid discuss user inform current possibl assign manag ident comput instanc credenti base access requir issu forward product team futur support"
    },
    {
        "Question_title":"Importing a file into jupyterlabs from s3",
        "Question_body":"<p>I have a file I want to import into a Sagemaker Jupyter notebook python 3 instance for use.  The exact code would be 'import lstm.'  I can store the file in s3 (which would probably be ideal) or locally, whichever you prefer.  I have been searching the internet for a while and have been unable to find a solution to this.  I am actually just trying to run\/understand this code from Suraj Raval's youtube channel: <a href=\"https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot\" rel=\"nofollow noreferrer\">https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot<\/a>.  The 'import lstm' line is failing when I run, and I am trying to figure out how to make this work.  <\/p>\n\n<p>I have tried:\nfrom s3:\/\/... import lstm.  failed\nI have tried some boto3 methods and wasn't able to get it to work.  <\/p>\n\n<pre><code>import time\nimport threading\nimport lstm, etl, json. ##this line\nimport numpy as np\nimport pandas as pd\nimport h5py\nimport matplotlib.pyplot as plt\nconfigs = json.loads(open('configs.json').read())\ntstart = time.time()\n<\/code><\/pre>\n\n<p>I would just like to be able to import the lstm file and all the others into a Jupyter notebook instance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1555242786160,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":1863.0,
        "Answer_body":"<p>I think you should  be cloning the Github repo in SageMaker instance and not importing the files from S3. I was able to reproduce the Bitcoin Trading Bot notebook from SageMaker by cloning it. You can follow the below steps<\/p>\n\n<h3>Cloning Github Repo to SageMaker Notebook<\/h3>\n\n<ol>\n<li>Open JupyterLab from the AWS SageMaker console.<\/li>\n<li>From the JupyterLab  Launcher, open the Terminal.<\/li>\n<li>Change directory to SageMaker<\/li>\n<\/ol>\n\n<pre><code>cd ~\/SageMaker\n<\/code><\/pre>\n\n<ol start=\"4\">\n<li>Clone the BitCoin Trading Bot <a href=\"https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot\" rel=\"nofollow noreferrer\">git repo<\/a><\/li>\n<\/ol>\n\n<pre><code>git clone https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot.git\ncd Bitcoin_Trading_Bot\n<\/code><\/pre>\n\n<ol start=\"5\">\n<li>Now you can open the notebook <code>Bitcoin LSTM Prediction.ipynb<\/code> and select the Tensorflow Kernel to run the notebook.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YbKic.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YbKic.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<h3>Adding files from local machine to SageMaker Notebook<\/h3>\n\n<p>To add files from your local machine to SageMaker Notebook instance, you can use <a href=\"https:\/\/jupyterlab.readthedocs.io\/en\/stable\/user\/files.html\" rel=\"nofollow noreferrer\">file upload<\/a> functionality in JupyterLab<\/p>\n\n<h3>Adding files from S3 to SageMaker Notebook<\/h3>\n\n<p>To add files from S3 to SageMaker Notebook instance, use AWS CLI or Python SDK to upload\/download files. <\/p>\n\n<p>For example, to download <code>lstm.py<\/code> file from S3 to SageMaker using AWS CLI<\/p>\n\n<pre><code>aws s3 cp s3:\/\/mybucket\/bot\/src\/lstm.py .\n<\/code><\/pre>\n\n<p>Using <code>boto3<\/code> API<\/p>\n\n<pre><code>import boto3\ns3 = boto3.resource('s3')\ns3.meta.client.download_file('mybucket', 'bot\/src\/lstm.py', '.\/lstm.py')\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1555346571920,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55674959",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1555252725128,
        "Question_original_content":"import file jupyterlab file want import jupyt notebook python instanc us exact code import lstm store file probabl ideal local whichev prefer search internet unabl solut actual try run understand code suraj raval youtub channel http github com llsourcel bitcoin trade bot import lstm line fail run try figur work tri import lstm fail tri boto method wasn abl work import time import thread import lstm etl json line import numpi import panda import hpy import matplotlib pyplot plt config json load open config json read tstart time time like abl import lstm file jupyt notebook instanc",
        "Question_preprocessed_content":"import file jupyterlab file want import jupyt notebook python instanc us exact code import store file local whichev prefer search internet unabl solut actual try code suraj raval youtub channel import lstm line fail run try figur work tri import lstm fail tri boto method wasn abl work like abl import lstm file jupyt notebook instanc",
        "Question_gpt_summary_original":"The user is facing challenges in importing a file named 'lstm' into a Sagemaker Jupyter notebook python 3 instance for use. The user has tried importing the file from s3 and using boto3 methods, but both have failed. The 'import lstm' line is failing when the user runs the code, and the user is trying to figure out how to make it work. The user wants to import the lstm file and all the others into a Jupyter notebook instance.",
        "Question_gpt_summary":"user face challeng import file name lstm jupyt notebook python instanc us user tri import file boto method fail import lstm line fail user run code user try figur work user want import lstm file jupyt notebook instanc",
        "Answer_original_content":"think clone github repo instanc import file abl reproduc bitcoin trade bot notebook clone follow step clone github repo notebook open jupyterlab consol jupyterlab launcher open termin chang directori clone bitcoin trade bot git repo git clone http github com llsourcel bitcoin trade bot git bitcoin trade bot open notebook bitcoin lstm predict ipynb select tensorflow kernel run notebook ad file local machin notebook add file local machin notebook instanc us file upload function jupyterlab ad file notebook add file notebook instanc us aw cli python sdk upload download file exampl download lstm file aw cli aw mybucket bot src lstm boto api import boto boto resourc meta client download file mybucket bot src lstm lstm",
        "Answer_preprocessed_content":"think clone github repo instanc import file abl reproduc bitcoin trade bot notebook clone follow step clone github repo notebook open jupyterlab consol jupyterlab launcher open termin chang directori clone bitcoin trade bot git repo open notebook select tensorflow kernel run notebook ad file local machin notebook add file local machin notebook instanc us file upload function jupyterlab ad file notebook add file notebook instanc us aw cli python sdk file exampl download file aw cli api",
        "Answer_gpt_summary_original":"The solution suggested in the discussion is to clone the Github repo in SageMaker instance instead of importing the files from S3. The user can follow the steps mentioned in the discussion to clone the BitCoin Trading Bot git repo and open the notebook Bitcoin LSTM Prediction.ipynb. The user can also add files from their local machine to SageMaker Notebook instance using file upload functionality in JupyterLab or use AWS CLI or Python SDK to upload\/download files from S3.",
        "Answer_gpt_summary":"solut suggest discuss clone github repo instanc instead import file user follow step mention discuss clone bitcoin trade bot git repo open notebook bitcoin lstm predict ipynb user add file local machin notebook instanc file upload function jupyterlab us aw cli python sdk upload download file"
    },
    {
        "Question_title":"How can I attach a managed disk to a Machine Learning Compute instance?",
        "Question_body":"Hello experts,\n\nI would like to attach a managed disk to my machine learning compute instance. Is that possible?\n\nThere is a possible overlap to the question Attach Disk to Virtual Machine, but steps doesn't seem to apply to ML compute instances.\n\nThanks in advance,",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1601637154150,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello,\n\nYou can attach your managed disk by following steps in Azure portal:\n\n\nMore details and limitation please see:\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-target#azure-machine-learning-compute-managed\n\n\n\n\nRegards,\nYutong",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/115201\/how-can-i-attach-a-managed-disk-to-a-machine-learn.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-04T08:05:55.917Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nYou can attach your managed disk by following steps in Azure portal:\n\n\nMore details and limitation please see:\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-target#azure-machine-learning-compute-managed\n\n\n\n\nRegards,\nYutong",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1604477155916,
        "Question_original_content":"attach manag disk machin learn comput instanc hello expert like attach manag disk machin learn comput instanc possibl possibl overlap question attach disk virtual machin step appli comput instanc thank advanc",
        "Question_preprocessed_content":"attach manag disk machin learn comput instanc hello expert like attach manag disk machin learn comput instanc possibl possibl overlap question attach disk virtual machin step appli comput instanc thank advanc",
        "Question_gpt_summary_original":"The user is facing a challenge in attaching a managed disk to their machine learning compute instance and is seeking guidance on whether it is possible and how to do it. They have found that the steps for attaching a disk to a virtual machine do not apply to ML compute instances.",
        "Question_gpt_summary":"user face challeng attach manag disk machin learn comput instanc seek guidanc possibl step attach disk virtual machin appli comput instanc",
        "Answer_original_content":"hello attach manag disk follow step azur portal detail limit http doc microsoft com azur machin learn concept comput target azur machin learn comput manag regard yutong",
        "Answer_preprocessed_content":"hello attach manag disk follow step azur portal detail limit regard yutong",
        "Answer_gpt_summary_original":"Solution: The solution provided is to follow the steps in the Azure portal to attach a managed disk to the machine learning compute instance. The user is directed to refer to the documentation for more details and limitations.",
        "Answer_gpt_summary":"solut solut provid follow step azur portal attach manag disk machin learn comput instanc user direct refer document detail limit"
    },
    {
        "Question_title":"How to create a Logs Router Sink when a Vertex AI training job failed (after 3 attempts)?",
        "Question_body":"<p>I am running a <code>Vertex AI custom training job<\/code> (machine learnin training using custom container) on <code>GCP<\/code>. I would like to create a <code>Pub\/Sub<\/code> message when the job failed so I can post a message on some chat like Slack. Logfile (<code>Cloud Logging)<\/code> is looking like that:<\/p>\n<pre><code>{\ninsertId: &quot;xxxxx&quot;\nlabels: {\nml.googleapis.com\/endpoint: &quot;&quot;\nml.googleapis.com\/job_state: &quot;FAILED&quot;\n}\nlogName: &quot;projects\/xxx\/logs\/ml.googleapis.com%2F1113875647681265664&quot;\nreceiveTimestamp: &quot;2021-07-09T15:05:52.702295640Z&quot;\nresource: {\nlabels: {\njob_id: &quot;1113875647681265664&quot;\nproject_id: &quot;xxx&quot;\ntask_name: &quot;service&quot;\n}\ntype: &quot;ml_job&quot;\n}\nseverity: &quot;INFO&quot;\ntextPayload: &quot;Job failed.&quot;\ntimestamp: &quot;2021-07-09T15:05:52.187968162Z&quot;\n}\n<\/code><\/pre>\n<p>I am creating a Logs Router Sink with the following query:<\/p>\n<pre><code>resource.type=&quot;ml_job&quot; AND textPayload:&quot;Job failed&quot; AND labels.&quot;ml.googleapis.com\/job_state&quot;:&quot;FAILED&quot;\n<\/code><\/pre>\n<p>The issue I am facing is that Vertex AI will retry the job 3 times before declaring the job as a failure but in the logfile the message is identical. Below you have 3 examples, only the last one that failed 3 times really failed at the end.\n<a href=\"https:\/\/i.stack.imgur.com\/NSGPb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NSGPb.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>In the logfile, I don't have any count id for example. Any idea how to solve this ? Creating a BigQuery table to keep track of the number of failure per <code>resource.labels.job_id<\/code> seems to be an overkill if I need to do that in all my project. Is there a way to do a group by <code>resource.labels.job_id<\/code> and count within Logs Router Sink ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1627312838200,
        "Question_favorite_count":null,
        "Question_last_edit_time":1627313437400,
        "Question_score":0.0,
        "Question_view_count":216.0,
        "Answer_body":"<p>The log sink is quite simple: provide a filter, it will publish in a PubSub topic each entry which match this filter. No group by, no count, nothing!!<\/p>\n<p>I propose you to use a combination of log-based metrics and Cloud monitoring.<\/p>\n<ol>\n<li>Firstly, create a <a href=\"https:\/\/cloud.google.com\/logging\/docs\/logs-based-metrics\" rel=\"nofollow noreferrer\">log based metrics<\/a> on your job failed log entry<\/li>\n<li>Create an <a href=\"https:\/\/cloud.google.com\/logging\/docs\/logs-based-metrics\/charts-and-alerts\" rel=\"nofollow noreferrer\">alert on this log based metrics<\/a> with the following key values<\/li>\n<\/ol>\n<ul>\n<li>Set the group by that you want, for example, the jobID (i don't know what is the relevant value for VertexAI job)<\/li>\n<li>Set an alert when the threshold is equal or above 3<\/li>\n<li>Add a notification channel and set a PubSub notification (still in beta)<\/li>\n<\/ul>\n<p>With this configuration, the alert will be posted only once in PubSub when 3 occurrences of the same jobID will occur.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68532457",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1627331282972,
        "Question_original_content":"creat log router sink train job fail attempt run custom train job machin learnin train custom contain gcp like creat pub sub messag job fail post messag chat like slack logfil cloud log look like insertid label googleapi com endpoint googleapi com job state fail lognam project log googleapi com receivetimestamp resourc label job project task servic type job sever info textpayload job fail timestamp creat log router sink follow queri resourc type job textpayload job fail label googleapi com job state fail issu face retri job time declar job failur logfil messag ident exampl fail time fail end logfil count exampl idea solv creat bigqueri tabl track number failur resourc label job overkil need project wai group resourc label job count log router sink",
        "Question_preprocessed_content":"creat log router sink train job fail run like creat messag job fail post messag chat like slack logfil look like creat log router sink follow queri issu face retri job time declar job failur logfil messag ident exampl fail time fail end logfil count exampl idea solv creat bigqueri tabl track number failur overkil need project wai group count log router sink",
        "Question_gpt_summary_original":"The user is facing a challenge in creating a Logs Router Sink to generate a Pub\/Sub message when a Vertex AI custom training job fails on GCP. The issue is that Vertex AI retries the job three times before declaring it as a failure, but the log message is identical. The user is looking for a solution to count the number of failures per resource.labels.job_id without creating a BigQuery table for each project.",
        "Question_gpt_summary":"user face challeng creat log router sink gener pub sub messag custom train job fail gcp issu retri job time declar failur log messag ident user look solut count number failur resourc label job creat bigqueri tabl project",
        "Answer_original_content":"log sink simpl provid filter publish pubsub topic entri match filter group count propos us combin log base metric cloud monitor firstli creat log base metric job fail log entri creat alert log base metric follow kei valu set group want exampl jobid know relev valu vertexai job set alert threshold equal add notif channel set pubsub notif beta configur alert post pubsub occurr jobid occur",
        "Answer_preprocessed_content":"log sink simpl provid filter publish pubsub topic entri match filter group count propos us combin metric cloud monitor firstli creat log base metric job fail log entri creat alert log base metric follow kei valu set group want exampl jobid set alert threshold equal add notif channel set pubsub notif configur alert post pubsub occurr jobid occur",
        "Answer_gpt_summary_original":"Solution:\n- Create a log-based metric on the job failed log entry.\n- Create an alert on this log-based metric with the relevant group by value (e.g., jobID) and set the threshold to 3.\n- Add a notification channel and set a PubSub notification.\n- With this configuration, the alert will be posted only once in PubSub when 3 occurrences of the same jobID will occur.",
        "Answer_gpt_summary":"solut creat log base metric job fail log entri creat alert log base metric relev group valu jobid set threshold add notif channel set pubsub notif configur alert post pubsub occurr jobid occur"
    },
    {
        "Question_title":"Organizaions using MLFlow",
        "Question_body":"Hi\n\nWe at\u00a0 Directorate General of Customs and Excise (https:\/\/www.beacukai.go.id) are using MLFlow extensively in an in-house Machine Learning platform. Please add our organization to the list on your website.\n\nRegards",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616546187000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":31.0,
        "Answer_body":"Hi\n\n\nAt DevSamurai (https:\/\/www.devsamurai.com) we are using MLFlow for our in-house Machine Learning platform and for our customers in Japan. Please add our organization to the list on your website.\n\n\nThanks,\nTam",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/h4GyDKFUWcg",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-25T09:08:55",
                "Answer_body":"Hi\n\n\nAt DevSamurai (https:\/\/www.devsamurai.com) we are using MLFlow for our in-house Machine Learning platform and for our customers in Japan. Please add our organization to the list on your website.\n\n\nThanks,\nTam"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"organizaion director gener custom excis http beacukai extens hous machin learn platform add organ list websit regard",
        "Question_preprocessed_content":"organizaion director gener custom excis extens machin learn platform add organ list websit regard",
        "Question_gpt_summary_original":"The user is requesting to add their organization, Directorate General of Customs and Excise, to the list of organizations using MLFlow on a website. No specific challenges are mentioned in the text.",
        "Question_gpt_summary":"user request add organ director gener custom excis list organ websit specif challeng mention text",
        "Answer_original_content":"devsamurai http devsamurai com hous machin learn platform custom japan add organ list websit thank tam",
        "Answer_preprocessed_content":"devsamurai machin learn platform custom japan add organ list websit thank tam",
        "Answer_gpt_summary_original":"Solution: No solutions provided.",
        "Answer_gpt_summary":"solut solut provid"
    },
    {
        "Question_title":"enhanced speech feature",
        "Question_body":"Hi I have a queryIn Dailogflow if we enable enhanced speech feature, specifically, credit card info (i.e. number), if that is spoken by user, is that stored by Google. Please help",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629336840000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":414.0,
        "Answer_body":"Hello,\n\nI understand you are looking to use the enhanced model in Dialogflow and you are looking to understand the Data Security. Please let me know if my understanding is wrong.\n\nIf that is what you are looking for, then I think you should read this section of this article[0] which addresses this concern. As explained in the doc[0], Google uses the data sent to Dialogflow on the project with data logging enabled. Google uses this data solely to train and improve Google products and services. So, while you'll maintain full ownership of all data that you upload to a project with data logging enabled, there are some terms[1] which I think you should be aware of.\n\n[0]https:\/\/cloud.google.com\/dialogflow\/es\/docs\/speech-enhanced-models#data-security\n[1]https:\/\/cloud.google.com\/dialogflow\/docs\/data-logging-terms\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/enhanced-speech-feature\/td-p\/167747\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hello,\n\nI understand you are looking to use the enhanced model in Dialogflow and you are looking to understand the Data Security. Please let me know if my understanding is wrong.\n\nIf that is what you are looking for, then I think you should read this section of this article[0] which addresses this concern. As explained in the doc[0], Google uses the data sent to Dialogflow on the project with data logging enabled. Google uses this data solely to train and improve Google products and services. So, while you'll maintain full ownership of all data that you upload to a project with data logging enabled, there are some terms[1] which I think you should be aware of.\n\n[0]https:\/\/cloud.google.com\/dialogflow\/es\/docs\/speech-enhanced-models#data-security\n[1]https:\/\/cloud.google.com\/dialogflow\/docs\/data-logging-terms\n\nView solution in original post"
            },
            {
                "Answer_creation_time":"2021-08-20T06:59:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hello,\n\nI understand you are looking to use the enhanced model in Dialogflow and you are looking to understand the Data Security. Please let me know if my understanding is wrong.\n\nIf that is what you are looking for, then I think you should read this section of this article[0] which addresses this concern. As explained in the doc[0], Google uses the data sent to Dialogflow on the project with data logging enabled. Google uses this data solely to train and improve Google products and services. So, while you'll maintain full ownership of all data that you upload to a project with data logging enabled, there are some terms[1] which I think you should be aware of.\n\n[0]https:\/\/cloud.google.com\/dialogflow\/es\/docs\/speech-enhanced-models#data-security\n[1]https:\/\/cloud.google.com\/dialogflow\/docs\/data-logging-terms"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"enhanc speech featur queryin dailogflow enabl enhanc speech featur specif credit card info number spoken user store googl help",
        "Question_preprocessed_content":"enhanc speech featur queryin dailogflow enabl enhanc speech featur specif credit card info spoken user store googl help",
        "Question_gpt_summary_original":"The user is seeking clarification on whether enabling the enhanced speech feature in Dialogflow will result in Google storing any credit card information spoken by the user.",
        "Question_gpt_summary":"user seek clarif enabl enhanc speech featur dialogflow result googl store credit card inform spoken user",
        "Answer_original_content":"hello understand look us enhanc model dialogflow look understand data secur let know understand wrong look think read section articl address concern explain doc googl us data sent dialogflow project data log enabl googl us data sole train improv googl product servic maintain ownership data upload project data log enabl term think awar http cloud googl com dialogflow doc speech enhanc model data secur http cloud googl com dialogflow doc data log term view solut origin post",
        "Answer_preprocessed_content":"hello understand look us enhanc model dialogflow look understand data secur let know understand wrong look think read section articl address concern explain doc googl us data sent dialogflow project data log enabl googl us data sole train improv googl product servic maintain ownership data upload project data log enabl term think awar view solut origin post",
        "Answer_gpt_summary_original":"Solution: The discussion provides a link to an article that addresses the user's concern about data security when using the enhanced speech feature in Dialogflow. The article explains that Google uses the data sent to Dialogflow on the project with data logging enabled solely to train and improve Google products and services. The user will maintain full ownership of all data uploaded to a project with data logging enabled, but there are some terms that the user should be aware of. No credit card information storage solution is mentioned in the discussion.",
        "Answer_gpt_summary":"solut discuss provid link articl address user concern data secur enhanc speech featur dialogflow articl explain googl us data sent dialogflow project data log enabl sole train improv googl product servic user maintain ownership data upload project data log enabl term user awar credit card inform storag solut mention discuss"
    },
    {
        "Question_title":"Running DVC on AWS Batch",
        "Question_body":"<p>I\u2019m trying to get a DVC job to run on AWS Batch, but I\u2019m having trouble getting dvc to access the s3 bucket.<\/p>\n<p>This is the script I\u2019m running:<\/p>\n<pre><code class=\"lang-auto\">#!\/bin\/bash\n\nAWS_CREDENTIALS=$(curl http:\/\/169.254.170.2$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI)\n\nexport AWS_DEFAULT_REGION=us-east-1\n\nexport AWS_ACCESS_KEY_ID=$(echo \"$AWS_CREDENTIALS\" | jq .AccessKeyId -r)\n\nexport AWS_SECRET_ACCESS_KEY=$(echo \"$AWS_CREDENTIALS\" | jq .SecretAccessKey -r)\n\nexport AWS_SESSION_TOKEN=$(echo \"$AWS_CREDENTIALS\" | jq .Token -r)\n\necho \"AWS_ACCESS_KEY_ID=&lt;$AWS_ACCESS_KEY_ID&gt;\"\n\necho \"AWS_SECRET_ACCESS_KEY=&lt;$AWS_SECRET_ACCESS_KEY&gt;\"\n\necho \"AWS_SECRET_ACCESS_KEY=&lt;$(cat &lt;(echo \"$AWS_SECRET_ACCESS_KEY\" | head -c 6) &lt;(echo -n \"...\") &lt;(echo \"$AWS_SECRET_ACCESS_KEY\" | tail -c 6))&gt;\"\n\necho \"AWS_SESSION_TOKEN=&lt;$(cat &lt;(echo \"$AWS_SESSION_TOKEN\" | head -c 6) &lt;(echo -n \"...\") &lt;(echo \"$AWS_SESSION_TOKEN\" | tail -c 6))&gt;\"\n\nalias python=python3\n\naws s3 ls s3:\/\/duolingo-dvc\/\n\naws s3 ls s3:\/\/duolingo-dvc\/det-grade\/\n\naws s3 cp s3:\/\/duolingo-dvc\/det-grade\/00\/0e4343c163bd70df0a6f9d81e1b4d2 mycopy.txt\n\ndvc remote modify s3 --local region us-east-1\n\ndvc remote modify s3 --local access_key_id $AWS_ACCESS_KEY_ID\n\ndvc remote modify s3 --local secret_access_key $AWS_SECRET_ACCESS_KEY\n\ndvc remote modify s3 --local session_token $AWS_SESSION_TOKEN\n\necho \"Starting DVC pull\"\n\ndvc pull -v scripts\/writing\/data\/$1\/responses.train-model.csv\ndvc pull -v scripts\/writing\/data\/$1\/responses.test.csv\ndvc pull -v scripts\/writing\/data\/$1\/corrected-responses.pkl\n\necho \"Stopping DVC pull\"\n\ndvc repro correct-responses@$1 --force --single-item\n<\/code><\/pre>\n<p>All the AWS CLI commands, accessing the same bucket, work.  However, when it tries to run the DVC pull commands, I get access denied errors, even though it should be using the same credentials.<\/p>\n<pre><code class=\"lang-auto\">botocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied\n<\/code><\/pre>\n<p>Just before that, it says its \u201cPreparing to collect status from \u2018duolingo-dvc\/det-grade\u2019\u201d.<\/p>\n<p>In case its relevant.  The role\u2019s permissions to the s3 bucket are defined as follows:<\/p>\n<pre><code class=\"lang-auto\">data \"aws_iam_policy_document\" \"standard-batch-job-role\" {\n  # S3 read access to related buckets\n  statement {\n    actions = [\n      \"s3:Get*\",\n      \"s3:List*\",\n    ]\n    resources = [\n      data.aws_s3_bucket.duolingo-dvc.arn,\n      \"${data.aws_s3_bucket.duolingo-dvc.arn}\/*\",\n    ]\n    effect = \"Allow\"\n  }\n}\n<\/code><\/pre>\n<p>AWS doesn\u2019t make it easy to copy the full stack trace, but here is a screenshot:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/7bd06124d7d53dd522e687473927233c3b909550.png\" data-download-href=\"\/uploads\/short-url\/hFjb9dvov2M6NvqQdUR86ZgtAZ2.png?dl=1\" title=\"Screen Shot 2023-01-30 at 10.36.42 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/7bd06124d7d53dd522e687473927233c3b909550_2_690x230.png\" alt=\"Screen Shot 2023-01-30 at 10.36.42 PM\" data-base62-sha1=\"hFjb9dvov2M6NvqQdUR86ZgtAZ2\" width=\"690\" height=\"230\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/7bd06124d7d53dd522e687473927233c3b909550_2_690x230.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/7bd06124d7d53dd522e687473927233c3b909550_2_1035x345.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/7bd06124d7d53dd522e687473927233c3b909550_2_1380x460.png 2x\" data-dominant-color=\"EDEDED\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2023-01-30 at 10.36.42 PM<\/span><span class=\"informations\">2772\u00d7926 393 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":10,
        "Question_comment_count":0,
        "Question_creation_time":1675135870597,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":87.0,
        "Answer_body":"<p>AWS\u2019s documentation for accessing the credentials within the AWS Batch container can be found <a href=\"https:\/\/docs.aws.amazon.com\/AmazonECS\/latest\/developerguide\/task-iam-roles.html\" rel=\"noopener nofollow ugc\">here<\/a>.  I\u2019m pretty sure I\u2019ve pulled these correctly, as otherwise the <code>aws s3<\/code> commands would fail, which they do not.  I verified this by setting the <code>AWS_<\/code> environment variables to incorrect values, and the <code>aws s3<\/code> commands do fail in that case.<\/p>. <p>From the docs (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/add#amazon-s3\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">remote add<\/a>)<br>\nMake sure you have the following permissions enabled:<\/p>\n<ul>\n<li>s3:ListBucket<\/li>\n<li>s3:GetObject<\/li>\n<li>s3:PutObject<\/li>\n<li>s3:DeleteObject.<\/li>\n<\/ul>\n<p>This enables the S3 API methods that are performed by DVC (list_objects_v2 or list_objects, head_object, upload_file, download_file, delete_object, copy).<\/p>. <p>It already has <code>s3:ListBucket<\/code> and <code>s3:GetObject<\/code>.  It\u2019s failing on <code>list_objects_v2<\/code>.  It should only need the other <code>s3:PutObject<\/code> and <code>s3:DeleteObject<\/code> if I\u2019m doing a <code>dvc push<\/code> or something that mutates the remote store, which I am not.<\/p>. <p>For the sake of thoroughness, I added DeleteObject and PutObject permissions to the role.  As expected, I still get the same error when it tries to call <code>ListObjectsV2<\/code>.<\/p>. <p>I\u2019ve traced this down to being an issue with s3fs.  Even though the access key ID, access secret, and session token are all being passed into s3fs, for some reason that library\u2019s call to <code>ListObjectsV2<\/code> fails, even though it works fine when I run <code>aws s3 ls<\/code> within the container.  I can even reproduce it by writing my own python script that calls <code>s3fs.S3FileSystem(key=...,secret=...,token=...).ls('duolingo-dvc')<\/code>, which fails when running inside the container (but works locally).  I\u2019m not sure where to look next.<\/p>. <p>Does it succeed if you also provide the region?<\/p>\n<pre><code class=\"lang-python\">fs = S3FileSystem(client_kwargs={\"region_name\": \"eu-west-1\"), ...)\n<\/code><\/pre>. <p>Good suggestion, but, no, it doesn\u2019t help.<\/p>\n<p>Here is my latest script:<\/p>\n<pre><code class=\"lang-auto\">import os\n\nimport s3fs\n\nprint(os.environ[\"AWS_ACCESS_KEY_ID\"])\nprint(os.environ[\"AWS_SECRET_ACCESS_KEY\"])\nprint(os.environ[\"AWS_SESSION_TOKEN\"])\n\nprint(\"running with credentials\")\nfs = s3fs.S3FileSystem(\n    key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n    secret=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n    token=os.environ[\"AWS_SESSION_TOKEN\"],\n    client_kwargs={\"region_name\": \"us-east-1\"}\n)\nprint(fs.exists(\"duolingo-dvc\/det-grade\/00\/0e4343c163bd70df0a6f9d81e1b4d2\"))\nprint(fs.ls(\"duolingo-dvc\/\"))\nprint(fs.ls(\"duolingo-dvc\/det-grade\/\"))\n<\/code><\/pre>\n<p>All the credentials print out, so they are there.  But, I still get an \u201caccess denied\u201d on <code>ListObjectsV2<\/code> when running <code>fs.exists<\/code>.<\/p>\n<p>The aws cli commands, using the same credentials, work fine.<\/p>. <p>I\u2019ve tried to reproduce it, but I can\u2019t.<\/p>\n<p>Could you try to run <code>aws s3api list-objects-v2 --bucket duolingo-dvc<\/code> ?<\/p>\n<p>Does <code>fs.ls<\/code> fail also?<\/p>. <p>Have you tried <code>fs = s3fs.S3FileSystem()<\/code>? That should make <code>s3fs<\/code> use <code>boto<\/code>\u2019s default credentials resolution.<\/p>\n<p>Another thing to try is to turn on s3fs <a href=\"https:\/\/s3fs.readthedocs.io\/en\/latest\/#logging\" rel=\"noopener nofollow ugc\">debug logs<\/a>.<\/p>. <p><code>aws s3api list-objects-v2 --bucket duolingo-dvc<\/code> succeeds.<\/p>\n<p>Using <code>fs = s3fs.S3FileSystem().exists()<\/code> and <code>fs = s3fs.S3FileSystem().ls()<\/code> does not.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/running-dvc-on-aws-batch\/1481",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-31T03:43:03.745Z",
                "Answer_body":"<p>AWS\u2019s documentation for accessing the credentials within the AWS Batch container can be found <a href=\"https:\/\/docs.aws.amazon.com\/AmazonECS\/latest\/developerguide\/task-iam-roles.html\" rel=\"noopener nofollow ugc\">here<\/a>.  I\u2019m pretty sure I\u2019ve pulled these correctly, as otherwise the <code>aws s3<\/code> commands would fail, which they do not.  I verified this by setting the <code>AWS_<\/code> environment variables to incorrect values, and the <code>aws s3<\/code> commands do fail in that case.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-31T10:31:17.831Z",
                "Answer_body":"<p>From the docs (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/add#amazon-s3\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">remote add<\/a>)<br>\nMake sure you have the following permissions enabled:<\/p>\n<ul>\n<li>s3:ListBucket<\/li>\n<li>s3:GetObject<\/li>\n<li>s3:PutObject<\/li>\n<li>s3:DeleteObject.<\/li>\n<\/ul>\n<p>This enables the S3 API methods that are performed by DVC (list_objects_v2 or list_objects, head_object, upload_file, download_file, delete_object, copy).<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-31T11:37:39.259Z",
                "Answer_body":"<p>It already has <code>s3:ListBucket<\/code> and <code>s3:GetObject<\/code>.  It\u2019s failing on <code>list_objects_v2<\/code>.  It should only need the other <code>s3:PutObject<\/code> and <code>s3:DeleteObject<\/code> if I\u2019m doing a <code>dvc push<\/code> or something that mutates the remote store, which I am not.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-31T15:21:34.212Z",
                "Answer_body":"<p>For the sake of thoroughness, I added DeleteObject and PutObject permissions to the role.  As expected, I still get the same error when it tries to call <code>ListObjectsV2<\/code>.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-01T01:22:20.147Z",
                "Answer_body":"<p>I\u2019ve traced this down to being an issue with s3fs.  Even though the access key ID, access secret, and session token are all being passed into s3fs, for some reason that library\u2019s call to <code>ListObjectsV2<\/code> fails, even though it works fine when I run <code>aws s3 ls<\/code> within the container.  I can even reproduce it by writing my own python script that calls <code>s3fs.S3FileSystem(key=...,secret=...,token=...).ls('duolingo-dvc')<\/code>, which fails when running inside the container (but works locally).  I\u2019m not sure where to look next.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-02T10:57:18.130Z",
                "Answer_body":"<p>Does it succeed if you also provide the region?<\/p>\n<pre><code class=\"lang-python\">fs = S3FileSystem(client_kwargs={\"region_name\": \"eu-west-1\"), ...)\n<\/code><\/pre>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-02T17:06:44.475Z",
                "Answer_body":"<p>Good suggestion, but, no, it doesn\u2019t help.<\/p>\n<p>Here is my latest script:<\/p>\n<pre><code class=\"lang-auto\">import os\n\nimport s3fs\n\nprint(os.environ[\"AWS_ACCESS_KEY_ID\"])\nprint(os.environ[\"AWS_SECRET_ACCESS_KEY\"])\nprint(os.environ[\"AWS_SESSION_TOKEN\"])\n\nprint(\"running with credentials\")\nfs = s3fs.S3FileSystem(\n    key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n    secret=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n    token=os.environ[\"AWS_SESSION_TOKEN\"],\n    client_kwargs={\"region_name\": \"us-east-1\"}\n)\nprint(fs.exists(\"duolingo-dvc\/det-grade\/00\/0e4343c163bd70df0a6f9d81e1b4d2\"))\nprint(fs.ls(\"duolingo-dvc\/\"))\nprint(fs.ls(\"duolingo-dvc\/det-grade\/\"))\n<\/code><\/pre>\n<p>All the credentials print out, so they are there.  But, I still get an \u201caccess denied\u201d on <code>ListObjectsV2<\/code> when running <code>fs.exists<\/code>.<\/p>\n<p>The aws cli commands, using the same credentials, work fine.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-09T01:04:21.546Z",
                "Answer_body":"<p>I\u2019ve tried to reproduce it, but I can\u2019t.<\/p>\n<p>Could you try to run <code>aws s3api list-objects-v2 --bucket duolingo-dvc<\/code> ?<\/p>\n<p>Does <code>fs.ls<\/code> fail also?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-09T13:52:28.055Z",
                "Answer_body":"<p>Have you tried <code>fs = s3fs.S3FileSystem()<\/code>? That should make <code>s3fs<\/code> use <code>boto<\/code>\u2019s default credentials resolution.<\/p>\n<p>Another thing to try is to turn on s3fs <a href=\"https:\/\/s3fs.readthedocs.io\/en\/latest\/#logging\" rel=\"noopener nofollow ugc\">debug logs<\/a>.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-11T04:14:32.849Z",
                "Answer_body":"<p><code>aws s3api list-objects-v2 --bucket duolingo-dvc<\/code> succeeds.<\/p>\n<p>Using <code>fs = s3fs.S3FileSystem().exists()<\/code> and <code>fs = s3fs.S3FileSystem().ls()<\/code> does not.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"run aw batch try job run aw batch have troubl get access bucket script run bin bash aw credenti curl http aw contain credenti rel uri export aw default region east export aw access kei echo aw credenti accesskeyid export aw secret access kei echo aw credenti secretaccesskei export aw session token echo aw credenti token echo aw access kei echo aw secret access kei echo aw secret access kei echo aw session token alia python python aw duolingo aw duolingo det grade aw duolingo det grade ecbddfafdebd mycopi txt remot modifi local region east remot modifi local access kei aw access kei remot modifi local secret access kei aw secret access kei remot modifi local session token aw session token echo start pull pull script write data respons train model csv pull script write data respons test csv pull script write data correct respons pkl echo stop pull repro correct respons forc singl item aw cli command access bucket work tri run pull command access deni error credenti botocor except clienterror error occur accessdeni call listobjectsv oper access deni sai prepar collect statu duolingo det grade case relev role permiss bucket defin follow data aw iam polici document standard batch job role read access relat bucket statement action list resourc data aw bucket duolingo arn data aw bucket duolingo arn effect allow aw doesnt easi copi stack trace screenshot screen shot",
        "Question_preprocessed_content":"run aw batch try job run aw batch have troubl get access bucket script run aw cli command access bucket work tri run pull command access deni error credenti sai prepar collect statu case relev role permiss bucket defin follow aw doesnt easi copi stack trace screenshot screen shot",
        "Question_gpt_summary_original":"The user is having trouble getting DVC to access an S3 bucket when running a job on AWS Batch. The AWS CLI commands accessing the same bucket work, but when running DVC pull commands, access denied errors occur, even though the same credentials should be used. The role's permissions to the S3 bucket are defined to allow S3 read access to related buckets.",
        "Question_gpt_summary":"user have troubl get access bucket run job aw batch aw cli command access bucket work run pull command access deni error occur credenti role permiss bucket defin allow read access relat bucket",
        "Answer_original_content":"awss document access credenti aw batch contain pretti sure iv pull correctli aw command fail verifi set aw environ variabl incorrect valu aw command fail case doc remot add sure follow permiss enabl listbucket getobject putobject deleteobject enabl api method perform list object list object head object upload file download file delet object copi listbucket getobject fail list object need putobject deleteobject push mutat remot store sake thorough ad deleteobject putobject permiss role expect error tri listobjectsv iv trace issu sf access kei access secret session token pass sf reason librari listobjectsv fail work fine run aw contain reproduc write python script call sf sfilesystem kei secret token duolingo fail run insid contain work local sure look succe provid region sfilesystem client kwarg region west good suggest doesnt help latest script import import sf print environ aw access kei print environ aw secret access kei print environ aw session token print run credenti sf sfilesystem kei environ aw access kei secret environ aw secret access kei token environ aw session token client kwarg region east print exist duolingo det grade ecbddfafdebd print duolingo print duolingo det grade credenti print access deni listobjectsv run exist aw cli command credenti work fine iv tri reproduc try run aw sapi list object bucket duolingo fail tri sf sfilesystem sf us boto default credenti resolut thing try turn sf debug log aw sapi list object bucket duolingo succe sf sfilesystem exist sf sfilesystem",
        "Answer_preprocessed_content":"awss document access credenti aw batch contain pretti sure iv pull correctli command fail verifi set environ variabl incorrect valu command fail case doc sure follow permiss enabl listbucket getobject putobject deleteobject enabl api method perform fail need mutat remot store sake thorough ad deleteobject putobject permiss role expect error tri iv trace issu access kei access secret session token pass reason librari fail work fine run contain reproduc write python script call fail run insid contain sure look succe provid region good suggest doesnt help latest script credenti print access deni run aw cli command credenti work fine iv tri reproduc try run fail tri us default credenti resolut thing try turn debug log succe",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Add the missing permissions to the role, such as s3:PutObject and s3:DeleteObject.\n- Provide the region when creating the S3FileSystem object.\n- Try using s3fs.S3FileSystem() to make s3fs use boto's default credentials resolution.\n- Turn on s3fs debug logs to identify the issue. \n\nNote that some of these solutions were suggested but did not work for the user.",
        "Answer_gpt_summary":"possibl solut mention discuss add miss permiss role putobject deleteobject provid region creat sfilesystem object try sf sfilesystem sf us boto default credenti resolut turn sf debug log identifi issu note solut suggest work user"
    },
    {
        "Question_title":"AzureML multiple models stored in dictionary",
        "Question_body":"<p><strong>Overview:<\/strong><\/p>\n\n<p>I have a unique Python model where we hold n trained random forest models in a dictionary. I tried to avoid this setup, but for the time being it's necessary. On my local, I can make predictions by passing a dataframe to a predict function and looping through the rows, calling the appropriate model for each row, like rf_models[model].predict().<\/p>\n\n<p>In AzureML I created a toy model that allows me to go:\nWeb Input -> Python Script -> Score Model -> Web output. <\/p>\n\n<p><strong>Challenge:<\/strong><\/p>\n\n<p>I need to be able to call the score_model, or specifically the predict method, from inside the \"Python Script\" function on AzureML so I can deal with the loops and n models stored in the dict. The results, either a JSON or dataframe, would be sent to AzureML's Web Output.<\/p>\n\n<p>I found a link online (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts<\/a>) that got me close, but the example shows a model being trained and used to predict at the same time inside the same Python script, thus calling the predict method as a local variable and not calling a previously trained model. I found only limited documentation online to solve this problem and I could not get the rest of the way there. I'm unsure if this type of customization is not yet available or if I'm completely overlooking some key functionality.<\/p>\n\n<p>Thank you for your assistance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1502218686207,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":200.0,
        "Answer_body":"<p>Here are two links that might help:<\/p>\n\n<ol>\n<li><a href=\"https:\/\/github.com\/Azure\/Machine-Learning-Operationalization\" rel=\"nofollow noreferrer\">AzureML Operationalization<\/a> <\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/Machine-Learning-Operationalization\/blob\/master\/samples\/python\/tutorials\/realtime\/digit_classification.ipynb\" rel=\"nofollow noreferrer\">Example notebook<\/a> that shows how to publish Python model as a web service. You would do a similar thing, only you would pickle the dictionary of your models instead. <\/li>\n<\/ol>\n\n<p>Note that this functionality is currently in preview mode.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45576092",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1503860922243,
        "Question_original_content":"multipl model store dictionari overview uniqu python model hold train random forest model dictionari tri avoid setup time necessari local predict pass datafram predict function loop row call appropri model row like model model predict creat toi model allow web input python script score model web output challeng need abl score model specif predict method insid python script function deal loop model store dict result json datafram sent web output link onlin http doc microsoft com azur machin learn machin learn execut python script got close exampl show model train predict time insid python script call predict method local variabl call previous train model limit document onlin solv problem rest wai unsur type custom avail complet overlook kei function thank assist",
        "Question_preprocessed_content":"multipl model store dictionari overview uniqu python model hold train random forest model dictionari tri avoid setup time necessari local predict pass datafram predict function loop row call appropri model row like creat toi model allow web input python script score model web output challeng need abl specif predict method insid python script function deal loop model store dict result json datafram sent web output link onlin got close exampl show model train predict time insid python script call predict method local variabl call previous train model limit document onlin solv problem rest wai unsur type custom avail complet overlook kei function thank assist",
        "Question_gpt_summary_original":"The user has a unique Python model where they hold n trained random forest models in a dictionary. They need to be able to call the predict method from inside the \"Python Script\" function on AzureML so they can deal with the loops and n models stored in the dict. The user has found limited documentation online to solve this problem and is unsure if this type of customization is not yet available or if they are completely overlooking some key functionality.",
        "Question_gpt_summary":"user uniqu python model hold train random forest model dictionari need abl predict method insid python script function deal loop model store dict user limit document onlin solv problem unsur type custom avail complet overlook kei function",
        "Answer_original_content":"link help operation exampl notebook show publish python model web servic similar thing pickl dictionari model instead note function current preview mode",
        "Answer_preprocessed_content":"link help operation exampl notebook show publish python model web servic similar thing pickl dictionari model instead note function current preview mode",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Check out AzureML Operationalization and the example notebook provided to publish the Python model as a web service. The user can pickle the dictionary of their models instead of a single model.\n- Note that this functionality is currently in preview mode.",
        "Answer_gpt_summary":"possibl solut mention discuss check operation exampl notebook provid publish python model web servic user pickl dictionari model instead singl model note function current preview mode"
    },
    {
        "Question_title":"Can I use AWS CLI to add tags to all processing jobs matching a certain regex",
        "Question_body":"<p>I have close to 100 processing jobs to which I want to add certain tags. I've found commands that you can use to tag one resource with a list of tags. Is there any way I can do this for multiple jobs? Through CLI or through python+boto?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1657515736330,
        "Question_favorite_count":null,
        "Question_last_edit_time":1657607287156,
        "Question_score":0.0,
        "Question_view_count":38.0,
        "Answer_body":"<p>You can <code>ResourceGroupsTaggingAPI<\/code>'s method <code>tag_resources()<\/code>.<br \/>\nThis is used to apply one or more tags to the specified list of resources.<\/p>\n<p>References:<\/p>\n<ol>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/resourcegroupstaggingapi.html#ResourceGroupsTaggingAPI.Client.tag_resources\" rel=\"nofollow noreferrer\">Tag Resources using boto3<\/a><\/li>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/resourcegroupstaggingapi.html#ResourceGroupsTaggingAPI.Client.untag_resources\" rel=\"nofollow noreferrer\">UnTag Resources using boto3<\/a><\/li>\n<\/ol>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72933908",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1657516175783,
        "Question_original_content":"us aw cli add tag process job match certain regex close process job want add certain tag command us tag resourc list tag wai multipl job cli python boto",
        "Question_preprocessed_content":"us aw cli add tag process job match certain regex close process job want add certain tag command us tag resourc list tag wai multipl job cli python boto",
        "Question_gpt_summary_original":"The user is facing a challenge of adding tags to close to 100 processing jobs. They are looking for a way to add tags to multiple jobs using AWS CLI or python+boto instead of tagging one resource at a time.",
        "Question_gpt_summary":"user face challeng ad tag close process job look wai add tag multipl job aw cli python boto instead tag resourc time",
        "Answer_original_content":"resourcegroupstaggingapi method tag resourc appli tag specifi list resourc refer tag resourc boto untag resourc boto",
        "Answer_preprocessed_content":"method appli tag specifi list resourc refer tag resourc boto untag resourc boto",
        "Answer_gpt_summary_original":"Solution: The user can use the `tag_resources()` method of `ResourceGroupsTaggingAPI` using boto3 to apply one or more tags to the specified list of resources. References to the documentation of `tag_resources()` and `untag_resources()` methods are provided.",
        "Answer_gpt_summary":"solut user us tag resourc method resourcegroupstaggingapi boto appli tag specifi list resourc refer document tag resourc untag resourc method provid"
    },
    {
        "Question_title":"How to understand output from a Multiclass Neural Network",
        "Question_body":"<p>Built a flow in Azure ML using a Neural network Multiclass module (for settings see picture). \n <a href=\"https:\/\/i.stack.imgur.com\/6xTYY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6xTYY.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Some more info about the Multiclass:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/lLUVC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lLUVC.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The data flow is simple, split of 80\/20. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/k72ZX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/k72ZX.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Preparation of the data is made before it goes into Azure. Data looks like this: <a href=\"https:\/\/i.stack.imgur.com\/SoSUa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/SoSUa.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>My problem comes when I want to make sense of the output and if possible transform\/calculate the output to probabilities. Output looks like this: <a href=\"https:\/\/i.stack.imgur.com\/kBZOM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kBZOM.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>My question: If scored probabilities output for my model is 0.6 and scored labels = 1, how sure is the model of the scored labels 1? And how sure can I be that actual outcome will be a 1?<\/p>\n\n<p>Can I safely assume that a scored probabilities of 0.80 = 80% chance of outcome? Or what type of outcomes should I watch out for?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1535108970810,
        "Question_favorite_count":null,
        "Question_last_edit_time":1535126329423,
        "Question_score":0.0,
        "Question_view_count":263.0,
        "Answer_body":"<p>To start with, your are in a <em>binary<\/em> classification setting, not in a multi-class one (we normally use this term when number of classes > 2).<\/p>\n\n<blockquote>\n  <p>If scored probabilities output for my model is 0.6 and scored labels = 1, how sure is the model of the scored labels 1? <\/p>\n<\/blockquote>\n\n<p>In <em>practice<\/em>, the scored probabilities are routinely interpreted as the <em>confidence<\/em> of the model; so, in this example, we would say that your model has 60% confidence that the particular sample belongs to class 1 (and, complementary, 40% confidence that it belongs to class 0).<\/p>\n\n<blockquote>\n  <p>And how sure can I be that actual outcome will be a 1?<\/p>\n<\/blockquote>\n\n<p>If you don't have any alternate means of computing such outcomes yourself (e.g. a different model), I cannot see how this question is different from your previous one.<\/p>\n\n<blockquote>\n  <p>Can I safely assume that a scored probabilities of 0.80 = 80% chance of outcome? <\/p>\n<\/blockquote>\n\n<p>This is the kind of statement that would drive a professional statistician mad; nevertheless, the clarifications above regarding the confidence should be enough for your purposes (they are enough indeed for ML practitioners).<\/p>\n\n<p>My answer in <a href=\"https:\/\/stackoverflow.com\/questions\/51367755\/predict-classes-or-class-probabilities\/51423325#51423325\">Predict classes or class probabilities?<\/a> should also be helpful.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1535111877860,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52003180",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1535111519160,
        "Question_original_content":"understand output multiclass neural network built flow neural network multiclass modul set pictur info multiclass data flow simpl split prepar data goe azur data look like problem come want sens output possibl transform calcul output probabl output look like question score probabl output model score label sure model score label sure actual outcom safe assum score probabl chanc outcom type outcom watch",
        "Question_preprocessed_content":"understand output multiclass neural network built flow neural network multiclass modul info multiclass data flow simpl split prepar data goe azur data look like problem come want sens output possibl output probabl output look like question score probabl output model score label sure model score label sure actual outcom safe assum score probabl chanc outcom type outcom watch",
        "Question_gpt_summary_original":"The user is facing challenges in understanding the output from a multiclass neural network and transforming\/calculating the output to probabilities. They are unsure about the certainty of the scored labels and the actual outcome, and are seeking clarification on how to interpret the scored probabilities and labels.",
        "Question_gpt_summary":"user face challeng understand output multiclass neural network transform calcul output probabl unsur certainti score label actual outcom seek clarif interpret score probabl label",
        "Answer_original_content":"start binari classif set multi class normal us term number class score probabl output model score label sure model score label practic score probabl routin interpret confid model exampl model confid particular sampl belong class complementari confid belong class sure actual outcom altern mean comput outcom differ model question differ previou safe assum score probabl chanc outcom kind statement drive profession statistician mad clarif confid purpos practition answer predict class class probabl help",
        "Answer_preprocessed_content":"start binari classif set score probabl output model score label sure model score label practic score probabl routin interpret confid model exampl model confid particular sampl belong class sure actual outcom altern mean comput outcom question differ previou safe assum score probabl chanc outcom kind statement drive profession statistician mad clarif confid purpos answer predict class class probabl help",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- The scored probabilities are routinely interpreted as the confidence of the model.\n- In the given example, the model has 60% confidence that the particular sample belongs to class 1 and 40% confidence that it belongs to class 0.\n- If there are no alternate means of computing the actual outcome, it is not possible to be sure about the actual outcome.\n- The statement that a scored probability of 0.80 equals an 80% chance of outcome is not a statistically sound statement, but the clarifications regarding confidence should be enough for practical purposes.\n- The answer in the provided link may also be helpful.",
        "Answer_gpt_summary":"possibl solut mention discuss score probabl routin interpret confid model given exampl model confid particular sampl belong class confid belong class altern mean comput actual outcom possibl sure actual outcom statement score probabl equal chanc outcom statist sound statement clarif confid practic purpos answer provid link help"
    },
    {
        "Question_title":"Does a call to \"Deploy web service(via API key) \" re run trained Azure ML model again",
        "Question_body":"<p>I wanted to know how exactly the following works in backend<\/p>\n\n<p><strong>Scenario :<\/strong> <\/p>\n\n<blockquote>\n  <p>-> We get data from Edgex foundry in UTC format and we it store it in Azure Document DB in (CST\/CDT timezone) format<\/p>\n  \n  <p>-> We trained ML model on data(with Date in CST\/CDT timezone) and Deploy web service.<\/p>\n<\/blockquote>\n\n<p><strong>So I have few basic doubts below<\/strong><\/p>\n\n<blockquote>\n  <ol>\n  <li><p>When web job hits our predictive webservice , will the trained ML model be run again?<\/p><\/li>\n  <li><p>Do we need to convert the UTC timezone for new incoming test data( which we want to predict) into CST\/CDT timezone, as TimeStamp does\n  matter for our prediction?<\/p><\/li>\n  <li><p>What happens in backend when predictive webservice API is called?<\/p><\/li>\n  <\/ol>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1520339633757,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":92.0,
        "Answer_body":"<p>This is only based on my experience with Azure ML, but I think I can help with your questions.<\/p>\n\n<blockquote>\n  <p>When web job hits our predictive webservice, will the trained ML model be run again?<\/p>\n<\/blockquote>\n\n<p>Yes, in the sense that it will call the <code>predict<\/code> (or similar) method on the model on the new data. For instance, in <code>scikit-learn<\/code> you would train your model using the <code>fit<\/code> method. Once the model is in production, only the <code>predict<\/code> method would be called.<\/p>\n\n<p>It will also run the whole workflow you have set up to be deployed as the web service. As an example below is a workflow I've played around with before. Each time the web service is run with new data, this whole thing will be run. This is like creating a Pipeline in <code>scikit-learn<\/code>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YMFZb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YMFZb.png\" alt=\"Azure ML Workflow\"><\/a><\/p>\n\n<blockquote>\n  <p>Do we need to convert the UTC timezone for new incoming test data( which we want to predict) into CST\/CDT timezone, as TimeStamp does matter for our prediction?<\/p>\n<\/blockquote>\n\n<p>I would say yes, you would need to convert to the timezone that was used when training in the model. This can be done by adding a step in your workflow then when you call the web service it will do the necessary converting for you before making a prediction.<\/p>\n\n<blockquote>\n  <p>What happens in backend when predictive webservice API is called?<\/p>\n<\/blockquote>\n\n<p>I'm not sure if anyone knows for sure other than the folks at Microsoft, but for sure it will run the workflow you have set up.<\/p>\n\n<hr>\n\n<p>I know it's not much, but I hope this helps or at least gets you on the right track for what you need.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49130977",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1521026355920,
        "Question_original_content":"deploi web servic api kei run train model want know exactli follow work backend scenario data edgex foundri utc format store azur document cst cdt timezon format train model data date cst cdt timezon deploi web servic basic doubt web job hit predict webservic train model run need convert utc timezon new incom test data want predict cst cdt timezon timestamp matter predict happen backend predict webservic api call",
        "Question_preprocessed_content":"deploi web servic run train model want know exactli follow work backend scenario data edgex foundri utc format store azur document format train model data deploi web servic basic doubt web job hit predict webservic train model run need convert utc timezon new incom test data timezon timestamp matter predict happen backend predict webservic api call",
        "Question_gpt_summary_original":"The user is seeking clarification on how a trained ML model works in the backend when a web service is deployed via an API key. They have specific doubts about whether the model is run again when the web job hits the predictive web service, whether incoming test data needs to be converted to the correct timezone, and what happens in the backend when the predictive web service API is called.",
        "Question_gpt_summary":"user seek clarif train model work backend web servic deploi api kei specif doubt model run web job hit predict web servic incom test data need convert correct timezon happen backend predict web servic api call",
        "Answer_original_content":"base experi think help question web job hit predict webservic train model run ye sens predict similar method model new data instanc scikit learn train model fit method model product predict method call run workflow set deploi web servic exampl workflow plai time web servic run new data thing run like creat pipelin scikit learn need convert utc timezon new incom test data want predict cst cdt timezon timestamp matter predict ye need convert timezon train model ad step workflow web servic necessari convert make predict happen backend predict webservic api call sure know sure folk microsoft sure run workflow set know hope help get right track need",
        "Answer_preprocessed_content":"base experi think help question web job hit predict webservic train model run ye sens method model new data instanc train model method model product method call run workflow set deploi web servic exampl workflow plai time web servic run new data thing run like creat pipelin need convert utc timezon new incom test data timezon timestamp matter predict ye need convert timezon train model ad step workflow web servic necessari convert make predict happen backend predict webservic api call sure know sure folk microsoft sure run workflow set know hope help get right track need",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- When the web job hits the predictive web service, the trained ML model will be run again by calling the predict method on the model with the new data.\n- The whole workflow that has been set up to be deployed as the web service will also be run each time the web service is called with new data.\n- It is recommended to convert incoming test data to the timezone that was used when training the model. This can be done by adding a step in the workflow.\n- It is unclear what exactly happens in the backend when the predictive web service API is called, but it is certain that the workflow set up for the web service will be executed.",
        "Answer_gpt_summary":"possibl solut mention discuss web job hit predict web servic train model run call predict method model new data workflow set deploi web servic run time web servic call new data recommend convert incom test data timezon train model ad step workflow unclear exactli happen backend predict web servic api call certain workflow set web servic execut"
    },
    {
        "Question_title":"Dvc and S3 permissions management",
        "Question_body":"<p>Hello dvc people!<\/p>\n<p>At my team we are experimenting with dvc as an interface to work with trained models and datasets. So far it has been working nicely, but we found a mayor roadblock when it comes to permissions management.<\/p>\n<p>In our ideal scenario, when a teammate gets access to one of our repositories in GitHub that person will only have access to the data from that particular repository. This permission should be given automatically by the fact that the person has access to the repository, with no need to modify roles in AWS.<\/p>\n<p>However, this seems challenging to achieve through S3 + DVC. Our considered solution right now is to have a single S3 bucket with all our DVC repositories. If we grant access to all teammates to the whole bucket it would mean that they will have access to all the company\u2019s data, which is less than ideal. So we were considering having a \u201cJunior\u201d role which we will have to give access manually to whatever specific S3 folders they need and a \u201cSenior\u201d role with access to everything.<\/p>\n<p>This solution is suboptimal as we will need to handle different permissions for GitHub and for S3, which adds overhead.<\/p>\n<p>Is there any other pattern we are missing here? Is there any easy way for a teammate to be given access to ONLY the dvc remote from the GitHub repository automatically?<\/p>\n<p>Thanks for the help <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/relaxed.png?v=10\" title=\":relaxed:\" class=\"emoji\" alt=\":relaxed:\"><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1631267338282,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":484.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/javi\">@Javi<\/a>, thank you for asking! This is a very good question and two possible solutions:<\/p>\n<ol>\n<li>\n<strong>Workflow level<\/strong>. You can allow <code>write<\/code>\/<code>push<\/code> access to the bucket only for bots\/virtual-users that will push data to the bucket through CI\/CD only after approved pull requests in GitHub. While all the users will have <code>read<\/code>\/<code>pull<\/code> access from the bucket.<\/li>\n<li>\n<strong>User level<\/strong>. The way you describe it. We are in the design stage for this feature. It would be great if you can help us with the design and requirements.<\/li>\n<\/ol>\n<p>I\u2019d be happy to help you and go deeper into the solutions. Please let me know if you are open to a chat - please shoot me a Hi email to my-first-name at iterative.ai<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-s3-permissions-management\/887",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-10T17:48:06.234Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/javi\">@Javi<\/a>, thank you for asking! This is a very good question and two possible solutions:<\/p>\n<ol>\n<li>\n<strong>Workflow level<\/strong>. You can allow <code>write<\/code>\/<code>push<\/code> access to the bucket only for bots\/virtual-users that will push data to the bucket through CI\/CD only after approved pull requests in GitHub. While all the users will have <code>read<\/code>\/<code>pull<\/code> access from the bucket.<\/li>\n<li>\n<strong>User level<\/strong>. The way you describe it. We are in the design stage for this feature. It would be great if you can help us with the design and requirements.<\/li>\n<\/ol>\n<p>I\u2019d be happy to help you and go deeper into the solutions. Please let me know if you are open to a chat - please shoot me a Hi email to my-first-name at iterative.ai<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"permiss manag hello peopl team experi interfac work train model dataset far work nice mayor roadblock come permiss manag ideal scenario teammat get access repositori github person access data particular repositori permiss given automat fact person access repositori need modifi role aw challeng achiev consid solut right singl bucket repositori grant access teammat bucket mean access compani data ideal consid have junior role access manual specif folder need senior role access solut suboptim need handl differ permiss github add overhead pattern miss easi wai teammat given access remot github repositori automat thank help",
        "Question_preprocessed_content":"permiss manag hello peopl team experi interfac work train model dataset far work nice mayor roadblock come permiss manag ideal scenario teammat get access repositori github person access data particular repositori permiss given automat fact person access repositori need modifi role aw challeng achiev consid solut right singl bucket repositori grant access teammat bucket mean access compani data ideal consid have junior role access manual specif folder need senior role access solut suboptim need handl differ permiss github add overhead pattern miss easi wai teammat given access remot github repositori automat thank help",
        "Question_gpt_summary_original":"The user is facing challenges with permissions management while using DVC as an interface to work with trained models and datasets. They want to ensure that a teammate only has access to the data from a particular repository when they get access to that repository in GitHub, without modifying roles in AWS. However, this seems challenging to achieve through S3 + DVC, and the user is considering having a \"Junior\" and \"Senior\" role with different access levels, which is suboptimal and adds overhead. The user is seeking advice on whether there is any other pattern they are missing or an easy way for a teammate to be given access to ONLY the DVC remote from the GitHub repository automatically.",
        "Question_gpt_summary":"user face challeng permiss manag interfac work train model dataset want ensur teammat access data particular repositori access repositori github modifi role aw challeng achiev user consid have junior senior role differ access level suboptim add overhead user seek advic pattern miss easi wai teammat given access remot github repositori automat",
        "Answer_original_content":"javi thank ask good question possibl solut workflow level allow write push access bucket bot virtual user push data bucket approv pull request github user read pull access bucket user level wai design stage featur great help design requir happi help deeper solut let know open chat shoot email iter",
        "Answer_preprocessed_content":"thank ask good question possibl solut workflow level allow access bucket push data bucket approv pull request github user access bucket user level wai design stage featur great help design requir happi help deeper solut let know open chat shoot email",
        "Answer_gpt_summary_original":"Two possible solutions were discussed in the conversation. The first solution is to allow write\/push access to the bucket only for bots\/virtual-users that will push data to the bucket through CI\/CD only after approved pull requests in GitHub. While all the users will have read\/pull access from the bucket. The second solution is to have a user-level permission system, which is currently in the design stage, where a teammate can be given access to ONLY the DVC remote from the GitHub repository automatically.",
        "Answer_gpt_summary":"possibl solut discuss convers solut allow write push access bucket bot virtual user push data bucket approv pull request github user read pull access bucket second solut user level permiss current design stage teammat given access remot github repositori automat"
    },
    {
        "Question_title":"When do you specify the Target variable in a SageMaker Training job?",
        "Question_body":"<p>I'm trying to create a Machine Learning algorithm following this tutorial : <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-console.html\" rel=\"nofollow noreferrer\">Get Started with Amazon SageMaker<\/a><\/p>\n\n<p>Unless I missed something in the tutorial, I didn't find any steps where we specify the target variable. Can someone explain where \/ when we specify our target variable when creating an ML model using SageMaker built-in algorithms? <\/p>\n\n<p>Thanks a lot! <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1579360480360,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":715.0,
        "Answer_body":"<p>It depends on the scientific paradigm you're using in SageMaker :)<\/p>\n\n<ul>\n<li>SageMaker Built-in algorithms all have their input specification,\ndescribed in their respective documentation. For example, for\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html#ll-input_output\" rel=\"nofollow noreferrer\">SageMaker Linear Learner<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#InputOutput-XGBoost\" rel=\"nofollow noreferrer\">SageMaker XGBoost<\/a> the target is assumed\nto be the first column.<\/li>\n<li>With custom code, such as Bring-Your-Own-Docker or SageMaker Framework containers (for Sklearn, TF, PyTorch, MXNet) since you are the one writing the code you can write any sort of logic, and the target can be any column of your dataset.<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59801874",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1579512805583,
        "Question_original_content":"specifi target variabl train job try creat machin learn algorithm follow tutori start miss tutori step specifi target variabl explain specifi target variabl creat model built algorithm thank lot",
        "Question_preprocessed_content":"specifi target variabl train job try creat machin learn algorithm follow tutori start miss tutori step specifi target variabl explain specifi target variabl creat model algorithm thank lot",
        "Question_gpt_summary_original":"The user is facing a challenge in creating a Machine Learning algorithm using SageMaker built-in algorithms as they are unable to find any steps in the tutorial where the target variable is specified. They are seeking clarification on when and where to specify the target variable.",
        "Question_gpt_summary":"user face challeng creat machin learn algorithm built algorithm unabl step tutori target variabl specifi seek clarif specifi target variabl",
        "Answer_original_content":"depend scientif paradigm built algorithm input specif describ respect document exampl linear learner xgboost target assum column custom code bring docker framework contain sklearn pytorch mxnet write code write sort logic target column dataset",
        "Answer_preprocessed_content":"depend scientif paradigm algorithm input specif describ respect document exampl linear learner xgboost target assum column custom code framework contain write code write sort logic target column dataset",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- For SageMaker built-in algorithms, the input specification is described in their respective documentation. For example, for SageMaker Linear Learner and SageMaker XGBoost, the target is assumed to be the first column.\n- With custom code, such as Bring-Your-Own-Docker or SageMaker Framework containers (for Sklearn, TF, PyTorch, MXNet), the user can write any sort of logic, and the target can be any column of their dataset.\n\nNo personal opinions or biases are included in the response.",
        "Answer_gpt_summary":"possibl solut mention discuss built algorithm input specif describ respect document exampl linear learner xgboost target assum column custom code bring docker framework contain sklearn pytorch mxnet user write sort logic target column dataset person opinion bias includ respons"
    },
    {
        "Question_title":"How to correctly write a sagemaker tensorflow input_handler() that returns a numpy array?",
        "Question_body":"<p>I am trying to implement a input_handler() in inference.py for a sagemaker inference container.<\/p>\n<p>The images\/arrays are very big (3D). So I want to pass in a S3 URI, then the input_handler() function should load the image\/array from s3 and return the actual numpy array for the model (which expects a tensor):<\/p>\n<pre><code>def input_handler(data, context):\n\n    d = data.read().decode('utf-8')\n\n    body = json.loads(d)\n    s3path = body['s3_path']\n\n    s3 = S3FileSystem()\n    df = np.load(s3.open(s3path))\n\n    return df\n<\/code><\/pre>\n<p>Returning a numpy array worked with the Sagemaker python api version &lt; 1.0 and input_fn(), but does not work with the new container used by sagemaker python api &gt; 2.0 that expects input_handler().<\/p>\n<p>The actual container image is &quot;763104351884.dkr.ecr.eu-central-1.amazonaws.com\/tensorflow-inference:1.15-gpu&quot;.<\/p>\n<p>During inference, I get the following error in CloudWatch thrown by the container:<\/p>\n<pre><code>ERROR:python_service:exception handling request: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(\n\nTraceback (most recent call last):\n  File &quot;\/sagemaker\/python_service.py&quot;, line 289, in _handle_invocation_post\n    res.body, res.content_type = self._handlers(data, context)\n  File &quot;\/sagemaker\/python_service.py&quot;, line 322, in handler\n    response = requests.post(context.rest_uri, data=processed_input)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/api.py&quot;, line 116, in post\n    return request('post', url, data=data, json=json, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/api.py&quot;, line 60, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/sessions.py&quot;, line 512, in request\n    data=data or \n{}\n,\n<\/code><\/pre>\n<p>What is the correct return type? All examples I found were for json &amp; text...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1599496754993,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1599548555527,
        "Question_score":0.0,
        "Question_view_count":636.0,
        "Answer_body":"<p>This seems to work:<\/p>\n<p><code>return json.dumps({&quot;inputs&quot;: df.tolist() }).<\/code><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63781356",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1600259722303,
        "Question_original_content":"correctli write tensorflow input handler return numpi arrai try implement input handler infer infer contain imag arrai big want pass uri input handler function load imag arrai return actual numpi arrai model expect tensor def input handler data context data read decod utf bodi json load spath bodi path sfilesystem load open spath return return numpi arrai work python api version expect input handler actual contain imag dkr ecr central amazonaw com tensorflow infer gpu infer follow error cloudwatch thrown contain error python servic except handl request truth valu arrai element ambigu us traceback recent file python servic line handl invoc post re bodi re content type self handler data context file python servic line handler respons request post context rest uri data process input file usr local lib python dist packag request api line post return request post url data data json json kwarg file usr local lib python dist packag request api line request return session request method method url url kwarg file usr local lib python dist packag request session line request data data correct return type exampl json text",
        "Question_preprocessed_content":"correctli write tensorflow return numpi arrai try implement infer contain big want pass uri function load return actual numpi arrai model return numpi arrai work python api version expect actual contain imag infer follow error cloudwatch thrown contain correct return type exampl json",
        "Question_gpt_summary_original":"The user is facing challenges in implementing an input_handler() function for a Sagemaker inference container that can load large 3D images\/arrays from S3 and return a numpy array for the model. The function worked with the Sagemaker python API version <1.0 and input_fn() but not with the new container used by Sagemaker python API >2.0 that expects input_handler(). The user is getting an error during inference, which suggests an issue with the return type. The user is seeking guidance on the correct return type for the input_handler() function.",
        "Question_gpt_summary":"user face challeng implement input handler function infer contain load larg imag arrai return numpi arrai model function work python api version expect input handler user get error infer suggest issu return type user seek guidanc correct return type input handler function",
        "Answer_original_content":"work return json dump input tolist",
        "Answer_preprocessed_content":"work",
        "Answer_gpt_summary_original":"Solution: The following solution was suggested in the discussion: \n\n- Return the numpy array as a JSON object using the following code: `return json.dumps({\"inputs\": df.tolist()})`.",
        "Answer_gpt_summary":"solut follow solut suggest discuss return numpi arrai json object follow code return json dump input tolist"
    },
    {
        "Question_title":"Use Azure ML methods like an API",
        "Question_body":"<p>Is that possible to use machine learning methods from Microsoft Azure Machine Learning  as an API from my own code (without ML Studio) with possibility to calculate everything on their side?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1450294016510,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":180.0,
        "Answer_body":"<p>You can <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-publish-a-machine-learning-web-service\/\" rel=\"nofollow\">publish<\/a> an experiment (machine learning functions you hooked together in Azure ML Studio) as an API. When you call that API in your custom code you give it your data and all the computation runs in the cloud in Azure ML. <\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34320449",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1450317613532,
        "Question_original_content":"us method like api possibl us machin learn method api code studio possibl calcul",
        "Question_preprocessed_content":"us method like api possibl us machin learn method api code possibl calcul",
        "Question_gpt_summary_original":"The user is facing a challenge of whether it is possible to use Azure ML methods as an API from their own code without using ML Studio and with the ability to perform calculations on the Azure side.",
        "Question_gpt_summary":"user face challeng possibl us method api code studio abil perform calcul azur",
        "Answer_original_content":"publish experi machin learn function hook studio api api custom code data comput run cloud",
        "Answer_preprocessed_content":"publish experi api api custom code data comput run cloud",
        "Answer_gpt_summary_original":"Solution: The user can publish an experiment as an API and call it in their custom code, which will allow them to perform calculations on the Azure side without using ML Studio.",
        "Answer_gpt_summary":"solut user publish experi api custom code allow perform calcul azur studio"
    },
    {
        "Question_title":"how many models can be deployed in single node in azure kubernetes service?",
        "Question_body":"Working on deployment of 170 ml models using ML studio and azure Kubernetes service which is referred on the below doc link \"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/how-to-deploy-azure-kubernetes-service.md\".\n\nWe are training the model using python script with the custom environment and we are registering the ml model on the Azure ML services. Once we register the mode we are deploying it on the AKS by using the container images.\n\nWhile deploying the ML model we are able to deploy up 10 to 11 models per pods for each Node in AKS. When we try to deploy the model on the same node we are getting deployment timeout error and we are getting the below error message.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630746472547,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi @suvedharan-5910\n\nThe number of models to be deployed is limited to 1,000 models per deployment (per container).\n\nAutoscaling for Azure ML model deployments is azureml-fe, which is a smart request router. Since all inference requests go through it, it has the necessary data to automatically scale the deployed model(s).\nmore details\n\n\n\n\n\nIf the Answer is helpful, please click Accept Answer and up-vote, so that it can help others in the community looking for help on similar topics.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/540001\/how-many-models-can-be-deployed-in-single-node-in.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-04T12:32:05.4Z",
                "Answer_score":0,
                "Answer_body":"Hi @suvedharan-5910\n\nThe number of models to be deployed is limited to 1,000 models per deployment (per container).\n\nAutoscaling for Azure ML model deployments is azureml-fe, which is a smart request router. Since all inference requests go through it, it has the necessary data to automatically scale the deployed model(s).\nmore details\n\n\n\n\n\nIf the Answer is helpful, please click Accept Answer and up-vote, so that it can help others in the community looking for help on similar topics.",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1630758725400,
        "Question_original_content":"model deploi singl node azur kubernet servic work deploy model studio azur kubernet servic refer doc link http github com microsoftdoc azur doc blob master articl machin learn deploi azur kubernet servic train model python script custom environ regist model servic regist mode deploi ak contain imag deploi model abl deploi model pod node ak try deploi model node get deploy timeout error get error messag",
        "Question_preprocessed_content":"model deploi singl node azur kubernet servic work deploy model studio azur kubernet servic refer doc link train model python script custom environ regist model servic regist mode deploi ak contain imag deploi model abl deploi model pod node ak try deploi model node get deploy timeout error get error messag",
        "Question_gpt_summary_original":"The user is facing challenges in deploying multiple ML models using Azure Kubernetes Service. They are able to deploy up to 10-11 models per pod for each node, but encounter deployment timeout errors when attempting to deploy more models on the same node.",
        "Question_gpt_summary":"user face challeng deploi multipl model azur kubernet servic abl deploi model pod node encount deploy timeout error attempt deploi model node",
        "Answer_original_content":"suvedharan number model deploi limit model deploy contain autosc model deploy smart request router infer request necessari data automat scale deploi model detail answer help click accept answer vote help commun look help similar topic",
        "Answer_preprocessed_content":"number model deploi limit model deploy autosc model deploy smart request router infer request necessari data automat scale deploi model detail answer help click accept answer help commun look help similar topic",
        "Answer_gpt_summary_original":"Solution: \n- The number of models to be deployed is limited to 1,000 models per deployment (per container).\n- Autoscaling for Azure ML model deployments is azureml-fe, which is a smart request router. It can automatically scale the deployed model(s) based on the inference requests.",
        "Answer_gpt_summary":"solut number model deploi limit model deploy contain autosc model deploy smart request router automat scale deploi model base infer request"
    },
    {
        "Question_title":"ModelUploadOp step failing with custom prediction container",
        "Question_body":"<p>I am currenlty trying to deploy a Vertex pipeline to achieve the following:<\/p>\n<ol>\n<li><p>Train a custom model (from a custom training python package) and dump model artifacts (trained model and data preprocessor that will be sed at prediction time). This is step is working fine as I can see new resources being created in the storage bucket.<\/p>\n<\/li>\n<li><p>Create a model resource via <code>ModelUploadOp<\/code>. This step fails for some reason when specifying <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code> with the error message in the <strong>errors<\/strong> section below. This is somewhat surprising as they are both needed by the prediction container and environment variables are passed as a dict as specified in the documentation.<br \/>\nThis step works just fine using <code>gcloud<\/code> commands:<\/p>\n<\/li>\n<\/ol>\n<pre class=\"lang-sh prettyprint-override\"><code>gcloud ai models upload \\\n    --region us-west1 \\\n    --display-name session_model_latest \\\n    --container-image-uri gcr.io\/and-reporting\/pred:latest \\\n    --container-env-vars=&quot;MODEL_BUCKET=ml_session_model&quot; \\\n    --container-health-route=\/\/health \\\n    --container-predict-route=\/\/predict \\\n    --container-ports=5000\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Create an endpoint.<\/li>\n<li>Deploy the model to the endpoint.<\/li>\n<\/ol>\n<p>There is clearly something that I am getting wrong with Vertex, the components <a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.2.2\/index.html\" rel=\"nofollow noreferrer\">documentation<\/a> doesn't help much in this case.<\/p>\n<h2>Pipeline<\/h2>\n<pre class=\"lang-py prettyprint-override\"><code>from datetime import datetime\n\nimport kfp\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import compiler\n\nPIPELINE_ROOT = &quot;gs:\/\/ml_model_bucket\/pipeline_root&quot;\n\n\n@kfp.dsl.pipeline(name=&quot;session-train-deploy&quot;, pipeline_root=PIPELINE_ROOT)\ndef pipeline():\n    training_op = gcc_aip.CustomPythonPackageTrainingJobRunOp(\n        project=&quot;my-project&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;train_session_model&quot;,\n        model_display_name=&quot;session_model&quot;,\n        service_account=&quot;name@my-project.iam.gserviceaccount.com&quot;,\n        environment_variables={&quot;MODEL_BUCKET&quot;: &quot;ml_session_model&quot;},\n        python_module_name=&quot;trainer.train&quot;,\n        staging_bucket=&quot;gs:\/\/ml_model_bucket\/&quot;,\n        base_output_dir=&quot;gs:\/\/ml_model_bucket\/&quot;,\n        args=[\n            &quot;--gcs-data-path&quot;,\n            &quot;gs:\/\/ml_model_data\/2019-Oct_short.csv&quot;,\n            &quot;--gcs-model-path&quot;,\n            &quot;gs:\/\/ml_model_bucket\/model\/model.joblib&quot;,\n            &quot;--gcs-preproc-path&quot;,\n            &quot;gs:\/\/ml_model_bucket\/model\/preproc.pkl&quot;,\n        ],\n        container_uri=&quot;us-docker.pkg.dev\/vertex-ai\/training\/scikit-learn-cpu.0-23:latest&quot;,\n        python_package_gcs_uri=&quot;gs:\/\/ml_model_bucket\/trainer-0.0.1.tar.gz&quot;,\n        model_serving_container_image_uri=&quot;gcr.io\/my-project\/pred&quot;,\n        model_serving_container_predict_route=&quot;\/predict&quot;,\n        model_serving_container_health_route=&quot;\/health&quot;,\n        model_serving_container_ports=[5000],\n        model_serving_container_environment_variables={\n            &quot;MODEL_BUCKET&quot;: &quot;ml_model_bucket\/model&quot;\n        },\n    )\n\n    model_upload_op = gcc_aip.ModelUploadOp(\n        project=&quot;and-reporting&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;session_model&quot;,\n        serving_container_image_uri=&quot;gcr.io\/my-project\/pred:latest&quot;,\n        # When passing the following 2 arguments this step fails...\n        serving_container_environment_variables={&quot;MODEL_BUCKET&quot;: &quot;ml_model_bucket\/model&quot;},\n        serving_container_ports=[5000],\n        serving_container_predict_route=&quot;\/predict&quot;,\n        serving_container_health_route=&quot;\/health&quot;,\n    )\n    model_upload_op.after(training_op)\n\n    endpoint_create_op = gcc_aip.EndpointCreateOp(\n        project=&quot;my-project&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;pipeline_endpoint&quot;,\n    )\n\n    model_deploy_op = gcc_aip.ModelDeployOp(\n        model=model_upload_op.outputs[&quot;model&quot;],\n        endpoint=endpoint_create_op.outputs[&quot;endpoint&quot;],\n        deployed_model_display_name=&quot;session_model&quot;,\n        traffic_split={&quot;0&quot;: 100},\n        service_account=&quot;name@my-project.iam.gserviceaccount.com&quot;,\n    )\n    model_deploy_op.after(endpoint_create_op)\n\n\nif __name__ == &quot;__main__&quot;:\n    ts = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n    compiler.Compiler().compile(pipeline, &quot;custom_train_pipeline.json&quot;)\n    pipeline_job = aiplatform.PipelineJob(\n        display_name=&quot;session_train_and_deploy&quot;,\n        template_path=&quot;custom_train_pipeline.json&quot;,\n        job_id=f&quot;session-custom-pipeline-{ts}&quot;,\n        enable_caching=True,\n    )\n    pipeline_job.submit()\n\n<\/code><\/pre>\n<h3>Errors and notes<\/h3>\n<ol>\n<li>When specifying <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code> the step fails with the following error:<\/li>\n<\/ol>\n<pre><code>{'code': 400, 'message': 'Invalid JSON payload received. Unknown name &quot;MODEL_BUCKET&quot; at \\'model.container_spec.env[0]\\': Cannot find field.\\nInvalid value at \\'model.container_spec.ports[0]\\' (type.googleapis.com\/google.cloud.aiplatform.v1.Port), 5000', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com\/google.rpc.BadRequest', 'fieldViolations': [{'field': 'model.container_spec.env[0]', 'description': 'Invalid JSON payload received. Unknown name &quot;MODEL_BUCKET&quot; at \\'model.container_spec.env[0]\\': Cannot find field.'}, {'field': 'model.container_spec.ports[0]', 'description': &quot;Invalid value at 'model.container_spec.ports[0]' (type.googleapis.com\/google.cloud.aiplatform.v1.Port), 5000&quot;}]}]}\n<\/code><\/pre>\n<p>When commenting out <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code>  the model resource gets created but deploying it manually to the endpoint results into a failed deployment with no output logs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1643879294320,
        "Question_favorite_count":null,
        "Question_last_edit_time":1644239389000,
        "Question_score":0.0,
        "Question_view_count":273.0,
        "Answer_body":"<p>After some time researching the problem I've stumbled upon <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/6848\" rel=\"nofollow noreferrer\">this<\/a> Github issue. The problem was originated by a mismatch between <a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.2.2\/index.html\" rel=\"nofollow noreferrer\"><code>google_cloud_pipeline_components<\/code><\/a> and <a href=\"https:\/\/kubernetes.io\/docs\/reference\/generated\/kubernetes-api\/v1.19\/#envvar-v1-core\" rel=\"nofollow noreferrer\"><code>kubernetes_api<\/code><\/a> docs. In this case, <code>serving_container_environment_variables<\/code> is typed as an <code>Optional[dict[str, str]]<\/code> whereas it should have been typed as a <code>Optional[list[dict[str, str]]]<\/code>. A similar mismatch can be found for <code>serving_container_ports<\/code> argument as well. Passing arguments following kubernetes documentation did the trick:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_upload_op = gcc_aip.ModelUploadOp(\n    project=&quot;my-project&quot;,\n    location=&quot;us-west1&quot;,\n    display_name=&quot;session_model&quot;,\n    serving_container_image_uri=&quot;gcr.io\/my-project\/pred:latest&quot;,\n    serving_container_environment_variables=[\n        {&quot;name&quot;: &quot;MODEL_BUCKET&quot;, &quot;value&quot;: &quot;ml_session_model&quot;}\n    ],\n    serving_container_ports=[{&quot;containerPort&quot;: 5000}],\n    serving_container_predict_route=&quot;\/predict&quot;,\n    serving_container_health_route=&quot;\/health&quot;,\n)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70968460",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1643965835692,
        "Question_original_content":"modeluploadop step fail custom predict contain currenlti try deploi vertex pipelin achiev follow train custom model custom train python packag dump model artifact train model data preprocessor sed predict time step work fine new resourc creat storag bucket creat model resourc modeluploadop step fail reason specifi serv contain environ variabl serv contain port error messag error section somewhat surpris need predict contain environ variabl pass dict specifi document step work fine gcloud command gcloud model upload region west displai session model latest contain imag uri gcr report pred latest contain env var model bucket session model contain health rout health contain predict rout predict contain port creat endpoint deploi model endpoint clearli get wrong vertex compon document help case pipelin datetim import datetim import kfp googl cloud import aiplatform googl cloud pipelin compon import aiplatform gcc aip kfp import compil pipelin root model bucket pipelin root kfp dsl pipelin session train deploi pipelin root pipelin root def pipelin train gcc aip custompythonpackagetrainingjobrunop project project locat west displai train session model model displai session model servic account project iam gserviceaccount com environ variabl model bucket session model python modul trainer train stage bucket model bucket base output dir model bucket arg gc data path model data oct short csv gc model path model bucket model model joblib gc preproc path model bucket model preproc pkl contain uri docker pkg dev vertex train scikit learn cpu latest python packag gc uri model bucket trainer tar model serv contain imag uri gcr project pred model serv contain predict rout predict model serv contain health rout health model serv contain port model serv contain environ variabl model bucket model bucket model model upload gcc aip modeluploadop project report locat west displai session model serv contain imag uri gcr project pred latest pass follow argument step fail serv contain environ variabl model bucket model bucket model serv contain port serv contain predict rout predict serv contain health rout health model upload train endpoint creat gcc aip endpointcreateop project project locat west displai pipelin endpoint model deploi gcc aip modeldeployop model model upload output model endpoint endpoint creat output endpoint deploi model displai session model traffic split servic account project iam gserviceaccount com model deploi endpoint creat main datetim strftime compil compil compil pipelin custom train pipelin json pipelin job aiplatform pipelinejob displai session train deploi templat path custom train pipelin json job session custom pipelin enabl cach true pipelin job submit error note specifi serv contain environ variabl serv contain port step fail follow error code messag invalid json payload receiv unknown model bucket model contain spec env field ninvalid valu model contain spec port type googleapi com googl cloud aiplatform port statu invalid argument detail type type googleapi com googl rpc badrequest fieldviol field model contain spec env descript invalid json payload receiv unknown model bucket model contain spec env field field model contain spec port descript invalid valu model contain spec port type googleapi com googl cloud aiplatform port comment serv contain environ variabl serv contain port model resourc get creat deploi manual endpoint result fail deploy output log",
        "Question_preprocessed_content":"modeluploadop step fail custom predict contain currenlti try deploi vertex pipelin achiev follow train custom model dump model artifact step work fine new resourc creat storag bucket creat model resourc step fail reason specifi error messag error section somewhat surpris need predict contain environ variabl pass dict specifi document step work fine command creat endpoint deploi model endpoint clearli get wrong vertex compon document help case pipelin error note specifi step fail follow error comment model resourc get creat deploi manual endpoint result fail deploy output log",
        "Question_gpt_summary_original":"The user is encountering challenges while deploying a Vertex pipeline. The ModelUploadOp step is failing when specifying serving_container_environment_variables and serving_container_ports. The error message indicates that the JSON payload received is invalid and the field \"MODEL_BUCKET\" cannot be found. The user has tried to create a model resource via gcloud commands, which works fine. However, when using Vertex, the documentation doesn't help much.",
        "Question_gpt_summary":"user encount challeng deploi vertex pipelin modeluploadop step fail specifi serv contain environ variabl serv contain port error messag indic json payload receiv invalid field model bucket user tri creat model resourc gcloud command work fine vertex document help",
        "Answer_original_content":"time research problem stumbl github issu problem origin mismatch googl cloud pipelin compon kubernet api doc case serv contain environ variabl type option dict str str type option list dict str str similar mismatch serv contain port argument pass argument follow kubernet document trick model upload gcc aip modeluploadop project project locat west displai session model serv contain imag uri gcr project pred latest serv contain environ variabl model bucket valu session model serv contain port containerport serv contain predict rout predict serv contain health rout health",
        "Answer_preprocessed_content":"time research problem stumbl github issu problem origin mismatch doc case type type similar mismatch argument pass argument follow kubernet document trick",
        "Answer_gpt_summary_original":"Solution:\nThe problem was caused by a mismatch between `google_cloud_pipeline_components` and `kubernetes_api` docs. The `serving_container_environment_variables` and `serving_container_ports` arguments should have been typed as `Optional[list[dict[str, str]]]` instead of `Optional[dict[str, str]]`. Passing arguments following kubernetes documentation resolved the issue.",
        "Answer_gpt_summary":"solut problem caus mismatch googl cloud pipelin compon kubernet api doc serv contain environ variabl serv contain port argument type option list dict str str instead option dict str str pass argument follow kubernet document resolv issu"
    },
    {
        "Question_title":"All records are lost in a project without any action",
        "Question_body":"<p>Dear Sir or Madam,<\/p>\n<p>Sorry for bothering you, I think there is an error in one of my wandb projects and the records of all runs were lost. The account is nbower0707, email 1155156871@link.cuhk.edu.hk, and the project name is ocp22.<\/p>\n<p>Everything worked fine before today, and I did a lot of experiments on this project. I\u2019m uploading records of my metric around every 5000 steps, and the result validation metric plot should be something like  figure 1 shows(continuous lines of records, with multiple data points) I\u2019m uploading the corresponding metrics every 2500 steps, and wandb displayed all results fine yesterday (either undergoing or finished runs)<\/p>\n<p>However, when I check the plot today, the record of metric in all runs were (completely or partly) lost, except for some small isolated data points left (as figure 2 and 3 shows).<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d.jpeg\" data-download-href=\"\/uploads\/short-url\/n0TMrYL9SyvpaH1YKsBmceDhhRb.jpeg?dl=1\" title=\"Picture 1\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_414x500.jpeg\" alt=\"Picture 1\" data-base62-sha1=\"n0TMrYL9SyvpaH1YKsBmceDhhRb\" width=\"414\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_414x500.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_621x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_828x1000.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Picture 1<\/span><span class=\"informations\">2337\u00d72818 348 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>I tried to use <strong>wandb sync<\/strong> from the local file, and upload the runs to a new project, the result is still the same.<\/p>\n<p>I didn\u2019t do any specific operations regarding wandb logging process or on the website. The project consist of runs uploaded from different machines, therefore it wouldn\u2019t be mistakenly deletion\/ false operation offline. And the phenomenon of lost of data also occurs on old runs that finished weeks ago.<\/p>\n<p>Please let me know if you have any suggestions on this error, and if the records could be recovered.<\/p>\n<p>Your time and patience are sincerely appreciated.<\/p>\n<p>Bowen Wang<\/p>",
        "Question_answer_count":13,
        "Question_comment_count":0,
        "Question_creation_time":1661403318517,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":162.0,
        "Answer_body":"<p>Hey all,<\/p>\n<p>Our engineering team looked into this and rolled back some changes, everything should be working fine now.<\/p>\n<p>Please let us know if this issue persists.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/all-records-are-lost-in-a-project-without-any-action\/2993",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-25T05:10:38.981Z",
                "Answer_body":"<p>I have the same problem\u2026<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T06:26:28.134Z",
                "Answer_body":"<p>Me too. From night to morning the runs graphs miss a lot of data points in the validation section and I also noted that the resize of the panels in that section doesn\u2019t work properly<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T06:27:35.035Z",
                "Answer_body":"<p>Same problem here, the plots are weird and loses a lot of data points<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T06:44:26.346Z",
                "Answer_body":"<p>Me too\u2026 It seem to be  same problem<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T07:43:03.396Z",
                "Answer_body":"<p>Hi Everyone,<\/p>\n<p>Apologies for the inconvenience here. We are looking into the issue - any links to workspaces where you see this currently would be greatly appreciated.<\/p>\n<p>Thanks,<br>\nRamit<br>\nW&amp;B Support<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T08:19:07.481Z",
                "Answer_body":"<p>Mine is <a href=\"https:\/\/wandb.ai\/niansong1996\/cot-codegen?workspace=user-niansong1996\" class=\"inline-onebox\">Weights &amp; Biases<\/a><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T08:20:00.625Z",
                "Answer_body":"<p>Same problem here, plots look similar to the ones shown<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T08:45:43.885Z",
                "Answer_body":"<p>Here\u2019s <a href=\"https:\/\/wandb.ai\/johnminelli\/TwoWaySinth\">mine<\/a><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T09:41:56.499Z",
                "Answer_body":"<p>The same issue\u2026<br>\nYesterday  they were fine\u2026<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T09:46:40.765Z",
                "Answer_body":"<p>Same issue  <a href=\"https:\/\/wandb.ai\/ecotoxformer\/fish-EC50-MOR?workspace=user-styrbjornkall\">here<\/a>. Though the charts look fine when opened in their respective run, just not in the combined workspace\u2026<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T16:49:12.578Z",
                "Answer_body":"<p>Now it\u2019s fine for me, thank you for the support<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T21:07:26.293Z",
                "Answer_body":"<p>Hey all,<\/p>\n<p>Our engineering team looked into this and rolled back some changes, everything should be working fine now.<\/p>\n<p>Please let us know if this issue persists.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-10-24T21:08:14.661Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1661461646292,
        "Question_original_content":"record lost project action dear sir madam sorri bother think error project record run lost account nbower email link cuhk edu project ocp work fine todai lot experi project upload record metric step result valid metric plot like figur show continu line record multipl data point upload correspond metric step displai result fine yesterdai undergo finish run check plot todai record metric run complet partli lost small isol data point left figur show pictur tri us sync local file upload run new project result didnt specif oper log process websit project consist run upload differ machin wouldnt mistakenli delet fals oper offlin phenomenon lost data occur old run finish week ago let know suggest error record recov time patienc sincer appreci bowen wang",
        "Question_preprocessed_content":"record lost project action dear sir madam sorri bother think error project record run lost account nbower email project ocp work fine todai lot experi project upload record metric step result valid metric plot like figur show upload correspond metric step displai result fine yesterdai check plot todai record metric run lost small isol data point left pictur tri us sync local file upload run new project result didnt specif oper log process websit project consist run upload differ machin wouldnt mistakenli delet fals oper offlin phenomenon lost data occur old run finish week ago let know suggest error record recov time patienc sincer appreci bowen wang",
        "Question_gpt_summary_original":"The user has encountered a challenge where all records of their runs in a wandb project named \"ocp22\" have been lost without any action on their part. The user had been uploading records of their metric every 5000 steps and the validation metric plot was displaying fine until today. The records of the metric in all runs were lost, except for some small isolated data points. The user tried to use \"wandb sync\" from the local file and upload the runs to a new project, but the result was still the same. The user did not perform any specific operations regarding the wandb logging process or on the website, and the phenomenon of lost data also occurred on old runs that finished weeks ago. The user is seeking suggestions on how to recover the lost records.",
        "Question_gpt_summary":"user encount challeng record run project name ocp lost action user upload record metric step valid metric plot displai fine todai record metric run lost small isol data point user tri us sync local file upload run new project result user perform specif oper log process websit phenomenon lost data occur old run finish week ago user seek suggest recov lost record",
        "Answer_original_content":"hei engin team look roll chang work fine let know issu persist thank ramit",
        "Answer_preprocessed_content":"hei engin team look roll chang work fine let know issu persist thank ramit",
        "Answer_gpt_summary_original":"Solution: The engineering team has looked into the issue and rolled back some changes, which should have resolved the problem. The user is advised to check if the issue persists and report it if it does. No other solutions were mentioned.",
        "Answer_gpt_summary":"solut engin team look issu roll chang resolv problem user advis check issu persist report solut mention"
    },
    {
        "Question_title":"Why package is not updated even the lifecycle script has been executed successfully in SageMaker?",
        "Question_body":"<p>I wanted to update pandas version in 'conda-python3' in SageMaker, I've followed the steps in this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">page<\/a>, and linked the new configuration to my instance, CloudWatch log shows me the script has been executed successfully, but when I restart my instance and print out the panda version, it's still showing the old version 0.24.2, I don't understand why?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fR82t.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fR82t.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This is the script in the lifecycle configuration:<\/p>\n<pre><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\npip install pandas\n\nconda update pandas\n\nsource deactivate\n\nEOF\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593901036277,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":106.0,
        "Answer_body":"<p>You are not activating any conda environment such as <a href=\"https:\/\/stackoverflow.com\/questions\/60036916\/sagemaker-lifecycle-configuration-for-installing-pandas-not-working\">python3<\/a>.<\/p>\n<pre><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\n# This will affect only the Jupyter kernel called &quot;conda_python3&quot;.\nsource activate python3\n\npip install pandas\n\nconda update pandas\n\nsource deactivate\n\nEOF\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1605563749572,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62734994",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1594359260763,
        "Question_original_content":"packag updat lifecycl script execut successfulli want updat panda version conda python follow step page link new configur instanc cloudwatch log show script execut successfulli restart instanc print panda version show old version understand script lifecycl configur bin bash sudo user eof pip instal panda conda updat panda sourc deactiv eof",
        "Question_preprocessed_content":"packag updat lifecycl script execut successfulli want updat panda version follow step page link new configur instanc cloudwatch log show script execut successfulli restart instanc print panda version show old version understand script lifecycl configur",
        "Question_gpt_summary_original":"The user encountered a challenge in updating the pandas version in 'conda-python3' in SageMaker. The user followed the steps in the provided page and linked the new configuration to their instance. The CloudWatch log showed that the script was executed successfully, but upon restarting the instance and checking the pandas version, it still showed the old version. The user is unsure why this is happening.",
        "Question_gpt_summary":"user encount challeng updat panda version conda python user follow step provid page link new configur instanc cloudwatch log show script execut successfulli restart instanc check panda version show old version user unsur happen",
        "Answer_original_content":"activ conda environ python bin bash sudo user eof affect jupyt kernel call conda python sourc activ python pip instal panda conda updat panda sourc deactiv eof",
        "Answer_preprocessed_content":"activ conda environ python",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to activate the conda environment such as 'python3' before installing and updating pandas. The provided script should be modified to include the command 'source activate python3' before installing and updating pandas, and 'source deactivate' after completing the installation and update.",
        "Answer_gpt_summary":"solut possibl solut mention discuss activ conda environ python instal updat panda provid script modifi includ command sourc activ python instal updat panda sourc deactiv complet instal updat"
    },
    {
        "Question_title":"How to configure comet (comet.ml) to track Keras?",
        "Question_body":"<p>im trying to setup <a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a> to log my experiment details <\/p>\n\n<p>getting strange error:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"train.py\", line 7, in &lt;module&gt;\n    from comet_ml import Experiment\nImportError: No module named comet_ml\n<\/code><\/pre>\n\n<p>trying in python 2 and python3<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1506066265147,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1506066568087,
        "Question_score":3.0,
        "Question_view_count":1208.0,
        "Answer_body":"<p>it seems like comet isn't installed on your machine.<\/p>\n\n<p>you can use :<\/p>\n\n<pre><code>pip3 install comet_ml\npip install comet_ml\n<\/code><\/pre>\n\n<p>take a look at the example projects at: <\/p>\n\n<p><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1513514919392,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46359436",
        "Tool":"Comet",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1506066553020,
        "Question_original_content":"configur track kera try setup http log experi detail get strang error traceback recent file train line import experi importerror modul name try python python",
        "Question_preprocessed_content":"configur track kera try setup log experi detail get strang error try python python",
        "Question_gpt_summary_original":"The user is encountering an error while trying to set up comet.ml to log their experiment details. They are receiving an ImportError message stating that there is no module named comet_ml, despite trying in both Python 2 and Python 3.",
        "Question_gpt_summary":"user encount error try set log experi detail receiv importerror messag state modul name despit try python python",
        "Answer_original_content":"like isn instal machin us pip instal pip instal look exampl project http github com quickstart guid",
        "Answer_preprocessed_content":"like isn instal machin us look exampl project",
        "Answer_gpt_summary_original":"Solution: The user can install comet_ml by running either \"pip3 install comet_ml\" or \"pip install comet_ml\" in their terminal. They can also refer to the example projects provided in the quickstart guide on GitHub.",
        "Answer_gpt_summary":"solut user instal run pip instal pip instal termin refer exampl project provid quickstart guid github"
    },
    {
        "Question_title":"Dvc get git@github.com:*.git folder giving ERROR: unexpected error - Response payload is not completed",
        "Question_body":"<p>I have successfully added a folder with 2gb of files using DVC to an s3 bucket. After that I tried to get the folder using the dvc get command. It downloaded half the folder correctly and after that the download stopped and gave me this message: ERROR: Unexpected error - Response payload not completed<br>\nAny suggestions on how I can resolve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634238227604,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":355.0,
        "Answer_body":"<p>For the record: this was discussed on discord <a href=\"https:\/\/discord.com\/channels\/485586884165107732\/485596304961962003\/898534803362701322\" class=\"inline-onebox\">Discord<\/a> and seems to have been a network issue.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-get-git-github-com-git-folder-giving-error-unexpected-error-response-payload-is-not-completed\/919",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-15T21:38:57.691Z",
                "Answer_body":"<p>For the record: this was discussed on discord <a href=\"https:\/\/discord.com\/channels\/485586884165107732\/485596304961962003\/898534803362701322\" class=\"inline-onebox\">Discord<\/a> and seems to have been a network issue.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"git github com git folder give error unexpect error respons payload complet successfulli ad folder file bucket tri folder command download half folder correctli download stop gave messag error unexpect error respons payload complet suggest resolv issu",
        "Question_preprocessed_content":"folder give error unexpect error respons payload complet successfulli ad folder file bucket tri folder command download half folder correctli download stop gave messag error unexpect error respons payload complet suggest resolv issu",
        "Question_gpt_summary_original":"The user encountered an error while trying to download a folder using the dvc get command. The download stopped halfway and displayed the message \"ERROR: Unexpected error - Response payload not completed.\" The folder contains 2GB of files and was successfully added to an s3 bucket using DVC. The user is seeking suggestions on how to resolve the issue.",
        "Question_gpt_summary":"user encount error try download folder command download stop halfwai displai messag error unexpect error respons payload complet folder contain file successfulli ad bucket user seek suggest resolv issu",
        "Answer_original_content":"record discuss discord discord network issu",
        "Answer_preprocessed_content":"record discuss discord discord network issu",
        "Answer_gpt_summary_original":"Solution: No solution provided. The discussion suggests that the issue was caused by a network problem.",
        "Answer_gpt_summary":"solut solut provid discuss suggest issu caus network problem"
    },
    {
        "Question_title":"Using MLflow and Sagemaker with preprocessing steps",
        "Question_body":"I'm deploying my models to Sagemaker using MLflow integration. However, my ML pipeline includes some basic preprocessing steps, such as scalers, and I need it to be part of my inference endpoint. Is there a way to do that with MLflow? I looked in the\u00a0mlflow_pyfunc\u00a0is closer to what I want, but I'm not sure if it is compatible with Sagemaker.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658304744000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":17.0,
        "Answer_body":"Yes, you can add preprocessing or post processing\u00a0steps around a logged MLflow model and then re-log the new extended pipeline model as a new wrapped MLflow model. Then this new MLflow model should work the same way as the original model with the additional functionality of doing pre\/post processing.\nAnd it should work in Sagemaker as well. Let me know if you have any further questions.\n\n\nYong\nhttps:\/\/www.linkedin.com\/in\/yongliu\/\n\n\n\nOn Wed, Jul 20, 2022 at 5:12 AM 'Paulo Vasconcellos' via mlflow-users <mlflow...@googlegroups.com> wrote:\n\nI'm deploying my models to Sagemaker using MLflow integration. However, my ML pipeline includes some basic preprocessing steps, such as scalers, and I need it to be part of my inference endpoint. Is there a way to do that with MLflow? I looked in the\u00a0mlflow_pyfunc\u00a0is closer to what I want, but I'm not sure if it is compatible with Sagemaker.\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/213752ed-f2d3-4ae2-ac4f-03492769799fn%40googlegroups.com.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/I7LdK_7KCIw",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-20T12:31:38",
                "Answer_body":"Yes, you can add preprocessing or post processing\u00a0steps around a logged MLflow model and then re-log the new extended pipeline model as a new wrapped MLflow model. Then this new MLflow model should work the same way as the original model with the additional functionality of doing pre\/post processing.\nAnd it should work in Sagemaker as well. Let me know if you have any further questions.\n\n\nYong\nhttps:\/\/www.linkedin.com\/in\/yongliu\/\n\n\n\nOn Wed, Jul 20, 2022 at 5:12 AM 'Paulo Vasconcellos' via mlflow-users <mlflow...@googlegroups.com> wrote:\n\nI'm deploying my models to Sagemaker using MLflow integration. However, my ML pipeline includes some basic preprocessing steps, such as scalers, and I need it to be part of my inference endpoint. Is there a way to do that with MLflow? I looked in the\u00a0mlflow_pyfunc\u00a0is closer to what I want, but I'm not sure if it is compatible with Sagemaker.\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/213752ed-f2d3-4ae2-ac4f-03492769799fn%40googlegroups.com."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"preprocess step deploi model integr pipelin includ basic preprocess step scaler need infer endpoint wai look pyfunci closer want sure compat",
        "Question_preprocessed_content":"preprocess step deploi model integr pipelin includ basic preprocess step scaler need infer endpoint wai look closer want sure compat",
        "Question_gpt_summary_original":"The user is facing a challenge in deploying their ML models to Sagemaker using MLflow integration as their ML pipeline includes basic preprocessing steps that need to be part of the inference endpoint. They are unsure if the mlflow_pyfunc is compatible with Sagemaker and are seeking a solution to include the preprocessing steps in the inference endpoint.",
        "Question_gpt_summary":"user face challeng deploi model integr pipelin includ basic preprocess step need infer endpoint unsur pyfunc compat seek solut includ preprocess step infer endpoint",
        "Answer_original_content":"ye add preprocess post processingstep log model log new extend pipelin model new wrap model new model work wai origin model addit function pre post process work let know question yong http linkedin com yongliu wed jul paulo vasconcello user wrote deploi model integr pipelin includ basic preprocess step scaler need infer endpoint wai look pyfunci closer want sure compat receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user acf googlegroup com",
        "Answer_preprocessed_content":"ye add preprocess post processingstep log model new extend pipelin model new wrap model new model work wai origin model addit function process work let know question yong wed jul paulo vasconcello user wrote deploi model integr pipelin includ basic preprocess step scaler need infer endpoint wai look closer want sure compat receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that the user can add preprocessing or post-processing steps around a logged MLflow model and then re-log the new extended pipeline model as a new wrapped MLflow model. This new MLflow model should work the same way as the original model with the additional functionality of doing pre\/post-processing. It is mentioned that this solution should work in Sagemaker as well.",
        "Answer_gpt_summary":"solut discuss suggest user add preprocess post process step log model log new extend pipelin model new wrap model new model work wai origin model addit function pre post process mention solut work"
    },
    {
        "Question_title":"Can I pass arguments to the entrypoint of a SageMaker estimator?",
        "Question_body":"<p>I'm using the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html#sagemaker.estimator.Framework\" rel=\"noreferrer\">SageMaker python sdk<\/a> and was hoping to pass in some arguments to be used by my entrypoint, I'm not seeing how to do this.<\/p>\n\n<pre><code>from sagemaker.sklearn.estimator import SKLearn  # sagemaker python sdk\n\nentrypoint = 'entrypoint_script.py'\n\nsklearn = SKLearn(entry_point=entrypoint,  # &lt;-- need to pass args to this\n                  train_instance_type=instance_class,\n                  role=role,\n                  sagemaker_session=sm)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1556867880610,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1556899526172,
        "Question_score":5.0,
        "Question_view_count":2780.0,
        "Answer_body":"<p>The answer is no as there is no parameter on the Estimator base class, or the fit method, that accepts arguments to pass to the entrypoint.<\/p>\n\n<p>I resolved this by passing the parameter as part of the hyperparameter dictionary. This gets passed to the entrypoint as arguments.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1578346552883,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55964972",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1556949846448,
        "Question_original_content":"pass argument entrypoint estim python sdk hope pass argument entrypoint see sklearn estim import sklearn python sdk entrypoint entrypoint script sklearn sklearn entri point entrypoint need pass arg train instanc type instanc class role role session",
        "Question_preprocessed_content":"pass argument entrypoint estim python sdk hope pass argument entrypoint see",
        "Question_gpt_summary_original":"The user is trying to pass arguments to the entrypoint of a SageMaker estimator using the SageMaker python sdk, but is unable to find a way to do so. They have provided a code snippet where they are using the SKLearn estimator and need to pass arguments to the entry_point parameter.",
        "Question_gpt_summary":"user try pass argument entrypoint estim python sdk unabl wai provid code snippet sklearn estim need pass argument entri point paramet",
        "Answer_original_content":"answer paramet estim base class fit method accept argument pass entrypoint resolv pass paramet hyperparamet dictionari get pass entrypoint argument",
        "Answer_preprocessed_content":"answer paramet estim base class fit method accept argument pass entrypoint resolv pass paramet hyperparamet dictionari get pass entrypoint argument",
        "Answer_gpt_summary_original":"Solution: One solution mentioned in the discussion is to pass the parameter as part of the hyperparameter dictionary, which gets passed to the entrypoint as arguments.",
        "Answer_gpt_summary":"solut solut mention discuss pass paramet hyperparamet dictionari get pass entrypoint argument"
    },
    {
        "Question_title":"Pipeline can't find nodes in kedro",
        "Question_body":"<p>I was following <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/04_create_pipelines.html\" rel=\"nofollow noreferrer\">pipelines tutorial<\/a>, create all needed files, started the kedro with <code>kedro run --node=preprocessing_data<\/code> but got stuck with such error message:<\/p>\n\n<pre><code>ValueError: Pipeline does not contain nodes named ['preprocessing_data'].\n<\/code><\/pre>\n\n<p>If I run kedro without <code>node<\/code> parameter, I receive<\/p>\n\n<pre><code>kedro.context.context.KedroContextError: Pipeline contains no nodes\n<\/code><\/pre>\n\n<p>Contents of the files:<\/p>\n\n<pre><code>src\/project\/pipelines\/data_engineering\/nodes.py\ndef preprocess_data(data: SparkDataSet) -&gt; None:\n    print(data)\n    return\n<\/code><\/pre>\n\n<pre><code>src\/project\/pipelines\/data_engineering\/pipeline.py\ndef create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                func=preprocess_data,\n                inputs=\"data\",\n                outputs=\"preprocessed_data\",\n                name=\"preprocessing_data\",\n            ),\n        ]\n    )\n<\/code><\/pre>\n\n<pre><code>src\/project\/pipeline.py\ndef create_pipelines(**kwargs) -&gt; Dict[str, Pipeline]:\n    de_pipeline = de.create_pipeline()\n    return {\n        \"de\": de_pipeline,\n        \"__default__\": Pipeline([])\n    }\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1582395101757,
        "Question_favorite_count":null,
        "Question_last_edit_time":1583175186660,
        "Question_score":4.0,
        "Question_view_count":2155.0,
        "Answer_body":"<p>I think it looks like you need to have the pipeline in <code>__default__<\/code>.\ne.g.<\/p>\n\n<pre><code>def create_pipelines(**kwargs) -&gt; Dict[str, Pipeline]:\n    de_pipeline = de.create_pipeline()\n    return {\n        \"de\": data_engineering_pipeline,\n        \"__default__\": data_engineering_pipeline\n    }\n<\/code><\/pre>\n\n<p>Then <code>kedro run --node=preprocessing_data<\/code> works for me.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":8.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60355240",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1582427680590,
        "Question_original_content":"pipelin node follow pipelin tutori creat need file start run node preprocess data got stuck error messag valueerror pipelin contain node name preprocess data run node paramet receiv context context contexterror pipelin contain node content file src project pipelin data engin node def preprocess data data sparkdataset print data return src project pipelin data engin pipelin def creat pipelin kwarg return pipelin node func preprocess data input data output preprocess data preprocess data src project pipelin def creat pipelin kwarg dict str pipelin pipelin creat pipelin return pipelin default pipelin",
        "Question_preprocessed_content":"pipelin node follow pipelin tutori creat need file start got stuck error messag run paramet receiv content file",
        "Question_gpt_summary_original":"The user encountered an error while following the pipelines tutorial in Kedro. They created all the necessary files and started Kedro with the node parameter, but received an error message stating that the pipeline does not contain nodes named 'preprocessing_data'. Running Kedro without the node parameter resulted in an error message stating that the pipeline contains no nodes. The user provided the contents of the relevant files, including the function to preprocess data, the pipeline creation function, and the function to create pipelines.",
        "Question_gpt_summary":"user encount error follow pipelin tutori creat necessari file start node paramet receiv error messag state pipelin contain node name preprocess data run node paramet result error messag state pipelin contain node user provid content relev file includ function preprocess data pipelin creation function function creat pipelin",
        "Answer_original_content":"think look like need pipelin default def creat pipelin kwarg dict str pipelin pipelin creat pipelin return data engin pipelin default data engin pipelin run node preprocess data work",
        "Answer_preprocessed_content":"think look like need pipelin work",
        "Answer_gpt_summary_original":"Solution: The user can try adding the pipeline to the `__default__` dictionary in the `create_pipelines` function. This can be done by modifying the function to include the following line of code: `\"__default__\": data_engineering_pipeline`. After making this change, running Kedro with the node parameter should work.",
        "Answer_gpt_summary":"solut user try ad pipelin default dictionari creat pipelin function modifi function includ follow line code default data engin pipelin make chang run node paramet work"
    },
    {
        "Question_title":"Is there a way to pass arguments to our own docker container in sagemaker?",
        "Question_body":"<p>I am trying to train my model using Bring your own container technique in sagemaker. My model training runs correctly without any issues locally. But my docker image takes env-file as an input that could change at different runs. But in sagemaker when passing the ECR image, I don't know how to pass this env-file. So instead, inside the <code>train<\/code> script, which is called by the sagemaker, I added <code>export KEY=value<\/code> statements to create my variables. Even that did not expose my variables. Another way I tried it was by executing <code>RUN source file.env<\/code> while building my image. Even this approach did not work out as I got an error <code>\/bin\/sh: 1: source: not found<\/code>.<\/p>\n<p>I could try <code>ENV<\/code> while building my image and that would probably work but this approach won't be flexible as my variables could change at different runs. Is there any way to pass docker run arguments from a sagemaker estimator or notebook? I checked out the documentation but I couldn't find anything.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1634032422110,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":292.0,
        "Answer_body":"<p>I've been passing environment variables along with the Docker image URL when creating the Training job using the SageMaker Python SDK. Documentation of the <code>train<\/code> method states that:<\/p>\n<pre><code>environment (dict[str, str]) : Environment variables to be set for\n            use during training job (default: ``None``): \n<\/code><\/pre>\n<p>For reference, the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/5bc3ccf\/src\/sagemaker\/session.py#L569\" rel=\"nofollow noreferrer\">SDK source<\/a>.<\/p>\n<p>Because the SDK is a wrapper on top of <a href=\"https:\/\/pypi.org\/project\/boto3\/\" rel=\"nofollow noreferrer\">Boto3<\/a>, I'm pretty sure that the same can be implemented with Boto3 alone, and that there is an equivalent for every other <a href=\"https:\/\/aws.amazon.com\/developer\/tools\/#SDKs\" rel=\"nofollow noreferrer\">Amazon Services SDK<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69538469",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1662661497360,
        "Question_original_content":"wai pass argument docker contain try train model bring contain techniqu model train run correctli issu local docker imag take env file input chang differ run pass ecr imag know pass env file instead insid train script call ad export kei valu statement creat variabl expos variabl wai tri execut run sourc file env build imag approach work got error bin sourc try env build imag probabl work approach won flexibl variabl chang differ run wai pass docker run argument estim notebook check document couldn",
        "Question_preprocessed_content":"wai pass argument docker contain try train model bring contain techniqu model train run correctli issu local docker imag take input chang differ run pass ecr imag know pass instead insid script call ad statement creat variabl expos variabl wai tri execut build imag approach work got error try build imag probabl work approach won flexibl variabl chang differ run wai pass docker run argument estim notebook check document couldn",
        "Question_gpt_summary_original":"The user is facing challenges in passing arguments to their own docker container in Sagemaker while using the Bring your own container technique. The docker image takes an env-file as an input that could change at different runs, but the user does not know how to pass this env-file while passing the ECR image. The user tried adding export KEY=value statements to create variables, but it did not expose the variables. The user also tried executing RUN source file.env while building the image, but it resulted in an error. The user is looking for a way to pass docker run arguments from a Sagemaker estimator or notebook, but could not find any documentation on it.",
        "Question_gpt_summary":"user face challeng pass argument docker contain bring contain techniqu docker imag take env file input chang differ run user know pass env file pass ecr imag user tri ad export kei valu statement creat variabl expos variabl user tri execut run sourc file env build imag result error user look wai pass docker run argument estim notebook document",
        "Answer_original_content":"pass environ variabl docker imag url creat train job python sdk document train method state environ dict str str environ variabl set us train job default refer sdk sourc sdk wrapper boto pretti sure implement boto equival amazon servic sdk",
        "Answer_preprocessed_content":"pass environ variabl docker imag url creat train job python sdk document method state refer sdk sourc sdk wrapper boto pretti sure implement boto equival amazon servic sdk",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to pass environment variables along with the Docker image URL when creating the training job using the SageMaker Python SDK. The SDK provides an option to set environment variables for use during the training job. It is also suggested that the same can be implemented with Boto3 alone, and that there is an equivalent for every other Amazon Services SDK.",
        "Answer_gpt_summary":"solut possibl solut mention discuss pass environ variabl docker imag url creat train job python sdk sdk provid option set environ variabl us train job suggest implement boto equival amazon servic sdk"
    },
    {
        "Question_title":"ERROR conda.core.link:_execute(502): An error occurred while installing package 'conda-forge::astor-0.7.1-py_0'",
        "Question_body":"<p>I am trying to follow a Python tutorial and I have been able to execute almost everything, until the point of Deploying an endpoint to Azure with python.<\/p>\n\n<p>In order to give some context I have uploaded the scripts to my git account:\n<a href=\"https:\/\/github.com\/levalencia\/MLTutorial\" rel=\"nofollow noreferrer\">https:\/\/github.com\/levalencia\/MLTutorial<\/a><\/p>\n\n<p>File 1 and 2 Work perfectly fine<\/p>\n\n<p>However the following section in File 3 fails:<\/p>\n\n<pre><code>%%time\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\n\ninference_config = InferenceConfig(runtime= \"python\", \n                                   entry_script=\"score.py\",\n                                   conda_file=\"myenv.yml\")\n\nservice = Model.deploy(workspace=ws, \n                       name='keras-mnist-svc2', \n                       models=[amlModel], \n                       inference_config=inference_config, \n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n\n<p>with below error:<\/p>\n\n<pre><code>ERROR - Service deployment polling reached non-successful terminal state, current service state: Transitioning\nOperation ID: 8353cad2-4218-450a-a03b-df418725acb1\nMore information can be found here: https:\/\/machinelearnin1143382465.blob.core.windows.net\/azureml\/ImageLogs\/8353cad2-4218-450a-a03b-df418725acb1\/build.log?sv=2018-03-28&amp;sr=b&amp;sig=UKzefxIrm3l7OsXxj%2FT4RsvUfAuhuaBwaz2P4mJu7vY%3D&amp;st=2020-03-11T12%3A23%3A33Z&amp;se=2020-03-11T20%3A28%3A33Z&amp;sp=r\nError:\n{\n  \"code\": \"EnvironmentBuildFailed\",\n  \"statusCode\": 400,\n  \"message\": \"Failed Building the Environment.\"\n}\n\nERROR - Service deployment polling reached non-successful terminal state, current service state: Transitioning\nOperation ID: 8353cad2-4218-450a-a03b-df418725acb1\nMore information can be found here: https:\/\/machinelearnin1143382465.blob.core.windows.net\/azureml\/ImageLogs\/8353cad2-4218-450a-a03b-df418725acb1\/build.log?sv=2018-03-28&amp;sr=b&amp;sig=UKzefxIrm3l7OsXxj%2FT4RsvUfAuhuaBwaz2P4mJu7vY%3D&amp;st=2020-03-11T12%3A23%3A33Z&amp;se=2020-03-11T20%3A28%3A33Z&amp;sp=r\nError:\n{\n  \"code\": \"EnvironmentBuildFailed\",\n  \"statusCode\": 400,\n  \"message\": \"Failed Building the Environment.\"\n}\n<\/code><\/pre>\n\n<p>When I download the logs, I got this:<\/p>\n\n<pre><code>wheel-0.34.2         | 24 KB     |            |   0% [0m[91m\nwheel-0.34.2         | 24 KB     | ########## | 100% [0m\nDownloading and Extracting Packages\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... failed\n[91m\nERROR conda.core.link:_execute(502): An error occurred while installing package 'conda-forge::astor-0.7.1-py_0'.\nFileNotFoundError(2, \"No such file or directory: '\/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56\/bin\/python3.6'\")\nAttempting to roll back.\n\n[0mRolling back transaction: ...working... done\n[91m\nFileNotFoundError(2, \"No such file or directory: '\/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56\/bin\/python3.6'\")\n\n\n[0mThe command '\/bin\/sh -c ldconfig \/usr\/local\/cuda\/lib64\/stubs &amp;&amp; conda env create -p \/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56 -f azureml-environment-setup\/mutated_conda_dependencies.yml &amp;&amp; rm -rf \"$HOME\/.cache\/pip\" &amp;&amp; conda clean -aqy &amp;&amp; CONDA_ROOT_DIR=$(conda info --root) &amp;&amp; rm -rf \"$CONDA_ROOT_DIR\/pkgs\" &amp;&amp; find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + &amp;&amp; ldconfig' returned a non-zero code: 1\n2020\/03\/11 12:28:11 Container failed during run: acb_step_0. No retries remaining.\nfailed to run step ID: acb_step_0: exit status 1\n\nRun ID: cb3 failed after 2m21s. Error: failed during run, err: exit status 1\n<\/code><\/pre>\n\n<p>Update 1:<\/p>\n\n<p>I tried to run:\nconda list    --name base  conda<\/p>\n\n<p>inside the notebook and I got this:<\/p>\n\n<pre><code> # packages in environment at \/anaconda:\n    #\n    # Name                    Version                   Build  Channel\n    _anaconda_depends         2019.03                  py37_0  \n    anaconda                  custom                   py37_1  \n    anaconda-client           1.7.2                    py37_0  \n    anaconda-navigator        1.9.6                    py37_0  \n    anaconda-project          0.8.4                      py_0  \n    conda                     4.8.2                    py37_0  \n    conda-build               3.17.6                   py37_0  \n    conda-env                 2.6.0                         1  \n    conda-package-handling    1.6.0            py37h7b6447c_0  \n    conda-verify              3.1.1                    py37_0  \n\n    Note: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n\n<p>However in the deployment log I got this:<\/p>\n\n<pre><code>Solving environment: ...working... \ndone\n[91m\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 4.5.11\n  latest version: 4.8.2\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1583931341127,
        "Question_favorite_count":null,
        "Question_last_edit_time":1584001089836,
        "Question_score":2.0,
        "Question_view_count":5656.0,
        "Answer_body":"<p>Unfortunately there seems to an issue with this version of Conda (4.5.11). To complete this task in the tutorial, you can just update the dependency for Tensorflow and Keras to be from <code>pip<\/code> and not <code>conda<\/code>. There are reasons why this is less than ideal for a production environment. The Azure ML <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py\" rel=\"nofollow noreferrer\">documentation states<\/a>:<\/p>\n\n<blockquote>\n  <p>\"If your dependency is available through both Conda and pip (from\n  PyPi), use the Conda version, as Conda packages typically come with\n  pre-built binaries that make installation more reliable.\"<\/p>\n<\/blockquote>\n\n<p>In this case though, if you update the following code block:<\/p>\n\n<pre><code>from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_conda_package(\"tensorflow\")\nmyenv.add_conda_package(\"keras\")\n\nwith open(\"myenv.yml\",\"w\") as f:\n    f.write(myenv.serialize_to_string())\n\n# Review environment file\nwith open(\"myenv.yml\",\"r\") as f:\n    print(f.read())\n<\/code><\/pre>\n\n<p>To be the following:<\/p>\n\n<pre><code>from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_pip_package(\"tensorflow==2.0.0\")\nmyenv.add_pip_package(\"azureml-defaults\")\nmyenv.add_pip_package(\"keras\")\n\nwith open(\"myenv.yml\", \"w\") as f:\n    f.write(myenv.serialize_to_string())\n\nwith open(\"myenv.yml\", \"r\") as f:\n    print(f.read())\n<\/code><\/pre>\n\n<p>The tutorial should be able to be completed.  Let me know if any of this does not work for you once this update has been made.<\/p>\n\n<p>I have also reported this issue to Microsoft (in regards to the Conda version).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60636558",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1584015256823,
        "Question_original_content":"error conda core link execut error occur instal packag conda forg astor try follow python tutori abl execut point deploi endpoint azur python order context upload script git account http github com levalencia mltutori file work perfectli fine follow section file fail time core webservic import webservic core model import inferenceconfig infer config inferenceconfig runtim python entri script score conda file myenv yml servic model deploi workspac kera mnist svc model amlmodel infer config infer config deploy config aciconfig servic wait deploy output true error error servic deploy poll reach non success termin state current servic state transit oper cad dfacb inform http machinelearnin blob core window net imagelog cad dfacb build log sig ukzefxirmlosxxj ftrsvufauhuabwazpmjuvi error code environmentbuildfail statuscod messag fail build environ error servic deploy poll reach non success termin state current servic state transit oper cad dfacb inform http machinelearnin blob core window net imagelog cad dfacb build log sig ukzefxirmlosxxj ftrsvufauhuabwazpmjuvi error code environmentbuildfail statuscod messag fail build environ download log got wheel wheel download extract packag prepar transact work verifi transact work execut transact work fail error conda core link execut error occur instal packag conda forg astor filenotfounderror file directori env abdeaccdbabbabb bin python attempt roll mroll transact work filenotfounderror file directori env abdeaccdbabbabb bin python mthe command bin ldconfig usr local cuda lib stub conda env creat env abdeaccdbabbabb environ setup mutat conda depend yml home cach pip conda clean aqi conda root dir conda info root conda root dir pkg conda root dir type pycach exec ldconfig return non zero code contain fail run acb step retri remain fail run step acb step exit statu run fail error fail run err exit statu updat tri run conda list base conda insid notebook got packag environ anaconda version build channel anaconda depend anaconda custom anaconda client anaconda navig anaconda project conda conda build conda env conda packag handl pyhbc conda verifi note need restart kernel us updat packag deploy log got solv environ work warn newer version conda exist current version latest version updat conda run conda updat base default conda",
        "Question_preprocessed_content":"error error occur instal packag try follow python tutori abl execut point deploi endpoint azur python order context upload script git account file work perfectli fine follow section file fail error download log got updat tri run conda list base conda insid notebook got deploy log got",
        "Question_gpt_summary_original":"The user encountered an error while trying to deploy an endpoint to Azure with Python. The error occurred during the installation of the package 'conda-forge::astor-0.7.1-py_0'. The deployment log showed that a newer version of conda exists and needs to be updated. The user also tried to run 'conda list --name base conda' inside the notebook, which showed a different version of conda than what was shown in the deployment log.",
        "Question_gpt_summary":"user encount error try deploi endpoint azur python error occur instal packag conda forg astor deploy log show newer version conda exist need updat user tri run conda list base conda insid notebook show differ version conda shown deploy log",
        "Answer_original_content":"unfortun issu version conda complet task tutori updat depend tensorflow kera pip conda reason ideal product environ document state depend avail conda pip pypi us conda version conda packag typic come pre built binari instal reliabl case updat follow code block core conda depend import condadepend myenv condadepend myenv add conda packag tensorflow myenv add conda packag kera open myenv yml write myenv serial string review environ file open myenv yml print read follow core conda depend import condadepend myenv condadepend myenv add pip packag tensorflow myenv add pip packag default myenv add pip packag kera open myenv yml write myenv serial string open myenv yml print read tutori abl complet let know work updat report issu microsoft regard conda version",
        "Answer_preprocessed_content":"unfortun issu version conda complet task tutori updat depend tensorflow kera reason ideal product environ document state depend avail conda pip us conda version conda packag typic come binari instal case updat follow code block follow tutori abl complet let know work updat report issu microsoft",
        "Answer_gpt_summary_original":"Solution: The user can update the dependency for Tensorflow and Keras to be from pip and not conda. The user needs to update the code block to add pip packages instead of conda packages. The updated code block is provided in the discussion. The tutorial should be able to be completed with this update. The issue with the Conda version has been reported to Microsoft.",
        "Answer_gpt_summary":"solut user updat depend tensorflow kera pip conda user need updat code block add pip packag instead conda packag updat code block provid discuss tutori abl complet updat issu conda version report microsoft"
    },
    {
        "Question_title":"Azure Machine Learning - Access uri_folder dataset (ML v2) from notebook (not job)",
        "Question_body":"Hi all,\n\nI've registered in Azure Machine Learning a Data Lake Gen2 datastore that point to a container with a hierarchy of folders that contain avro files and on top of it I registered a folder_uri dataset (ML v2).\n\nNow I want to access to these folders from a notebook, convert them in a pandas dataframe in order to do some data exploration.\n\nI search on the documentation, and I only found examples that run job and using this type of dataset as input, but I need to be able to explore it using notebook.\n\nIs it possible? How can I do it?\n\nThanks",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1666794361980,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@GCocci Thanks for the question. Currently it's not supported to access the avro files. Here is the document for accessing the datastore using folder_uri dataset.\nhttps:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-to-v2-resource-datastore\n\nMapping Data Flow supports AVRO as a source type https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/data-flow-source#supported-sources",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1063867\/azure-machine-learning-access-uri-folder-dataset-m.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-27T12:45:05.14Z",
                "Answer_score":0,
                "Answer_body":"@GCocci Thanks for the question. Currently it's not supported to access the avro files. Here is the document for accessing the datastore using folder_uri dataset.\nhttps:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-to-v2-resource-datastore\n\nMapping Data Flow supports AVRO as a source type https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/data-flow-source#supported-sources",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":18.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1666874705140,
        "Question_original_content":"access uri folder dataset notebook job regist data lake gen datastor point contain hierarchi folder contain avro file regist folder uri dataset want access folder notebook convert panda datafram order data explor search document exampl run job type dataset input need abl explor notebook possibl thank",
        "Question_preprocessed_content":"access dataset notebook regist data lake gen datastor point contain hierarchi folder contain avro file regist dataset want access folder notebook convert panda datafram order data explor search document exampl run job type dataset input need abl explor notebook possibl thank",
        "Question_gpt_summary_original":"The user is facing a challenge in accessing a folder_uri dataset (ML v2) from a notebook in Azure Machine Learning. They have registered a Data Lake Gen2 datastore that points to a container with a hierarchy of folders containing avro files. The user wants to convert the folders into a pandas dataframe for data exploration but has only found examples that run jobs using this type of dataset as input. The user is seeking guidance on how to explore the dataset using a notebook.",
        "Question_gpt_summary":"user face challeng access folder uri dataset notebook regist data lake gen datastor point contain hierarchi folder contain avro file user want convert folder panda datafram data explor exampl run job type dataset input user seek guidanc explor dataset notebook",
        "Answer_original_content":"gcocci thank question current support access avro file document access datastor folder uri dataset http learn microsoft com azur machin learn migrat resourc datastor map data flow support avro sourc type http learn microsoft com azur data factori data flow sourc support sourc",
        "Answer_preprocessed_content":"thank question current support access avro file document access datastor dataset map data flow support avro sourc type",
        "Answer_gpt_summary_original":"Solution: One solution mentioned in the discussion is to use Mapping Data Flow, which supports AVRO as a source type. Another solution is to access the Data Lake Gen2 datastore using the folder_uri dataset, as described in the provided documentation. However, it is currently not supported to access the AVRO files directly.",
        "Answer_gpt_summary":"solut solut mention discuss us map data flow support avro sourc type solut access data lake gen datastor folder uri dataset describ provid document current support access avro file directli"
    },
    {
        "Question_title":"DVC for analytics pipeline with runtime parameters and variable dependencies",
        "Question_body":"<p>Hi all! I\u2019m quite new to DVC and still in the process of fully understanding its capabilities and benefits. Similar to the discussion <a href=\"https:\/\/discuss.dvc.org\/t\/need-to-build-non-ml-data-pipeline-is-dvc-good-fit\/849\" class=\"inline-onebox\">Need to build non-ML data pipeline, is DVC good fit?<\/a>, I am still struggling with finding out whether DVC is a good fit for our data processing workflow or not.<\/p>\n<p>Obviously, DVC suits perfectly for straightforward ML projects, where inputs are data and a set of hyper-parameters and the output is a trained model that can then be deployed somewhere. However, things seem to be a bit more complex for different types of use cases.<\/p>\n<h2>\n<a name=\"use-case-example-pipeline-1\" class=\"anchor\" href=\"#use-case-example-pipeline-1\"><\/a>Use Case + Example Pipeline<\/h2>\n<p>Consider this example.<\/p>\n<p>We\u2019re building a data processing pipeline to analyze geo- and time-referenced images and also want to deploy it to a production server, where it then gets triggered on-demand or on a regular basis.<\/p>\n<p>On a high level, input to that process are images plus some additional meta data. Output is the processing result, e.g. in form of a small snippet of JSON data.<\/p>\n<p>The pipeline would approximately consist of these steps:<\/p>\n<ol>\n<li>\n<strong>Download data:<\/strong> Given two parameters (<code>location<\/code> (an <a href=\"https:\/\/en.wikipedia.org\/wiki\/Military_Grid_Reference_System\" rel=\"noopener nofollow ugc\">MGRS<\/a> code) and <code>dates<\/code> (as comma-separated list)), downloads all images for that location and day from a web service and saves them them to a network share in a file structure like this:\n<ul>\n<li>\n<code>image_data<\/code>\n<ul>\n<li>\n<code>4QFJ<\/code> (the location code)\n<ul>\n<li>\n<code>2022<\/code>\n<ul>\n<li>\n<code>02<\/code>\n<ul>\n<li>\n<code>16<\/code>\n<ul>\n<li><code>image.jpg<\/code><\/li>\n<li>\n<code>...<\/code> (some auxiliary data, skipped for brevity)<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<li>\n<strong>Filter data:<\/strong> Given a parameter <code>filtering_method<\/code>, it filters such of these images, that satisfy certain criteria. The result could, for instance, be a JSON list of image paths. The method parameter would probably also go inside <code>params.yaml<\/code>, I presume.<\/li>\n<li>\n<strong>Process data:<\/strong> Takes the set of valid images from the previous step, loads them from the network drive (see first step), analyzes and processes them. Depending on the <code>filtering_method<\/code> this list is longer or shorter. In reality, this step is actually multiple separate steps, each of which has in- and outputs itself, but for simplicity that part is skipped. Output is a JSON file that is then saved to the network drive again, e.g. at <code>results\/4QFJ_2022-02-16.json<\/code>.<\/li>\n<\/ol>\n<h2>\n<a name=\"issues-2\" class=\"anchor\" href=\"#issues-2\"><\/a>Issues<\/h2>\n<p>As you can see, this is quite different from a classical ML <em>preprocess - train - evaluate<\/em> flow. Specifically, what makes me doubt if DVC is easily applicable here is:<\/p>\n<ol>\n<li>The pipeline depends on <strong>\u201cruntime\u201d parameters<\/strong> (<code>location<\/code>, <code>date<\/code>) as input, that likely change for each run and are passed by the user (or an automated script, rather). How to model these? Update <code>params.yaml<\/code> each time?\n<ul>\n<li>Additional question for general understanding: day <code>params.yaml<\/code> contained only one key, say <code>foo<\/code>. If I run the pipeline with <code>foo: 4<\/code>, then with <code>foo: 5<\/code> and once again with <code>foo: 4<\/code>, would the last run be skipped, because it had earlier run with the same set of dependencies already?<\/li>\n<\/ul>\n<\/li>\n<li>The files to be used as <strong>dependencies are not a static set<\/strong>, but rather vary, depending on input parameters. It\u2019s not always <code>data\/train.csv<\/code> or so, but instead the set of dependencies (the filenames) to stage 2 vary with varying input parameters to step 1. How to help with this? Maybe using template variables in <code>dvc.yaml<\/code>?<\/li>\n<li>Preferably, we\u2019d also like to send a notification for each pipeline run by calling an online REST API. In other words, some of the pipeline steps have <strong>side effects<\/strong>, that can\u2019t really be tracked with files. This is probably also a point where DVC falls short for us, because, if I ran the pipeline twice with same parameters and data, only one notification would be sent. However, this point is not too crucial.<\/li>\n<\/ol>\n<p>What do you think, is DVC a good fit for us and if yes, how do I work around these \u201cissues\u201d?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1645031335208,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":396.0,
        "Answer_body":"<blockquote>\n<p>The pipeline depends on <strong>\u201cruntime\u201d parameters<\/strong> ( <code>location<\/code> , <code>date<\/code> ) as input, that likely change for each run and are passed by the user (or an automated script, rather). How to model these? Update <code>params.yaml<\/code> each time?<\/p>\n<\/blockquote>\n<p>Updating <code>params.yaml<\/code> would be the way to do this in DVC pipelines. If you are already generating the location and date via an automated script, what about just having the script write the appropriate values into <code>params.yaml<\/code>? So the automated script would be separate from your pipeline, and it would potentially do update <code>params.yaml<\/code> and then trigger <code>dvc repro<\/code>?<\/p>\n<blockquote>\n<ul>\n<li>Additional question for general understanding: day <code>params.yaml<\/code> contained only one key, say <code>foo<\/code> . If I run the pipeline with <code>foo: 4<\/code> , then with <code>foo: 5<\/code> and once again with <code>foo: 4<\/code> , would the last run be skipped, because it had earlier run with the same set of dependencies already?<\/li>\n<\/ul>\n<\/blockquote>\n<p>Yes, in this case DVC would checkout the outputs generated by the previous <code>foo: 4<\/code> run instead of re-executing the stage. If your stage is non-deterministic (and you expect different outputs each time even if the parameter value is unchanged) you can force DVC to always execute the stage by setting the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/stage\/add#--always-changed\">always_changed<\/a> flag.<\/p>\n<blockquote>\n<p>The files to be used as <strong>dependencies are not a static set<\/strong> , but rather vary, depending on input parameters. It\u2019s not always <code>data\/train.csv<\/code> or so, but instead the set of dependencies (the filenames) to stage 2 vary with varying input parameters to step 1. How to help with this? Maybe using template variables in <code>dvc.yaml<\/code> ?<\/p>\n<\/blockquote>\n<p>Are the files consistent based on the input parameters, meaning for a given <code>location<\/code>, <code>date<\/code> combination, the set of files will always be the same (but the files\/filenames will be different for other <code>location<\/code>, <code>date<\/code> combinations)?<\/p>\n<p>In this case, you can just tell DVC to track a single output directory path, and have your \u201cdownload data\u201d stage download all of the files into that directory. You don\u2019t need to tell DVC each individual path to track. You would then use that directory path as the dependency for the next (filtering) stage in your pipeline.<\/p>\n<blockquote>\n<p>Preferably, we\u2019d also like to send a notification for each pipeline run by calling an online REST API. In other words, some of the pipeline steps have <strong>side effects<\/strong> , that can\u2019t really be tracked with files. This is probably also a point where DVC falls short for us, because, if I ran the pipeline twice with same parameters and data, only one notification would be sent. However, this point is not too crucial.<\/p>\n<\/blockquote>\n<p>This could also potentially be accomplished using the <code>always_changed<\/code> flag to force DVC to run stages with these types of side effects even when dependencies haven\u2019t changed<\/p>. <p>Thanks a lot for your comprehensive answer and for helping me understand DVC better!<\/p>\n<blockquote>\n<p>Are the files consistent based on the input parameters, meaning for a given <code>location<\/code> , <code>date<\/code> combination, the set of files will always be the same<\/p>\n<\/blockquote>\n<p>Yes. Their paths will include both parameters, as well as another component that is determined by the script at runtime. Although it\u2019s not known upfront, it\u2019s always the same for the same input parameters.<\/p>\n<blockquote>\n<p>You would then use that directory path as the dependency for the next (filtering) stage in your pipeline.<\/p>\n<\/blockquote>\n<p>But then I couldn\u2019t really benefit from the dependency tracking \/ hashing feature in its entirety anymore, could I?<\/p>\n<p>Let\u2019s say I specify <code>~\/data<\/code> as a dependency. Let the pipeline then run for some date <code>d1<\/code>, resulting, e.g. in <code>~\/data\/d1\/*<\/code> being created. Let it then run for <code>d2<\/code>, resulting in <code>~\/data\/d2\/*<\/code> and once again for <code>d1<\/code>. Since contents of <code>~\/data<\/code> have changed, the third run would still be performed, although the data relevant for <code>d1<\/code> hasn\u2019t changed, right?<\/p>\n<p>In addition, I have one more question. I\u2019d like to know whether it\u2019s a good practice to use DVC with a <strong>shared file system<\/strong> (e.g. NFS \/ CIFS mount, WebDAV, \u2026)? Can I run my DVC pipeline on two different hosts, that both access the same file system, and benefit from DVC\u2019s benefits? Should <code>dvc.lock<\/code> also reside on the shared drive then?<\/p>. <pre><code class=\"lang-auto\">But then I couldn\u2019t really benefit from the dependency tracking \/ hashing feature in its entirety anymore, could I?\n\nLet\u2019s say I specify ~\/data as a dependency. Let the pipeline then run for some date d1, resulting, e.g. in ~\/data\/d1\/* being created. Let it then run for d2, resulting in ~\/data\/d2\/* and once again for d1. Since contents of ~\/data have changed, the third run would still be performed, although the data relevant for d1 hasn\u2019t changed, right?\n<\/code><\/pre>\n<p>My suggestion was to just use <code>data\/<\/code> to store a single dependency\u2019s dataset. So rather than have <code>data\/d1\/*<\/code>, <code>data\/d2\/*<\/code>, <code>data\/d3\/*<\/code> your fetch stage would just clear the entire contents of <code>data\/<\/code> and then download the files directly into <code>data\/*<\/code> (rather than appending each fetch dataset on top of the previous datasets). After the first run, <code>data\/<\/code> would only contain the <code>d1<\/code> files, after the second run <code>data\/<\/code> would only contain the <code>d2<\/code> files, and so on.<\/p>\n<p>Doing it this way means that the dependency tracking\/hashing in DVC will still work, since DVC will be aware of the multiple different possible \u201cdep states\u201d that it has seen for <code>data\/<\/code> (since data will have a different possible hash for each of <code>d1<\/code>, <code>d2<\/code>, <code>d3<\/code>, \u2026). So the next time you fetch <code>d1<\/code>, DVC would see that it has executed a run for this <code>data\/<\/code> hash (via the DVC <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#run-cache\">run-cache<\/a>) and then the subsequent (non-fetch) stages in your pipeline would not be re-run.<\/p>\n<blockquote>\n<p>I\u2019d like to know whether it\u2019s a good practice to use DVC with a <strong>shared file system<\/strong> (e.g. NFS \/ CIFS mount, WebDAV, \u2026)?<\/p>\n<\/blockquote>\n<p>The recommended way of doing this would be to keep your DVC cache directory on the network mount, and configure DVC to use symlinks to that cache directory (see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/share-a-dvc-cache#how-to-share-a-dvc-cache\">https:\/\/dvc.org\/doc\/user-guide\/how-to\/share-a-dvc-cache#how-to-share-a-dvc-cache<\/a>). This way, all of your DVC-tracked data would remain on the network mount, but your main repo workspace would still be on a local drive. This also allows you to re-use one shared cache directory across multiple DVC repos if that is something you are looking for. (Note that the shared cache scenario works for shared local directories as well, using network storage isn\u2019t required)<\/p>\n<p>It is also possible to use the network mount as your main repo workspace directory, but this comes with some caveats. In general, DVC is not optimized for use on network filesystems, and will likely come with a significant performance hit (compared to keeping your repo workspace on local storage).<\/p>\n<p>There are also a few <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#config\">DVC config<\/a> options which may need to be set depending on your use case and specific network filesystem in this case:<\/p>\n<ul>\n<li>\n<code>core.hardlink_lock<\/code> - specifically for NFS<\/li>\n<li>\n<code>state.dir<\/code>, <code>index.dir<\/code> - DVC uses sqlite for caching certain internal lookups, but sqlite does not work properly on certain network filesystems, so you will likely need to set these options to point to a local filesystem directory (like <code>\/tmp<\/code>)<\/li>\n<\/ul>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-for-analytics-pipeline-with-runtime-parameters-and-variable-dependencies\/1065",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-17T05:19:15.265Z",
                "Answer_body":"<blockquote>\n<p>The pipeline depends on <strong>\u201cruntime\u201d parameters<\/strong> ( <code>location<\/code> , <code>date<\/code> ) as input, that likely change for each run and are passed by the user (or an automated script, rather). How to model these? Update <code>params.yaml<\/code> each time?<\/p>\n<\/blockquote>\n<p>Updating <code>params.yaml<\/code> would be the way to do this in DVC pipelines. If you are already generating the location and date via an automated script, what about just having the script write the appropriate values into <code>params.yaml<\/code>? So the automated script would be separate from your pipeline, and it would potentially do update <code>params.yaml<\/code> and then trigger <code>dvc repro<\/code>?<\/p>\n<blockquote>\n<ul>\n<li>Additional question for general understanding: day <code>params.yaml<\/code> contained only one key, say <code>foo<\/code> . If I run the pipeline with <code>foo: 4<\/code> , then with <code>foo: 5<\/code> and once again with <code>foo: 4<\/code> , would the last run be skipped, because it had earlier run with the same set of dependencies already?<\/li>\n<\/ul>\n<\/blockquote>\n<p>Yes, in this case DVC would checkout the outputs generated by the previous <code>foo: 4<\/code> run instead of re-executing the stage. If your stage is non-deterministic (and you expect different outputs each time even if the parameter value is unchanged) you can force DVC to always execute the stage by setting the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/stage\/add#--always-changed\">always_changed<\/a> flag.<\/p>\n<blockquote>\n<p>The files to be used as <strong>dependencies are not a static set<\/strong> , but rather vary, depending on input parameters. It\u2019s not always <code>data\/train.csv<\/code> or so, but instead the set of dependencies (the filenames) to stage 2 vary with varying input parameters to step 1. How to help with this? Maybe using template variables in <code>dvc.yaml<\/code> ?<\/p>\n<\/blockquote>\n<p>Are the files consistent based on the input parameters, meaning for a given <code>location<\/code>, <code>date<\/code> combination, the set of files will always be the same (but the files\/filenames will be different for other <code>location<\/code>, <code>date<\/code> combinations)?<\/p>\n<p>In this case, you can just tell DVC to track a single output directory path, and have your \u201cdownload data\u201d stage download all of the files into that directory. You don\u2019t need to tell DVC each individual path to track. You would then use that directory path as the dependency for the next (filtering) stage in your pipeline.<\/p>\n<blockquote>\n<p>Preferably, we\u2019d also like to send a notification for each pipeline run by calling an online REST API. In other words, some of the pipeline steps have <strong>side effects<\/strong> , that can\u2019t really be tracked with files. This is probably also a point where DVC falls short for us, because, if I ran the pipeline twice with same parameters and data, only one notification would be sent. However, this point is not too crucial.<\/p>\n<\/blockquote>\n<p>This could also potentially be accomplished using the <code>always_changed<\/code> flag to force DVC to run stages with these types of side effects even when dependencies haven\u2019t changed<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-17T13:44:24.125Z",
                "Answer_body":"<p>Thanks a lot for your comprehensive answer and for helping me understand DVC better!<\/p>\n<blockquote>\n<p>Are the files consistent based on the input parameters, meaning for a given <code>location<\/code> , <code>date<\/code> combination, the set of files will always be the same<\/p>\n<\/blockquote>\n<p>Yes. Their paths will include both parameters, as well as another component that is determined by the script at runtime. Although it\u2019s not known upfront, it\u2019s always the same for the same input parameters.<\/p>\n<blockquote>\n<p>You would then use that directory path as the dependency for the next (filtering) stage in your pipeline.<\/p>\n<\/blockquote>\n<p>But then I couldn\u2019t really benefit from the dependency tracking \/ hashing feature in its entirety anymore, could I?<\/p>\n<p>Let\u2019s say I specify <code>~\/data<\/code> as a dependency. Let the pipeline then run for some date <code>d1<\/code>, resulting, e.g. in <code>~\/data\/d1\/*<\/code> being created. Let it then run for <code>d2<\/code>, resulting in <code>~\/data\/d2\/*<\/code> and once again for <code>d1<\/code>. Since contents of <code>~\/data<\/code> have changed, the third run would still be performed, although the data relevant for <code>d1<\/code> hasn\u2019t changed, right?<\/p>\n<p>In addition, I have one more question. I\u2019d like to know whether it\u2019s a good practice to use DVC with a <strong>shared file system<\/strong> (e.g. NFS \/ CIFS mount, WebDAV, \u2026)? Can I run my DVC pipeline on two different hosts, that both access the same file system, and benefit from DVC\u2019s benefits? Should <code>dvc.lock<\/code> also reside on the shared drive then?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-18T01:23:53.339Z",
                "Answer_body":"<pre><code class=\"lang-auto\">But then I couldn\u2019t really benefit from the dependency tracking \/ hashing feature in its entirety anymore, could I?\n\nLet\u2019s say I specify ~\/data as a dependency. Let the pipeline then run for some date d1, resulting, e.g. in ~\/data\/d1\/* being created. Let it then run for d2, resulting in ~\/data\/d2\/* and once again for d1. Since contents of ~\/data have changed, the third run would still be performed, although the data relevant for d1 hasn\u2019t changed, right?\n<\/code><\/pre>\n<p>My suggestion was to just use <code>data\/<\/code> to store a single dependency\u2019s dataset. So rather than have <code>data\/d1\/*<\/code>, <code>data\/d2\/*<\/code>, <code>data\/d3\/*<\/code> your fetch stage would just clear the entire contents of <code>data\/<\/code> and then download the files directly into <code>data\/*<\/code> (rather than appending each fetch dataset on top of the previous datasets). After the first run, <code>data\/<\/code> would only contain the <code>d1<\/code> files, after the second run <code>data\/<\/code> would only contain the <code>d2<\/code> files, and so on.<\/p>\n<p>Doing it this way means that the dependency tracking\/hashing in DVC will still work, since DVC will be aware of the multiple different possible \u201cdep states\u201d that it has seen for <code>data\/<\/code> (since data will have a different possible hash for each of <code>d1<\/code>, <code>d2<\/code>, <code>d3<\/code>, \u2026). So the next time you fetch <code>d1<\/code>, DVC would see that it has executed a run for this <code>data\/<\/code> hash (via the DVC <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#run-cache\">run-cache<\/a>) and then the subsequent (non-fetch) stages in your pipeline would not be re-run.<\/p>\n<blockquote>\n<p>I\u2019d like to know whether it\u2019s a good practice to use DVC with a <strong>shared file system<\/strong> (e.g. NFS \/ CIFS mount, WebDAV, \u2026)?<\/p>\n<\/blockquote>\n<p>The recommended way of doing this would be to keep your DVC cache directory on the network mount, and configure DVC to use symlinks to that cache directory (see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/share-a-dvc-cache#how-to-share-a-dvc-cache\">https:\/\/dvc.org\/doc\/user-guide\/how-to\/share-a-dvc-cache#how-to-share-a-dvc-cache<\/a>). This way, all of your DVC-tracked data would remain on the network mount, but your main repo workspace would still be on a local drive. This also allows you to re-use one shared cache directory across multiple DVC repos if that is something you are looking for. (Note that the shared cache scenario works for shared local directories as well, using network storage isn\u2019t required)<\/p>\n<p>It is also possible to use the network mount as your main repo workspace directory, but this comes with some caveats. In general, DVC is not optimized for use on network filesystems, and will likely come with a significant performance hit (compared to keeping your repo workspace on local storage).<\/p>\n<p>There are also a few <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#config\">DVC config<\/a> options which may need to be set depending on your use case and specific network filesystem in this case:<\/p>\n<ul>\n<li>\n<code>core.hardlink_lock<\/code> - specifically for NFS<\/li>\n<li>\n<code>state.dir<\/code>, <code>index.dir<\/code> - DVC uses sqlite for caching certain internal lookups, but sqlite does not work properly on certain network filesystems, so you will likely need to set these options to point to a local filesystem directory (like <code>\/tmp<\/code>)<\/li>\n<\/ul>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"analyt pipelin runtim paramet variabl depend new process fulli understand capabl benefit similar discuss need build non data pipelin good fit struggl find good fit data process workflow obvious suit perfectli straightforward project input data set hyper paramet output train model deploi thing bit complex differ type us case us case exampl pipelin consid exampl build data process pipelin analyz geo time referenc imag want deploi product server get trigger demand regular basi high level input process imag plu addit meta data output process result form small snippet json data pipelin approxim consist step download data given paramet locat mgr code date comma separ list download imag locat dai web servic save network share file structur like imag data qfj locat code imag jpg auxiliari data skip breviti filter data given paramet filter method filter imag satisfi certain criteria result instanc json list imag path method paramet probabl insid param yaml presum process data take set valid imag previou step load network drive step analyz process depend filter method list longer shorter realiti step actual multipl separ step output simplic skip output json file save network drive result qfj json issu differ classic preprocess train evalu flow specif make doubt easili applic pipelin depend runtim paramet locat date input like chang run pass user autom script model updat param yaml time addit question gener understand dai param yaml contain kei foo run pipelin foo foo foo run skip earlier run set depend file depend static set vari depend input paramet data train csv instead set depend filenam stage vari vari input paramet step help mayb templat variabl yaml prefer wed like send notif pipelin run call onlin rest api word pipelin step effect track file probabl point fall short ran pipelin twice paramet data notif sent point crucial think good fit ye work issu",
        "Question_preprocessed_content":"analyt pipelin runtim paramet variabl depend new process fulli understand capabl benefit similar discuss need build data pipelin good fit struggl find good fit data process workflow obvious suit perfectli straightforward project input data set output train model deploi thing bit complex differ type us case us case exampl pipelin consid exampl build data process pipelin analyz geo imag want deploi product server get trigger regular basi high level input process imag plu addit meta data output process result form small snippet json data pipelin approxim consist step download data given paramet download imag locat dai web servic save network share file structur like filter data given paramet filter imag satisfi certain criteria result instanc json list imag path method paramet probabl insid presum process data take set valid imag previou step load network drive analyz process depend list longer shorter realiti step actual multipl separ step output simplic skip output json file save network drive issu differ classic preprocess train evalu flow specif make doubt easili applic pipelin depend runtim paramet input like chang run pass user model updat time addit question gener understand dai contain kei run pipelin run skip earlier run set depend file depend static set vari depend input paramet instead set depend stage vari vari input paramet step help mayb templat variabl prefer wed like send notif pipelin run call onlin rest api word pipelin step effect track file probabl point fall short ran pipelin twice paramet data notif sent point crucial think good fit ye work issu",
        "Question_gpt_summary_original":"The user is struggling to determine if DVC is a good fit for their data processing workflow, which involves analyzing geo- and time-referenced images and deploying the pipeline to a production server. The pipeline has runtime parameters that change for each run and are passed by the user, and the set of dependencies varies depending on input parameters. Additionally, some pipeline steps have side effects that cannot be tracked with files, making it difficult to send notifications for each pipeline run. The user is seeking advice on how to model these issues if DVC is a good fit for their workflow.",
        "Question_gpt_summary":"user struggl determin good fit data process workflow involv analyz geo time referenc imag deploi pipelin product server pipelin runtim paramet chang run pass user set depend vari depend input paramet addition pipelin step effect track file make difficult send notif pipelin run user seek advic model issu good fit workflow",
        "Answer_original_content":"pipelin depend runtim paramet locat date input like chang run pass user autom script model updat param yaml time updat param yaml wai pipelin gener locat date autom script have script write appropri valu param yaml autom script separ pipelin potenti updat param yaml trigger repro addit question gener understand dai param yaml contain kei foo run pipelin foo foo foo run skip earlier run set depend ye case checkout output gener previou foo run instead execut stage stage non determinist expect differ output time paramet valu unchang forc execut stage set chang flag file depend static set vari depend input paramet data train csv instead set depend filenam stage vari vari input paramet step help mayb templat variabl yaml file consist base input paramet mean given locat date combin set file file filenam differ locat date combin case tell track singl output directori path download data stage download file directori dont need tell individu path track us directori path depend filter stage pipelin prefer wed like send notif pipelin run call onlin rest api word pipelin step effect track file probabl point fall short ran pipelin twice paramet data notif sent point crucial potenti accomplish chang flag forc run stage type effect depend havent chang thank lot comprehens answer help understand better file consist base input paramet mean given locat date combin set file ye path includ paramet compon determin script runtim known upfront input paramet us directori path depend filter stage pipelin benefit depend track hash featur entireti anymor let specifi data depend let pipelin run date result data creat let run result data content data chang run perform data relev chang right addit question like know good practic us share file nf cif mount webdav run pipelin differ host access file benefit benefit lock resid share drive benefit depend track hash featur entireti anymor let specifi data depend let pipelin run date result data creat let run result data content data chang run perform data relev chang right suggest us data store singl depend dataset data data data fetch stage clear entir content data download file directli data append fetch dataset previou dataset run data contain file second run data contain file wai mean depend track hash work awar multipl differ possibl dep state seen data data differ possibl hash time fetch execut run data hash run cach subsequ non fetch stage pipelin run like know good practic us share file nf cif mount webdav recommend wai cach directori network mount configur us symlink cach directori http org doc user guid share cach share cach wai track data remain network mount main repo workspac local drive allow us share cach directori multipl repo look note share cach scenario work share local directori network storag isnt requir possibl us network mount main repo workspac directori come caveat gener optim us network filesystem like come signific perform hit compar keep repo workspac local storag config option need set depend us case specif network filesystem case core hardlink lock specif nf state dir index dir us sqlite cach certain intern lookup sqlite work properli certain network filesystem like need set option point local filesystem directori like tmp",
        "Answer_preprocessed_content":"pipelin depend runtim paramet input like chang run pass user model updat time updat wai pipelin gener locat date autom script have script write appropri valu autom script separ pipelin potenti updat trigger addit question gener understand dai contain kei run pipelin run skip earlier run set depend ye case checkout output gener previou run instead stage stage forc execut stage set flag file depend static set vari depend input paramet instead set depend stage vari vari input paramet step help mayb templat variabl file consist base input paramet mean given combin set file case tell track singl output directori path download data stage download file directori dont need tell individu path track us directori path depend stage pipelin prefer wed like send notif pipelin run call onlin rest api word pipelin step effect track file probabl point fall short ran pipelin twice paramet data notif sent point crucial potenti accomplish flag forc run stage type effect depend havent chang thank lot comprehens answer help understand better file consist base input paramet mean given combin set file ye path includ paramet compon determin script runtim known upfront input paramet us directori path depend stage pipelin benefit depend track hash featur entireti anymor let specifi depend let pipelin run date result creat let run result content chang run perform data relev chang right addit question like know good practic us share file run pipelin differ host access file benefit benefit resid share drive suggest us store singl depend dataset fetch stage clear entir content download file directli run contain file second run contain file wai mean depend work awar multipl differ possibl dep state seen time fetch execut run hash subsequ stage pipelin like know good practic us share file recommend wai cach directori network mount configur us symlink cach directori wai track data remain network mount main repo workspac local drive allow share cach directori multipl repo look possibl us network mount main repo workspac directori come caveat gener optim us network filesystem like come signific perform hit config option need set depend us case specif network filesystem case specif nf us sqlite cach certain intern lookup sqlite work properli certain network filesystem like need set option point local filesystem directori",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Updating `params.yaml` each time to model the runtime parameters.\n- Using template variables in `dvc.yaml` to help with the dynamic set of dependencies.\n- Potentially using the `always_changed` flag to force DVC to run stages with side effects even when dependencies haven\u2019t changed.\n- Keeping the DVC cache directory on the network mount and configuring DVC to use symlinks to that cache directory to use DVC with a shared file system.\n- Using the network mount as the main repo workspace directory, but this comes with some caveats and may require setting specific DVC config options. \n\nNote that there are no solutions provided for sending notifications for each pipeline run with side effects that",
        "Answer_gpt_summary":"possibl solut mention discuss updat param yaml time model runtim paramet templat variabl yaml help dynam set depend potenti chang flag forc run stage effect depend havent chang keep cach directori network mount configur us symlink cach directori us share file network mount main repo workspac directori come caveat requir set specif config option note solut provid send notif pipelin run effect"
    },
    {
        "Question_title":"Import-url on local network",
        "Question_body":"<p>Hi,<br>\nI have a question regarding the import-url function of dvc. I have windows machines on a local network and want to import  (logfiles) files from one machine to another. I have the windows path, password and ip address and I am already able to <code>robocopy<\/code> the files I want. Is it now possible to import those files?<br>\nThe copy command that works is:<br>\n<code>robocopy \\\\{ipaddress}\\c$\\data\\file.txt C:\\Path\\file.txt<\/code><\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1621351703852,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":231.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jimmy\">@jimmy<\/a> !<\/p>\n<p><code>import-url<\/code> should work to my mind in that case. Do you see any problems with it?<\/p>\n<p>Could you also clarify the scenario a bit? Do you want to keep that data outside <code>\\\\{ipaddress}\\c$\\data\\file.txt<\/code> and keep importing newer versions when updates are available? Or do you want to take the current state of it and add into DVC?<\/p>. <p>Hi,<br>\nthank you for your quick reply. I have no Idea what was the issue yesterday, but today it seems to work. (Maybe the error message could be more verbose?) The idea is to copy the new logfiles from time to time to the local machine, so that the network usage is minimal. Secondly I want to be able to  easily check if there are new logfiles. This should work with <code>dvc update data\\file.txt.dvc<\/code>, right?<\/p>\n<p>Another question: Will the file be sent over the network to calculate the md5 sum? I really want to reduce network usage. Thanks again.<\/p>. <aside class=\"quote no-group\" data-username=\"jimmy\" data-post=\"3\" data-topic=\"749\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/j\/ee59a6\/40.png\" class=\"avatar\"> jimmy:<\/div>\n<blockquote>\n<p>Hi,<br>\nthank you for your quick reply. I have no Idea what was the issue yesterday, but today it seems to work.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Perfect!<\/p>\n<blockquote>\n<p>Secondly I want to be able to easily check if there are new logfiles. This should work with <code>dvc update data\\file.txt.dvc<\/code> , right?<\/p>\n<\/blockquote>\n<p>yes, it\u2019ll handle this<\/p>\n<blockquote>\n<p>Another question: Will the file be sent over the network to calculate the md5 sum? I really want to reduce network usage. Thanks again.<\/p>\n<\/blockquote>\n<p>I think yes unfortunately, it\u2019ll be reading them via network to calculate the md5 again (to see if update is needed, and what files to we need to pass) <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a> can confirm this.<\/p>\n<hr>\n<p>A workaround here is to write a custom stages (<code>dvc stage add<\/code>) and do not rely on <code>import-url<\/code>. First stage would check if there are new updates. For example, if that log directory is immutable, append-only you could for example run <code>ls dir | wc -l &gt; files.count<\/code> or just <code>ls dir &gt; files<\/code>:<\/p>\n<pre><code class=\"lang-auto\">dvc stage add -o files \"ls dir &gt; files.list\"\n<\/code><\/pre>\n<p>Then second stage that would depend on the <code>files.list<\/code>:<\/p>\n<pre><code class=\"lang-auto\">dvc stage add -d files.list -o data [command to sync missing files to data]\n<\/code><\/pre>\n<p>In the second stage file, you can consider using <code>--outs-persist<\/code> to keep the previous version of the data when you run the stage (so that you can calculate delta and copy only missing files).<\/p>. <p>Ok, maybe it would make sense to just compare by timestamps in these cases? I just settled on robocopy now, which seems to work:<\/p>\n<pre><code class=\"lang-auto\">stages:\n  ImportData:\n    cmd: (robocopy \\\\{ip-address}\\c$\\data data *.*  \/XO\n      \/E) ^&amp; IF %ERRORLEVEL% LEQ 8 exit \/B 0\n    always_changed: true\n    outs:\n    - data:\n        persist: true\n    frozen: true\n<\/code><\/pre>\n<p>The command is not pretty because robocopy uses more exit codes for success. Is there an easier way to update the data? At moment, I have to <code>dvc unfreeze $$ dvc repro ImportData $$ dvc freeze $$ git commit ...<\/code><\/p>. <p>I\u2019m not familiar with <code>robocopy<\/code> - does it always copies all the data no matter what you have locally?<\/p>. <p>Hi,<br>\ndue to the flag <code>\/XO<\/code>, robocopy copies just new and changed files.<\/p>. <p>So, then it depends on how exactly does it detect the changes? If it (like DVC has to go and calculate hash) then we need to improve this (and we can discuss how). If it relies on timestamps only or some other simple way - then you should be fine, right? That single stage that you have should be efficient to bring only new files when they come. Unless I\u2019m missing something.<\/p>. <p>Sorry that I didn\u2019t point out my questions very well. In general, it is now working as I want. Two little notes:<\/p>\n<ul>\n<li>For me it would be convenient, if dvc could filter the external files by timestamps first.  (Maybe as an option.)<\/li>\n<li>With the stage as in one of the previous answers,  it would be convenient, if I could do something like <code>dvc update ImportData<\/code>. Right now, I have to <code>dvc unfreeze ImportData &amp;&amp; dvc repro ImportData &amp;&amp; dvc unfreeze ImportData<\/code>, just to update one data folder\/file. It is probably not a good idea, to make wrap this command into a new stage, right?<\/li>\n<\/ul>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/import-url-on-local-network\/749",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-19T00:43:41.286Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jimmy\">@jimmy<\/a> !<\/p>\n<p><code>import-url<\/code> should work to my mind in that case. Do you see any problems with it?<\/p>\n<p>Could you also clarify the scenario a bit? Do you want to keep that data outside <code>\\\\{ipaddress}\\c$\\data\\file.txt<\/code> and keep importing newer versions when updates are available? Or do you want to take the current state of it and add into DVC?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-19T07:27:24.311Z",
                "Answer_body":"<p>Hi,<br>\nthank you for your quick reply. I have no Idea what was the issue yesterday, but today it seems to work. (Maybe the error message could be more verbose?) The idea is to copy the new logfiles from time to time to the local machine, so that the network usage is minimal. Secondly I want to be able to  easily check if there are new logfiles. This should work with <code>dvc update data\\file.txt.dvc<\/code>, right?<\/p>\n<p>Another question: Will the file be sent over the network to calculate the md5 sum? I really want to reduce network usage. Thanks again.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-20T01:46:20.291Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"jimmy\" data-post=\"3\" data-topic=\"749\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/j\/ee59a6\/40.png\" class=\"avatar\"> jimmy:<\/div>\n<blockquote>\n<p>Hi,<br>\nthank you for your quick reply. I have no Idea what was the issue yesterday, but today it seems to work.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Perfect!<\/p>\n<blockquote>\n<p>Secondly I want to be able to easily check if there are new logfiles. This should work with <code>dvc update data\\file.txt.dvc<\/code> , right?<\/p>\n<\/blockquote>\n<p>yes, it\u2019ll handle this<\/p>\n<blockquote>\n<p>Another question: Will the file be sent over the network to calculate the md5 sum? I really want to reduce network usage. Thanks again.<\/p>\n<\/blockquote>\n<p>I think yes unfortunately, it\u2019ll be reading them via network to calculate the md5 again (to see if update is needed, and what files to we need to pass) <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a> can confirm this.<\/p>\n<hr>\n<p>A workaround here is to write a custom stages (<code>dvc stage add<\/code>) and do not rely on <code>import-url<\/code>. First stage would check if there are new updates. For example, if that log directory is immutable, append-only you could for example run <code>ls dir | wc -l &gt; files.count<\/code> or just <code>ls dir &gt; files<\/code>:<\/p>\n<pre><code class=\"lang-auto\">dvc stage add -o files \"ls dir &gt; files.list\"\n<\/code><\/pre>\n<p>Then second stage that would depend on the <code>files.list<\/code>:<\/p>\n<pre><code class=\"lang-auto\">dvc stage add -d files.list -o data [command to sync missing files to data]\n<\/code><\/pre>\n<p>In the second stage file, you can consider using <code>--outs-persist<\/code> to keep the previous version of the data when you run the stage (so that you can calculate delta and copy only missing files).<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-20T12:12:46.933Z",
                "Answer_body":"<p>Ok, maybe it would make sense to just compare by timestamps in these cases? I just settled on robocopy now, which seems to work:<\/p>\n<pre><code class=\"lang-auto\">stages:\n  ImportData:\n    cmd: (robocopy \\\\{ip-address}\\c$\\data data *.*  \/XO\n      \/E) ^&amp; IF %ERRORLEVEL% LEQ 8 exit \/B 0\n    always_changed: true\n    outs:\n    - data:\n        persist: true\n    frozen: true\n<\/code><\/pre>\n<p>The command is not pretty because robocopy uses more exit codes for success. Is there an easier way to update the data? At moment, I have to <code>dvc unfreeze $$ dvc repro ImportData $$ dvc freeze $$ git commit ...<\/code><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-21T02:24:34.401Z",
                "Answer_body":"<p>I\u2019m not familiar with <code>robocopy<\/code> - does it always copies all the data no matter what you have locally?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-21T06:35:38.537Z",
                "Answer_body":"<p>Hi,<br>\ndue to the flag <code>\/XO<\/code>, robocopy copies just new and changed files.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-21T20:19:17.163Z",
                "Answer_body":"<p>So, then it depends on how exactly does it detect the changes? If it (like DVC has to go and calculate hash) then we need to improve this (and we can discuss how). If it relies on timestamps only or some other simple way - then you should be fine, right? That single stage that you have should be efficient to bring only new files when they come. Unless I\u2019m missing something.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-22T07:18:56.340Z",
                "Answer_body":"<p>Sorry that I didn\u2019t point out my questions very well. In general, it is now working as I want. Two little notes:<\/p>\n<ul>\n<li>For me it would be convenient, if dvc could filter the external files by timestamps first.  (Maybe as an option.)<\/li>\n<li>With the stage as in one of the previous answers,  it would be convenient, if I could do something like <code>dvc update ImportData<\/code>. Right now, I have to <code>dvc unfreeze ImportData &amp;&amp; dvc repro ImportData &amp;&amp; dvc unfreeze ImportData<\/code>, just to update one data folder\/file. It is probably not a good idea, to make wrap this command into a new stage, right?<\/li>\n<\/ul>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"import url local network question import url function window machin local network want import logfil file machin window path password address abl robocopi file want possibl import file copi command work robocopi ipaddress data file txt path file txt thank",
        "Question_preprocessed_content":"local network question function window machin local network want import file machin window path password address abl file want possibl import file copi command work thank",
        "Question_gpt_summary_original":"The user is facing a challenge in importing log files from one Windows machine to another on a local network using the import-url function of dvc. They have the necessary information such as the Windows path, password, and IP address, and have successfully used the robocopy command to copy the files they need. The user is seeking guidance on whether it is possible to import these files using dvc.",
        "Question_gpt_summary":"user face challeng import log file window machin local network import url function necessari inform window path password address successfulli robocopi command copi file need user seek guidanc possibl import file",
        "Answer_original_content":"jimmi import url work mind case problem clarifi scenario bit want data outsid ipaddress data file txt import newer version updat avail want current state add thank quick repli idea issu yesterdai todai work mayb error messag verbos idea copi new logfil time time local machin network usag minim secondli want abl easili check new logfil work updat data file txt right question file sent network calcul sum want reduc network usag thank jimmi thank quick repli idea issu yesterdai todai work perfect secondli want abl easili check new logfil work updat data file txt right ye itll handl question file sent network calcul sum want reduc network usag thank think ye unfortun itll read network calcul updat need file need pass kuprus confirm workaround write custom stage stage add reli import url stage check new updat exampl log directori immut append exampl run dir file count dir file stage add file dir file list second stage depend file list stage add file list data command sync miss file data second stage file consid out persist previou version data run stage calcul delta copi miss file mayb sens compar timestamp case settl robocopi work stage importdata cmd robocopi address data data errorlevel leq exit chang true out data persist true frozen true command pretti robocopi us exit code success easier wai updat data moment unfreez repro importdata freez git commit familiar robocopi copi data matter local flag robocopi copi new chang file depend exactli detect chang like calcul hash need improv discuss reli timestamp simpl wai fine right singl stage effici bring new file come miss sorri didnt point question gener work want littl note conveni filter extern file timestamp mayb option stage previou answer conveni like updat importdata right unfreez importdata repro importdata unfreez importdata updat data folder file probabl good idea wrap command new stage right",
        "Answer_preprocessed_content":"work mind case problem clarifi scenario bit want data outsid import newer version updat avail want current state add thank quick repli idea issu yesterdai todai work idea copi new logfil time time local machin network usag minim secondli want abl easili check new logfil work right question file sent network calcul sum want reduc network usag thank jimmi thank quick repli idea issu yesterdai todai work perfect secondli want abl easili check new logfil work right ye itll handl question file sent network calcul sum want reduc network usag thank think ye unfortun itll read network calcul confirm workaround write custom stage reli stage check new updat exampl log directori immut exampl run second stage depend second stage file consid previou version data run stage mayb sens compar timestamp case settl robocopi work command pretti robocopi us exit code success easier wai updat data moment familiar copi data matter local flag robocopi copi new chang file depend exactli detect chang need improv reli timestamp simpl wai fine right singl stage effici bring new file come miss sorri didnt point question gener work want littl note conveni filter extern file timestamp stage previou answer conveni like right updat data probabl good idea wrap command new stage right",
        "Answer_gpt_summary_original":"Solution:\n- The user can use the `import-url` function of dvc to import log files from one Windows machine to another on a local network.\n- The user can write a custom stage using `dvc stage add` and not rely on `import-url`. The first stage would check if there are new updates, and the second stage would depend on the `files.list`.\n- The user can use the `robocopy` command to copy new and changed files from one machine to another.\n- The user can filter the external files by timestamps first by adding an option to dvc.\n- The user can update one data folder\/file by running `dvc unfreeze ImportData && dvc repro ImportData && dvc unfreeze Import",
        "Answer_gpt_summary":"solut user us import url function import log file window machin local network user write custom stage stage add reli import url stage check new updat second stage depend file list user us robocopi command copi new chang file machin user filter extern file timestamp ad option user updat data folder file run unfreez importdata repro importdata unfreez import"
    },
    {
        "Question_title":"Is there a way to install R libraries in SageMaker that receive a non-zero exit status?",
        "Question_body":"Hi all, I'm having an issue with an R kernel\/Jupyter notebook. I've come across two different libraries that result in the following error:\n\nWarning message in install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\"):\n\u201cinstallation of package \u2018XML\u2019 had non-zero exit status\u201dUpdating HTML index of packages in '.Library'\nMaking 'packages.html' ... done\n\n\nXML is the second package that I have run into this issue with. The other is rJava.\n\nI found a workaround that could work if I had root access, which is installing via the command line in a terminal. Which involves commands such as:\n\nyum install r-cran-rjava\n\n\nHowever, I don't have root access and cannot install as I get the message \"You need to be root to perform this command.\" So this workaround hasn't been possible.\n\nAfter checking the documentation for rJava and XML, I am running the requirements for JDK and other system requirements in SageMaker. This issue wasn't reproducible on a local RStudio environment. XML is a dependency for multiple R libraries (as is rJava). Is there a way that I can still install these packages?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1527798496000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":464.0,
        "Answer_body":"For Amazon SageMaker notebook Instances, you have the ability to assume root privileges, so instead of:\n\n$ yum install r-cran-rjava\n\nyou can try:\n\n$ sudo yum install r-cran-rjava\n\nwhich will allow you to impersonate the superuser (ie. root) for that command 1\n\nBut I don't believe that package exists in the available repos (ie. may be valid for another distro of Linux, but does not appear to be available in the yum repos -- running yum search 'r-cran-rjava' returned no results 2)\n\nInstead, from a prompt, install R + the necessary development files for later installation of R packages:\n\n$ sudo yum install -y R-java-devel.x86_64\n\nAnd finally, install the necessary XML libraries to support the XML package in R:\n\n$ sudo yum install -y libxml2-devel 3\n\nAfter which you can then open R (either as root user...)\n\n$ sudo R\n\nor personal\/local user\n\n$ R\n\nand execute the package installation:\n\n> install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\")\n\nEDITED TO FIX RJAVA PACKAGE INSTALLATION\n\nIt looks like the installation is requiring libgomp.spec\/libgomp.a files, so you can first find that file:\n\n$ sudo find \/ -iname libgomp.spec\n\nwhich should be located at \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec -- if so, you can manually create symlinks to fix this:\n\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec \/usr\/lib64\/\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.a \/usr\/lib64\/\n\n\nIf that ran correctly, you should now see both files in \/usr\/lib64 path:\n\n$ ls \/usr\/lib64\/libgomp*\n\nOnce confirmed, you can run the install.package('rJava') command.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE-0c9SwxRViGhd-lAYtsuw\/is-there-a-way-to-install-r-libraries-in-sage-maker-that-receive-a-non-zero-exit-status",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-06-01T02:52:15.000Z",
                "Answer_score":0,
                "Answer_body":"For Amazon SageMaker notebook Instances, you have the ability to assume root privileges, so instead of:\n\n$ yum install r-cran-rjava\n\nyou can try:\n\n$ sudo yum install r-cran-rjava\n\nwhich will allow you to impersonate the superuser (ie. root) for that command 1\n\nBut I don't believe that package exists in the available repos (ie. may be valid for another distro of Linux, but does not appear to be available in the yum repos -- running yum search 'r-cran-rjava' returned no results 2)\n\nInstead, from a prompt, install R + the necessary development files for later installation of R packages:\n\n$ sudo yum install -y R-java-devel.x86_64\n\nAnd finally, install the necessary XML libraries to support the XML package in R:\n\n$ sudo yum install -y libxml2-devel 3\n\nAfter which you can then open R (either as root user...)\n\n$ sudo R\n\nor personal\/local user\n\n$ R\n\nand execute the package installation:\n\n> install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\")\n\nEDITED TO FIX RJAVA PACKAGE INSTALLATION\n\nIt looks like the installation is requiring libgomp.spec\/libgomp.a files, so you can first find that file:\n\n$ sudo find \/ -iname libgomp.spec\n\nwhich should be located at \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec -- if so, you can manually create symlinks to fix this:\n\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec \/usr\/lib64\/\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.a \/usr\/lib64\/\n\n\nIf that ran correctly, you should now see both files in \/usr\/lib64 path:\n\n$ ls \/usr\/lib64\/libgomp*\n\nOnce confirmed, you can run the install.package('rJava') command.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1527821535000,
        "Question_original_content":"wai instal librari receiv non zero exit statu have issu kernel jupyt notebook come differ librari result follow error warn messag instal packag xml repo http cran project org instal packag xml non zero exit statusupd html index packag librari make packag html xml second packag run issu rjava workaround work root access instal command line termin involv command yum instal cran rjava root access instal messag need root perform command workaround hasn possibl check document rjava xml run requir jdk requir issu wasn reproduc local rstudio environ xml depend multipl librari rjava wai instal packag",
        "Question_preprocessed_content":"wai instal librari receiv exit statu have issu notebook come differ librari result follow error warn messag repo instal packag xml exit statusupd html index packag make xml second packag run issu rjava workaround work root access instal command line termin involv command yum instal root access instal messag need root perform workaround hasn possibl check document rjava xml run requir jdk requir issu wasn reproduc local rstudio environ xml depend multipl librari wai instal packag",
        "Question_gpt_summary_original":"The user is facing challenges in installing R libraries, specifically XML and rJava, in SageMaker due to non-zero exit status errors. The user has tried a workaround that involves installing via the command line in a terminal, but cannot do so as they do not have root access. The issue was not reproducible on a local RStudio environment, and the user is running the requirements for JDK and other system requirements in SageMaker. The user is seeking a solution to install these packages without root access.",
        "Question_gpt_summary":"user face challeng instal librari specif xml rjava non zero exit statu error user tri workaround involv instal command line termin root access issu reproduc local rstudio environ user run requir jdk requir user seek solut instal packag root access",
        "Answer_original_content":"notebook instanc abil assum root privileg instead yum instal cran rjava try sudo yum instal cran rjava allow imperson superus root command believ packag exist avail repo valid distro linux appear avail yum repo run yum search cran rjava return result instead prompt instal necessari develop file later instal packag sudo yum instal java devel final instal necessari xml librari support xml packag sudo yum instal libxml devel open root user sudo person local user execut packag instal instal packag xml repo http cran project org edit fix rjava packag instal look like instal requir libgomp spec libgomp file file sudo inam libgomp spec locat usr lib gcc amazon linux libgomp spec manual creat symlink fix sudo usr lib gcc amazon linux libgomp spec usr lib sudo usr lib gcc amazon linux libgomp usr lib ran correctli file usr lib path usr lib libgomp confirm run instal packag rjava command",
        "Answer_preprocessed_content":"notebook instanc abil assum root privileg instead yum instal try sudo yum instal allow imperson superus command believ packag exist avail repo instead prompt instal necessari develop file later instal packag sudo yum instal final instal necessari xml librari support xml packag sudo yum instal open sudo user execut packag instal repo edit fix rjava packag instal look like instal requir file file sudo inam locat manual creat symlink fix sudo sudo ran correctli file path confirm run command",
        "Answer_gpt_summary_original":"The solution suggested in the discussion involves assuming root privileges by using the command \"sudo yum install\" instead of \"yum install\" to install the necessary packages. If the required package is not available in the available repositories, the user can install R and the necessary development files, followed by installing the necessary XML libraries to support the XML package in R. The user can then open R and execute the package installation. Additionally, the user can manually create symlinks to fix the issue of missing libgomp.spec\/libgomp.a files and then run the install.package('rJava') command.",
        "Answer_gpt_summary":"solut suggest discuss involv assum root privileg command sudo yum instal instead yum instal instal necessari packag requir packag avail avail repositori user instal necessari develop file follow instal necessari xml librari support xml packag user open execut packag instal addition user manual creat symlink fix issu miss libgomp spec libgomp file run instal packag rjava command"
    },
    {
        "Question_title":"How can I match my local azure automl python sdk version to the remote version?",
        "Question_body":"<p>I'm using the azure automl python sdk to download and save a model then reload it. I get the following error:<\/p>\n<pre><code>anaconda3\\envs\\automl_21\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator Pipeline from version 0.22.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n<\/code><\/pre>\n<p>How can I ensure that the versions match?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617943623520,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":62.0,
        "Answer_body":"<p>My Microsoft contact says -<\/p>\n<p>&quot;For this, their best  bet is probably to see what the training env was pinned to and install those same pins. They can get that env by running child_run.get_environment() and then pip install all the pkgs listed in there with the pins listed there.&quot;<\/p>\n<p>A useful code snippet.<\/p>\n<pre><code>for run in experiment.get_runs():\n    tags_dictionary = run.get_tags()\n    best_run = AutoMLRun(experiment, tags_dictionary['automl_best_child_run_id'])\n    env = best_run.get_environment()\n    print(env.python.conda_dependencies.serialize_to_string())\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1618178182616,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67015185",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1618176409272,
        "Question_original_content":"match local azur automl python sdk version remot version azur automl python sdk download save model reload follow error anaconda env automl lib site packag sklearn base userwarn try unpickl estim pipelin version version post lead break code invalid result us risk userwarn ensur version match",
        "Question_preprocessed_content":"match local azur automl python sdk version remot version azur automl python sdk download save model reload follow error ensur version match",
        "Question_gpt_summary_original":"The user is encountering an error while using the Azure AutoML Python SDK to download and save a model, which warns about trying to unpickle an estimator Pipeline from version 0.22.1 when using version 0.22.2.post1. The user is seeking guidance on how to match the local and remote versions to avoid breaking code or invalid results.",
        "Question_gpt_summary":"user encount error azur automl python sdk download save model warn try unpickl estim pipelin version version post user seek guidanc match local remot version avoid break code invalid result",
        "Answer_original_content":"microsoft contact sai best bet probabl train env pin instal pin env run child run environ pip instal pkg list pin list us code snippet run experi run tag dictionari run tag best run automlrun experi tag dictionari automl best child run env best run environ print env python conda depend serial string",
        "Answer_preprocessed_content":"microsoft contact sai best bet probabl train env pin instal pin env run pip instal pkg list pin list us code snippet",
        "Answer_gpt_summary_original":"Solution: The Microsoft contact suggests that the user should check the training environment and install the same pinned packages. The user can get the environment by running child_run.get_environment() and then pip install all the packages listed in there with the pins listed there. A code snippet is also provided to get the environment.",
        "Answer_gpt_summary":"solut microsoft contact suggest user check train environ instal pin packag user environ run child run environ pip instal packag list pin list code snippet provid environ"
    },
    {
        "Question_title":"Using Sagemaker predictor in a Spark UDF function",
        "Question_body":"<p>I am trying to run inference on a Tensorflow model deployed on SageMaker from a Python Spark job.\nI am running a (Databricks) notebook which has the following cell:<\/p>\n\n<pre><code>def call_predict():\n        batch_size = 1\n        data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n        tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n        prediction = predictor.predict(tensor_proto)\n        print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n<\/code><\/pre>\n\n<p>If I just call call_predict() it works fine:<\/p>\n\n<pre><code>call_predict()\n<\/code><\/pre>\n\n<p>and I get the output:<\/p>\n\n<pre><code>Process time: 65.261396\nOut[61]: {'model_spec': {'name': u'generic_model',\n  'signature_name': u'serving_default',\n  'version': {'value': 1578909324L}},\n 'outputs': {u'ages': {'dtype': 1,\n   'float_val': [5.680944442749023],\n   'tensor_shape': {'dim': [{'size': 1L}]}}}}\n<\/code><\/pre>\n\n<p>but when I try to call from a Spark context (in a UDF) I get a serialization error.\nThe code I'm trying to run is:<\/p>\n\n<pre><code>dataRange = range(1, 10001)\nrangeRDD = sc.parallelize(dataRange, 8)\nnew_data = rangeRDD.map(lambda x : call_predict())\nnew_data.count()\n<\/code><\/pre>\n\n<p>and the error I get is:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nPicklingError                             Traceback (most recent call last)\n&lt;command-2282434&gt; in &lt;module&gt;()\n      2 rangeRDD = sc.parallelize(dataRange, 8)\n      3 new_data = rangeRDD.map(lambda x : call_predict())\n----&gt; 4 new_data.count()\n      5 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in count(self)\n   1094         3\n   1095         \"\"\"\n-&gt; 1096         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n   1097 \n   1098     def stats(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in sum(self)\n   1085         6.0\n   1086         \"\"\"\n-&gt; 1087         return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n   1088 \n   1089     def count(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in fold(self, zeroValue, op)\n    956         # zeroValue provided to each partition is unique from the one provided\n    957         # to the final reduce call\n--&gt; 958         vals = self.mapPartitions(func).collect()\n    959         return reduce(op, vals, zeroValue)\n    960 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in collect(self)\n    829         # Default path used in OSS Spark \/ for non-credential passthrough clusters:\n    830         with SCCallSiteSync(self.context) as css:\n--&gt; 831             sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n    832         return list(_load_from_socket(sock_info, self._jrdd_deserializer))\n    833 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _jrdd(self)\n   2573 \n   2574         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n-&gt; 2575                                       self._jrdd_deserializer, profiler)\n   2576         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n   2577                                              self.preservesPartitioning, self.is_barrier)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _wrap_function(sc, func, deserializer, serializer, profiler)\n   2475     assert serializer, \"serializer should not be empty\"\n   2476     command = (func, profiler, deserializer, serializer)\n-&gt; 2477     pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n   2478     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n   2479                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _prepare_for_python_RDD(sc, command)\n   2461     # the serialized command will be compressed by broadcast\n   2462     ser = CloudPickleSerializer()\n-&gt; 2463     pickled_command = ser.dumps(command)\n   2464     if len(pickled_command) &gt; sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  # Default 1M\n   2465         # The broadcast will have same life cycle as created PythonRDD\n\n\/databricks\/spark\/python\/pyspark\/serializers.pyc in dumps(self, obj)\n    709                 msg = \"Could not serialize object: %s: %s\" % (e.__class__.__name__, emsg)\n    710             cloudpickle.print_exec(sys.stderr)\n--&gt; 711             raise pickle.PicklingError(msg)\n    712 \n    713 \n\nPicklingError: Could not serialize object: TypeError: can't pickle _ssl._SSLSocket objects\n<\/code><\/pre>\n\n<p>Not sure what is this serialization error - does is complain about failing to deserialize the Predictor<\/p>\n\n<p>My notebook has a cell which was called prior to the above cells with the following imports:<\/p>\n\n<pre><code>import sagemaker\nimport boto3\nfrom sagemaker.tensorflow.model import TensorFlowPredictor\nimport tensorflow as tf\nimport numpy as np\nimport time\n<\/code><\/pre>\n\n<p>The Predictor was created with the following code:<\/p>\n\n<pre><code>sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY,\n                                aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\nsagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY,\n                                        aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\nboto_session = boto3.Session(region_name='us-east-1')\nsagemaker_session = sagemaker.Session(boto_session, sagemaker_client=sagemaker_client, sagemaker_runtime_client=sagemaker_runtime_client)\n\npredictor = TensorFlowPredictor('endpoint-poc', sagemaker_session)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579190415880,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":322.0,
        "Answer_body":"<p>The udf function will be executed by multiple spark tasks in parallel. Those tasks run in completely isolated python processes and they are scheduled to physically different machines. Hence each data, those functions reference, must be on the same node. This is the case for everything created within the udf.<\/p>\n\n<p>Whenever you reference any object outside of the udf from the function, this data structure needs to be serialised (pickled) to each executor. Some object state, like open connections to a socket, cannot be pickled.<\/p>\n\n<p>You need to make sure, that connections are lazily opened each executor. It must happen only on the first function call on that executor. The <a href=\"https:\/\/spark.apache.org\/docs\/latest\/streaming-programming-guide.html#design-patterns-for-using-foreachrdd\" rel=\"nofollow noreferrer\">connection pooling topic<\/a> is covered in the docs, however only in the spark streaming guide (though it also applies for normal batch jobs).<\/p>\n\n<p>Normally one can use the Singleton Pattern for this. But in python people use the Borgh pattern.<\/p>\n\n<pre><code>class Env:\n    _shared_state = {\n        \"sagemaker_client\": None\n        \"sagemaker_runtime_client\": None\n        \"boto_session\": None\n        \"sagemaker_session\": None\n        \"predictor\": None\n    }\n    def __init__(self):\n        self.__dict__ = self._shared_state\n        if not self.predictor:\n            self.sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n            self.sagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\n            self.boto_session = boto3.Session(region_name='us-east-1')\n            self.sagemaker_session = sagemaker.Session(self.boto_session, sagemaker_client=self.sagemaker_client, sagemaker_runtime_client=self.sagemaker_runtime_client)\n\n            self.predictor = TensorFlowPredictor('endpoint-poc', self.sagemaker_session)\n\n\n#....\ndef call_predict():\n   env = Env()\n   batch_size = 1\n   data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n   tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n   prediction = env.predictor.predict(tensor_proto)\n\n   print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n\nnew_data = rangeRDD.map(lambda x : call_predict())\n<\/code><\/pre>\n\n<p>The Env class is defined on the master node. Its <code>_shared_state<\/code> has empty entries. When then Env object is instantiated first time, it shares the state with all further instances of Env on any subsequent call to the udf. On each separate parallel running process this will happen exactly one time. This way the sessions are shared and do not need to pickled. <\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1579506845700,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59773503",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1579206604950,
        "Question_original_content":"predictor spark udf function try run infer tensorflow model deploi python spark job run databrick notebook follow cell def predict batch size data tensor proto tensor proto valu asarrai data shape batch size len data dtype float predict predictor predict tensor proto print process time format time clock start return predict predict work fine predict output process time model spec gener model signatur serv default version valu output ag dtype float val tensor shape dim size try spark context udf serial error code try run datarang rang rangerdd parallel datarang new data rangerdd map lambda predict new data count error picklingerror traceback recent rangerdd parallel datarang new data rangerdd map lambda predict new data count databrick spark python pyspark rdd pyc count self return self mappartit lambda sum sum def stat self databrick spark python pyspark rdd pyc sum self return self mappartit lambda sum fold oper add def count self databrick spark python pyspark rdd pyc fold self zerovalu zerovalu provid partit uniqu provid final reduc val self mappartit func collect return reduc val zerovalu databrick spark python pyspark rdd pyc collect self default path oss spark non credenti passthrough cluster sccallsitesync self context css sock info self ctx jvm pythonrdd collectandserv self jrdd rdd return list load socket sock info self jrdd deseri databrick spark python pyspark rdd pyc jrdd self wrap func wrap function self ctx self func self prev jrdd deseri self jrdd deseri profil python rdd self ctx jvm pythonrdd self prev jrdd rdd wrap func self preservespartit self barrier databrick spark python pyspark rdd pyc wrap function func deseri serial profil assert serial serial command func profil deseri serial pickl command broadcast var env includ prepar python rdd command return jvm pythonfunct bytearrai pickl command env includ pythonexec pythonv broadcast var javaaccumul databrick spark python pyspark rdd pyc prepar python rdd command serial command compress broadcast ser cloudpickleseri pickl command ser dump command len pickl command jvm pythonutil getbroadcastthreshold jsc default broadcast life cycl creat pythonrdd databrick spark python pyspark serial pyc dump self obj msg serial object class emsg cloudpickl print exec sy stderr rais pickl picklingerror msg picklingerror serial object typeerror pickl ssl sslsocket object sure serial error complain fail deseri predictor notebook cell call prior cell follow import import import boto tensorflow model import tensorflowpredictor import tensorflow import numpi import time predictor creat follow code client boto client aw access kei access kei aw secret access kei secret kei region east runtim client boto client runtim aw access kei access kei aw secret access kei secret kei region east boto session boto session region east session session boto session client client runtim client runtim client predictor tensorflowpredictor endpoint poc session",
        "Question_preprocessed_content":"predictor spark udf function try run infer tensorflow model deploi python spark job run notebook follow cell work fine output try spark context serial error code try run error sure serial error complain fail deseri predictor notebook cell call prior cell follow import predictor creat follow code",
        "Question_gpt_summary_original":"The user is encountering a serialization error when trying to call a Sagemaker predictor from a Spark context in a UDF. The error message suggests that the issue may be related to failing to deserialize the predictor. The user has imported the necessary libraries and created the predictor successfully in a previous cell.",
        "Question_gpt_summary":"user encount serial error try predictor spark context udf error messag suggest issu relat fail deseri predictor user import necessari librari creat predictor successfulli previou cell",
        "Answer_original_content":"udf function execut multipl spark task parallel task run complet isol python process schedul physic differ machin data function refer node case creat udf refer object outsid udf function data structur need serialis pickl executor object state like open connect socket pickl need sure connect lazili open executor happen function executor connect pool topic cover doc spark stream guid appli normal batch job normal us singleton pattern python peopl us borgh pattern class env share state client runtim client boto session session predictor def init self self dict self share state self predictor self client boto client aw access kei access kei aw secret access kei secret kei region east self runtim client boto client runtim aw access kei access kei aw secret access kei secret kei region east self boto session boto session region east self session session self boto session client self client runtim client self runtim client self predictor tensorflowpredictor endpoint poc self session def predict env env batch size data tensor proto tensor proto valu asarrai data shape batch size len data dtype float predict env predictor predict tensor proto print process time format time clock start return predict new data rangerdd map lambda predict env class defin master node share state entri env object instanti time share state instanc env subsequ udf separ parallel run process happen exactli time wai session share need pickl",
        "Answer_preprocessed_content":"udf function execut multipl spark task parallel task run complet isol python process schedul physic differ machin data function refer node case creat udf refer object outsid udf function data structur need serialis executor object state like open connect socket pickl need sure connect lazili open executor happen function executor connect pool topic cover doc spark stream guid normal us singleton pattern python peopl us borgh pattern env class defin master node entri env object instanti time share state instanc env subsequ udf separ parallel run process happen exactli time wai session share need pickl",
        "Answer_gpt_summary_original":"Solution:\n- The issue may be related to failing to deserialize the predictor when calling a Sagemaker predictor from a Spark context in a UDF.\n- The udf function will be executed by multiple spark tasks in parallel, and those tasks run in completely isolated python processes and are scheduled to physically different machines. Hence each data, those functions reference, must be on the same node.\n- Whenever you reference any object outside of the udf from the function, this data structure needs to be serialized (pickled) to each executor. Some object state, like open connections to a socket, cannot be pickled.\n- You need to make sure that connections are lazily opened each executor. It must happen only on the first function call on that executor",
        "Answer_gpt_summary":"solut issu relat fail deseri predictor call predictor spark context udf udf function execut multipl spark task parallel task run complet isol python process schedul physic differ machin data function refer node refer object outsid udf function data structur need serial pickl executor object state like open connect socket pickl need sure connect lazili open executor happen function executor"
    },
    {
        "Question_title":"How to deploy sagemaker.workflow.pipeline.Pipeline?",
        "Question_body":"<p>I have a <code>sagemaker.workflow.pipeline.Pipeline<\/code> which contains multiple <code>sagemaker.workflow.steps.ProcessingStep<\/code> and each <code>ProcessingStep<\/code> contains <code>sagemaker.processing.ScriptProcessor<\/code>.<\/p>\n<p>The current pipeline graph look like the below shown image. It will take data from multiple sources from S3, process it and create a final dataset using the data from previous steps.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6XImq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6XImq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As the <code>Pipeline<\/code> object doesn't support <code>.deploy<\/code> method, how to deploy this pipeline?<\/p>\n<p>While inference\/scoring, When we receive a raw data(single row for each source), how to trigger the pipeline?<\/p>\n<p>or Sagemaker Pipeline is designed for only data processing and model training on huge\/batch data? Not for the inference with the single data point?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639040387083,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":313.0,
        "Answer_body":"<blockquote>\n<p>As the Pipeline object doesn't support .deploy method, how to deploy this pipeline?<\/p>\n<\/blockquote>\n<p>Pipeline does not have a <code>.deploy()<\/code> method, no<\/p>\n<p>Use <code>pipeline.upsert(role_arn='...')<\/code> to create\/update the pipeline definition to SageMaker, then call <code>pipeline.start()<\/code> . Docs <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#pipeline\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<blockquote>\n<p>While inference\/scoring, When we receive a raw data(single row for each source), how to trigger the pipeline?<\/p>\n<\/blockquote>\n<p>There are actually two types of pipelines in SageMaker. Model Building Pipelines (which you have in your question), and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>, which are used for Inference. AWS definitely should have called the former &quot;workflows&quot;<\/p>\n<p>You can use a model building pipeline to setup a serial inference pipeline<\/p>\n<p>To do pre-processing in a serial inference pipeline, you want to train an encoder\/estimator (such as SKLearn) and save its model. Then train a learning algorithm, and save its model, then create a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/pipeline.html\" rel=\"nofollow noreferrer\">PipelineModel<\/a> using both models<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70287087",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1639073178300,
        "Question_original_content":"deploi workflow pipelin pipelin workflow pipelin pipelin contain multipl workflow step processingstep processingstep contain process scriptprocessor current pipelin graph look like shown imag data multipl sourc process creat final dataset data previou step pipelin object support deploi method deploi pipelin infer score receiv raw data singl row sourc trigger pipelin pipelin design data process model train huge batch data infer singl data point",
        "Question_preprocessed_content":"deploi contain multipl contain current pipelin graph look like shown imag data multipl sourc process creat final dataset data previou step object support method deploi pipelin receiv raw data trigger pipelin pipelin design data process model train data infer singl data point",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a sagemaker.workflow.pipeline.Pipeline that contains multiple sagemaker.workflow.steps.ProcessingStep, each with sagemaker.processing.ScriptProcessor. The user is unsure how to trigger the pipeline during inference\/scoring with a single data point and is questioning whether Sagemaker Pipeline is only designed for data processing and model training on batch data.",
        "Question_gpt_summary":"user face challeng deploi workflow pipelin pipelin contain multipl workflow step processingstep process scriptprocessor user unsur trigger pipelin infer score singl data point question pipelin design data process model train batch data",
        "Answer_original_content":"pipelin object support deploi method deploi pipelin pipelin deploi method us pipelin upsert role arn creat updat pipelin definit pipelin start doc infer score receiv raw data singl row sourc trigger pipelin actual type pipelin model build pipelin question serial infer pipelin infer aw definit call workflow us model build pipelin setup serial infer pipelin pre process serial infer pipelin want train encod estim sklearn save model train learn algorithm save model creat pipelinemodel model",
        "Answer_preprocessed_content":"pipelin object support deploi method deploi pipelin pipelin method us pipelin definit doc receiv raw data trigger pipelin actual type pipelin model build pipelin serial infer pipelin infer aw definit call workflow us model build pipelin setup serial infer pipelin serial infer pipelin want train save model train learn algorithm save model creat pipelinemodel model",
        "Answer_gpt_summary_original":"Solution:\n- To deploy the pipeline, use `pipeline.upsert(role_arn='...')` to create\/update the pipeline definition to SageMaker, then call `pipeline.start()`.\n- There are two types of pipelines in SageMaker: Model Building Pipelines and Serial Inference Pipelines. To do pre-processing in a serial inference pipeline, train an encoder\/estimator and save its model, then train a learning algorithm and save its model. Finally, create a `PipelineModel` using both models.",
        "Answer_gpt_summary":"solut deploi pipelin us pipelin upsert role arn creat updat pipelin definit pipelin start type pipelin model build pipelin serial infer pipelin pre process serial infer pipelin train encod estim save model train learn algorithm save model final creat pipelinemodel model"
    },
    {
        "Question_title":"sanic python - instead of flask",
        "Question_body":"Hey,\n\n\nWe are not able to use the mlflow server in our production API due to the fact it's using flask.\nWe have create our own API which is taking the pickel from mlflow and uses\u00a0sanic python for the API which is much faster.\n\n\nCan you please change your flask to\u00a0sanic python?\n\n\nIt will help us a lot.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1560415286000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":6.0,
        "Answer_body":"We definitely welcome contributions of this sort! If interested in contributing, it would also be great to point out the pros\/cons of sanic vs flask, and if there are backwards-compatibility issues that might arise from switching.\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/56a48ce0-6460-4f8f-91c8-a9feec6d5a1c%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/KjKwpgi8EjI",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-06-13T11:53:27",
                "Answer_body":"We definitely welcome contributions of this sort! If interested in contributing, it would also be great to point out the pros\/cons of sanic vs flask, and if there are backwards-compatibility issues that might arise from switching.\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/56a48ce0-6460-4f8f-91c8-a9feec6d5a1c%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"sanic python instead flask hei abl us server product api fact flask creat api take pickel usessan python api faster chang flask tosan python help lot",
        "Question_preprocessed_content":"sanic python instead flask hei abl us server product api fact flask creat api take pickel usessan python api faster chang flask tosan python help lot",
        "Question_gpt_summary_original":"The user is facing challenges in using the mlflow server in their production API because it is built on Flask. They have created their own API using Sanic Python, which is faster, but they are requesting the other party to change their Flask to Sanic Python to better integrate with their API.",
        "Question_gpt_summary":"user face challeng server product api built flask creat api sanic python faster request parti chang flask sanic python better integr api",
        "Answer_original_content":"definit welcom contribut sort interest contribut great point pro con sanic flask backward compat issu aris switch receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user ac afeecdac googlegroup com option visit http group googl com optout",
        "Answer_preprocessed_content":"definit welcom contribut sort interest contribut great point sanic flask issu aris switch receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that the other party can contribute by pointing out the pros\/cons of Sanic vs Flask and if there are any backward-compatibility issues that might arise from switching. However, no specific solution is provided to the challenge of using mlflow server in production API built on Flask.",
        "Answer_gpt_summary":"solut discuss suggest parti contribut point pro con sanic flask backward compat issu aris switch specif solut provid challeng server product api built flask"
    },
    {
        "Question_title":"Google Translate API",
        "Question_body":"Hi,I would like to use Google Translate API in plain javascript.As far as I understand from this guide, the supported languages are : ... and some additional languages :Does translate API is supported for Javascript as well? If so, where is the guide?Thanks in advance.",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1640861160000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":272.0,
        "Answer_body":"Here is a third-party solution you may find useful [1].\u00a0You may also report it to the Public Issue Tracker (PIT) [2]\u00a0 as well as feature request.\n\n[1]\u00a0 https:\/\/github.com\/topics\/javascript-translate\n\n[2]\u00a0https:\/\/cloud.google.com\/support\/docs\/issue-trackers\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translate-API\/td-p\/181620\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-10T14:09:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Here is a third-party solution you may find useful [1].\u00a0You may also report it to the Public Issue Tracker (PIT) [2]\u00a0 as well as feature request.\n\n[1]\u00a0 https:\/\/github.com\/topics\/javascript-translate\n\n[2]\u00a0https:\/\/cloud.google.com\/support\/docs\/issue-trackers\n\nView solution in original post"
            },
            {
                "Answer_creation_time":"2021-12-30T17:04:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"I could not find any official documentation\/tutorial on Google Translate API for plain JavaScript. It looks like the previously used \"Google Transliterate API\" which was officially deprecated as of May 26, 2011 had support for plain JavaScript [1].\nThe client libraries are currently available for seven popular programming languages \u2013 C#, Go, Java, Node.js, PHP, Python, and Ruby.\n\nHowever, the following un-official link [2] (July 20, 2021) may help.\n\n[1] https:\/\/developers.google.com\/transliterate\/v1\/getting_started\n[2] https:\/\/rapidapi.com\/blog\/google-translate-api-tutorial\/"
            },
            {
                "Answer_creation_time":"2021-12-31T05:36:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Thanks for the reply sf.\u00a0\n\nHow can it be that there is not documentation for one of the most popular programming languages such as Javascript?"
            },
            {
                "Answer_creation_time":"2022-01-10T14:09:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Here is a third-party solution you may find useful [1].\u00a0You may also report it to the Public Issue Tracker (PIT) [2]\u00a0 as well as feature request.\n\n[1]\u00a0 https:\/\/github.com\/topics\/javascript-translate\n\n[2]\u00a0https:\/\/cloud.google.com\/support\/docs\/issue-trackers"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1641823740000,
        "Question_original_content":"googl translat api like us googl translat api plain javascript far understand guid support languag addit languag translat api support javascript guid thank advanc",
        "Question_preprocessed_content":"googl translat api like us googl translat api plain far understand guid support languag addit languag translat api support javascript guid thank advanc",
        "Question_gpt_summary_original":"The user is seeking guidance on whether Google Translate API is supported for plain Javascript and where to find the guide. They are also seeking clarification on the supported languages.",
        "Question_gpt_summary":"user seek guidanc googl translat api support plain javascript guid seek clarif support languag",
        "Answer_original_content":"parti solut us report public issu tracker pit featur request http github com topic javascript translat http cloud googl com support doc issu tracker view solut origin post",
        "Answer_preprocessed_content":"solut us report public issu tracker featur request view solut origin post",
        "Answer_gpt_summary_original":"Solutions provided:\n- A third-party solution for Google Translate API in plain Javascript is suggested and a link to it is provided.\n- The user can report the issue to the Public Issue Tracker (PIT) and make a feature request.\n- No clarification on the supported languages is provided.",
        "Answer_gpt_summary":"solut provid parti solut googl translat api plain javascript suggest link provid user report issu public issu tracker pit featur request clarif support languag provid"
    },
    {
        "Question_title":"Can't find scoring.py when using PythonScriptStep() in Databricks",
        "Question_body":"<p>We are defining in Databricks a PythonScriptStep(). When using PythonScriptStep() within our pipeline script we can't find the scoring.py file.<\/p>\n<pre><code>scoring_step = PythonScriptStep(\n    name=&quot;Scoring_Step&quot;,\n    source_directory=os.getenv(&quot;DATABRICKS_NOTEBOOK_PATH&quot;, &quot;\/Users\/USER_NAME\/source_directory&quot;),\n    script_name=&quot;.\/scoring.py&quot;,\n    arguments=[&quot;--input_dataset&quot;, ds_consumption],\n    compute_target=pipeline_cluster,\n    runconfig=pipeline_run_config,\n    allow_reuse=False)\n<\/code><\/pre>\n<p>We getting the following error message:<\/p>\n<pre><code>Step [Scoring_Step]: script not found at: \/databricks\/driver\/scoring.py. Make sure to specify an appropriate source_directory on the Step or default_source_directory on the Pipeline.\n<\/code><\/pre>\n<p>For some reason Databricks is searching for the file in '\/databricks\/driver\/' instead of the folder we entered.<\/p>\n<p>There is also the way to use DatabricksStep() instead of PythonScriptStep(), but because of specific reasons we need to use the PythonSriptStep() class.<\/p>\n<p>Could anybody help us with this specific problem?<\/p>\n<p>Thank you very much for any help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655997421433,
        "Question_favorite_count":null,
        "Question_last_edit_time":1656039942703,
        "Question_score":0.0,
        "Question_view_count":68.0,
        "Answer_body":"<pre><code>scoring_step = PythonScriptStep(\n    name=&quot;Scoring_Step&quot;,\n    source_directory=os.getenv(&quot;DATABRICKS_NOTEBOOK_PATH&quot;, &quot;\/Users\/USER_NAME\/source_directory&quot;),\n    script_name=&quot;.\/scoring.py&quot;,\n    arguments=[&quot;--input_dataset&quot;, ds_consumption],\n    compute_target=pipeline_cluster,\n    runconfig=pipeline_run_config,\n    allow_reuse=False)\n<\/code><\/pre>\n<p>Change the above code block with below code block. It will resolve the error<\/p>\n<pre><code>data_ref = OutputFileDatasetConfig(\n    name='data_ref',\n    destination=(ds, '\/data')\n).as_upload()\n\n\ndata_prep_step = PythonScriptStep(\n    name='data_prep',\n    script_name='pipeline_steps\/data_prep.py',\n    source_directory='\/.',\n    arguments=[\n        '--main_path', main_ref,\n        '--data_ref_folder', data_ref\n                ],\n    inputs=[main_ref, data_ref],\n    outputs=[data_ref],\n    runconfig=arbitrary_run_config,\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p>Reference link for the <a href=\"https:\/\/scoring_step%20=%20PythonScriptStep(%20%20%20%20%20name=%22Scoring_Step%22,%20%20%20%20%20source_directory=os.getenv(%22DATABRICKS_NOTEBOOK_PATH%22,%20%22\/Users\/USER_NAME\/source_directory%22),%20%20%20%20%20script_name=%22.\/scoring.py%22,%20%20%20%20%20arguments=%5B%22--input_dataset%22,%20ds_consumption%5D,%20%20%20%20%20compute_target=pipeline_cluster,%20%20%20%20%20runconfig=pipeline_run_config,%20%20%20%20%20allow_reuse=False)\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72732616",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1656324774632,
        "Question_original_content":"score pythonscriptstep databrick defin databrick pythonscriptstep pythonscriptstep pipelin script score file score step pythonscriptstep score step sourc directori getenv databrick notebook path user user sourc directori script score argument input dataset consumpt comput target pipelin cluster runconfig pipelin run config allow reus fals get follow error messag step score step script databrick driver score sure specifi appropri sourc directori step default sourc directori pipelin reason databrick search file databrick driver instead folder enter wai us databricksstep instead pythonscriptstep specif reason need us pythonsriptstep class anybodi help specif problem thank help",
        "Question_preprocessed_content":"pythonscriptstep databrick defin databrick pythonscriptstep pythonscriptstep pipelin script file get follow error messag reason databrick search file instead folder enter wai us databricksstep instead pythonscriptstep specif reason need us pythonsriptstep class anybodi help specif problem thank help",
        "Question_gpt_summary_original":"The user is encountering a challenge when using PythonScriptStep() in Databricks to find the scoring.py file. The error message indicates that Databricks is searching for the file in '\/databricks\/driver\/' instead of the specified folder. The user needs to use PythonScriptStep() instead of DatabricksStep() due to specific reasons.",
        "Question_gpt_summary":"user encount challeng pythonscriptstep databrick score file error messag indic databrick search file databrick driver instead specifi folder user need us pythonscriptstep instead databricksstep specif reason",
        "Answer_original_content":"score step pythonscriptstep score step sourc directori getenv databrick notebook path user user sourc directori script score argument input dataset consumpt comput target pipelin cluster runconfig pipelin run config allow reus fals chang code block code block resolv error data ref outputfiledatasetconfig data ref destin data upload data prep step pythonscriptstep data prep script pipelin step data prep sourc directori argument main path main ref data ref folder data ref input main ref data ref output data ref runconfig arbitrari run config allow reus fals refer link document",
        "Answer_preprocessed_content":"chang code block code block resolv error refer link document",
        "Answer_gpt_summary_original":"Solution: The solution provided in the discussion is to change the code block with the new code block provided. The new code block includes the use of OutputFileDatasetConfig() and PythonScriptStep() to resolve the error. No other solutions are mentioned in the discussion.",
        "Answer_gpt_summary":"solut solut provid discuss chang code block new code block provid new code block includ us outputfiledatasetconfig pythonscriptstep resolv error solut mention discuss"
    },
    {
        "Question_title":"Why is my GCP Vertex pipeline api_endpoint not right?",
        "Question_body":"<p>My <code>API_ENDPOINT<\/code> is set to <code>europe-west1-aiplatform.googleapis.com<\/code>.<\/p>\n<p>I define a pipeline:<\/p>\n<pre><code>def pipeline(project: str = PROJECT_ID, region: str = REGION, api_endpoint: str = API_ENDPOINT):\n<\/code><\/pre>\n<p>when I run it:<\/p>\n<pre><code>job = aip.PipelineJob(\ndisplay_name=DISPLAY_NAME,\ntemplate_path=&quot;image classification_pipeline.json&quot;.replace(&quot; &quot;, &quot;_&quot;),)\njob.run()\n<\/code><\/pre>\n<p>it is always created in USandA:<\/p>\n<pre><code>INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. \nResource name: projects\/my_proj_id\/locations\/us-central1\/pipelineJobs\/automl-image-training-v2-anumber\n<\/code><\/pre>\n<p>How do I get it into Europe?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1641293452093,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":92.0,
        "Answer_body":"<p>The <code>location<\/code> parameter in the <code>aip.PipelineJob()<\/code> class can be used to specify in which region the pipeline will be deployed. Refer to this <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/aiplatform#class-googlecloudaiplatformpipelinejobdisplayname-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-templatepath-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-jobid-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-pipelineroot-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-parametervalues-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-any--none-enablecaching-optionalboolhttpspythonreadthedocsioenlatestlibraryfunctionshtmlbool--none-encryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-labels-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-credentials-optionalgoogleauthcredentialscredentialshttpsgoogleapisdevpythongoogle-authlatestreferencegoogleauthcredentialshtmlgoogleauthcredentialscredentials--none-project-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-location-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none\" rel=\"nofollow noreferrer\">documentation<\/a> for more information about the <code>PipelineJob()<\/code> method.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>REGION = &quot;europe-west1&quot;\n\njob = aip.PipelineJob(\n          display_name=DISPLAY_NAME,\n          template_path=&quot;image classification_pipeline.json&quot;.replace(&quot; &quot;, &quot;_&quot;),\n          location=REGION)\n\njob.run()\n<\/code><\/pre>\n<p>The above code will deploy a pipeline in the <code>europe-west1<\/code> region. The code returns the following output. The job is now deployed in the specified region.<\/p>\n<pre><code>INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\nINFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects\/&lt;project-id&gt;\/locations\/europe-west1\/pipelineJobs\/hello-world-pipeline\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1647760091087,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70577610",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1641305260580,
        "Question_original_content":"gcp vertex pipelin api endpoint right api endpoint set europ west aiplatform googleapi com defin pipelin def pipelin project str project region str region api endpoint str api endpoint run job aip pipelinejob displai displai templat path imag classif pipelin json replac job run creat usanda info googl cloud aiplatform pipelin job pipelinejob creat resourc project proj locat central pipelinejob automl imag train anumb europ",
        "Question_preprocessed_content":"gcp vertex pipelin right set defin pipelin run creat usanda europ",
        "Question_gpt_summary_original":"The user is facing a challenge with their GCP Vertex pipeline API endpoint, which is set to \"europe-west1-aiplatform.googleapis.com\". Despite this, when they run the pipeline, it is always created in the US. The user is seeking a solution to get the pipeline created in Europe.",
        "Question_gpt_summary":"user face challeng gcp vertex pipelin api endpoint set europ west aiplatform googleapi com despit run pipelin creat user seek solut pipelin creat europ",
        "Answer_original_content":"locat paramet aip pipelinejob class specifi region pipelin deploi refer document inform pipelinejob method region europ west job aip pipelinejob displai displai templat path imag classif pipelin json replac locat region job run code deploi pipelin europ west region code return follow output job deploi specifi region info googl cloud aiplatform pipelin job creat pipelinejob info googl cloud aiplatform pipelin job pipelinejob creat resourc project locat europ west pipelinejob hello world pipelin",
        "Answer_preprocessed_content":"paramet class specifi region pipelin deploi refer document inform method code deploi pipelin region code return follow output job deploi specifi region",
        "Answer_gpt_summary_original":"Solution: The user can use the `location` parameter in the `aip.PipelineJob()` class to specify the region where the pipeline will be deployed. By setting the `location` parameter to \"europe-west1\", the pipeline will be created in the Europe region. The documentation provides more information about the `PipelineJob()` method.",
        "Answer_gpt_summary":"solut user us locat paramet aip pipelinejob class specifi region pipelin deploi set locat paramet europ west pipelin creat europ region document provid inform pipelinejob method"
    },
    {
        "Question_title":"Resource Limit Exceeded- xgb.deploy",
        "Question_body":"<p>When I tried to deploy example from Amazon SageMaker<\/p>\n\n<pre><code>xgb_predictor = xgb.deploy(initial_instance_count=1,\n                           instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.<\/p>\n\n<p>Any idea how to fix this? <\/p>\n\n<p>Thank you<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1556178339113,
        "Question_favorite_count":null,
        "Question_last_edit_time":1556182433123,
        "Question_score":0.0,
        "Question_view_count":1093.0,
        "Answer_body":"<p>AWS is using soft limits to prevent customers from making a mistake that might cause them more money than they expected. When you are starting to use a new service, such as Amazon SageMaker, you will hit these soft limits and you need to ask specifically to raise them using the \"Support\" link on the top right side of your AWS management console. <\/p>\n\n<p>Here is a link to guide on how to do that: <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/manage-service-limits\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/manage-service-limits\/<\/a><\/p>\n\n<p>You will usually get the limit increased within a couple of days. In the meanwhile, you can choose a smaller instance (such as t2) that are often available.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55844248",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1556224263716,
        "Question_original_content":"resourc limit exceed xgb deploi tri deploi exampl xgb predictor xgb deploi initi instanc count instanc type xlarg resourcelimitexceed error occur resourcelimitexceed call createendpoint oper account level servic limit xlarg endpoint usag instanc current util instanc request delta instanc contact aw support request increas limit idea fix thank",
        "Question_preprocessed_content":"resourc limit exceed tri deploi exampl resourcelimitexceed error occur call createendpoint oper servic limit endpoint usag instanc current util instanc request delta instanc contact aw support request increas limit idea fix thank",
        "Question_gpt_summary_original":"The user encountered a Resource Limit Exceeded error when trying to deploy an example from Amazon SageMaker. The error message indicated that the account-level service limit for endpoint usage of 'ml.m4.xlarge' is 0 instances, with current utilization of 0 instances and a request delta of 1 instance. The user is seeking advice on how to fix this issue.",
        "Question_gpt_summary":"user encount resourc limit exceed error try deploi exampl error messag indic account level servic limit endpoint usag xlarg instanc current util instanc request delta instanc user seek advic fix issu",
        "Answer_original_content":"aw soft limit prevent custom make mistak caus monei expect start us new servic hit soft limit need ask specif rais support link right aw manag consol link guid http aw amazon com premiumsupport knowledg center manag servic limit usual limit increas coupl dai choos smaller instanc avail",
        "Answer_preprocessed_content":"aw soft limit prevent custom make mistak caus monei expect start us new servic hit soft limit need ask specif rais support link right aw manag consol link guid usual limit increas coupl dai choos smaller instanc avail",
        "Answer_gpt_summary_original":"Solution: The user needs to request an increase in the account-level service limit for endpoint usage of 'ml.m4.xlarge' by using the \"Support\" link on the top right side of their AWS management console. They can refer to the guide provided in the discussion for instructions on how to do that. In the meantime, they can choose a smaller instance (such as t2) that are often available.",
        "Answer_gpt_summary":"solut user need request increas account level servic limit endpoint usag xlarg support link right aw manag consol refer guid provid discuss instruct meantim choos smaller instanc avail"
    },
    {
        "Question_title":"Push image error: Get \"https:\/\/https\/v2\/\": dial tcp: lookup https on SOME-IP: no such host",
        "Question_body":"This is question\/issue from Polyaxon Slack that was resolved. I am posting it for visibility if someone stumbles on the same issue with answer down below.\n\nGiven setup:\nDocker-registry provider: Amazon Elastic Container Registry (ECR)\nPolyaxon version: 1.7.3 CE\nDeployed with Kubernetes on AWS\nAnd Kaniko configuration:\nconnections:\n  - name: docker-registry\n    kind: registry\n    description: \"aws docker repository\"\n    schema:\n      url: https:\/\/ID.dkr.ecr.SOME-REGION.amazonaws.com\n secret:\n      name: aws-secret\n      mountPath: \/root\/.aws\/\n    configMap:\n      name: docker-config\n      mountPath: \/kaniko\/.docker\nAnd polyaxonfile for the build:\nversion: 1.1\nkind: operation\nname: build\nparams:\n  destination:\n    connection: docker-registry\n    value: polyaxon\nrunPatch:\n  init:\n  - dockerfile:\n      image: \"tensorflow\/tensorflow:2.0.1-py3\"\n      run:\n      - 'pip3 install --no-cache-dir -U polyaxon[\"polyboard\",\"polytune\"]'\n      langEnv: 'en_US.UTF-8'\nhubRef: kaniko\nThen raised error:\nConditions:\nTYPE    STATUS    REASON                MESSAGE                                      LAST_UPDATE_TIME    LAST_TRANSITION_TIME\n------  --------  --------------------  -------------------------------------------  ------------------  ----------------------\nfailed  True      BackoffLimitExceeded  Job has reached the specified backoff limit  a few seconds ago   a few seconds ago\n2021-04-19 08:18:08.805683+02:00 | error checking push permissions -- make sure you entered the correct tag name, and that you are authenticated correctly,\n and try again: checking push permission for \"https:\/\/ID.dkr.ecr.REGION.amazonaws.com\/SOME-NAME:SOME-TAG\": creating push check transport for https: failed: Get \"https:\/\/https\/v2\/\": dial tcp: lookup https on SOME-IP: no such host",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619178414000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Answer_body":"Resolution:\n\nThis was error on the ECR side. The repo SOME-NAME:SOME-TAG was not existing inside docker registry. Creating SOME-NAME in ECR allowed successful build and push.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1292",
        "Tool":"Polyaxon",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-23T12:58:58Z",
                "Answer_score":1,
                "Answer_body":"Resolution:\n\nThis was error on the ECR side. The repo SOME-NAME:SOME-TAG was not existing inside docker registry. Creating SOME-NAME in ECR allowed successful build and push."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0,
        "Question_closed_time":null,
        "Question_original_content":"push imag error http http dial tcp lookup http host question issu slack resolv post visibl stumbl issu answer given setup docker registri provid amazon elast contain registri ecr version deploi kubernet aw kaniko configur connect docker registri kind registri descript aw docker repositori schema url http dkr ecr region amazonaw com secret aw secret mountpath root aw configmap docker config mountpath kaniko docker file build version kind oper build param destin connect docker registri valu runpatch init dockerfil imag tensorflow tensorflow run pip instal cach dir polyboard polytun langenv utf hubref kaniko rais error condit type statu reason messag updat time transit time fail true backofflimitexceed job reach specifi backoff limit second ago second ago error check push permiss sure enter correct tag authent correctli try check push permiss http dkr ecr region amazonaw com tag creat push check transport http fail http http dial tcp lookup http host",
        "Question_preprocessed_content":"push imag error dial tcp lookup http host slack resolv post visibl stumbl issu answer given setup provid amazon elast contain registri version deploi kubernet aw kaniko configur connect kind registri descript aw docker repositori schema url secret mountpath configmap mountpath file build version kind oper build param destin connect valu runpatch init dockerfil imag run pip instal langenv hubref kaniko rais error condit type statu reason messag fail true backofflimitexceed job reach specifi backoff limit second ago second ago error check push permiss sure enter correct tag authent correctli try check push permiss creat push check transport http fail dial tcp lookup http host",
        "Question_gpt_summary_original":"The user encountered an error while trying to push an image to a Docker registry provider, Amazon Elastic Container Registry (ECR), using Polyaxon version 1.7.3 CE deployed with Kubernetes on AWS. The error message indicated that there was an issue with the push permissions and that the push check transport for https failed due to a lookup error on SOME-IP.",
        "Question_gpt_summary":"user encount error try push imag docker registri provid amazon elast contain registri ecr version deploi kubernet aw error messag indic issu push permiss push check transport http fail lookup error",
        "Answer_original_content":"resolut error ecr repo tag exist insid docker registri creat ecr allow success build push",
        "Answer_preprocessed_content":"resolut error ecr repo exist insid docker registri creat ecr allow success build push",
        "Answer_gpt_summary_original":"Solution: The issue was resolved by creating the missing repository in Amazon Elastic Container Registry (ECR). Once the repository was created, the user was able to successfully build and push the image to ECR using Polyaxon version 1.7.3 CE deployed with Kubernetes on AWS.",
        "Answer_gpt_summary":"solut issu resolv creat miss repositori amazon elast contain registri ecr repositori creat user abl successfulli build push imag ecr version deploi kubernet aw"
    },
    {
        "Question_title":"How can I quickly debug a SageMaker training script?",
        "Question_body":"<p>When running an ML training job in Amazon SageMaker, the training script is \"deployed\" and given an ML training instance, which takes about 10 minutes to spin up and get the data it needs. <\/p>\n\n<p>I can only get one error message from the training job, then it dies and the instance is killed along with it. <\/p>\n\n<p>After I make a change to the training script to fix it, I need to deploy and run it which takes another 10 minutes or so.<\/p>\n\n<p>How can I accomplish this faster, or keep the training instance running?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1548272148563,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":1902.0,
        "Answer_body":"<p>It seems that you are running a training job using one of the SageMaker frameworks. Given that, you can use the \"local mode\" feature of SageMaker, which will run your training job (specifically the container) locally in your notebook instance. That way, you can iterate on your script until it works. Then you can move on to the remote training cluster to train the model against the whole dataset if needed. To use local mode, you just set the instance type to \"local\". More details about local mode can be found at <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview<\/a> and the blog post: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54334462",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1548282026927,
        "Question_original_content":"quickli debug train script run train job train script deploi given train instanc take minut spin data need error messag train job di instanc kill chang train script fix need deploi run take minut accomplish faster train instanc run",
        "Question_preprocessed_content":"quickli debug train script run train job train script deploi given train instanc take minut spin data need error messag train job di instanc kill chang train script fix need deploi run take minut accomplish faster train instanc run",
        "Question_gpt_summary_original":"The user is facing challenges in quickly debugging a SageMaker training script as the training instance takes about 10 minutes to spin up and get the data it needs. The user can only get one error message from the training job, and after making a change to the script, they need to redeploy and run it, which takes another 10 minutes. The user is seeking ways to accomplish this faster or keep the training instance running.",
        "Question_gpt_summary":"user face challeng quickli debug train script train instanc take minut spin data need user error messag train job make chang script need redeploi run take minut user seek wai accomplish faster train instanc run",
        "Answer_original_content":"run train job framework given us local mode featur run train job specif contain local notebook instanc wai iter script work remot train cluster train model dataset need us local mode set instanc type local detail local mode http github com aw python sdk python sdk overview blog post http aw amazon com blog machin learn us amazon local mode train notebook instanc",
        "Answer_preprocessed_content":"run train job framework given us local mode featur run train job local notebook instanc wai iter script work remot train cluster train model dataset need us local mode set instanc type local detail local mode blog post",
        "Answer_gpt_summary_original":"Solution: The user can use the \"local mode\" feature of SageMaker, which will run the training job locally in the notebook instance. This way, the user can iterate on the script until it works and then move on to the remote training cluster to train the model against the whole dataset if needed. To use local mode, the user just needs to set the instance type to \"local\". More details about local mode can be found at https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview and the blog post: https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/.",
        "Answer_gpt_summary":"solut user us local mode featur run train job local notebook instanc wai user iter script work remot train cluster train model dataset need us local mode user need set instanc type local detail local mode http github com aw python sdk python sdk overview blog post http aw amazon com blog machin learn us amazon local mode train notebook instanc"
    },
    {
        "Question_title":"Use tensorboard with object detection API in sagemaker",
        "Question_body":"<p>with <a href=\"https:\/\/github.com\/svpino\/tensorflow-object-detection-sagemaker\" rel=\"nofollow noreferrer\">this<\/a> I successfully created a training job on sagemaker using the Tensorflow Object Detection API in a docker container. Now I'd like to monitor the training job using sagemaker, but cannot find anything explaining how to do it. I don't use a sagemaker notebook.\nI think I can do it by saving the logs into a S3 bucket and point there a local tensorboard instance .. but don't know how to tell the tensorflow object detection API where to save the logs (is there any command line argument for this ?).\nSomething like <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/keras_script_mode_pipe_mode_horovod\/tensorflow_keras_CIFAR10.ipynb\" rel=\"nofollow noreferrer\">this<\/a>, but the script <code>generate_tensorboard_command.py<\/code> fails because my training job don't have the <code>sagemaker_submit_directory<\/code> parameter..<\/p>\n<p>The fact is when I start the training job nothing is created on my s3 until the job finish and upload everything. There should be a way tell tensorflow where to save the logs (s3) during the training, hopefully without modifying the API source code..<\/p>\n<p><strong>Edit<\/strong><\/p>\n<p>I can finally make it works with the accepted solution (tensorflow natively supports read\/write to s3), there are however additional steps to do:<\/p>\n<ol>\n<li>Disable network isolation in the training job configuration<\/li>\n<li>Provide credentials to the docker image to write to S3 bucket<\/li>\n<\/ol>\n<p>The only thing is that Tensorflow continuously polls filesystem (i.e. looking for an updated model in serving mode) and this cause useless requests to S3, that you will have to pay (together with a buch of errors in the console). I opened a new question <a href=\"https:\/\/stackoverflow.com\/q\/64969198\/4267439\">here<\/a> for this. At least it works.<\/p>\n<p><strong>Edit 2<\/strong><\/p>\n<p>I was wrong, TF just write logs, is not polling so it's an expected behavior and the extra costs are minimal.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590408759343,
        "Question_favorite_count":null,
        "Question_last_edit_time":1606323198012,
        "Question_score":2.0,
        "Question_view_count":311.0,
        "Answer_body":"<p>Looking through the example you posted, it looks as though the <code>model_dir<\/code> passed to the TensorFlow Object Detection package is configured to <code>\/opt\/ml\/model<\/code>:<\/p>\n<pre><code># These are the paths to where SageMaker mounts interesting things in your container.\nprefix = '\/opt\/ml\/'\ninput_path = os.path.join(prefix, 'input\/data')\noutput_path = os.path.join(prefix, 'output')\nmodel_path = os.path.join(prefix, 'model')\nparam_path = os.path.join(prefix, 'input\/config\/hyperparameters.json')\n<\/code><\/pre>\n<p>During the training process, tensorboard logs will be written to <code>\/opt\/ml\/model<\/code>, and then uploaded to s3 as a final model artifact AFTER training: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html<\/a>.<\/p>\n<p>You <em>might<\/em> be able to side-step the SageMaker artifact upload step and point the <code>model_dir<\/code> of TensorFlow Object Detection API directly at an s3 location during training:<\/p>\n<pre><code>model_path = &quot;s3:\/\/your-bucket\/path\/here\n<\/code><\/pre>\n<p>This means that the TensorFlow library within the SageMaker job is directly writing to S3 instead of the filesystem inside of it's container. Assuming the underlying TensorFlow Object Detection code can write directly to S3 (something you'll have to verify), you should be able to see the tensorboard logs and checkpoints there in realtime.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62002183",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1594137982056,
        "Question_original_content":"us tensorboard object detect api successfulli creat train job tensorflow object detect api docker contain like monitor train job explain us notebook think save log bucket point local tensorboard instanc know tell tensorflow object detect api save log command line argument like script gener tensorboard command fail train job submit directori paramet fact start train job creat job finish upload wai tell tensorflow save log train hopefulli modifi api sourc code edit final work accept solut tensorflow nativ support read write addit step disabl network isol train job configur provid credenti docker imag write bucket thing tensorflow continu poll filesystem look updat model serv mode caus useless request pai buch error consol open new question work edit wrong write log poll expect behavior extra cost minim",
        "Question_preprocessed_content":"us tensorboard object detect api successfulli creat train job tensorflow object detect api docker contain like monitor train job explain us notebook think save log bucket point local tensorboard instanc know tell tensorflow object detect api save log like script fail train job fact start train job creat job finish upload wai tell tensorflow save log train hopefulli modifi api sourc edit final work accept solut addit step disabl network isol train job configur provid credenti docker imag write bucket thing tensorflow continu poll filesystem caus useless request pai open new question work edit wrong write log poll expect behavior extra cost minim",
        "Question_gpt_summary_original":"The user successfully created a training job on sagemaker using the Tensorflow Object Detection API in a docker container, but is facing challenges in monitoring the training job using sagemaker. The user is unable to find any information on how to do it and is considering saving the logs into an S3 bucket and pointing to a local tensorboard instance. However, the user does not know how to tell the Tensorflow Object Detection API where to save the logs. The user has tried using a script but it failed because the training job does not have the required parameter. The user was able to make it work with the accepted solution, but had to disable network isolation in the training job configuration and provide credentials to the docker image to write to the S3 bucket. The user also discovered that Tensorflow continuously writes logs and does not poll the filesystem, which results in minimal extra costs.",
        "Question_gpt_summary":"user successfulli creat train job tensorflow object detect api docker contain face challeng monitor train job user unabl inform consid save log bucket point local tensorboard instanc user know tell tensorflow object detect api save log user tri script fail train job requir paramet user abl work accept solut disabl network isol train job configur provid credenti docker imag write bucket user discov tensorflow continu write log poll filesystem result minim extra cost",
        "Answer_original_content":"look exampl post look model dir pass tensorflow object detect packag configur opt model path mount interest thing contain prefix opt input path path join prefix input data output path path join prefix output model path path join prefix model param path path join prefix input config hyperparamet json train process tensorboard log written opt model upload final model artifact train http doc aw amazon com latest algorithm train algo envvari html abl step artifact upload step point model dir tensorflow object detect api directli locat train model path bucket path mean tensorflow librari job directli write instead filesystem insid contain assum underli tensorflow object detect code write directli verifi abl tensorboard log checkpoint realtim",
        "Answer_preprocessed_content":"look exampl post look pass tensorflow object detect packag configur train process tensorboard log written upload final model artifact train abl artifact upload step point tensorflow object detect api directli locat train mean tensorflow librari job directli write instead filesystem insid contain assum underli tensorflow object detect code write directli abl tensorboard log checkpoint realtim",
        "Answer_gpt_summary_original":"Solution: The user can configure the `model_dir` passed to the TensorFlow Object Detection package to `\/opt\/ml\/model`, where tensorboard logs will be written during the training process and then uploaded to S3 as a final model artifact after training. Alternatively, the user can point the `model_dir` of TensorFlow Object Detection API directly at an S3 location during training by setting `model_path = \"s3:\/\/your-bucket\/path\/here\"`. This means that the TensorFlow library within the SageMaker job is directly writing to S3 instead of the filesystem inside of its container. Assuming the underlying TensorFlow Object Detection code can write directly to S3, the user should be able to see the tensorboard logs and checkpoints there in real-time.",
        "Answer_gpt_summary":"solut user configur model dir pass tensorflow object detect packag opt model tensorboard log written train process upload final model artifact train altern user point model dir tensorflow object detect api directli locat train set model path bucket path mean tensorflow librari job directli write instead filesystem insid contain assum underli tensorflow object detect code write directli user abl tensorboard log checkpoint real time"
    },
    {
        "Question_title":"Azure ML's web service asking for label?",
        "Question_body":"<p>I built a linear regression algorithm in Azure ML. On the &quot;Score Model&quot; module I can actually see the predictions and the rest of the features. However, when I deploy this project as a web service, the service is expecting the actual label of the data (e.g. I'm trying to predict a house's price and it asks me for the price of the house to make the prediction), which doesn't make any sense to me... What am I doing wrong? On the &quot;Train Model&quot; module I set that the label column is the HousePrice, which is what I'm trying to predict...<\/p>\n<p>This is my model:\n<a href=\"https:\/\/i.stack.imgur.com\/kI8qu.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kI8qu.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I tried leaving that field blank but the prediction returns null...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1476382562787,
        "Question_favorite_count":null,
        "Question_last_edit_time":1592644375060,
        "Question_score":4.0,
        "Question_view_count":1012.0,
        "Answer_body":"<p>The input schema (names\/types of required input) based on the location in the graph where you attach the \"Web Service Input\" module. To get the schema you want, you will need to find -- or if necessary, create -- a place in the experiment where the data has the column names\/types you desire.<\/p>\n\n<p>Consider this simple example experiment that predicts whether a field called \"income\" will be above or below $50k\/year:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/nWaN2.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/nWaN2.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When we click \"Set up web service\", the following graph is automatically generated:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/NMMpV.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/NMMpV.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Since the input dataset and \"Web service input\" modules are connected to the same port, the web service schema will perfectly match the schema of the input dataset. This is unfortunate because the input dataset contains a column called \"income\", which is what our web service is supposed to predict -- this is equivalent to the problem that you are having.<\/p>\n\n<p>To get around it, we need to create a place in our experiment graph where we've dropped the unneeded \"income\" field from the input dataset, and attach the \"Web service input\" module there:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WPeSB.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/WPeSB.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>With this arrangement, the web service only requests the features actually needed to score the model. I'm sure you can use a similar method to create a predictive experiment with whatever input schema you need for your own work.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40028165",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1476730527012,
        "Question_original_content":"web servic ask label built linear regress algorithm score model modul actual predict rest featur deploi project web servic servic expect actual label data try predict hous price ask price hous predict sens wrong train model modul set label column housepric try predict model tri leav field blank predict return null",
        "Question_preprocessed_content":"web servic ask label built linear regress algorithm score model modul actual predict rest featur deploi project web servic servic expect actual label data sens wrong train model modul set label column housepric try model tri leav field blank predict return",
        "Question_gpt_summary_original":"The user built a linear regression algorithm in Azure ML and is facing challenges while deploying the project as a web service. The web service is asking for the actual label of the data, which is not making sense to the user. The user has set the label column as HousePrice in the Train Model module, but leaving the field blank results in null predictions.",
        "Question_gpt_summary":"user built linear regress algorithm face challeng deploi project web servic web servic ask actual label data make sens user user set label column housepric train model modul leav field blank result null predict",
        "Answer_original_content":"input schema name type requir input base locat graph attach web servic input modul schema want need necessari creat place experi data column name type desir consid simpl exampl experi predict field call incom year click set web servic follow graph automat gener input dataset web servic input modul connect port web servic schema perfectli match schema input dataset unfortun input dataset contain column call incom web servic suppos predict equival problem have need creat place experi graph drop unneed incom field input dataset attach web servic input modul arrang web servic request featur actual need score model sure us similar method creat predict experi input schema need work",
        "Answer_preprocessed_content":"input schema base locat graph attach web servic input modul schema want need necessari creat place experi data column desir consid simpl exampl experi predict field call incom click set web servic follow graph automat gener input dataset web servic input modul connect port web servic schema perfectli match schema input dataset unfortun input dataset contain column call incom web servic suppos predict equival problem have need creat place experi graph drop unneed incom field input dataset attach web servic input modul arrang web servic request featur actual need score model sure us similar method creat predict experi input schema need work",
        "Answer_gpt_summary_original":"Solution: The user needs to create a place in the experiment graph where the unneeded column is dropped from the input dataset and attach the \"Web service input\" module there. This will ensure that the web service only requests the features actually needed to score the model.",
        "Answer_gpt_summary":"solut user need creat place experi graph unneed column drop input dataset attach web servic input modul ensur web servic request featur actual need score model"
    },
    {
        "Question_title":"Azure ML Studio error while testing real-time endpoint - list index out of range",
        "Question_body":"I am new to the Azure ML Studio and just deployed the bike-rental regression model. When I tried to test it using the built in test tool in the studio, I am getting the attached error. Similar results running the Python code as well. Can someone please help me?",
        "Question_answer_count":5,
        "Question_comment_count":4,
        "Question_creation_time":1645577589217,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@KumarPriya-6121 Thanks for the question. It's known issue and the product team working on the fix to change in the UI.\n\nWorkaround: As shown below please set the GlobalParameters flag to 1.0 or a float number or remove it.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/746784\/azure-ml-studio-error-while-testing-real-time-endp.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-24T09:35:29.74Z",
                "Answer_score":3,
                "Answer_body":"@KumarPriya-6121 Thanks for the question. It's known issue and the product team working on the fix to change in the UI.\n\nWorkaround: As shown below please set the GlobalParameters flag to 1.0 or a float number or remove it.",
                "Answer_comment_count":3,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-03-01T07:41:18.553Z",
                "Answer_score":0,
                "Answer_body":"Change GlobalParameters flag value from integer 1 to decimal 1.0 to make it works.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-16T03:22:52.183Z",
                "Answer_score":0,
                "Answer_body":"What about when making GET requests to the score endpoint or doing a POST? Is there a way of setting the GlobalParameters flag there? I get this just by doing an authorized GET to my endpoint from postman.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-25T21:41:46.547Z",
                "Answer_score":2,
                "Answer_body":"@FernandoJosRibeiroJnior-4000 I had to modify the notebook as follows to get it to work:\n\n x = [[1,1,2022,1,0,6,0,2,0.344167,0.363625,0.805833,0.160446], \n    \n     [2,1,2022,1,0,0,0,2,0.363478,0.353739,0.696087,0.248539], \n    \n     [3,1,2022,1,0,1,1,1,0.196364,0.189405,0.437273,0.248309], \n    \n     [4,1,2022,1,0,2,1,1,0.2,0.212122,0.590435,0.160296], \n    \n     [5,1,2022,1,0,3,1,1,0.226957,0.22927,0.436957,0.1869]] \n    \n columns = [\"day\",\"mnth\", \"year\", \"season\", \"holiday\", \"weekday\", \"workingday\", \"weathersit\", \"temp\", \"atemp\", \"hum\", \"windspeed\"]\n data = [{columns[i]: v for i, v in enumerate(x_i)} for x_i in x]\n    \n request = {\n   \"Inputs\": {\n     \"data\": data\n   },\n   \"GlobalParameters\": 0.0 # not sure if this value matters\n }\n    \n input_json = json.dumps(request)",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-10T06:25:59.75Z",
                "Answer_score":0,
                "Answer_body":"Azure ML Studio error while testing real-time endpoint - Input data are inconsistent with schema\n\n\n\n\n\nKindly help me to resolve this issue.\n\n\n\n\nThanks in Advance\n\nRegards,\nPrashanth K",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1645695329740,
        "Question_original_content":"studio error test real time endpoint list index rang new studio deploi bike rental regress model tri test built test tool studio get attach error similar result run python code help",
        "Question_preprocessed_content":"studio error test endpoint list index rang new studio deploi regress model tri test built test tool studio get attach error similar result run python code help",
        "Question_gpt_summary_original":"The user encountered an error while testing a bike-rental regression model in Azure ML Studio, both through the built-in test tool and Python code. The error message indicates a \"list index out of range\" issue. The user is seeking assistance to resolve the problem.",
        "Question_gpt_summary":"user encount error test bike rental regress model studio built test tool python code error messag indic list index rang issu user seek assist resolv problem",
        "Answer_original_content":"kumarpriya thank question known issu product team work fix chang workaround shown set globalparamet flag float number remov",
        "Answer_preprocessed_content":"thank question known issu product team work fix chang workaround shown set globalparamet flag float number remov",
        "Answer_gpt_summary_original":"Solution: The discussion provides a workaround solution to the \"list index out of range\" error encountered while testing a bike-rental regression model in Azure ML Studio. The solution involves setting the GlobalParameters flag to 1.0 or a float number or removing it. The product team is also working on a fix to change in the UI.",
        "Answer_gpt_summary":"solut discuss provid workaround solut list index rang error encount test bike rental regress model studio solut involv set globalparamet flag float number remov product team work fix chang"
    },
    {
        "Question_title":"Is there IO functionality to store trained models in kedro?",
        "Question_body":"<p>In the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.html#\" rel=\"nofollow noreferrer\">IO section of the kedro API docs<\/a> I could not find functionality w.r.t. storing trained models (e.g. <code>.pkl<\/code>, <code>.joblib<\/code>, <code>ONNX<\/code>, <code>PMML<\/code>)? Have I missed something?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1589224779627,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":795.0,
        "Answer_body":"<p>There is the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.PickleLocalDataSet.html#kedro.io.PickleLocalDataSet\" rel=\"nofollow noreferrer\"><code>pickle<\/code><\/a> dataset in <code>kedro.io<\/code>, that you can use to save trained models and\/or anything you want to pickle and is serialisable (models being a common object). It accepts a <code>backend<\/code> that defaults to <code>pickle<\/code> but can be set to <code>joblib<\/code> if you want to use <code>joblib<\/code> instead.<\/p>\n\n<p>I'm just going to quickly note that Kedro is moving to <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.extras.datasets.html\" rel=\"nofollow noreferrer\"><code>kedro.extras.datasets<\/code><\/a> for its datasets and moving away from having non-core datasets in <code>kedro.io<\/code>. You might want to look at <code>kedro.extras.datasets<\/code> and in Kedro 0.16 onwards <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/kedro.extras.datasets.pickle.PickleDataSet.html#kedro.extras.datasets.pickle.PickleDataSet\" rel=\"nofollow noreferrer\"><code>pickle.PickleDataSet<\/code><\/a> with <code>joblib<\/code> support.<\/p>\n\n<p>The Kedro <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/02_tutorial_template.html\" rel=\"nofollow noreferrer\"><code>spaceflights<\/code><\/a> tutorial in the documentation actually saves the trained linear regression model using the <code>pickle<\/code> dataset if you want to see an example of it. The relevant section is <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/04_create_pipelines.html#working-with-multiple-pipelines\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1589277661208,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61737613",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1589225919163,
        "Question_original_content":"function store train model section api doc function store train model pkl joblib onnx pmml miss",
        "Question_preprocessed_content":"function store train model section api doc function store train model miss",
        "Question_gpt_summary_original":"The user is facing a challenge in finding IO functionality to store trained models in kedro and is seeking clarification on whether such functionality exists.",
        "Question_gpt_summary":"user face challeng find function store train model seek clarif function exist",
        "Answer_original_content":"pickl dataset us save train model want pickl serialis model common object accept backend default pickl set joblib want us joblib instead go quickli note move extra dataset dataset move awai have non core dataset want look extra dataset onward pickl pickledataset joblib support spaceflight tutori document actual save train linear regress model pickl dataset want exampl relev section",
        "Answer_preprocessed_content":"dataset us save train model want pickl serialis accept default set want us instead go quickli note move dataset move awai have dataset want look onward support tutori document actual save train linear regress model dataset want exampl relev section",
        "Answer_gpt_summary_original":"Solution:\n- The user can use the `pickle` dataset in `kedro.io` to save trained models and anything that can be pickled and serialized.\n- The `pickle` dataset accepts a `backend` that defaults to `pickle` but can be set to `joblib` if the user wants to use `joblib` instead.\n- The user can also look at `kedro.extras.datasets` and use `pickle.PickleDataSet` with `joblib` support in Kedro 0.16 onwards.\n- The Kedro `spaceflights` tutorial in the documentation provides an example of saving a trained linear regression model using the `pickle` dataset.",
        "Answer_gpt_summary":"solut user us pickl dataset save train model pickl serial pickl dataset accept backend default pickl set joblib user want us joblib instead user look extra dataset us pickl pickledataset joblib support onward spaceflight tutori document provid exampl save train linear regress model pickl dataset"
    },
    {
        "Question_title":"Is there a way to pass arguments to multiple jobs in optuna?",
        "Question_body":"<p>I am trying to use optuna for searching hyper parameter spaces.<\/p>\n\n<p>In one particular scenario I train a model on a machine with a few GPUs.\nThe model and batch size allows me to run 1 training per 1 GPU.\nSo, ideally I would like to let optuna spread all trials across the available GPUs\nso that there is always 1 trial running on each GPU.<\/p>\n\n<p>In the <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#how-can-i-use-two-gpus-for-evaluating-two-trials-simultaneously\" rel=\"nofollow noreferrer\">docs<\/a> it says, I should just start one process per GPU in a separate terminal like:<\/p>\n\n<pre><code>CUDA_VISIBLE_DEVICES=0 optuna study optimize foo.py objective --study foo --storage sqlite:\/\/\/example.db\n<\/code><\/pre>\n\n<p>I want to avoid that because the whole hyper parameter search continues in multiple rounds after that. I don't want to always manually start a process per GPU, check when all are finished, then start the next round.<\/p>\n\n<p>I saw <code>study.optimize<\/code> has a <code>n_jobs<\/code> argument.\nAt first glance this seems to be perfect.\n<em>E.g.<\/em> I could do this:<\/p>\n\n<pre><code>import optuna\n\ndef objective(trial):\n    # the actual model would be trained here\n    # the trainer here would need to know which GPU\n    # it should be using\n    best_val_loss = trainer(**trial.params)\n    return best_val_loss\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100, n_jobs=8)\n<\/code><\/pre>\n\n<p>This starts multiple threads each starting a training.\nHowever, the trainer within <code>objective<\/code> somehow needs to know which GPU it should be using.\nIs there a trick to accomplish that?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1589323176827,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":2684.0,
        "Answer_body":"<p>After a few mental breakdowns I figured out that I can do what I want using a <code>multiprocessing.Queue<\/code>. To get it into the objective function I need to define it as a lambda function or as a class (I guess partial also works). <em>E.g.<\/em><\/p>\n\n<pre><code>from contextlib import contextmanager\nimport multiprocessing\nN_GPUS = 2\n\nclass GpuQueue:\n\n    def __init__(self):\n        self.queue = multiprocessing.Manager().Queue()\n        all_idxs = list(range(N_GPUS)) if N_GPUS &gt; 0 else [None]\n        for idx in all_idxs:\n            self.queue.put(idx)\n\n    @contextmanager\n    def one_gpu_per_process(self):\n        current_idx = self.queue.get()\n        yield current_idx\n        self.queue.put(current_idx)\n\n\nclass Objective:\n\n    def __init__(self, gpu_queue: GpuQueue):\n        self.gpu_queue = gpu_queue\n\n    def __call__(self, trial: Trial):\n        with self.gpu_queue.one_gpu_per_process() as gpu_i:\n            best_val_loss = trainer(**trial.params, gpu=gpu_i)\n            return best_val_loss\n\nif __name__ == '__main__':\n    study = optuna.create_study()\n    study.optimize(Objective(GpuQueue()), n_trials=100, n_jobs=8)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61763206",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1589464897583,
        "Question_original_content":"wai pass argument multipl job try us search hyper paramet space particular scenario train model machin gpu model batch size allow run train gpu ideal like let spread trial avail gpu trial run gpu doc sai start process gpu separ termin like cuda visibl devic studi optim foo object studi foo storag sqlite exampl want avoid hyper paramet search continu multipl round want manual start process gpu check finish start round saw studi optim job argument glanc perfect import def object trial actual model train trainer need know gpu best val loss trainer trial param return best val loss studi creat studi studi optim object trial job start multipl thread start train trainer object need know gpu trick accomplish",
        "Question_preprocessed_content":"wai pass argument multipl job try us search hyper paramet space particular scenario train model machin gpu model batch size allow run train gpu ideal like let spread trial avail gpu trial run gpu doc sai start process gpu separ termin like want avoid hyper paramet search continu multipl round want manual start process gpu check finish start round saw argument glanc perfect start multipl thread start train trainer need know gpu trick accomplish",
        "Question_gpt_summary_original":"The user is trying to use optuna for hyperparameter search and wants to spread all trials across available GPUs. The documentation suggests starting one process per GPU in a separate terminal, but the user wants to avoid that. The user has tried using the `n_jobs` argument in `study.optimize` to start multiple threads, but the trainer within `objective` needs to know which GPU to use. The user is seeking a solution to this challenge.",
        "Question_gpt_summary":"user try us hyperparamet search want spread trial avail gpu document suggest start process gpu separ termin user want avoid user tri job argument studi optim start multipl thread trainer object need know gpu us user seek solut challeng",
        "Answer_original_content":"mental breakdown figur want multiprocess queue object function need defin lambda function class guess partial work contextlib import contextmanag import multiprocess gpu class gpuqueu def init self self queue multiprocess manag queue idx list rang gpu gpu idx idx self queue idx contextmanag def gpu process self current idx self queue yield current idx self queue current idx class object def init self gpu queue gpuqueu self gpu queue gpu queue def self trial trial self gpu queue gpu process gpu best val loss trainer trial param gpu gpu return best val loss main studi creat studi studi optim object gpuqueu trial job",
        "Answer_preprocessed_content":"mental breakdown figur want object function need defin lambda function class",
        "Answer_gpt_summary_original":"Solution: The user can use a `multiprocessing.Queue` to spread all trials across available GPUs. To get it into the objective function, the user needs to define it as a lambda function or as a class. The user can define a `GpuQueue` class that initializes a queue with available GPU indices and uses a context manager to assign one GPU per process. The `Objective` class can then use this `GpuQueue` class to assign a GPU to each trial. The user can then use `study.optimize` with the `n_jobs` argument to start multiple threads.",
        "Answer_gpt_summary":"solut user us multiprocess queue spread trial avail gpu object function user need defin lambda function class user defin gpuqueu class initi queue avail gpu indic us context manag assign gpu process object class us gpuqueu class assign gpu trial user us studi optim job argument start multipl thread"
    },
    {
        "Question_title":"How to assign users in SageMaker Studio?",
        "Question_body":"<p>I have successfully created SageMaker Studio- Status in service. However, it asks me to asks to assign users to it and I don't have any listed. Are these users from my IAM (I have many) or my organization (I have a couple).<\/p>\n\n<p>Where am I supposed to find these users?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1576591296847,
        "Question_favorite_count":null,
        "Question_last_edit_time":1576641179672,
        "Question_score":1.0,
        "Question_view_count":1029.0,
        "Answer_body":"<hr \/>\n<p>Did you setup SageMaker Studio to use AWS SSO or IAM for the authentication method?<\/p>\n<p>From what I have gathered, the SageMaker Studio users, when setup using IAM for the authentication method are not actually users. They just provide partitions within Studio for different work environments. You can then control access to these partitions using IAM policies for your IAM users \/ roles for federated users.<\/p>\n<p>Each Studio user has it's own URL to access that environment.<\/p>\n<p>Here is the SageMaker developer guide: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf#%5B%7B%22num%22%3A14340%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C533.986%2Cnull%5D\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf#%5B%7B%22num%22%3A14340%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C533.986%2Cnull%5D<\/a><\/p>\n<p>Page 36 discusses on-boarding with IAM.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59375896",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1603758115467,
        "Question_original_content":"assign user studio successfulli creat studio statu servic ask ask assign user list user iam organ coupl suppos user",
        "Question_preprocessed_content":"assign user studio successfulli creat studio statu servic ask ask assign user list user iam organ suppos user",
        "Question_gpt_summary_original":"The user has encountered a challenge in assigning users to their SageMaker Studio as they are not listed and they are unsure where to find them. They are unsure if the users are from their IAM or organization.",
        "Question_gpt_summary":"user encount challeng assign user studio list unsur unsur user iam organ",
        "Answer_original_content":"setup studio us aw sso iam authent method gather studio user setup iam authent method actual user provid partit studio differ work environ control access partit iam polici iam user role feder user studio user url access environ develop guid http doc aw amazon com latest pdf num gen xyz cnull page discuss board iam",
        "Answer_preprocessed_content":"setup studio us aw sso iam authent method gather studio user setup iam authent method actual user provid partit studio differ work environ control access partit iam polici iam user role feder user studio user url access environ develop guid page discuss iam",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that if SageMaker Studio is set up using IAM for authentication, the users are not actually users but provide partitions within Studio for different work environments. Access to these partitions can be controlled using IAM policies for IAM users or roles for federated users. Each Studio user has its own URL to access that environment. The SageMaker developer guide provides more information on on-boarding with IAM on page 36.",
        "Answer_gpt_summary":"solut discuss suggest studio set iam authent user actual user provid partit studio differ work environ access partit control iam polici iam user role feder user studio user url access environ develop guid provid inform board iam page"
    },
    {
        "Question_title":"Sagemaker and Data on Databases",
        "Question_body":"A customer has a question about data sources\n\n\u201cmost of our data is stored in SQL databases, while the SageMaker docs say that I have to put it all in S3. It\u2019s not obvious what the best way to do this is. I can think for example of splitting my analysis code in two; one pre-processing step to go from SQL queries to tabular data, and e.g. store that as Parquet files. For high-dimensional tensor data it\u2019s even less obvious.\u201d\n\nCan someone comment on that?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1533317474000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":208.0,
        "Answer_body":"We have an example notebook for interacting from Redshift data from a SageMaker managed notebook, which I believe is suitable for an Exploratory Data Analysis (EDA) use-case: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/working_with_redshift_data\/working_with_redshift_data.ipynb\n\nFor production purposes, the customer should consider separating the job of first extracting data from relational databases to S3 (to build out a data lake), and then using that for downstream processing\/machine learning (including SageMaker, EMR, Athena, Spectrum, etc.). Customers can build extraction pipelines from popular relational databases using AWS Glue, EMR, or their preferred ETL engines like those on the AWS Marketplace.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUh_P30-iXTKmzZv0D4vtLOA\/sagemaker-and-data-on-databases",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-08-03T17:52:46.000Z",
                "Answer_score":0,
                "Answer_body":"We have an example notebook for interacting from Redshift data from a SageMaker managed notebook, which I believe is suitable for an Exploratory Data Analysis (EDA) use-case: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/working_with_redshift_data\/working_with_redshift_data.ipynb\n\nFor production purposes, the customer should consider separating the job of first extracting data from relational databases to S3 (to build out a data lake), and then using that for downstream processing\/machine learning (including SageMaker, EMR, Athena, Spectrum, etc.). Customers can build extraction pipelines from popular relational databases using AWS Glue, EMR, or their preferred ETL engines like those on the AWS Marketplace.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-02-12T01:50:00.007Z",
                "Answer_score":0,
                "Answer_body":"I'd recommend using SageMaker Data Wrangler to connects the dots of different SageMaker services. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-wrangler-import.html",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1533318766000,
        "Question_original_content":"data databas custom question data sourc data store sql databas doc obviou best wai think exampl split analysi code pre process step sql queri tabular data store parquet file high dimension tensor data obviou comment",
        "Question_preprocessed_content":"data databas custom question data sourc data store sql databas doc obviou best wai think exampl split analysi code step sql queri tabular data store parquet file tensor data obviou comment",
        "Question_gpt_summary_original":"The user is facing challenges in using Sagemaker as most of their data is stored in SQL databases, while Sagemaker requires data to be stored in S3. The user is unsure of the best way to transfer the data and is considering splitting their analysis code into two steps. The user is also facing difficulties in transferring high-dimensional tensor data.",
        "Question_gpt_summary":"user face challeng data store sql databas requir data store user unsur best wai transfer data consid split analysi code step user face difficulti transfer high dimension tensor data",
        "Answer_original_content":"exampl notebook interact redshift data manag notebook believ suitabl exploratori data analysi eda us case http github com awslab amazon exampl blob master advanc function work redshift data work redshift data ipynb product purpos custom consid separ job extract data relat databas build data lake downstream process machin learn includ emr athena spectrum custom build extract pipelin popular relat databas aw glue emr prefer etl engin like aw marketplac",
        "Answer_preprocessed_content":"exampl notebook interact redshift data manag notebook believ suitabl exploratori data analysi product purpos custom consid separ job extract data relat databas downstream learn custom build extract pipelin popular relat databas aw glue emr prefer etl engin like aw marketplac",
        "Answer_gpt_summary_original":"Solution: The user can use AWS Glue, EMR, or their preferred ETL engines to build extraction pipelines from SQL databases to S3. They can then use the data stored in S3 for downstream processing\/machine learning, including SageMaker. For an exploratory data analysis use-case, the user can refer to the example notebook for interacting with Redshift data from a SageMaker managed notebook.",
        "Answer_gpt_summary":"solut user us aw glue emr prefer etl engin build extract pipelin sql databas us data store downstream process machin learn includ exploratori data analysi us case user refer exampl notebook interact redshift data manag notebook"
    },
    {
        "Question_title":"Is it necessary to commit DVC files from our CI pipelines?",
        "Question_body":"<p>DVC uses git commits to save the experiments and navigate between experiments.<\/p>\n<p>Is it possible to avoid making auto-commits in CI\/CD (to save data artifacts after <code>dvc repro<\/code> in CI\/CD side).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1587023782283,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1594640021920,
        "Question_score":6.0,
        "Question_view_count":1047.0,
        "Answer_body":"<blockquote>\n  <p>will you make it part of CI pipeline<\/p>\n<\/blockquote>\n\n<p>DVC often serves as a part of MLOps infrastructure. There is a popular <a href=\"https:\/\/martinfowler.com\/articles\/cd4ml.html\" rel=\"noreferrer\">blog post about CI\/CD for ML<\/a> where DVC is used under the hood. <a href=\"https:\/\/blog.codecentric.de\/en\/2020\/01\/remote-training-gitlab-ci-dvc\/\" rel=\"noreferrer\">Another example<\/a> but with GitLab CI\/CD.<\/p>\n\n<blockquote>\n  <p>scenario where you will integrate dvc commit command with CI\n  pipelines?<\/p>\n<\/blockquote>\n\n<p>If you mean <code>git commit<\/code> of DVC files (not <code>dvc commit<\/code>) then yes, you need to commit dvc-files into Git during CI\/CD process. Auto-commit is not the best practice.<\/p>\n\n<p>How to avoid Git commit in CI\/CD:<\/p>\n\n<ol>\n<li>After ML model training in CI\/CD, save changed dvc-files in external storage (for example GitLab artifact\/releases), then get the files to a developer machine and commit there. Users usually write scripts to automate it.<\/li>\n<li>Wait for DVC 1.0 release when <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1234\" rel=\"noreferrer\">run-cache (like build-cache)<\/a> will be implemented. Run-cache makes dvc-files ephemeral and no additional Git commits will be required. Technically, run-cache is an associative storage <code>repo state --&gt; run results<\/code> outside of Git repo (in data remote).<\/li>\n<\/ol>\n\n<p>Disclaimer: I'm one of the creators of DVC.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":null,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61245284",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1587029761967,
        "Question_original_content":"necessari commit file pipelin us git commit save experi navig experi possibl avoid make auto commit save data artifact repro",
        "Question_preprocessed_content":"necessari commit file pipelin us git commit save experi navig experi possibl avoid make",
        "Question_gpt_summary_original":"The user is facing a challenge of whether or not to commit DVC files from their CI pipelines, as DVC uses git commits to save experiments and navigate between them. They are exploring the possibility of avoiding auto-commits in CI\/CD to save data artifacts after running \"dvc repro\" in the CI\/CD side.",
        "Question_gpt_summary":"user face challeng commit file pipelin us git commit save experi navig explor possibl avoid auto commit save data artifact run repro",
        "Answer_original_content":"pipelin serv mlop infrastructur popular blog post hood exampl gitlab scenario integr commit command pipelin mean git commit file commit ye need commit file git process auto commit best practic avoid git commit model train save chang file extern storag exampl gitlab artifact releas file develop machin commit user usual write script autom wait releas run cach like build cach implement run cach make file ephemer addit git commit requir technic run cach associ storag repo state run result outsid git repo data remot disclaim creator",
        "Answer_preprocessed_content":"pipelin serv mlop infrastructur popular blog post hood exampl gitlab scenario integr commit command pipelin mean file ye need commit file git process best practic avoid git commit model train save chang file extern storag file develop machin commit user usual write script autom wait releas implement make file ephemer addit git commit requir technic associ storag outsid git repo disclaim creator",
        "Answer_gpt_summary_original":"Two possible solutions were discussed in the conversation. The first solution is to save changed DVC files in external storage after ML model training in CI\/CD, then get the files to a developer machine and commit there. The second solution is to wait for DVC 1.0 release when run-cache will be implemented, which makes DVC files ephemeral and no additional Git commits will be required.",
        "Answer_gpt_summary":"possibl solut discuss convers solut save chang file extern storag model train file develop machin commit second solut wait releas run cach implement make file ephemer addit git commit requir"
    },
    {
        "Question_title":"Invalid graph - invalid dataset",
        "Question_body":"Invalid graph - Invalid dataset",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1595587512910,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Thanks @ShowndaryaMadhavan for your quick response.\n\nI found that I had to press Generate Profile as in the picture below and then it worked",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/51962\/invalid-graph-invalid-dataset.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-25T07:57:18.377Z",
                "Answer_score":0,
                "Answer_body":"Thanks @ShowndaryaMadhavan for your quick response.\n\nI found that I had to press Generate Profile as in the picture below and then it worked",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2020-07-24T11:30:32.627Z",
                "Answer_score":0,
                "Answer_body":"Hi @ThalanayarMuthukumar-6317\n\nThere might be multiple reasons for this error:\n\nYour input dataset has invalid characters, bad values, or out of range values\n\n\nSome column is empty or contains too many missing values. ( You can use Clean Missing Data module to handle missing data in your dataset before you split )\n\n\nIf the data format is not supported.\n\n\nIf there are atleast 2 rows for Split Data to work\n\n\nIf you have specified desired rows to be split, make sure the number is less than the total rows,\n\nAnd, what mode of splitting have you chosen in Split Data? If you are splitting by rows, check if Stratified split is set to false. If it is true, check the target column you have chosen.\n\nHope this helps!",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":1.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1595663838376,
        "Question_original_content":"invalid graph invalid dataset invalid graph invalid dataset",
        "Question_preprocessed_content":"invalid graph invalid dataset invalid graph invalid dataset",
        "Question_gpt_summary_original":"The user has encountered challenges with an invalid dataset, resulting in an invalid graph.",
        "Question_gpt_summary":"user encount challeng invalid dataset result invalid graph",
        "Answer_original_content":"thank showndaryamadhavan quick respons press gener profil pictur work",
        "Answer_preprocessed_content":"thank quick respons press gener profil pictur work",
        "Answer_gpt_summary_original":"Solution: The user found a solution by pressing the \"Generate Profile\" button, which resolved the issue with the invalid dataset and resulted in a valid graph.",
        "Answer_gpt_summary":"solut user solut press gener profil button resolv issu invalid dataset result valid graph"
    },
    {
        "Question_title":"Deletion of storage",
        "Question_body":"<p>Hello.<br>\nI have created a toy project with google drive storage. I created 3 versions of the dataset and then deleted the drive folder to create a new storage folder. Then create 3 more dataset versions. When I want to download the first version it throws me an error:<\/p>\n<blockquote>\n<p>Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 176fc4479b1573160fd05222c20976d2.dir<\/p>\n<\/blockquote>\n<p>Is there a way to recover or rebuild the versions before the storage change?<\/p>\n<p>Thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1666479337651,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":57.0,
        "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/aaron\">@aaron<\/a>!<br>\nIf you want to retreive the first version, the one that you deleted you would need to try to recover data on drive itself - <a href=\"https:\/\/support.google.com\/drive\/answer\/2375102?hl=en&amp;co=GENIE.Platform%3DDesktop\" class=\"inline-onebox\">Delete and restore files in Google Drive - Computer - Google Drive Help<\/a><br>\nThey might be in Trash folder, but that holds only for 30 days after \u201cdeletion\u201d. You would need to retrieve it  and merge with the \u201cnew\u201d folder.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/deletion-of-storage\/1370",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-27T11:51:39.358Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/aaron\">@aaron<\/a>!<br>\nIf you want to retreive the first version, the one that you deleted you would need to try to recover data on drive itself - <a href=\"https:\/\/support.google.com\/drive\/answer\/2375102?hl=en&amp;co=GENIE.Platform%3DDesktop\" class=\"inline-onebox\">Delete and restore files in Google Drive - Computer - Google Drive Help<\/a><br>\nThey might be in Trash folder, but that holds only for 30 days after \u201cdeletion\u201d. You would need to retrieve it  and merge with the \u201cnew\u201d folder.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"delet storag hello creat toi project googl drive storag creat version dataset delet drive folder creat new storag folder creat dataset version want download version throw error cach file exist local remot miss cach file fcbfdcd dir wai recov rebuild version storag chang thank advanc",
        "Question_preprocessed_content":"delet storag hello creat toi project googl drive storag creat version dataset delet drive folder creat new storag folder creat dataset version want download version throw error cach file exist local remot miss cach file wai recov rebuild version storag chang thank advanc",
        "Question_gpt_summary_original":"The user encountered a challenge with their toy project on Google Drive storage. They created three versions of a dataset, deleted the drive folder to create a new storage folder, and then created three more dataset versions. However, when attempting to download the first version, an error occurred indicating missing cache files. The user is seeking a way to recover or rebuild the versions before the storage change.",
        "Question_gpt_summary":"user encount challeng toi project googl drive storag creat version dataset delet drive folder creat new storag folder creat dataset version attempt download version error occur indic miss cach file user seek wai recov rebuild version storag chang",
        "Answer_original_content":"hello aaron want retreiv version delet need try recov data drive delet restor file googl drive googl drive help trash folder hold dai delet need retriev merg new folder",
        "Answer_preprocessed_content":"hello want retreiv version delet need try recov data drive delet restor file googl drive googl drive help trash folder hold dai delet need retriev merg new folder",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to try to recover the deleted data on Google Drive itself by following the steps provided in the Google Drive Help article on deleting and restoring files. The deleted files might be in the Trash folder, but this only holds for 30 days after deletion. The user would need to retrieve the deleted files and merge them with the new folder.",
        "Answer_gpt_summary":"solut possibl solut mention discuss try recov delet data googl drive follow step provid googl drive help articl delet restor file delet file trash folder hold dai delet user need retriev delet file merg new folder"
    },
    {
        "Question_title":"Any recommendation on republish an endpoint to remain same",
        "Question_body":"<p>For AzureML we\u2019re using the REST api provided in published pipelines to launch them as part of scheduled jobs.<\/p>\n<p>It looks like if we republish an endpoint the GUID at the end of the URL changes.\nDo you have any recommendations for how to alias this so the URL can remain the same for a caller or keep it constant?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640914782270,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":124.0,
        "Answer_body":"<p>These are static, unique URLs that can be associated with multiple published pipeline versions (you can make one pipeline the default).<\/p>\n<p>Pipeline Endpoints:<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipelineendpoint?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.pipeline.core.PipelineEndpoint class - Azure Machine Learning Python | Microsoft Docs<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70538271",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1640952460163,
        "Question_original_content":"recommend republish endpoint remain rest api provid publish pipelin launch schedul job look like republish endpoint guid end url chang recommend alia url remain caller constant",
        "Question_preprocessed_content":"recommend republish endpoint remain rest api provid publish pipelin launch schedul job look like republish endpoint guid end url chang recommend alia url remain caller constant",
        "Question_gpt_summary_original":"The user is facing a challenge with AzureML's REST API as the GUID at the end of the URL changes when an endpoint is republished. They are seeking recommendations on how to alias the endpoint to keep the URL constant for the caller.",
        "Question_gpt_summary":"user face challeng rest api guid end url chang endpoint republish seek recommend alia endpoint url constant caller",
        "Answer_original_content":"static uniqu url associ multipl publish pipelin version pipelin default pipelin endpoint pipelin core pipelineendpoint class python microsoft doc",
        "Answer_preprocessed_content":"static uniqu url associ multipl publish pipelin version pipelin class python microsoft doc",
        "Answer_gpt_summary_original":"Solution: The discussion suggests using Pipeline Endpoints, which are static, unique URLs that can be associated with multiple published pipeline versions. By making one pipeline the default, the user can keep the URL constant for the caller.",
        "Answer_gpt_summary":"solut discuss suggest pipelin endpoint static uniqu url associ multipl publish pipelin version make pipelin default user url constant caller"
    },
    {
        "Question_title":"\"Failure Exception: OSError: [Errno 30] Read-only file system\" when using AzureML in Python Azure Function",
        "Question_body":"<h2>Issue<\/h2>\n<p>I am trying prepare and then submit a new experiment to Azure Machine Learning from an Azure Function in Python. I therefore register a new dataset for my Azure ML workspace, which contains the training data for my ML model using <code>dataset.register(...<\/code>. However, when I try to create this dataset with the following line of code<\/p>\n<pre><code>dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)\n<\/code><\/pre>\n<p>then I get a <code>Failure Exception: OSError: [Errno 30] Read-only file system ...<\/code>.<\/p>\n<h2>Ideas<\/h2>\n<ol>\n<li>I know that I shouldn't write to the file system from within an Azure function if possible. But I actually don't want to write anything to the local file system. I only want to create the dataset as a reference to my blob storage under <code>datastore_path<\/code> and then register this to my Azure Machine Learning workspace. But it seems that the method <code>from_delimited_files<\/code> is trying to write to the file system anyway (maybe some caching?).<\/li>\n<li>I also know that there is a temp folder in which writing temporary files is permitted. However, I belive I cannot really control where this method is writing data. I already tried changing the current working directory to this temp folder just before the function call using <code>os.chdir(tempfile.gettempdir())<\/code>, but that didn't help.<\/li>\n<\/ol>\n<p>Any other ideas? I don't think I am doing something particularly unusually...<\/p>\n<h2>Details<\/h2>\n<p>I am using python 3.7 and azureml-sdk 1.9.0 and I can run the python script locally without problems. I currently deploy from VSCode using the Azure Functions extension version 0.23.0 (and an Azure DevOps pipeline for CI\/CD).<\/p>\n<p>Here is my full stack trace:<\/p>\n<pre><code>Microsoft.Azure.WebJobs.Host.FunctionInvocationException: Exception while executing function: Functions.HttpTrigger_Train\n ---&gt; Microsoft.Azure.WebJobs.Script.Workers.Rpc.RpcException: Result: Failure\nException: OSError: [Errno 30] Read-only file system: '\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/bin\/deps.lock'\nStack:   File &quot;\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py&quot;, line 345, in _handle__invocation_request\n    self.__run_sync_func, invocation_id, fi.func, args)\n  File &quot;\/usr\/local\/lib\/python3.7\/concurrent\/futures\/thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py&quot;, line 480, in __run_sync_func\n    return func(**params)\n  File &quot;\/home\/site\/wwwroot\/HttpTrigger_Train\/__init__.py&quot;, line 11, in main\n    train()\n  File &quot;\/home\/site\/wwwroot\/shared_code\/train.py&quot;, line 70, in train\n    dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/_loggerfactory.py&quot;, line 126, in wrapper\n    return func(*args, **kwargs)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/dataset_factory.py&quot;, line 308, in from_delimited_files\n    quoting=support_multi_line)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/readers.py&quot;, line 100, in read_csv\n    df = Dataflow._path_to_get_files_block(path, archive_options)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/dataflow.py&quot;, line 2387, in _path_to_get_files_block\n    return datastore_to_dataflow(path)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 41, in datastore_to_dataflow\n    datastore, datastore_value = get_datastore_value(source)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 83, in get_datastore_value\n    _set_auth_type(workspace)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 134, in _set_auth_type\n    get_engine_api().set_aml_auth(SetAmlAuthMessageArgument(AuthType.SERVICEPRINCIPAL, json.dumps(auth)))\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py&quot;, line 18, in get_engine_api\n    _engine_api = EngineAPI()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py&quot;, line 55, in __init__\n    self._message_channel = launch_engine()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/engine.py&quot;, line 300, in launch_engine\n    dependencies_path = runtime.ensure_dependencies()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 141, in ensure_dependencies\n    with _FileLock(deps_lock_path, raise_on_timeout=timeout_exception):\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 113, in __enter__\n    self.acquire()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 72, in acquire\n    self.lockfile = os.open(self.lockfile_path, os.O_CREAT | os.O_EXCL | os.O_RDWR)\n\n   at Microsoft.Azure.WebJobs.Script.Description.WorkerFunctionInvoker.InvokeCore(Object[] parameters, FunctionInvocationContext context) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/Workers\/WorkerFunctionInvoker.cs:line 85\n   at Microsoft.Azure.WebJobs.Script.Description.FunctionInvokerBase.Invoke(Object[] parameters) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/FunctionInvokerBase.cs:line 85\n   at Microsoft.Azure.WebJobs.Script.Description.FunctionGenerator.Coerce[T](Task`1 src) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/FunctionGenerator.cs:line 225\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionInvoker`2.InvokeAsync(Object instance, Object[] arguments) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionInvoker.cs:line 52\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.InvokeAsync(IFunctionInvoker invoker, ParameterHelper parameterHelper, CancellationTokenSource timeoutTokenSource, CancellationTokenSource functionCancellationTokenSource, Boolean throwOnTimeout, TimeSpan timerInterval, IFunctionInstance instance) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 587\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithWatchersAsync(IFunctionInstanceEx instance, ParameterHelper parameterHelper, ILogger logger, CancellationTokenSource functionCancellationTokenSource) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 532\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, ParameterHelper parameterHelper, IFunctionOutputDefinition outputDefinition, ILogger logger, CancellationTokenSource functionCancellationTokenSource) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 470\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, FunctionStartedMessage message, FunctionInstanceLogEntry instanceLogEntry, ParameterHelper parameterHelper, ILogger logger, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 278\n   --- End of inner exception stack trace ---\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, FunctionStartedMessage message, FunctionInstanceLogEntry instanceLogEntry, ParameterHelper parameterHelper, ILogger logger, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 325\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.TryExecuteAsyncCore(IFunctionInstanceEx functionInstance, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 117\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597357236433,
        "Question_favorite_count":null,
        "Question_last_edit_time":1613407306920,
        "Question_score":2.0,
        "Question_view_count":1092.0,
        "Answer_body":"<p>The issue was an incompatible OS version in my virtual environment.<\/p>\n<p>A huge thanks goes to <a href=\"https:\/\/docs.microsoft.com\/answers\/users\/111253\/pramodvalavala-msft.html\" rel=\"nofollow noreferrer\">PramodValavala-MSFT<\/a> for his idea to create a docker container! Following his suggestion, I suddenly got the following error message for the  <code>dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)<\/code> command:<\/p>\n<blockquote>\n<p>Exception: NotImplementedError: Unsupported Linux distribution debian 10.<\/p>\n<\/blockquote>\n<p>which reminded me of the following warning in the azure machine learning documentation:<\/p>\n<blockquote>\n<p>Some dataset classes have dependencies on the azureml-dataprep\npackage, which is only compatible with 64-bit Python. For Linux users,\nthese classes are supported only on the following distributions: Red\nHat Enterprise Linux (7, 8), Ubuntu (14.04, 16.04, 18.04), Fedora (27,\n28), Debian (8, 9), and CentOS (7).<\/p>\n<\/blockquote>\n<p>Choosing the predefined docker image <code>2.0-python3.7<\/code> (running Debian 9) instead of  <code>3.0-python3.7<\/code> (running Debian 10) solved the issue (see <a href=\"https:\/\/hub.docker.com\/_\/microsoft-azure-functions-python\" rel=\"nofollow noreferrer\">https:\/\/hub.docker.com\/_\/microsoft-azure-functions-python<\/a>).<\/p>\n<p>I suspect that the default virtual environment, which I was using originally, also ran on an incompatible OS.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63403985",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1597616847400,
        "Question_original_content":"failur except oserror errno read file python azur function issu try prepar submit new experi azur function python regist new dataset workspac contain train data model dataset regist try creat dataset follow line code dataset dataset tabular delimit file path datastor path failur except oserror errno read file idea know shouldn write file azur function possibl actual want write local file want creat dataset refer blob storag datastor path regist workspac method delimit file try write file mayb cach know temp folder write temporari file permit beliv control method write data tri chang current work directori temp folder function chdir tempfil gettempdir help idea think particularli unusu detail python sdk run python script local problem current deploi vscode azur function extens version azur devop pipelin stack trace microsoft azur webjob host functioninvocationexcept except execut function function httptrigger train microsoft azur webjob script worker rpc rpcexcept result failur except oserror errno read file home site root python packag lib site packag dotnetcor bin dep lock stack file azur function host worker python linux azur function worker dispatch line handl invoc request self run sync func invoc func arg file usr local lib python concurr futur thread line run result self self arg self kwarg file azur function host worker python linux azur function worker dispatch line run sync func return func param file home site root httptrigger train init line main train file home site root share code train line train dataset dataset tabular delimit file path datastor path file home site root python packag lib site packag data loggerfactori line wrapper return func arg kwarg file home site root python packag lib site packag data dataset factori line delimit file quot support multi line file home site root python packag lib site packag dataprep api reader line read csv dataflow path file block path archiv option file home site root python packag lib site packag dataprep api dataflow line path file block return datastor dataflow path file home site root python packag lib site packag dataprep api datastor helper line datastor dataflow datastor datastor valu datastor valu sourc file home site root python packag lib site packag dataprep api datastor helper line datastor valu set auth type workspac file home site root python packag lib site packag dataprep api datastor helper line set auth type engin api set aml auth setamlauthmessageargu authtyp serviceprincip json dump auth file home site root python packag lib site packag dataprep api engineapi api line engin api engin api engineapi file home site root python packag lib site packag dataprep api engineapi api line init self messag channel launch engin file home site root python packag lib site packag dataprep api engineapi engin line launch engin depend path runtim ensur depend file home site root python packag lib site packag dotnetcor runtim line ensur depend filelock dep lock path rais timeout timeout except file home site root python packag lib site packag dotnetcor runtim line enter self acquir file home site root python packag lib site packag dotnetcor runtim line acquir self lockfil open self lockfil path creat excl rdwr microsoft azur webjob script descript workerfunctioninvok invokecor object paramet functioninvocationcontext context src azur function host src webjob script descript worker workerfunctioninvok line microsoft azur webjob script descript functioninvokerbas invok object paramet src azur function host src webjob script descript functioninvokerbas line microsoft azur webjob script descript functiongener coerc task src src azur function host src webjob script descript functiongener line microsoft azur webjob host executor functioninvok invokeasync object instanc object argument project azur webjob sdk rqmt src microsoft azur webjob host executor functioninvok line microsoft azur webjob host executor functionexecutor invokeasync ifunctioninvok invok parameterhelp parameterhelp cancellationtokensourc timeouttokensourc cancellationtokensourc functioncancellationtokensourc boolean throwontimeout timespan timerinterv ifunctioninst instanc project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line microsoft azur webjob host executor functionexecutor executewithwatchersasync ifunctioninstanceex instanc parameterhelp parameterhelp ilogg logger cancellationtokensourc functioncancellationtokensourc project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line microsoft azur webjob host executor functionexecutor executewithloggingasync ifunctioninstanceex instanc parameterhelp parameterhelp ifunctionoutputdefinit outputdefinit ilogg logger cancellationtokensourc functioncancellationtokensourc project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line microsoft azur webjob host executor functionexecutor executewithloggingasync ifunctioninstanceex instanc functionstartedmessag messag functioninstancelogentri instancelogentri parameterhelp parameterhelp ilogg logger cancellationtoken cancellationtoken project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line end inner except stack trace microsoft azur webjob host executor functionexecutor executewithloggingasync ifunctioninstanceex instanc functionstartedmessag messag functioninstancelogentri instancelogentri parameterhelp parameterhelp ilogg logger cancellationtoken cancellationtoken project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line microsoft azur webjob host executor functionexecutor tryexecuteasynccor ifunctioninstanceex functioninst cancellationtoken cancellationtoken project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line",
        "Question_preprocessed_content":"failur except oserror file python azur function issu try prepar submit new experi azur function python regist new dataset workspac contain train data model try creat dataset follow line code idea know shouldn write file azur function possibl actual want write local file want creat dataset refer blob storag regist workspac method try write file know temp folder write temporari file permit beliv control method write data tri chang current work directori temp folder function help idea think particularli detail python sdk run python script local problem current deploi vscode azur function extens version stack trace",
        "Question_gpt_summary_original":"The user is encountering a \"Failure Exception: OSError: [Errno 30] Read-only file system\" error when trying to create a dataset for their Azure Machine Learning workspace from an Azure Function in Python. They have tried changing the current working directory to the temp folder and are unsure why the method is trying to write to the file system. They are using python 3.7 and azureml-sdk 1.9.0 and can run the python script locally without problems.",
        "Question_gpt_summary":"user encount failur except oserror errno read file error try creat dataset workspac azur function python tri chang current work directori temp folder unsur method try write file python sdk run python script local problem",
        "Answer_original_content":"issu incompat version virtual environ huge thank goe pramodvalavala msft idea creat docker contain follow suggest suddenli got follow error messag dataset dataset tabular delimit file path datastor path command except notimplementederror unsupport linux distribut debian remind follow warn document dataset class depend dataprep packag compat bit python linux user class support follow distribut red hat enterpris linux ubuntu fedora debian cento choos predefin docker imag python run debian instead python run debian solv issu http hub docker com microsoft azur function python suspect default virtual environ origin ran incompat",
        "Answer_preprocessed_content":"issu incompat version virtual environ huge thank goe idea creat docker contain follow suggest suddenli got follow error messag command except notimplementederror unsupport linux distribut debian remind follow warn document dataset class depend dataprep packag compat python linux user class support follow distribut red hat enterpris linux ubuntu fedora debian cento choos predefin docker imag instead solv issu suspect default virtual environ origin ran incompat",
        "Answer_gpt_summary_original":"Solution: The issue was caused by an incompatible OS version in the virtual environment. The solution was to create a docker container using the predefined docker image \"2.0-python3.7\" (running Debian 9) instead of \"3.0-python3.7\" (running Debian 10) which solved the issue.",
        "Answer_gpt_summary":"solut issu caus incompat version virtual environ solut creat docker contain predefin docker imag python run debian instead python run debian solv issu"
    },
    {
        "Question_title":"MLflow proxied artifact access: Unable to locate credentials",
        "Question_body":"<p>I am using MLflow to track my experiments. I am using an S3 bucket as an artifact store. For acessing it, I want to use <em>proxied artifact access<\/em>, as described in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>, however this does not work for me, since it locally looks for credentials (but the server should handle this).<\/p>\n<h2>Expected Behaviour<\/h2>\n<p>As described in the docs, I would expect that locally, I do not need to specify my AWS credentials, since the server handles this for me. From <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n<blockquote>\n<p>This eliminates the need to allow end users to have direct path access to a remote object store (e.g., s3, adls, gcs, hdfs) for artifact handling and eliminates the need for an end-user to provide access credentials to interact with an underlying object store.<\/p>\n<\/blockquote>\n<h2>Actual Behaviour \/ Error<\/h2>\n<p>Whenever I run an experiment on my machine, I am running into the following error:<\/p>\n<p><code>botocore.exceptions.NoCredentialsError: Unable to locate credentials<\/code><\/p>\n<p>So the error is local. However, this should not happen since the server should handle the auth instead of me needing to store my credentials locally. Also, I would expect that I would not even need library <code>boto3<\/code> locally.<\/p>\n<h2>Solutions Tried<\/h2>\n<p>I am aware that I need to create a new experiment, because existing experiments might still use a different artifact location which is proposed in <a href=\"https:\/\/stackoverflow.com\/a\/71417933\/10465165\">this SO answer<\/a> as well as in the note in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>. Creating a new experiment did not solve the error for me. Whenever I run the experiment, I get an explicit log in the console validating this:<\/p>\n<p><code>INFO mlflow.tracking.fluent: Experiment with name 'test' does not exist. Creating a new experiment.<\/code><\/p>\n<p>Related Questions (<a href=\"https:\/\/stackoverflow.com\/questions\/72206086\/cant-log-mlflow-artifacts-to-s3-with-docker-based-tracking-server\">#1<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/72236258\/mlflow-unable-to-store-artifacts-to-s3\/72261826#comment128726676_72261826\">#2<\/a>) refer to a different scenario, which is also <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-4-mlflow-with-remote-tracking-server-backend-and-artifact-stores\" rel=\"nofollow noreferrer\">described in the docs<\/a><\/p>\n<h2>Server Config<\/h2>\n<p>The server runs on a kubernetes pod with the following config:<\/p>\n<pre><code>mlflow server \\\n    --host 0.0.0.0 \\\n    --port 5000 \\\n    --backend-store-uri postgresql:\/\/user:pw@endpoint \\\n    --artifacts-destination s3:\/\/my_bucket\/artifacts \\\n    --serve-artifacts \\\n    --default-artifact-root s3:\/\/my_bucket\/artifacts \\\n<\/code><\/pre>\n<p>I would expect my config to be correct, looking at doc <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">page 1<\/a> and <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#using-the-tracking-server-for-proxied-artifact-access\" rel=\"nofollow noreferrer\">page 2<\/a><\/p>\n<p>I am able to see the mlflow UI if I forward the port to my local machine. I also see the experiment runs as failed, because of the error I sent above.<\/p>\n<h2>My Code<\/h2>\n<p>The relevant part of my code which fails is the logging of the model:<\/p>\n<pre><code>mlflow.set_tracking_uri(&quot;http:\/\/localhost:5000&quot;)\nmlflow.set_experiment(&quot;test2)\n\n...\n\n# this works\nmlflow.log_params(hyperparameters)\n                        \nmodel = self._train(model_name, hyperparameters, X_train, y_train)\ny_pred = model.predict(X_test)\nself._evaluate(y_test, y_pred)\n\n# this fails with the error from above\nmlflow.sklearn.log_model(model, &quot;artifacts&quot;)\n\n<\/code><\/pre>\n<h2>Question<\/h2>\n<p>I am probably overlooking something. Is there a need to locally indicate that I want to use proxied artified access? If yes, how do I do this? Is there something I have missed?<\/p>\n<h2>Full Traceback<\/h2>\n<pre><code>  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/models\/model.py&quot;, line 295, in log\n    mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 726, in log_artifacts\n    MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py&quot;, line 1001, in log_artifacts\n    self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 346, in log_artifacts\n    self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/store\/artifact\/s3_artifact_repo.py&quot;, line 141, in log_artifacts\n    self._upload_file(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/store\/artifact\/s3_artifact_repo.py&quot;, line 117, in _upload_file\n    s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/boto3\/s3\/inject.py&quot;, line 143, in upload_file\n    return transfer.upload_file(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/boto3\/s3\/transfer.py&quot;, line 288, in upload_file\n    future.result()\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/futures.py&quot;, line 103, in result\n    return self._coordinator.result()\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/futures.py&quot;, line 266, in result\n    raise self._exception\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/tasks.py&quot;, line 139, in __call__\n    return self._execute_main(kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/tasks.py&quot;, line 162, in _execute_main\n    return_value = self._main(**kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/upload.py&quot;, line 758, in _main\n    client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 508, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 898, in _make_api_call\n    http, parsed_response = self._make_request(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 921, in _make_request\n    return self._endpoint.make_request(operation_model, request_dict)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 119, in make_request\n    return self._send_request(request_dict, operation_model)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 198, in _send_request\n    request = self.create_request(request_dict, operation_model)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 134, in create_request\n    self._event_emitter.emit(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 412, in emit\n    return self._emitter.emit(aliased_event_name, **kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 256, in emit\n    return self._emit(event_name, kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 239, in _emit\n    response = handler(**kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/signers.py&quot;, line 103, in handler\n    return self.sign(operation_name, request)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/signers.py&quot;, line 187, in sign\n    auth.add_auth(request)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/auth.py&quot;, line 407, in add_auth\n    raise NoCredentialsError()\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1657122030593,
        "Question_favorite_count":null,
        "Question_last_edit_time":1657122771947,
        "Question_score":0.0,
        "Question_view_count":237.0,
        "Answer_body":"<p>The problem is that the server is running on wrong run parameters, the <code>--default-artifact-root<\/code> needs to either be removed or set to <code>mlflow-artifacts:\/<\/code>.<\/p>\n<p>From <code>mlflow server --help<\/code>:<\/p>\n<pre><code>  --default-artifact-root URI  Directory in which to store artifacts for any\n                               new experiments created. For tracking server\n                               backends that rely on SQL, this option is\n                               required in order to store artifacts. Note that\n                               this flag does not impact already-created\n                               experiments with any previous configuration of\n                               an MLflow server instance. By default, data\n                               will be logged to the mlflow-artifacts:\/ uri\n                               proxy if the --serve-artifacts option is\n                               enabled. Otherwise, the default location will\n                               be .\/mlruns.\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72886409",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1657186814368,
        "Question_original_content":"proxi artifact access unabl locat credenti track experi bucket artifact store acess want us proxi artifact access describ doc work local look credenti server handl expect behaviour describ doc expect local need specifi aw credenti server handl doc elimin need allow end user direct path access remot object store adl gc hdf artifact handl elimin need end user provid access credenti interact underli object store actual behaviour error run experi machin run follow error botocor except nocredentialserror unabl locat credenti error local happen server handl auth instead need store credenti local expect need librari boto local solut tri awar need creat new experi exist experi us differ artifact locat propos answer note doc creat new experi solv error run experi explicit log consol valid info track fluent experi test exist creat new experi relat question refer differ scenario describ doc server config server run kubernet pod follow config server host port backend store uri postgresql user endpoint artifact destin bucket artifact serv artifact default artifact root bucket artifact expect config correct look doc page page abl forward port local machin experi run fail error sent code relev code fail log model set track uri http localhost set experi test work log param hyperparamet model self train model hyperparamet train train pred model predict test self evalu test pred fail error sklearn log model model artifact question probabl overlook need local indic want us proxi artifi access ye miss traceback file dir venv lib python site packag model model line log track fluent log artifact local path artifact path file dir venv lib python site packag track fluent line log artifact client log artifact run local dir artifact path file dir venv lib python site packag track client line log artifact self track client log artifact run local dir artifact path file dir venv lib python site packag track track servic client line log artifact self artifact repo run log artifact local dir artifact path file dir venv lib python site packag store artifact artifact repo line log artifact self upload file file dir venv lib python site packag store artifact artifact repo line upload file client upload file filenam local file bucket bucket kei kei extraarg extra arg file dir venv lib python site packag boto inject line upload file return transfer upload file file dir venv lib python site packag boto transfer line upload file futur result file dir venv lib python site packag stransfer futur line result return self coordin result file dir venv lib python site packag stransfer futur line result rais self except file dir venv lib python site packag stransfer task line return self execut main kwarg file dir venv lib python site packag stransfer task line execut main return valu self main kwarg file dir venv lib python site packag stransfer upload line main client object bucket bucket kei kei bodi bodi extra arg file dir venv lib python site packag botocor client line api return self api oper kwarg file dir venv lib python site packag botocor client line api http pars respons self request file dir venv lib python site packag botocor client line request return self endpoint request oper model request dict file dir venv lib python site packag botocor endpoint line request return self send request request dict oper model file dir venv lib python site packag botocor endpoint line send request request self creat request request dict oper model file dir venv lib python site packag botocor endpoint line creat request self event emitt emit file dir venv lib python site packag botocor hook line emit return self emitt emit alias event kwarg file dir venv lib python site packag botocor hook line emit return self emit event kwarg file dir venv lib python site packag botocor hook line emit respons handler kwarg file dir venv lib python site packag botocor signer line handler return self sign oper request file dir venv lib python site packag botocor signer line sign auth add auth request file dir venv lib python site packag botocor auth line add auth rais nocredentialserror botocor except nocredentialserror unabl locat credenti",
        "Question_preprocessed_content":"proxi artifact access unabl locat credenti track experi bucket artifact store acess want us proxi artifact access describ doc work local look credenti expect behaviour describ doc expect local need specifi aw credenti server handl doc elimin need allow end user direct path access remot object store artifact handl elimin need provid access credenti interact underli object store actual behaviour error run experi machin run follow error error local happen server handl auth instead need store credenti local expect need librari local solut tri awar need creat new experi exist experi us differ artifact locat propos answer note doc creat new experi solv error run experi explicit log consol valid relat question refer differ scenario describ doc server config server run kubernet pod follow config expect config correct look doc page page abl forward port local machin experi run fail error sent code relev code fail log model question probabl overlook need local indic want us proxi artifi access ye miss traceback",
        "Question_gpt_summary_original":"The user is encountering an error while using MLflow to track experiments with an S3 bucket as an artifact store. They are attempting to use proxied artifact access, but are receiving a \"NoCredentialsError\" locally, despite the server being configured to handle authentication. The user has tried creating a new experiment and ensuring their server configuration is correct, but the error persists. They are seeking assistance in resolving the issue.",
        "Question_gpt_summary":"user encount error track experi bucket artifact store attempt us proxi artifact access receiv nocredentialserror local despit server configur handl authent user tri creat new experi ensur server configur correct error persist seek assist resolv issu",
        "Answer_original_content":"problem server run wrong run paramet default artifact root need remov set artifact server help default artifact root uri directori store artifact new experi creat track server backend reli sql option requir order store artifact note flag impact creat experi previou configur server instanc default data log artifact uri proxi serv artifact option enabl default locat mlrun",
        "Answer_preprocessed_content":"problem server run wrong run paramet need remov set",
        "Answer_gpt_summary_original":"Solution: The solution to the problem is to ensure that the server is running on the correct run parameters. The user needs to either remove the <code>--default-artifact-root<\/code> or set it to <code>mlflow-artifacts:\/<\/code>. This can be done by referring to the <code>mlflow server --help<\/code> command.",
        "Answer_gpt_summary":"solut solut problem ensur server run correct run paramet user need remov default artifact root set artifact refer server help command"
    },
    {
        "Question_title":"Brewing up custom ML models on AWS SageMaker",
        "Question_body":"<p>Iam new with SageMaker and I try to use my own sickit-learn algorithm . For this I use Docker.\nI try to do the same task as described here in this github account : <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a><\/p>\n\n<p>My question is should I create manually the repository <strong><code>\/opt\/ml<\/code><\/strong>  (I work with windows OS) ?<\/p>\n\n<p>Can you explain me please?<\/p>\n\n<p>thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1533919042793,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":106.0,
        "Answer_body":"<p>You don't need to create <code>\/opt\/ml<\/code>, SageMaker will do it for you when it launches your training job.<\/p>\n\n<p>The contents of the <code>\/opt\/ml<\/code> directory are determined by the parameters you pass to the CreateTrainingJob API call. The scikit example notebook you linked to describes this (look at the <strong>Running your container<\/strong> sections). You can find more info about this in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">Create a Training Job<\/a> section of the main SageMaker documentation.<\/p>\n\n<hr>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51790720",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1533928824743,
        "Question_original_content":"brew custom model iam new try us sickit learn algorithm us docker try task describ github account http github com awslab amazon exampl blob master advanc function scikit bring scikit bring ipynb question creat manual repositori opt work window explain thank",
        "Question_preprocessed_content":"brew custom model iam new try us algorithm us docker try task describ github account question creat manual repositori explain thank",
        "Question_gpt_summary_original":"The user is facing a challenge in using their own sickit-learn algorithm on AWS SageMaker through Docker. They are unsure whether they need to manually create the repository \"\/opt\/ml\" and are seeking clarification on this issue.",
        "Question_gpt_summary":"user face challeng sickit learn algorithm docker unsur need manual creat repositori opt seek clarif issu",
        "Answer_original_content":"need creat opt launch train job content opt directori determin paramet pass createtrainingjob api scikit exampl notebook link describ look run contain section info creat train job section main document",
        "Answer_preprocessed_content":"need creat launch train job content directori determin paramet pass createtrainingjob api scikit exampl notebook link describ info creat train job section main document",
        "Answer_gpt_summary_original":"Solution: The user does not need to manually create the repository \"\/opt\/ml\" as AWS SageMaker will do it for them when launching the training job. The contents of the directory are determined by the parameters passed to the CreateTrainingJob API call. The scikit example notebook and the Create a Training Job section of the main SageMaker documentation provide more information on this.",
        "Answer_gpt_summary":"solut user need manual creat repositori opt launch train job content directori determin paramet pass createtrainingjob api scikit exampl notebook creat train job section main document provid inform"
    },
    {
        "Question_title":"What is the best way to deploy my machine learning model using GPUs, specifically as a web based API?",
        "Question_body":"I am trying to find the best way to run my machine learning models on GPUs for inference as an http request. Do Azure functions support GPUs? if not, what are other options I can look into?\n\nnote: I also want to use packaged models, not necessarily ones of my own creation (such as easyOCR for python)",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630916125883,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi,\n\nIf you need GPU support on ML inference the only supported option is the Azure Kubernetes Service as stated in this documentation\n\nFor guidance on deploying an ML model to AKS, please refer to this documenation on deploying to AKS",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/541074\/what-is-a-the-best-way-to-deploy-my-machine-learni.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-06T09:42:39.623Z",
                "Answer_score":1,
                "Answer_body":"Hi,\n\nIf you need GPU support on ML inference the only supported option is the Azure Kubernetes Service as stated in this documentation\n\nFor guidance on deploying an ML model to AKS, please refer to this documenation on deploying to AKS",
                "Answer_comment_count":4,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1630921359623,
        "Question_original_content":"best wai deploi machin learn model gpu specif web base api try best wai run machin learn model gpu infer http request azur function support gpu option look note want us packag model necessarili on creation easyocr python",
        "Question_preprocessed_content":"best wai deploi machin learn model gpu specif web base api try best wai run machin learn model gpu infer http request azur function support gpu option look note want us packag model necessarili on creation",
        "Question_gpt_summary_original":"The user is facing challenges in finding the best way to deploy their machine learning models on GPUs for inference as an http request. They are specifically looking for options to use packaged models and are unsure if Azure functions support GPUs.",
        "Question_gpt_summary":"user face challeng find best wai deploi machin learn model gpu infer http request specif look option us packag model unsur azur function support gpu",
        "Answer_original_content":"need gpu support infer support option azur kubernet servic state document guidanc deploi model ak refer documen deploi ak",
        "Answer_preprocessed_content":"need gpu support infer support option azur kubernet servic state document guidanc deploi model ak refer documen deploi ak",
        "Answer_gpt_summary_original":"Solution: The only supported option for GPU support on ML inference is the Azure Kubernetes Service (AKS). The user can refer to the documentation on deploying an ML model to AKS for guidance. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut support option gpu support infer azur kubernet servic ak user refer document deploi model ak guidanc solut mention discuss"
    },
    {
        "Question_title":"Multiple overlapping datasets",
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m considering using dvc for a project and am wondering if it fits the following use case. Say I have N images which form a dataset, but for any given experiment I may only want to train on some subset of the images. What I would like is to be able to determine which images in the subset are missing locally and only pull those images from the file store. Ideally these subsets could also have names and I\u2019d be able to just do dvc pull datasubset3. Is something like this possible?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607351479222,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":253.0,
        "Answer_body":"<p>Hello,<\/p>\n<aside class=\"quote no-group\" data-username=\"kazimpal\" data-post=\"1\" data-topic=\"576\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/k\/d07c76\/40.png\" class=\"avatar\"> kazimpal:<\/div>\n<blockquote>\n<p>Say I have N images which form a dataset<\/p>\n<\/blockquote>\n<\/aside>\n<p>OK so let\u2019s assume that dataset is a <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#adding-entire-directories\">directory which is tracked<\/a> by DVC.<\/p>\n<aside class=\"quote no-group\" data-username=\"kazimpal\" data-post=\"1\" data-topic=\"576\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/k\/d07c76\/40.png\" class=\"avatar\"> kazimpal:<\/div>\n<blockquote>\n<p>What I would like is to be able to determine which images in the subset are missing locally and only pull those images<\/p>\n<\/blockquote>\n<\/aside>\n<p>Once you have determined which files you can pull those specifically, yes. But it has to be done one by one (you can print the entire list to a file and then use a shell script to <code>dvc pull<\/code> each name). This is what we call \u201cgranularity support\u201d in most of our commands, including <code>push<\/code> and <code>pull<\/code>.<\/p>\n<aside class=\"quote no-group\" data-username=\"kazimpal\" data-post=\"1\" data-topic=\"576\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/k\/d07c76\/40.png\" class=\"avatar\"> kazimpal:<\/div>\n<blockquote>\n<p>Ideally these subsets could also have names and I\u2019d be able to just do dvc pull datasubset3<\/p>\n<\/blockquote>\n<\/aside>\n<p>DVC doesn\u2019t have such a feature, but we\u2019re open to feature requests in <a href=\"http:\/\/github.com\/iterative\/dvc\">github.com\/iterative\/dvc<\/a>.<\/p>\n<p>Note that we are also about to release a wildcard feature called \u201cglobbing\u201d which you can see here <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/5032\">https:\/\/github.com\/iterative\/dvc\/pull\/5032<\/a>, maybe that will be enough for your case? Depending on your dataset naming\/file structure.<\/p>\n<p>Best<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-overlapping-datasets\/576",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-07T18:59:43.636Z",
                "Answer_body":"<p>Hello,<\/p>\n<aside class=\"quote no-group\" data-username=\"kazimpal\" data-post=\"1\" data-topic=\"576\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/k\/d07c76\/40.png\" class=\"avatar\"> kazimpal:<\/div>\n<blockquote>\n<p>Say I have N images which form a dataset<\/p>\n<\/blockquote>\n<\/aside>\n<p>OK so let\u2019s assume that dataset is a <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#adding-entire-directories\">directory which is tracked<\/a> by DVC.<\/p>\n<aside class=\"quote no-group\" data-username=\"kazimpal\" data-post=\"1\" data-topic=\"576\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/k\/d07c76\/40.png\" class=\"avatar\"> kazimpal:<\/div>\n<blockquote>\n<p>What I would like is to be able to determine which images in the subset are missing locally and only pull those images<\/p>\n<\/blockquote>\n<\/aside>\n<p>Once you have determined which files you can pull those specifically, yes. But it has to be done one by one (you can print the entire list to a file and then use a shell script to <code>dvc pull<\/code> each name). This is what we call \u201cgranularity support\u201d in most of our commands, including <code>push<\/code> and <code>pull<\/code>.<\/p>\n<aside class=\"quote no-group\" data-username=\"kazimpal\" data-post=\"1\" data-topic=\"576\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/k\/d07c76\/40.png\" class=\"avatar\"> kazimpal:<\/div>\n<blockquote>\n<p>Ideally these subsets could also have names and I\u2019d be able to just do dvc pull datasubset3<\/p>\n<\/blockquote>\n<\/aside>\n<p>DVC doesn\u2019t have such a feature, but we\u2019re open to feature requests in <a href=\"http:\/\/github.com\/iterative\/dvc\">github.com\/iterative\/dvc<\/a>.<\/p>\n<p>Note that we are also about to release a wildcard feature called \u201cglobbing\u201d which you can see here <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/5032\">https:\/\/github.com\/iterative\/dvc\/pull\/5032<\/a>, maybe that will be enough for your case? Depending on your dataset naming\/file structure.<\/p>\n<p>Best<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"multipl overlap dataset consid project wonder fit follow us case imag form dataset given experi want train subset imag like abl determin imag subset miss local pull imag file store ideal subset name abl pull datasubset like possibl thank",
        "Question_preprocessed_content":"multipl overlap dataset consid project wonder fit follow us case imag form dataset given experi want train subset imag like abl determin imag subset miss local pull imag file store ideal subset name abl pull datasubset like possibl thank",
        "Question_gpt_summary_original":"The user is facing a challenge of managing multiple overlapping datasets using dvc for a project. They want to be able to determine which images in a subset are missing locally and only pull those images from the file store. Additionally, they would like to name these subsets and be able to pull them using a specific name.",
        "Question_gpt_summary":"user face challeng manag multipl overlap dataset project want abl determin imag subset miss local pull imag file store addition like subset abl pull specif",
        "Answer_original_content":"hello kazimp imag form dataset let assum dataset directori track kazimp like abl determin imag subset miss local pull imag determin file pull specif ye print entir list file us shell script pull granular support command includ push pull kazimp ideal subset name abl pull datasubset doesnt featur open featur request github com iter note releas wildcard featur call glob http github com iter pull mayb case depend dataset name file structur best",
        "Answer_preprocessed_content":"hello kazimp imag form dataset let assum dataset directori track kazimp like abl determin imag subset miss local pull imag determin file pull specif ye granular support command includ kazimp ideal subset name abl pull datasubset doesnt featur open featur request note releas wildcard featur call glob mayb case depend dataset structur best",
        "Answer_gpt_summary_original":"Solutions provided:\n- The user can determine which images in a subset are missing locally and only pull those images by printing the entire list to a file and then using a shell script to dvc pull each name.\n- The user can request a feature to name subsets and be able to pull them using a specific name in the DVC Github repository.\n- The user can use the upcoming wildcard feature called \"globbing\" in DVC, which can be seen in the DVC Github repository, to manage their dataset depending on their naming\/file structure.\n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"solut provid user determin imag subset miss local pull imag print entir list file shell script pull user request featur subset abl pull specif github repositori user us upcom wildcard featur call glob seen github repositori manag dataset depend name file structur person opinion bias includ respons"
    },
    {
        "Question_title":"Explicitly define pipeline execution order",
        "Question_body":"<p>Is there a way to specify a dependency between two pipeline stages except for a <code>outs<\/code> and <code>deps<\/code> combination? My use case is to run two stages consecutive, although they don\u2019t read \/ write files, that could be tracked by DVC, but depend on each other in a different way, e.g. by one stage writing a value to an external database and the other one reading it again.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645444562953,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":129.0,
        "Answer_body":"<p>The only way to define stage dependencies is via <code>outs<\/code> and <code>deps<\/code> files.<\/p>\n<p>For your situation, you could just write an empty\/temporary file as an <code>out<\/code> for the first stage, and also mark it as <code>cache: false<\/code> (since you don\u2019t actually need DVC to care about the contents of the file).<\/p>\n<p>Then in the second stage you could add the empty file as a <code>dep<\/code>, and then also mark the stage as <code>always_changed: true<\/code> so that the second stage is always executed (even though the content of the empty file won\u2019t ever be modified)<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/explicitly-define-pipeline-execution-order\/1073",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-22T02:20:47.287Z",
                "Answer_body":"<p>The only way to define stage dependencies is via <code>outs<\/code> and <code>deps<\/code> files.<\/p>\n<p>For your situation, you could just write an empty\/temporary file as an <code>out<\/code> for the first stage, and also mark it as <code>cache: false<\/code> (since you don\u2019t actually need DVC to care about the contents of the file).<\/p>\n<p>Then in the second stage you could add the empty file as a <code>dep<\/code>, and then also mark the stage as <code>always_changed: true<\/code> so that the second stage is always executed (even though the content of the empty file won\u2019t ever be modified)<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"explicitli defin pipelin execut order wai specifi depend pipelin stage out dep combin us case run stage consecut dont read write file track depend differ wai stage write valu extern databas read",
        "Question_preprocessed_content":"explicitli defin pipelin execut order wai specifi depend pipelin stage combin us case run stage consecut dont read write file track depend differ wai stage write valu extern databas read",
        "Question_gpt_summary_original":"The user is facing a challenge in specifying a dependency between two pipeline stages, which do not read or write files that can be tracked by DVC. The stages depend on each other in a different way, such as one stage writing a value to an external database and the other stage reading it again. The user is seeking a way to run these stages consecutively without using the <code>outs<\/code> and <code>deps<\/code> combination.",
        "Question_gpt_summary":"user face challeng specifi depend pipelin stage read write file track stage depend differ wai stage write valu extern databas stage read user seek wai run stage consecut out dep combin",
        "Answer_original_content":"wai defin stage depend out dep file situat write temporari file stage mark cach fals dont actual need care content file second stage add file dep mark stage chang true second stage execut content file wont modifi",
        "Answer_preprocessed_content":"wai defin stage depend file situat write file stage mark second stage add file mark stage second stage execut",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to write an empty\/temporary file as an <code>out<\/code> for the first stage, and mark it as <code>cache: false<\/code>. Then, in the second stage, add the empty file as a <code>dep<\/code> and mark the stage as <code>always_changed: true<\/code> so that the second stage is always executed.",
        "Answer_gpt_summary":"solut possibl solut mention discuss write temporari file stage mark cach fals second stage add file dep mark stage chang true second stage execut"
    },
    {
        "Question_title":"Deploying custom model on Azure ML Studio",
        "Question_body":"<p>In Azure ML Studio, we have the option of choosing a number of inbuilt ML models like Classification, Regression, etc. , which we can drag and drop to our workflow.<\/p>\n\n<p>My question is, can I upload a custom ML model that I have built locally on my system in Python, and add it to the workflow?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1565761282107,
        "Question_favorite_count":null,
        "Question_last_edit_time":1565767277470,
        "Question_score":1.0,
        "Question_view_count":1187.0,
        "Answer_body":"<ol>\n<li>Take the model.pkl file, zip it, and upload it into Azure Machine Learning Studio. Click the \u201cNew\u201d icon in the bottom left:\n<a href=\"https:\/\/i.stack.imgur.com\/Iwvhi.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Iwvhi.jpg\" alt=\"\"><\/a><\/li>\n<li>In the pane that comes up, click on dataset, and then \u201cFrom Local File\u201d:\n<a href=\"https:\/\/i.stack.imgur.com\/DvyjO.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DvyjO.jpg\" alt=\"\"><\/a><\/li>\n<li>Select the zip file where you stored your serialized model and click the tick. You expirement should look like this:\n<a href=\"https:\/\/i.stack.imgur.com\/0efka.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0efka.jpg\" alt=\"\"><\/a><\/li>\n<li>Put the following code to run your classification experiment:<\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport sys\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    sys.path.insert(0,\".\\Script Bundle\")\n    model = pickle.load(open(\".\\Script Bundle\\model.pkl\", 'rb'))\n    pred = model.predict(dataframe1)\n    return pd.DataFrame([pred[0]])\n<\/code><\/pre>\n\n<p><strong>Update<\/strong> \nIf you want to declare this experiment as an API you need to add web input and output to the Python script module.\n<a href=\"https:\/\/i.stack.imgur.com\/eqV8W.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eqV8W.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":7.0,
        "Answer_last_edit_time":1566565696980,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57488706",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1566202471556,
        "Question_original_content":"deploi custom model studio studio option choos number inbuilt model like classif regress drag drop workflow question upload custom model built local python add workflow",
        "Question_preprocessed_content":"deploi custom model studio studio option choos number inbuilt model like classif regress drag drop workflow question upload custom model built local python add workflow",
        "Question_gpt_summary_original":"The user is facing a challenge of deploying a custom machine learning model on Azure ML Studio and wants to know if it is possible to upload a locally built model in Python to the workflow.",
        "Question_gpt_summary":"user face challeng deploi custom machin learn model studio want know possibl upload local built model python workflow",
        "Answer_original_content":"model pkl file zip upload studio click new icon left pane come click dataset local file select zip file store serial model click tick expir look like follow code run classif experi import panda import sy import pickl def main datafram datafram sy path insert script bundl model pickl load open script bundl model pkl pred model predict datafram return datafram pred updat want declar experi api need add web input output python script modul",
        "Answer_preprocessed_content":"file zip upload studio click new icon left pane come click dataset local file select zip file store serial model click tick expir look like follow code run classif experi updat want declar experi api need add web input output python script modul",
        "Answer_gpt_summary_original":"Solution: The user can upload the locally built model in Python to Azure ML Studio by zipping the model.pkl file and uploading it as a dataset in the workflow. Then, the user can run the classification experiment by adding the provided code to the Python script module. If the user wants to declare this experiment as an API, they need to add web input and output to the Python script module.",
        "Answer_gpt_summary":"solut user upload local built model python studio zip model pkl file upload dataset workflow user run classif experi ad provid code python script modul user want declar experi api need add web input output python script modul"
    },
    {
        "Question_title":"Query a table\/database in Athena from a Notebook instance",
        "Question_body":"<p>I have developed different Athena Workgroups for different teams so that I can separate their queries and their query results. The users would like to query the tables available to them from their notebook instances (JupyterLab). I am having difficulty finding code which successfully covers the requirement of querying a table from the user's specific workgroup. I have only found code that will query the table from the primary workgroup. <\/p>\n\n<p>The code I have currently used is added below. <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from pyathena import connect\nimport pandas as pd\nconn = connect(s3_staging_dir='&lt;ATHENA QUERY RESULTS LOCATION&gt;',\nregion_name='&lt;YOUR REGION, for example, us-west-2&gt;')\n\n\ndf = pd.read_sql(\"SELECT * FROM &lt;DATABASE-NAME&gt;.&lt;YOUR TABLE NAME&gt; limit 8;\", conn)\ndf\n<\/code><\/pre>\n\n<p>This code does not work as the users only have access to perform queries from their specific workgroups hence get errors when this code is run. It also does not cover the requirement of separating the user's queries in user specific workgroups. <\/p>\n\n<p>Any suggestions on how I can add alter the code so that I can run the queries within a specific workgroup from the notebook instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1565704743610,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":5367.0,
        "Answer_body":"<p>Documentation of <code>pyathena<\/code> is not super extensive, but after looking into source code we can see that <code>connect<\/code> simply creates instance of <code>Connection<\/code> class.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def connect(*args, **kwargs):\n    from pyathena.connection import Connection\n    return Connection(*args, **kwargs)\n<\/code><\/pre>\n\n<p>Now, after looking into signature of <code>Connection.__init__<\/code> on <a href=\"https:\/\/github.com\/laughingman7743\/PyAthena\/blob\/master\/pyathena\/connection.py\" rel=\"nofollow noreferrer\">GitHub<\/a> we can see parameter <code>work_group=None<\/code> which name in the same way as one of the parameters for <code>start_query_execution<\/code> from the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/athena.html#Athena.Client.start_query_execution\" rel=\"nofollow noreferrer\">official<\/a> AWS Python API <code>boto3<\/code>. Here is what their documentation say about it:<\/p>\n\n<blockquote>\n  <p>WorkGroup (string) -- The name of the workgroup in which the query is being started.<\/p>\n<\/blockquote>\n\n<p>After following through usages and imports in <code>Connection<\/code> we endup with <a href=\"https:\/\/github.com\/laughingman7743\/PyAthena\/blob\/master\/pyathena\/common.py\" rel=\"nofollow noreferrer\">BaseCursor<\/a> class that under the hood makes a call to <code>start_query_execution<\/code> while unpacking a dictionary with parameters assembled by <code>BaseCursor._build_start_query_execution_request<\/code> method. That is excatly where we can see familar syntax for submitting queries to AWS Athena, in particular the following part:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>if self._work_group or work_group:\n    request.update({\n        'WorkGroup': work_group if work_group else self._work_group\n    })\n<\/code><\/pre>\n\n<p>So this should do a trick for your case:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom pyathena import connect\n\n\nconn = connect(\n    s3_staging_dir='&lt;ATHENA QUERY RESULTS LOCATION&gt;',\n    region_name='&lt;YOUR REGION, for example, us-west-2&gt;',\n    work_group='&lt;USER SPECIFIC WORKGROUP&gt;'\n)\n\ndf = pd.read_sql(\"SELECT * FROM &lt;DATABASE-NAME&gt;.&lt;YOUR TABLE NAME&gt; limit 8;\", conn)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1565966644207,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57479389",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1565963773252,
        "Question_original_content":"queri tabl databas athena notebook instanc develop differ athena workgroup differ team separ queri queri result user like queri tabl avail notebook instanc jupyterlab have difficulti find code successfulli cover requir queri tabl user specif workgroup code queri tabl primari workgroup code current ad pyathena import connect import panda conn connect stage dir region read sql select limit conn code work user access perform queri specif workgroup error code run cover requir separ user queri user specif workgroup suggest add alter code run queri specif workgroup notebook instanc",
        "Question_preprocessed_content":"queri athena notebook instanc develop differ athena workgroup differ team separ queri queri result user like queri tabl avail notebook instanc have difficulti find code successfulli cover requir queri tabl user specif workgroup code queri tabl primari workgroup code current ad code work user access perform queri specif workgroup error code run cover requir separ user queri user specif workgroup suggest add alter code run queri specif workgroup notebook instanc",
        "Question_gpt_summary_original":"The user is facing challenges in querying tables from their specific workgroup in Athena using JupyterLab. The current code they have only queries tables from the primary workgroup, which results in errors for users who only have access to their specific workgroup. The user is seeking suggestions on how to modify the code to query tables within a specific workgroup from the notebook instance.",
        "Question_gpt_summary":"user face challeng queri tabl specif workgroup athena jupyterlab current code queri tabl primari workgroup result error user access specif workgroup user seek suggest modifi code queri tabl specif workgroup notebook instanc",
        "Answer_original_content":"document pyathena super extens look sourc code connect simpli creat instanc connect class def connect arg kwarg pyathena connect import connect return connect arg kwarg look signatur connect init github paramet work group wai paramet start queri execut offici aw python api boto document workgroup string workgroup queri start follow usag import connect endup basecursor class hood make start queri execut unpack dictionari paramet assembl basecursor build start queri execut request method excatli familar syntax submit queri aw athena particular follow self work group work group request updat workgroup work group work group self work group trick case import panda pyathena import connect conn connect stage dir region work group read sql select limit conn",
        "Answer_preprocessed_content":"document super extens look sourc code simpli creat instanc class look signatur github paramet wai paramet offici aw python api document workgroup workgroup queri start follow usag import endup basecursor class hood make unpack dictionari paramet assembl method excatli familar syntax submit queri aw athena particular follow trick case",
        "Answer_gpt_summary_original":"The solution suggested in the discussion is to modify the code by adding the parameter \"work_group\" to the \"connect\" function and specifying the user-specific workgroup. This can be done by importing pandas and pyathena, creating a connection object with the specified workgroup, and then using the connection object to query tables from the specific workgroup.",
        "Answer_gpt_summary":"solut suggest discuss modifi code ad paramet work group connect function specifi user specif workgroup import panda pyathena creat connect object specifi workgroup connect object queri tabl specif workgroup"
    },
    {
        "Question_title":"SageMaker gives CannotStartContainerError although I specified an entrypoint",
        "Question_body":"<p>I want to train a custom ML model with SageMaker. The model is written in Python and should be shipped to SageMaker in a Docker image. Here is a simplified version of my Dockerfile (the model sits in the train.py file):<\/p>\n\n<pre><code>FROM amazonlinux:latest\n\n# Install Python 3\nRUN yum -y update &amp;&amp; yum install -y python3-pip python3-devel gcc &amp;&amp; yum clean all\n\n# Install sagemaker-containers (the official SageMaker utils package)\nRUN pip3 install --target=\/usr\/local\/lib\/python3.7\/site-packages sagemaker-containers &amp;&amp; rm -rf \/root\/.cache\n\n# Bring the script with the model to the image \nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n\n<p>Now, if I initialize this image as a SageMaker estimator and then run the <code>fit<\/code> method on this estimator I get the following error:<\/p>\n\n<p>\"AlgorithmError: CannotStartContainerError. Please make sure the container can be run with 'docker run  train'.\"<\/p>\n\n<p>In other words: SageMaker is not able to get into the container and run the train.py file. But why? The way I am specifying the entrypoint with <code>ENV SAGEMAKER_PROGRAM train.py<\/code> is recommended in the <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\/blob\/master\/README.rst\" rel=\"nofollow noreferrer\">docs of the sagemaker-containers package<\/a> (see 'How a script is executed inside the container').<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1585665709967,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1133.0,
        "Answer_body":"<p>I found a hint in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">the AWS docs<\/a> and came up with this solution:<\/p>\n\n<pre><code>ENTRYPOINT [\"python3.7\", \"\/opt\/ml\/code\/train.py\"]\n<\/code><\/pre>\n\n<p>With this the container <a href=\"https:\/\/docs.docker.com\/engine\/reference\/builder\/#entrypoint\" rel=\"nofollow noreferrer\">will run as an executable<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60953289",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1585671760392,
        "Question_original_content":"give cannotstartcontainererror specifi entrypoint want train custom model model written python ship docker imag simplifi version dockerfil model sit train file amazonlinux latest instal python run yum updat yum instal python pip python devel gcc yum clean instal contain offici util packag run pip instal target usr local lib python site packag contain root cach bring script model imag copi train opt code train env program train initi imag estim run fit method estim follow error algorithmerror cannotstartcontainererror sure contain run docker run train word abl contain run train file wai specifi entrypoint env program train recommend doc contain packag script execut insid contain",
        "Question_preprocessed_content":"give cannotstartcontainererror specifi entrypoint want train custom model model written python ship docker imag simplifi version dockerfil initi imag estim run method estim follow error algorithmerror cannotstartcontainererror sure contain run docker run word abl contain run file wai specifi entrypoint recommend doc contain packag",
        "Question_gpt_summary_original":"The user is encountering a challenge while training a custom ML model with SageMaker. The error message \"CannotStartContainerError\" is displayed when the user runs the \"fit\" method on the estimator. The user has specified the entrypoint using \"ENV SAGEMAKER_PROGRAM train.py\" which is recommended in the sagemaker-containers package documentation.",
        "Question_gpt_summary":"user encount challeng train custom model error messag cannotstartcontainererror displai user run fit method estim user specifi entrypoint env program train recommend contain packag document",
        "Answer_original_content":"hint aw doc came solut entrypoint python opt code train contain run execut",
        "Answer_preprocessed_content":"hint aw doc came solut contain run execut",
        "Answer_gpt_summary_original":"Solution: The user found a solution in the AWS documentation and specified the entrypoint using \"ENTRYPOINT [\"python3.7\", \"\/opt\/ml\/code\/train.py\"]\" which will allow the container to run as an executable.",
        "Answer_gpt_summary":"solut user solut aw document specifi entrypoint entrypoint python opt code train allow contain run execut"
    },
    {
        "Question_title":"Workaround for timeout error in Dataset.download()",
        "Question_body":"<p>azureml-sdk version: <code>1.0.85<\/code><\/p>\n\n<p>Calling below (as given in the Dataset UI), I get this<\/p>\n\n<pre><code>ds_split = Dataset.get_by_name(workspace, name='ret- holdout-split')\nds_split.download(target_path=dir_outputs, overwrite=True)\n<\/code><\/pre>\n\n<pre><code>UnexpectedError:\n{'errorCode': 'Microsoft.DataPrep.ErrorCodes.Unknown', 'message':\n    'The client could not finish the operation within specified timeout.',\n    'errorData': {}}\n<\/code><\/pre>\n\n<p>The <code>FileDataset<\/code> 1GB pickled file stored in blob.\n<a href=\"https:\/\/gist.github.com\/swanderz\/c608ced5f2c6a2802b7553bc9ead0762\" rel=\"nofollow noreferrer\">Here's a gist with the full traceback<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1581384771000,
        "Question_favorite_count":null,
        "Question_last_edit_time":1581436827487,
        "Question_score":0.0,
        "Question_view_count":217.0,
        "Answer_body":"<p>Tried again this AM and it worked. let's file this under \"transient error\"<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60160773",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1581436892352,
        "Question_original_content":"workaround timeout error dataset download sdk version call given dataset split dataset workspac ret holdout split split download target path dir output overwrit true unexpectederror errorcod microsoft dataprep errorcod unknown messag client finish oper specifi timeout errordata filedataset pickl file store blob gist traceback",
        "Question_preprocessed_content":"workaround timeout error sdk version call pickl file store blob gist traceback",
        "Question_gpt_summary_original":"The user is encountering a timeout error while trying to download a 1GB pickled file stored in a blob using the Dataset.get_by_name() function in azureml-sdk version 1.0.85. The error message received is \"The client could not finish the operation within specified timeout.\"",
        "Question_gpt_summary":"user encount timeout error try download pickl file store blob dataset function sdk version error messag receiv client finish oper specifi timeout",
        "Answer_original_content":"tri work let file transient error",
        "Answer_preprocessed_content":"tri work let file transient error",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut mention discuss"
    },
    {
        "Question_title":"Control tracked version of external dependency",
        "Question_body":"<p>I am trying to set up a DVC repository for machine learning data with different tagged versions of the dataset. I do this with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd \/raid\/ml_data  # folder on a data drive\n$ git init\n$ dvc init\n$ [add data]\n$ [commit to dvc, git]\n$ git tag -a 1.0.0\n$ [add or change data]\n$ [commit to dvc, git]\n$ git tag -a 1.1.0\n<\/code><\/pre>\n<p>I have multiple projects that each need to reference some version of this dataset. The problem is I can't figure out how to set up those projects to reference a specific version. I'm able to track the <code>HEAD<\/code> of the repo with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd ~\/my_proj  # different drive than the remote\n$ mkdir data\n$ git init\n$ dvc init\n$ dvc remote add -d local \/raid\/ml_data  # add the remote on my data drive\n$ dvc cache dir \/raid\/ml_data\/.dvc\/cache  # tell DVC to use the remote cache\n$ dvc checkout\n$ dvc run --external -d \/raid\/ml_data -o data\/ cp -r \/raid\/ml_data data\n<\/code><\/pre>\n<p>This gets me the latest version of the dataset, symlinked into my <code>data<\/code> folder, but what if I want some projects to use the <code>1.0.0<\/code> version and some to use the <code>1.1.0<\/code> version, or another version? Or for that matter, if I update the dataset to <code>2.0.0<\/code> but don't want my existing projects to necessarily track <code>HEAD<\/code> and instead keep the version with which they were set up?<\/p>\n<p>It's important to me to not create a ton of local copies of my dataset as the <code>\/home<\/code> drive is much smaller than the <code>\/raid<\/code> drive and some of these datasets are huge.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1604349754297,
        "Question_favorite_count":null,
        "Question_last_edit_time":1604361023336,
        "Question_score":2.0,
        "Question_view_count":139.0,
        "Answer_body":"<p>I think you are looking for the <a href=\"https:\/\/dvc.org\/doc\/start\/data-access\" rel=\"nofollow noreferrer\">data access<\/a> set of commands.<\/p>\n<p>In your particular case, <code>dvc import<\/code> makes sense:<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data\n<\/code><\/pre>\n<p>if you want to get the most recent version (HEAD). Then you will be able to update it with the <code>dvc update<\/code> command (if 2.0.0 is released, for example).<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data --rev 1.0.0\n<\/code><\/pre>\n<p>if you'd like to &quot;fix&quot; it to the specific version.<\/p>\n<h3>Avoiding copies<\/h3>\n<p>Make sure also, that <code>symlinks<\/code> are set for the second project, as described in the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"nofollow noreferrer\">Large Dataset Optimization<\/a>:<\/p>\n<pre><code>$ dvc config cache.type reflink,hardlink,symlink,copy\n<\/code><\/pre>\n<p>(there are config modifiers <code>--global<\/code>, <code>--local<\/code>, <code>--system<\/code> to set this setting for everyone at once, or just for one project, etc)<\/p>\n<p>Check the details instruction <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization#configuring-dvc-cache-file-link-type\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<hr \/>\n<p>Overall, it's a great setup, and looks like you got pretty much everything right. Please, don't hesitate to follow up and\/or create other questions here- we'll help you with this.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1604362563343,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64653042",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1604351561432,
        "Question_original_content":"control track version extern depend try set repositori machin learn data differ tag version dataset like raid data folder data drive git init init add data commit git git tag add chang data commit git git tag multipl project need refer version dataset problem figur set project refer specif version abl track head repo like proj differ drive remot mkdir data git init init remot add local raid data add remot data drive cach dir raid data cach tell us remot cach checkout run extern raid data data raid data data get latest version dataset symlink data folder want project us version us version version matter updat dataset want exist project necessarili track head instead version set import creat ton local copi dataset home drive smaller raid drive dataset huge",
        "Question_preprocessed_content":"control track version extern depend try set repositori machin learn data differ tag version dataset like multipl project need refer version dataset problem figur set project refer specif version abl track repo like get latest version dataset symlink folder want project us version us version version matter updat dataset want exist project necessarili track instead version set import creat ton local copi dataset drive smaller drive dataset huge",
        "Question_gpt_summary_original":"The user is facing challenges in setting up a DVC repository for machine learning data with different tagged versions of the dataset. They are unable to figure out how to set up multiple projects to reference a specific version of the dataset and avoid creating local copies of the dataset due to limited storage space.",
        "Question_gpt_summary":"user face challeng set repositori machin learn data differ tag version dataset unabl figur set multipl project refer specif version dataset avoid creat local copi dataset limit storag space",
        "Answer_original_content":"think look data access set command particular case import make sens import raid data data want recent version head abl updat updat command releas exampl import raid data data rev like fix specif version avoid copi sure symlink set second project describ larg dataset optim config cach type reflink hardlink symlink copi config modifi global local set set project check detail instruct overal great setup look like got pretti right hesit follow creat question help",
        "Answer_preprocessed_content":"think look data access set command particular case make sens want recent version abl updat command like fix specif version avoid copi sure set second project describ larg dataset optim config modifi set set project check detail instruct overal great setup look like got pretti right hesit follow creat question help",
        "Answer_gpt_summary_original":"Solution:\n- Use the \"data access\" set of commands, specifically the \"dvc import\" command to reference a specific version of the dataset.\n- Use the \"dvc update\" command to update the dataset to a newer version.\n- Set symlinks for the second project to avoid creating local copies of the dataset.\n- Use the \"dvc config cache.type\" command to set the cache type for the project. \n\nNo personal opinions or biases were included in the summary.",
        "Answer_gpt_summary":"solut us data access set command specif import command refer specif version dataset us updat command updat dataset newer version set symlink second project avoid creat local copi dataset us config cach type command set cach type project person opinion bias includ summari"
    },
    {
        "Question_title":"C# async\/await to F# using Azure ML example",
        "Question_body":"<p>I am working with Azure ML and I have the code sample to invoke my web  service (alas it is only in C#).  Can someone help me translate this to F#?  I have everything but the async and await done.<\/p>\n\n<pre><code> static async Task InvokeRequestResponseService()\n        {\n            using (var client = new HttpClient())\n            {\n                ScoreData scoreData = new ScoreData()\n                {\n                    FeatureVector = new Dictionary&lt;string, string&gt;() \n                    {\n                        { \"Zip Code\", \"0\" },\n                        { \"Race\", \"0\" },\n                        { \"Party\", \"0\" },\n                        { \"Gender\", \"0\" },\n                        { \"Age\", \"0\" },\n                        { \"Voted Ind\", \"0\" },\n                    },\n                    GlobalParameters = new Dictionary&lt;string, string&gt;() \n                    {\n                    }\n                };\n\n                ScoreRequest scoreRequest = new ScoreRequest()\n                {\n                    Id = \"score00001\",\n                    Instance = scoreData\n                };\n\n                const string apiKey = \"abc123\"; \/\/ Replace this with the API key for the web service\n                client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue( \"Bearer\", apiKey);\n\n                client.BaseAddress = new Uri(\"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/19a2e623b6a944a3a7f07c74b31c3b6d\/services\/f51945a42efa42a49f563a59561f5014\/score\");\n                HttpResponseMessage response = await client.PostAsJsonAsync(\"\", scoreRequest);\n                if (response.IsSuccessStatusCode)\n                {\n                    string result = await response.Content.ReadAsStringAsync();\n                    Console.WriteLine(\"Result: {0}\", result);\n                }\n                else\n                {\n                    Console.WriteLine(\"Failed with status code: {0}\", response.StatusCode);\n                }\n            }\n<\/code><\/pre>\n\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1410732744493,
        "Question_favorite_count":null,
        "Question_last_edit_time":1446025307110,
        "Question_score":2.0,
        "Question_view_count":607.0,
        "Answer_body":"<p>I was not able to compile and run the code, but you probably need something like this:<\/p>\n\n<pre><code>let invokeRequestResponseService() = async {\n    use client = new HttpClient()\n    let scoreData = (...)\n    let apiKey = \"abc123\"\n    client.DefaultRequestHeaders.Authorization &lt;- \n        new AuthenticationHeaderValue(\"Bearer\", apiKey)\n    client.BaseAddress &lt;- Uri(\"https:\/\/ussouthcentral....\/score\");\n    let! response = client.PostAsJsonAsync(\"\", scoreRequest) |&gt; Async.AwaitTask\n    if response.IsSuccessStatusCode then\n        let! result = response.Content.ReadAsStringAsync() |&gt; Async.AwaitTask\n        Console.WriteLine(\"Result: {0}\", result);\n    else\n        Console.WriteLine(\"Failed with status code: {0}\", response.StatusCode) }\n<\/code><\/pre>\n\n<ul>\n<li><p>Wrapping the code in the <code>async { .. }<\/code> block makes it asynchronous and lets you use <code>let!<\/code> inside the block to perform asynchronous waiting (i.e. in places where you'd use <code>await<\/code> in C#)<\/p><\/li>\n<li><p>F# uses type <code>Async&lt;T&gt;<\/code> instead of .NET Task, so when you're awaiting a task, you need to insert <code>Async.AwaitTask<\/code> (or you can write wrappers for the most frequently used operations)<\/p><\/li>\n<li><p>The <code>invokeRequestResponseService()<\/code> function returns F# async, so if you need to pass it to some other library function (or if it needs to return a task), you can use <code>Async.StartAsTask<\/code><\/p><\/li>\n<\/ul>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/25838512",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1410733358503,
        "Question_original_content":"async await exampl work code sampl invok web servic ala help translat async await static async task invokerequestresponseservic var client new httpclient scoredata scoredata new scoredata featurevector new dictionari zip code race parti gender ag vote ind globalparamet new dictionari scorerequest scorerequest new scorerequest score instanc scoredata const string apikei abc replac api kei web servic client defaultrequesthead author new authenticationheadervalu bearer apikei client baseaddress new uri http ussouthcentr servic net workspac aebaaafcbcbd servic faefaafaf score httpresponsemessag respons await client postasjsonasync scorerequest respons issuccetatuscod string result await respons content readasstringasync consol writelin result result consol writelin fail statu code respons statuscod thank",
        "Question_preprocessed_content":"exampl work code sampl invok web servic help translat async await thank",
        "Question_gpt_summary_original":"The user is facing a challenge in translating a C# code sample to F# for invoking a web service in Azure ML. The user has everything except for the async and await functions.",
        "Question_gpt_summary":"user face challeng translat code sampl invok web servic user async await function",
        "Answer_original_content":"abl compil run code probabl need like let invokerequestresponseservic async us client new httpclient let scoredata let apikei abc client defaultrequesthead author async awaittask respons issuccetatuscod let result respons content readasstringasync async awaittask consol writelin result result consol writelin fail statu code respons statuscod wrap code async block make asynchron let us let insid block perform asynchron wait place us await us type async instead net task await task need insert async awaittask write wrapper frequent oper invokerequestresponseservic function return async need pass librari function need return task us async startastask",
        "Answer_preprocessed_content":"abl compil run code probabl need like wrap code block make asynchron let us insid block perform asynchron wait us type instead net task await task need insert function return async need pass librari function us",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Wrap the code in the `async { .. }` block to make it asynchronous and use `let!` inside the block to perform asynchronous waiting.\n- Use type `Async<T>` instead of .NET Task, and insert `Async.AwaitTask` when awaiting a task.\n- The `invokeRequestResponseService()` function returns F# async, so use `Async.StartAsTask` if it needs to return a task.\n\nThese solutions were provided to help the user translate a C# code sample to F# for invoking a web service in Azure ML.",
        "Answer_gpt_summary":"possibl solut mention discuss wrap code async block asynchron us let insid block perform asynchron wait us type async instead net task insert async awaittask await task invokerequestresponseservic function return async us async startastask need return task solut provid help user translat code sampl invok web servic"
    },
    {
        "Question_title":"Which Amazon SageMaker algorithms can only use GPU for training?",
        "Question_body":"I read somewhere that some Amazon SageMaker's built-in algorithms can only be trained using GPU, whereas some can use either GPU or CPU, and some can only be used on CPU.\n\nIs there any official documentation explicitly stating which algorithms can only use GPU or both?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597251203000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":121.0,
        "Answer_body":"Documentation for Amazon SageMaker built-in algorithms provides recommendations around choice of Amazon EC2 instances and whether given algorithm supports GPU or CPU devices.\n\nLet's take Image Classification as an example. Here is a excerpt from online documentation:\n\nFor image classification, we support the following GPU instances for training: ml.p2.xlarge, ml.p2.8xlarge, ml.p2.16xlarge, ml.p3.2xlarge, ml.p3.8xlargeand ml.p3.16xlarge. We recommend using GPU instances with more memory for training with large batch sizes. However, both CPU (such as C4) and GPU (such as P2 and P3) instances can be used for the inference. You can also run the algorithm on multi-GPU and multi-machine settings for distributed training.\n\nFor more complex scenarios, such as Script or BYO Container modes, customers have flexibility to choose which device (GPU or CPU) to utilize for which operation. This is configured as part of their training scripts.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdTLbPM2STGelSj1g3TIjpA\/which-amazon-sage-maker-algorithms-can-only-use-gpu-for-training",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-08-12T17:02:17.000Z",
                "Answer_score":0,
                "Answer_body":"Documentation for Amazon SageMaker built-in algorithms provides recommendations around choice of Amazon EC2 instances and whether given algorithm supports GPU or CPU devices.\n\nLet's take Image Classification as an example. Here is a excerpt from online documentation:\n\nFor image classification, we support the following GPU instances for training: ml.p2.xlarge, ml.p2.8xlarge, ml.p2.16xlarge, ml.p3.2xlarge, ml.p3.8xlargeand ml.p3.16xlarge. We recommend using GPU instances with more memory for training with large batch sizes. However, both CPU (such as C4) and GPU (such as P2 and P3) instances can be used for the inference. You can also run the algorithm on multi-GPU and multi-machine settings for distributed training.\n\nFor more complex scenarios, such as Script or BYO Container modes, customers have flexibility to choose which device (GPU or CPU) to utilize for which operation. This is configured as part of their training scripts.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1597251737000,
        "Question_original_content":"algorithm us gpu train read built algorithm train gpu us gpu cpu cpu offici document explicitli state algorithm us gpu",
        "Question_preprocessed_content":"algorithm us gpu train read algorithm train gpu us gpu cpu cpu offici document explicitli state algorithm us gpu",
        "Question_gpt_summary_original":"The user is seeking official documentation that explicitly states which Amazon SageMaker algorithms can only be trained using GPU, as opposed to those that can use either GPU or CPU, or only CPU.",
        "Question_gpt_summary":"user seek offici document explicitli state algorithm train gpu oppos us gpu cpu cpu",
        "Answer_original_content":"document built algorithm provid recommend choic amazon instanc given algorithm support gpu cpu devic let imag classif exampl excerpt onlin document imag classif support follow gpu instanc train xlarg xlarg xlarg xlarg xlargeand xlarg recommend gpu instanc memori train larg batch size cpu gpu instanc infer run algorithm multi gpu multi machin set distribut train complex scenario script byo contain mode custom flexibl choos devic gpu cpu util oper configur train script",
        "Answer_preprocessed_content":"document algorithm provid recommend choic amazon instanc given algorithm support gpu cpu devic let imag classif exampl excerpt onlin document imag classif support follow gpu instanc train recommend gpu instanc memori train larg batch size cpu gpu instanc infer run algorithm set distribut train complex scenario script byo contain mode custom flexibl choos devic util oper configur train script",
        "Answer_gpt_summary_original":"Summary: The Amazon SageMaker documentation provides recommendations for which Amazon EC2 instances to use for training with GPU or CPU devices for various built-in algorithms, including Image Classification. For more complex scenarios, customers have flexibility to choose which device to utilize for which operation.",
        "Answer_gpt_summary":"summari document provid recommend amazon instanc us train gpu cpu devic built algorithm includ imag classif complex scenario custom flexibl choos devic util oper"
    },
    {
        "Question_title":"Admin Tool",
        "Question_body":"Hi ,\n\n\nI hope you are good.\n\n\nIs there any governance or admin tool in mlflow by which we can give only access to certain people to register a model or change a model status?\n\n\nThanks\u00a0\nSonalee",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1589950750000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":36.0,
        "Answer_body":"Hello Sonalee,\n\n\nAt the moment the controlled access policies to model registry is part of the enterprise managed MLFlow feature.\u00a0\n\n\nWe welcome community contributions for a generic RBAC for the OSS MLflow.\u00a0\n\n\nDo share with the community if you have any ideas.\u00a0\n\n\nCheers\nJules\u00a0\n\n\n\n\nSent from my iPhone\nPardon the dumb thumb typos :)\n\n\nOn May 20, 2020, at 1:59 AM, Sonalee Naina <sonale...@gmail.com> wrote:\n\n\n\ufeff\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/df679645-25d6-48d4-84c8-6ec9eb81009d%40googlegroups.com.. Hi Jules,\n\n\nCan\u00a0you share more details about the enterprise managed MLFlow feature?\n\n\nThanks,\n\n\nHien\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/3F296C11-B9C5-40DF-B4EF-DB75CFB9B171%40databricks.com.. Yes, Sonalee,\n\n\nIncidentally, I just finished doing a part-3 (on MLflow Model Registry) of a\u00a0 three-part series. You\u00a0can\u00a0\nhttps:\/\/www.youtube.com\/watch?v=AxYmj8ufKKY&feature=youtu.be\n\n\nThere's also a blog where we explain the enterprise features:\u00a0\nhttps:\/\/databricks.com\/blog\/2020\/04\/15\/databricks-extends-mlflow-model-registry-with-enterprise-features.html\n\n\n\nHTH!\n\n\nCheers\nJules\n\n\n\n\n\n\n\n\n\u2013\u2013\n\nThe Best Ideas are Simple\n\nJules S. Damji\n\nDeveloper Advocate\n\nDatabricks, Inc.\n\nju...@databricks.com\n\n(510) 304-7686\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/KHc7WI97kJE",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-05-20T11:09:47",
                "Answer_body":"Hello Sonalee,\n\n\nAt the moment the controlled access policies to model registry is part of the enterprise managed MLFlow feature.\u00a0\n\n\nWe welcome community contributions for a generic RBAC for the OSS MLflow.\u00a0\n\n\nDo share with the community if you have any ideas.\u00a0\n\n\nCheers\nJules\u00a0\n\n\n\n\nSent from my iPhone\nPardon the dumb thumb typos :)\n\n\nOn May 20, 2020, at 1:59 AM, Sonalee Naina <sonale...@gmail.com> wrote:\n\n\n\ufeff\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/df679645-25d6-48d4-84c8-6ec9eb81009d%40googlegroups.com."
            },
            {
                "Answer_creation_time":"2020-05-20T12:30:02",
                "Answer_body":"Hi Jules,\n\n\nCan\u00a0you share more details about the enterprise managed MLFlow feature?\n\n\nThanks,\n\n\nHien\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/3F296C11-B9C5-40DF-B4EF-DB75CFB9B171%40databricks.com."
            },
            {
                "Answer_creation_time":"2020-05-20T13:20:51",
                "Answer_body":"Yes, Sonalee,\n\n\nIncidentally, I just finished doing a part-3 (on MLflow Model Registry) of a\u00a0 three-part series. You\u00a0can\u00a0\nhttps:\/\/www.youtube.com\/watch?v=AxYmj8ufKKY&feature=youtu.be\n\n\nThere's also a blog where we explain the enterprise features:\u00a0\nhttps:\/\/databricks.com\/blog\/2020\/04\/15\/databricks-extends-mlflow-model-registry-with-enterprise-features.html\n\n\n\nHTH!\n\n\nCheers\nJules\n\n\n\n\n\n\n\n\n\u2013\u2013\n\nThe Best Ideas are Simple\n\nJules S. Damji\n\nDeveloper Advocate\n\nDatabricks, Inc.\n\nju...@databricks.com\n\n(510) 304-7686\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"admin tool hope good govern admin tool access certain peopl regist model chang model statu thank sonale",
        "Question_preprocessed_content":"admin tool hope good govern admin tool access certain peopl regist model chang model statu thank sonale",
        "Question_gpt_summary_original":"The user is inquiring about the availability of an admin tool in mlflow that can restrict access to certain individuals for registering or changing the status of a model.",
        "Question_gpt_summary":"user inquir avail admin tool restrict access certain individu regist chang statu model",
        "Answer_original_content":"hello sonale moment control access polici model registri enterpris manag featur welcom commun contribut gener rbac oss share commun idea cheer jule sent iphon pardon dumb thumb typo sonale naina wrote receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user ecebd googlegroup com jule canyou share detail enterpris manag featur thank hien view discuss web visit http group googl com msgid user bef dbcfbb databrick com ye sonale incident finish model registri seri youcan http youtub com watch axymjufkki featur youtu blog explain enterpris featur http databrick com blog databrick extend model registri enterpris featur html hth cheer jule best idea simpl jule damji develop advoc databrick databrick com",
        "Answer_preprocessed_content":"hello sonale moment control access polici model registri enterpris manag featur welcom commun contribut gener rbac oss share commun idea cheer jule sent iphon pardon dumb thumb typo sonale naina wrote receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit jule canyou share detail enterpris manag featur thank hien view discuss web visit ye sonale incident finish seri youcan blog explain enterpris featur hth cheer jule best idea simpl jule damji develop advoc databrick",
        "Answer_gpt_summary_original":"Solution: \n- The controlled access policies to model registry is currently only available in the enterprise managed MLFlow feature. \n- The community is open to contributions for a generic RBAC for the OSS MLflow.",
        "Answer_gpt_summary":"solut control access polici model registri current avail enterpris manag featur commun open contribut gener rbac oss"
    },
    {
        "Question_title":"mlflow.exceptions.MlflowException: Changing param values is not allowed. Param with key='input_rows' was already logged with value='32205'",
        "Question_body":"<p>I am using Mlflow as a work orchestration tool. I have a Machine Learning pipeline. In this pipeline, I have real-time data. I'm listening this data with Apache Kafka. Also, I'm doing this: Whenever 250 message comes to this topic, I'm gathering them, and I'm appending this message my previous data. After that, my training function is triggered. Thus, I am able to making new training in every 250 new data. With Mlflow, I can show the results, metrics and any other parameters of trained models. But After training occurred one time, the second one doesn't occurs, and It throws me this error which I have shown in title. Here it is my consumer:<\/p>\n<pre><code>topic_name = 'twitterdata'\ntrain_every = 250\n\n\ndef consume_tweets():\n    consumer = KafkaConsumer(\n        topic_name,\n        bootstrap_servers=['localhost:9093'],\n        auto_offset_reset='latest',\n        enable_auto_commit=True,\n        auto_commit_interval_ms=5000,\n        fetch_max_bytes=128,\n        max_poll_records=100,\n        value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n\n    tweet_counter = 0\n    for message in consumer:\n        tweets = json.loads(json.dumps(message.value))\n        # print(tweets['text'])\n        tweet_sentiment = make_prediction(tweets['text'])\n\n        if tweet_counter == train_every:\n            update_df()\n            data_path = 'data\/updated_tweets.csv'\n            train(data_path)\n            print(&quot;\\nTraining with new data is completed!\\n&quot;)\n            tweet_counter = 0\n\n        else:\n            tweet_counter += 1\n\n        publish_prediction(tweet_sentiment, tweets['text'])\n\n<\/code><\/pre>\n<p>And here it is my train.py:<\/p>\n<pre><code>train_tweets = pd.read_csv(DATA_PATH)\n    # train_tweets = train_tweets[:20000]\n\n    tweets = train_tweets.tweet.values\n    labels = train_tweets.label.values\n\n    # Log data params\n    mlflow.log_param('input_rows', train_tweets.shape[0])\n\n    # Do preprocessing and return vectorizer with it\n    vectorizer, processed_features = embedding(tweets)\n\n    # Saving vectorizer\n    save_vectorizer(vectorizer)\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)\n\n    # Handle imbalanced data by using 'Smote' and log to Mlflow\n    smote = SMOTE('minority')\n    mlflow.log_param(&quot;over-sampling&quot;, smote)\n\n    X_train, y_train = smote.fit_sample(X_train, y_train)\n\n    # text_classifier = MultinomialNB()\n    text_classifier = LogisticRegression(max_iter=10000)\n    text_classifier.fit(X_train, y_train)\n    predictions = text_classifier.predict(X_test)\n\n    # Model metrics\n    (rmse, mae, r2) = eval_metrics(y_test, predictions)\n\n    mlflow.log_param('os-row-Xtrain', X_train.shape[0])\n    mlflow.log_param('os-row-ytrain', y_train.shape[0])\n    mlflow.log_param(&quot;model_name&quot;, text_classifier)\n    mlflow.log_metric(&quot;rmse&quot;, rmse)\n    mlflow.log_metric(&quot;r2&quot;, r2)\n    mlflow.log_metric(&quot;mae&quot;, mae)\n    mlflow.log_metric('acc_score', accuracy_score(y_test, predictions))\n\n    mlflow.sklearn.log_model(text_classifier, &quot;model&quot;)\n<\/code><\/pre>\n<p>I couldn't solve the problem. MLflow is one of the newest tool, so issues and examples of Mlflow are very few.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1614014592930,
        "Question_favorite_count":null,
        "Question_last_edit_time":1614015098323,
        "Question_score":1.0,
        "Question_view_count":2716.0,
        "Answer_body":"<p>I think you need an MLflow &quot;run&quot; for every new batch of data, so that your parameters are logged independently for each new training.<\/p>\n<p>So, try the following in your consumer:<\/p>\n<pre><code>if tweet_counter == train_every:\n            update_df()\n            data_path = 'data\/updated_tweets.csv'\n            with mlflow.start_run() as mlrun:\n               train(data_path)\n            print(&quot;\\nTraining with new data is completed!\\n&quot;)\n            tweet_counter = 0\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66320435",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1614350710747,
        "Question_original_content":"except except chang param valu allow param kei input row log valu work orchestr tool machin learn pipelin pipelin real time data listen data apach kafka messag come topic gather append messag previou data train function trigger abl make new train new data result metric paramet train model train occur time second occur throw error shown titl consum topic twitterdata train def consum tweet consum kafkaconsum topic bootstrap server localhost auto offset reset latest enabl auto commit true auto commit interv fetch max byte max poll record valu deseri lambda json load decod utf tweet counter messag consum tweet json load json dump messag valu print tweet text tweet sentiment predict tweet text tweet counter train updat data path data updat tweet csv train data path print ntrain new data complet tweet counter tweet counter publish predict tweet sentiment tweet text train train tweet read csv data path train tweet train tweet tweet train tweet tweet valu label train tweet label valu log data param log param input row train tweet shape preprocess return vector vector process featur embed tweet save vector save vector vector split data train test train test train test split process featur label test size random state handl imbalanc data smote log smote smote minor log param sampl smote train train smote fit sampl train train text classifi multinomialnb text classifi logisticregress max iter text classifi fit train train predict text classifi predict test model metric rmse mae eval metric test predict log param row xtrain train shape log param row ytrain train shape log param model text classifi log metric rmse rmse log metric log metric mae mae log metric acc score accuraci score test predict sklearn log model text classifi model couldn solv problem newest tool issu exampl",
        "Question_preprocessed_content":"chang param valu allow param log valu work orchestr tool machin learn pipelin pipelin data listen data apach kafka messag come topic gather append messag previou data train function trigger abl make new train new data result metric paramet train model train occur time second occur throw error shown titl consum couldn solv problem newest tool issu exampl",
        "Question_gpt_summary_original":"The user is encountering an error while using Mlflow as a work orchestration tool for a machine learning pipeline. The pipeline involves real-time data gathered from Apache Kafka, and the training function is triggered every time 250 new data is received. However, after the first training, the second one does not occur, and the error \"mlflow.exceptions.MlflowException: Changing param values is not allowed. Param with key='input_rows' was already logged with value='32205'\" is thrown. The user has provided the code for the consumer and train.py but has been unable to solve the problem.",
        "Question_gpt_summary":"user encount error work orchestr tool machin learn pipelin pipelin involv real time data gather apach kafka train function trigger time new data receiv train second occur error except except chang param valu allow param kei input row log valu thrown user provid code consum train unabl solv problem",
        "Answer_original_content":"think need run new batch data paramet log independ new train try follow consum tweet counter train updat data path data updat tweet csv start run mlrun train data path print ntrain new data complet tweet counter",
        "Answer_preprocessed_content":"think need run new batch data paramet log independ new train try follow consum",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to create a new MLflow \"run\" for every new batch of data, so that the parameters are logged independently for each new training. The suggested code involves starting a new MLflow run for every 250 new data received and training the model with the updated data.",
        "Answer_gpt_summary":"solut possibl solut mention discuss creat new run new batch data paramet log independ new train suggest code involv start new run new data receiv train model updat data"
    },
    {
        "Question_title":"Continue stopped run in MLflow",
        "Question_body":"<p>We run our experiment on AWS spot instances. Sometimes the experiments are stopped, and we would prefer to continue logging to the same run. How can you set the run-id of the active run?<\/p>\n<p>Something like this pseudocode (not working):<\/p>\n<pre><code>if new:\n    mlflow.start_run(experiment_id=1, run_name=x)\nelse:\n    mlflow.set_run(run_id)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618244956103,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":131.0,
        "Answer_body":"<p>You can pass the run_id directly to <code>start_run<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.start_run(experiment_id=1,\n                 run_name=x,\n                 run_id=&lt;run_id_of_interrupted_run&gt; # pass None to start a new run\n                 ) \n<\/code><\/pre>\n<p>Of course, you have to store the run_id for this. You can get it with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.entities.html#mlflow.entities.RunInfo.run_id\" rel=\"nofollow noreferrer\"><code>run.info.run_id<\/code><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67062145",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1631884865500,
        "Question_original_content":"continu stop run run experi aw spot instanc experi stop prefer continu log run set run activ run like pseudocod work new start run experi run set run run",
        "Question_preprocessed_content":"continu stop run run experi aw spot instanc experi stop prefer continu log run set activ run like pseudocod",
        "Question_gpt_summary_original":"The user is facing a challenge of continuing a stopped run in MLflow while logging to the same run. They are seeking a way to set the run-id of the active run to continue logging.",
        "Question_gpt_summary":"user face challeng continu stop run log run seek wai set run activ run continu log",
        "Answer_original_content":"pass run directli start run start run experi run run pass start new run cours store run run info run",
        "Answer_preprocessed_content":"pass directli cours store",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to pass the run_id directly to `start_run` function and store the run_id for this. The run_id can be obtained using `run.info.run_id` function.",
        "Answer_gpt_summary":"solut solut suggest discuss pass run directli start run function store run run obtain run info run function"
    },
    {
        "Question_title":"Is there a way for Optuna `suggest_categorical`to return multiple choices from list?",
        "Question_body":"<p>I am using Optuna for hyperparametrization of my model. And i have a field where I want to test multiple combinations from a list. For example: I have <code>[&quot;lisa&quot;,&quot;adam&quot;,&quot;test&quot;]<\/code> and i want <code>suggest_categorical<\/code> to return not just one, but a random combination: maybe <code>[&quot;lisa&quot;, &quot;adam&quot;]<\/code>, maybe <code>[&quot;adam&quot;]<\/code>, maybe <code>[&quot;lisa&quot;, &quot;adam&quot;, &quot;test&quot;]<\/code>. Is there a way to get this with built in Optuna function?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660737627797,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":56.0,
        "Answer_body":"<p>You could use <code>itertools.combinations<\/code> to generate all possible combinations of list items and then pass them to optuna's <code>suggest_categorical<\/code> as choices:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import optuna\nimport itertools\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# generate the combinations\niterable = ['lisa', 'adam', 'test']\ncombinations = []\nfor r in range(1, len(iterable) + 1):\n    combinations.extend([list(x) for x in itertools.combinations(iterable=iterable, r=r)])\nprint(combinations)\n# [['lisa'], ['adam'], ['test'], ['lisa', 'adam'], ['lisa', 'test'], ['adam', 'test'], ['lisa', 'adam', 'test']]\n\n# sample the combinations\ndef objective(trial):\n    combination = trial.suggest_categorical(name='combination', choices=combinations)\n    return round(random.random(), 2)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=3)\n# [I 2022-08-18 08:03:51,658] A new study created in memory with name: no-name-3874ce95-2394-4526-bb19-0d9822d7e45c\n# [I 2022-08-18 08:03:51,659] Trial 0 finished with value: 0.94 and parameters: {'combination': ['adam']}. Best is trial 0 with value: 0.94.\n# [I 2022-08-18 08:03:51,660] Trial 1 finished with value: 0.87 and parameters: {'combination': ['lisa', 'test']}. Best is trial 1 with value: 0.87.\n# [I 2022-08-18 08:03:51,660] Trial 2 finished with value: 0.29 and parameters: {'combination': ['lisa', 'adam']}. Best is trial 2 with value: 0.29.\n<\/code><\/pre>\n<p>Using lists as choices in optuna's <code>suggest_categorical<\/code> throws a warning message, but apparently this is mostly inconsequential (see <a href=\"https:\/\/github.com\/optuna\/optuna\/issues\/2341\" rel=\"nofollow noreferrer\">this issue<\/a> in optuna's GitHub repository).<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1660803027380,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73388133",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1660802705056,
        "Question_original_content":"wai suggest categor return multipl choic list hyperparametr model field want test multipl combin list exampl lisa adam test want suggest categor return random combin mayb lisa adam mayb adam mayb lisa adam test wai built function",
        "Question_preprocessed_content":"wai return multipl choic list hyperparametr model field want test multipl combin list exampl want return random combin mayb mayb mayb wai built function",
        "Question_gpt_summary_original":"The user is facing a challenge with using Optuna for hyperparameter optimization of their model. They want to test multiple combinations from a list using the `suggest_categorical` function, but the function only returns one choice at a time. The user is looking for a way to get a random combination of choices from the list using the built-in Optuna function.",
        "Question_gpt_summary":"user face challeng hyperparamet optim model want test multipl combin list suggest categor function function return choic time user look wai random combin choic list built function",
        "Answer_original_content":"us itertool combin gener possibl combin list item pass suggest categor choic import import itertool import random import warn warn filterwarn ignor gener combin iter lisa adam test combin rang len iter combin extend list itertool combin iter iter print combin lisa adam test lisa adam lisa test adam test lisa adam test sampl combin def object trial combin trial suggest categor combin choic combin return round random random studi creat studi studi optim object trial new studi creat memori ddec trial finish valu paramet combin adam best trial valu trial finish valu paramet combin lisa test best trial valu trial finish valu paramet combin lisa adam best trial valu list choic suggest categor throw warn messag appar inconsequenti issu github repositori",
        "Answer_preprocessed_content":"us gener possibl combin list item pass choic list choic throw warn messag appar inconsequenti",
        "Answer_gpt_summary_original":"Solution: One possible solution is to use `itertools.combinations` to generate all possible combinations of list items and then pass them to Optuna's `suggest_categorical` as choices. The combinations can be sampled using the `objective` function and the `create_study` function from Optuna. However, using lists as choices in Optuna's `suggest_categorical` throws a warning message, but this is mostly inconsequential.",
        "Answer_gpt_summary":"solut possibl solut us itertool combin gener possibl combin list item pass suggest categor choic combin sampl object function creat studi function list choic suggest categor throw warn messag inconsequenti"
    },
    {
        "Question_title":"Sagemaker with tensorflow 2 not saving model",
        "Question_body":"<p>I am working with Keras and I am trying to train a model using Sagemaker. I have the following issue:\nWhen I train my model using TensorFlow 1.12 everything works fine:<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='entrypoint-2.py',\n                            base_job_name='mlearning-test',\n                         role=role,\n                         train_instance_count=1,\n                         input_mode='Pipe',\n                         train_instance_type='ml.p2.xlarge',\n                         framework_version='1.12.0')\n<\/code><\/pre>\n\n<p>My model is trained and the model is saved in S3. Not problems.<\/p>\n\n<p>However, if I changed the framework version to be 2.0.0<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='entrypoint-2.py',\n                                base_job_name='mlearning-test',\n                             role=role,\n                             train_instance_count=1,\n                             input_mode='Pipe',\n                             train_instance_type='ml.p2.xlarge',\n                             framework_version='2.0.0')\n<\/code><\/pre>\n\n<p>I get the following error: <\/p>\n\n<pre><code>2020-02-12 13:54:36,601 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path \/opt\/ml\/model. Your training job will not save any model files to S3.\nFor details of how to construct your training script see:\nhttps:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html#adapting-your-local-tensorflow-script\n<\/code><\/pre>\n\n<p>The training job is marked as successful but there is nothing in the S3 bucket and indeed there was not training.<\/p>\n\n<p>As an alternative, I tried putting the py_version='py3' but this keeps happening. is there a major difference that I am not aware of when using TF2 on sagemaker? <\/p>\n\n<p>I don't think the entry point is needed since it works fine with version 1.12 but in case you are curious or can spot something here it is:<\/p>\n\n<pre><code>import tensorflow as tf\nfrom sagemaker_tensorflow import PipeModeDataset\n#from tensorflow.contrib.data import map_and_batch\n\nINPUT_TENSOR_NAME = 'inputs_input'  \nBATCH_SIZE = 64\nNUM_CLASSES = 5\nBUFFER_SIZE = 50\nPREFETCH_SIZE = 1\nLENGHT = 512\nSEED = 26\nEPOCHS = 1\nWIDTH = 512\n\ndef keras_model_fn(hyperparameters):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(WIDTH, 'relu', input_shape=(None, WIDTH), name = 'inputs'),\n        #tf.keras.layers.InputLayer(input_shape=(None, WIDTH), name=INPUT_TENSOR_NAME),\n        tf.keras.layers.Dense(256, 'relu'),\n        tf.keras.layers.Dense(128, 'relu'),\n        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n    ])\n\n    opt = tf.keras.optimizers.RMSprop()\n\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n    return model\n\ndef serving_input_fn(hyperparameters):\n    # Notice that the input placeholder has the same input shape as the Keras model input\n    tensor = tf.placeholder(tf.float32, shape=[None, WIDTH])\n\n    # The inputs key INPUT_TENSOR_NAME matches the Keras InputLayer name\n    inputs = {INPUT_TENSOR_NAME: tensor}\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\ndef train_input_fn(training_dir, params):\n    \"\"\"Returns input function that would feed the model during training\"\"\"\n    return _input_fn('train')\n\ndef eval_input_fn(training_dir, params):\n    \"\"\"Returns input function that would feed the model during evaluation\"\"\"\n    return _input_fn('eval')\n\ndef _input_fn(channel):\n    \"\"\"Returns a Dataset for reading from a SageMaker PipeMode channel.\"\"\"\n    print(\"DATA \"+channel)\n    features={\n        'question': tf.FixedLenFeature([WIDTH], tf.float32),\n        'label': tf.FixedLenFeature([1], tf.int64)\n    }\n\n    def parse(record):\n        parsed = tf.parse_single_example(record, features)\n        #print(\"--------&gt;\"+str(tf.cast(parsed['question'], tf.float32))\n        return {\n            INPUT_TENSOR_NAME: tf.cast(parsed['question'], tf.float32)\n        }, parsed['label']\n\n    ds = PipeModeDataset(channel)\n    if EPOCHS &gt; 1:\n        ds = ds.repeat(EPOCHS)\n    ds = ds.prefetch(PREFETCH_SIZE)\n    #ds = ds.apply(map_and_batch(parse, batch_size=BATCH_SIZE,\n    #                            num_parallel_batches=BUFFER_SIZE))\n    ds = ds.map(parse, num_parallel_calls=NUM_PARALLEL_BATCHES)\n    ds = ds.batch(BATCH_SIZE)\n\n    return ds\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1581516927893,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":913.0,
        "Answer_body":"<p>you're correct, <strong>there has been a major, beneficial change last year in the SageMaker TensorFlow experience named the <em>Script Mode<\/em> formalism<\/strong>. As you can see in the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">SDK Documentation<\/a>:<\/p>\n\n<p><em>\"Warning.\nWe have added a new format of your TensorFlow training script with TensorFlow version 1.11. This new way gives the user script more flexibility. This new format is called Script Mode, as opposed to Legacy Mode, which is what we support with TensorFlow 1.11 and older versions. In addition we are adding Python 3 support with Script Mode. The last supported version of Legacy Mode will be TensorFlow 1.12. Script Mode is available with TensorFlow version 1.11 and newer. Make sure you refer to the correct version of this README when you prepare your script. You can find the Legacy Mode README here.\"<\/em><\/p>\n\n<p>with TensorFlow 2, you need to follow that <em>Script Mode<\/em> formalism and save your model in the <code>opt\/ml\/model<\/code> path, otherwise nothing will be sent to S3. <em>Script Mode<\/em> is quite straightforward to implement and gives better flexibility and portability, and this spec is shared with SageMaker Sklearn container, SageMaker Pytorch container and SageMaker MXNet container so definitely worth adopting<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60190365",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1581724456863,
        "Question_original_content":"tensorflow save model work kera try train model follow issu train model tensorflow work fine estim tensorflow entri point entrypoint base job mlearn test role role train instanc count input mode pipe train instanc type xlarg framework version model train model save problem chang framework version estim tensorflow entri point entrypoint base job mlearn test role role train instanc count input mode pipe train instanc type xlarg framework version follow error tensorflow contain train warn model artifact save path opt model train job save model file detail construct train script http readthedoc stabl html adapt local tensorflow script train job mark success bucket train altern tri put version keep happen major differ awar think entri point need work fine version case curiou spot import tensorflow tensorflow import pipemodedataset tensorflow contrib data import map batch input tensor input input batch size num class buffer size prefetch size lenght seed epoch width def kera model hyperparamet model kera sequenti kera layer dens width relu input shape width input kera layer inputlay input shape width input tensor kera layer dens relu kera layer dens relu kera layer dens num class activ softmax opt kera optim rmsprop model compil loss categor crossentropi optim opt metric accuraci return model def serv input hyperparamet notic input placehold input shape kera model input tensor placehold float shape width input kei input tensor match kera inputlay input input tensor tensor return estim export servinginputreceiv input input def train input train dir param return input function feed model train return input train def eval input train dir param return input function feed model evalu return input eval def input channel return dataset read pipemod channel print data channel featur question fixedlenfeatur width float label fixedlenfeatur int def pars record pars pars singl exampl record featur print str cast pars question float return input tensor cast pars question float pars label pipemodedataset channel epoch repeat epoch prefetch prefetch size appli map batch pars batch size batch size num parallel batch buffer size map pars num parallel call num parallel batch batch batch size return",
        "Question_preprocessed_content":"tensorflow save model work kera try train model follow issu train model tensorflow work fine model train model save problem chang framework version follow error train job mark success bucket train altern tri put keep happen major differ awar think entri point need work fine version case curiou spot",
        "Question_gpt_summary_original":"The user is facing challenges while trying to save a model in Sagemaker using TensorFlow 2.0.0. The training job is marked as successful but the model is not saved in the S3 bucket. The user has tried changing the py_version to 'py3' but the issue persists. The user has shared the entry point code for reference.",
        "Question_gpt_summary":"user face challeng try save model tensorflow train job mark success model save bucket user tri chang version issu persist user share entri point code refer",
        "Answer_original_content":"correct major benefici chang year tensorflow experi name script mode formal sdk document warn ad new format tensorflow train script tensorflow version new wai give user script flexibl new format call script mode oppos legaci mode support tensorflow older version addit ad python support script mode support version legaci mode tensorflow script mode avail tensorflow version newer sure refer correct version readm prepar script legaci mode readm tensorflow need follow script mode formal save model opt model path sent script mode straightforward implement give better flexibl portabl spec share sklearn contain pytorch contain mxnet contain definit worth adopt",
        "Answer_preprocessed_content":"correct major benefici chang year tensorflow experi name script mode formal sdk document warn ad new format tensorflow train script tensorflow version new wai give user script flexibl new format call script mode oppos legaci mode support tensorflow older version addit ad python support script mode support version legaci mode tensorflow script mode avail tensorflow version newer sure refer correct version readm prepar script legaci mode readm tensorflow need follow script mode formal save model path sent script mode straightforward implement give better flexibl portabl spec share sklearn contain pytorch contain mxnet contain definit worth adopt",
        "Answer_gpt_summary_original":"Solution: The user needs to follow the Script Mode formalism while saving the model in Sagemaker using TensorFlow 2.0.0. The model needs to be saved in the opt\/ml\/model path, otherwise, it will not be sent to S3. The Script Mode is easy to implement and provides better flexibility and portability.",
        "Answer_gpt_summary":"solut user need follow script mode formal save model tensorflow model need save opt model path sent script mode easi implement provid better flexibl portabl"
    },
    {
        "Question_title":"SageMaker Pipe Mode",
        "Question_body":"Does SageMaker pipe mode serve as a cost saving measure? Or is is just faster than file mode but generally not much cheaper? The cost savings of it might be 1. no need to copy data to training instances and 2. training instances need less space. Are these savings generally significant for customers?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590161458000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":33.0,
        "Answer_body":"To the best of my understanding, pipe mode decreases startup times, but frequently increases the bill.\n\nThe SageMaker billing starts after the data has been copied onto the container in File mode and control is transferred to the user script.\n\nReading the data in pipe mode starts after control is transferred, so the data transfer happens during the billable time.\n\nFurther the data is, to the best of my knowledge, not hitting the disk (EBS). This is fast, but also means that if you pass over your data multiple times, you have to re-read it again, on your dime (S3 requests and container wait times).\n\nPipe mode is still a good idea. For example if you have only few passes over the data and the data is rather large, so that it would not fit on an EBS volume.\n\nAlso, in PyTorch for example, data loading can happen in parallel. So while the GPU is chucking away on one batch, the CPUs load and prepare the data for the next batch.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QURbsBp9m5TsqKWWDdP8VJyw\/sage-maker-pipe-mode",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-05-22T16:44:47.000Z",
                "Answer_score":0,
                "Answer_body":"To the best of my understanding, pipe mode decreases startup times, but frequently increases the bill.\n\nThe SageMaker billing starts after the data has been copied onto the container in File mode and control is transferred to the user script.\n\nReading the data in pipe mode starts after control is transferred, so the data transfer happens during the billable time.\n\nFurther the data is, to the best of my knowledge, not hitting the disk (EBS). This is fast, but also means that if you pass over your data multiple times, you have to re-read it again, on your dime (S3 requests and container wait times).\n\nPipe mode is still a good idea. For example if you have only few passes over the data and the data is rather large, so that it would not fit on an EBS volume.\n\nAlso, in PyTorch for example, data loading can happen in parallel. So while the GPU is chucking away on one batch, the CPUs load and prepare the data for the next batch.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1590165887000,
        "Question_original_content":"pipe mode pipe mode serv cost save measur faster file mode gener cheaper cost save need copi data train instanc train instanc need space save gener signific custom",
        "Question_preprocessed_content":"pipe mode pipe mode serv cost save measur faster file mode gener cheaper cost save need copi data train instanc train instanc need space save gener signific custom",
        "Question_gpt_summary_original":"The user is questioning whether SageMaker pipe mode is a cost-saving measure or just faster than file mode. They suggest that the cost savings may come from not needing to copy data to training instances and requiring less space on training instances. The user is unsure if these savings are significant for customers.",
        "Question_gpt_summary":"user question pipe mode cost save measur faster file mode suggest cost save come need copi data train instanc requir space train instanc user unsur save signific custom",
        "Answer_original_content":"best understand pipe mode decreas startup time frequent increas bill start data copi contain file mode control transfer user script read data pipe mode start control transfer data transfer happen billabl time data best knowledg hit disk eb fast mean pass data multipl time read dime request contain wait time pipe mode good idea exampl pass data data larg fit eb volum pytorch exampl data load happen parallel gpu chuck awai batch cpu load prepar data batch",
        "Answer_preprocessed_content":"best understand pipe mode decreas startup time frequent increas bill start data copi contain file mode control transfer user script read data pipe mode start control transfer data transfer happen billabl time data best knowledg hit disk fast mean pass data multipl time dime pipe mode good idea exampl pass data data larg fit eb volum pytorch exampl data load happen parallel gpu chuck awai batch cpu load prepar data batch",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Pipe mode decreases startup times, but frequently increases the bill.\n- Reading the data in pipe mode starts after control is transferred, so the data transfer happens during the billable time.\n- Pipe mode is still a good idea if you have only few passes over the data and the data is rather large, so that it would not fit on an EBS volume.\n- In PyTorch for example, data loading can happen in parallel. So while the GPU is chucking away on one batch, the CPUs load and prepare the data for the next batch.\n\nNo solution is mentioned regarding the cost savings of not needing to copy data to training instances and requiring less space on training instances.",
        "Answer_gpt_summary":"possibl solut mention discuss pipe mode decreas startup time frequent increas read data pipe mode start control transfer data transfer happen billabl time pipe mode good idea pass data data larg fit eb volum pytorch exampl data load happen parallel gpu chuck awai batch cpu load prepar data batch solut mention cost save need copi data train instanc requir space train instanc"
    },
    {
        "Question_title":"WandB not using user PID when updating",
        "Question_body":"<p>Hello,<\/p>\n<p>I used  <code>tempfile.mkdtemp() <\/code> to create a temporary directory for my runs (as I don\u2019t want a persistent folder with tons of runs)<\/p>\n<p>For training everything works fine but when resuming the run to do some validation \/ evaluation updates, and using <code>run.summary.update({\"key\": value})<\/code> I got a<\/p>\n<pre><code class=\"lang-auto\">wandb: WARNING Path \/tmp\/tmpq5uafy4d\/wandb\/ wasn't writable, using system temp directory\n<\/code><\/pre>\n<p>with obviously<\/p>\n<pre><code class=\"lang-auto\">File \"\/mnt\/Projets\/nlp\/.venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 855, in _update_summary\n    with open(summary_path, \"w\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: '\/tmp\/tmpq5uafy4d\/wandb\/run-20211102_153311-37264m5k\/files\/wandb-summary.json'\n<\/code><\/pre>\n<p>As in the doc of <a href=\"https:\/\/docs.python.org\/3.9\/library\/tempfile.html#tempfile.mkdtemp\" rel=\"noopener nofollow ugc\"><code>mkdtemp<\/code><\/a> :<\/p>\n<pre><code class=\"lang-auto\"> The directory is readable, writable, and searchable only by the creating user ID.\n<\/code><\/pre>\n<p>So I guess WandB is not using the user ID and thus is not able to write in the directory for updating.<br>\nNote that this directory is different from the training one (as it\u2019s random at each init)<\/p>\n<p>Thanks in advance for any help.<br>\nHave a great day.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1635882593147,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":289.0,
        "Answer_body":"<p>As of now, there isn\u2019t an option to enable that by default. One issue you should be aware of is that if you are currently logging a run when you call <code>wandb sync --clean<\/code> bad things will happen. We\u2019re working on improving the the robustness of these features and will likely support an automatic clean option in the future.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-not-using-user-pid-when-updating\/1204",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-03T15:31:05.874Z",
                "Answer_body":"<p>For the posterity :<\/p>\n<p>After searching for a long time with it, wandb firstly deletes my class object (which call deletion of the temp folder) <strong>and then<\/strong> try to update the run.<\/p>\n<p>To avoid that you need to first call <code>run.finish() or wandb.finish()<\/code> which will first update and then delete your object (or let the garbage collector do it)  thus it will be synced before the object is destroyed and the temp file removed.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-03T16:45:59.689Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ierezell\">@ierezell<\/a>, there are some reasons why you don\u2019t want to automatically delete your run folders, especially if there is an issue with the run. However, I do understand your desire to manage the clutter in your file system. It sounds like having a feature where you could tell wandb to delete the local files it created after a successful run would be the most preferable option for you. Currently, calling <code>wandb sync --clean<\/code> will sync any unsynced runs and then remove those run folders from your computer. The wandb dir will still be there and there but it will only be taking up bytes of space as most of the information will have been deleted. If you still find it annoying to have mostly empty <code>.\/wandb<\/code> folders in your project dirs, you can set the <code>WANDB_DIR<\/code> environment variable to an absolute path where all of your run data will be stored until you call <code>wandb sync --clean<\/code>.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-03T18:08:15.337Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/aidanjd\">@aidanjd<\/a>,<\/p>\n<p>I didn\u2019t know the <code>wandb sync --clean<\/code> option!<br>\nIt\u2019s quite what I wanted to do, saving space and your solution will only delete finished runs which is nice <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>I don\u2019t mind deleting failed run as I can just relaunch them again (it\u2019s small models on only one machine). This is why I put all my wandb folders in <code>\/tmp<\/code> which means I keep the folder until I reboot.<\/p>\n<p>Is there any option to have the <code>clean<\/code> feature enabled by default? I mean for any run if sync is complete: delete the folder.<\/p>\n<p>Thanks for the response, I guess it solves it but we can continue discussing it.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-03T18:21:20.198Z",
                "Answer_body":"<p>As of now, there isn\u2019t an option to enable that by default. One issue you should be aware of is that if you are currently logging a run when you call <code>wandb sync --clean<\/code> bad things will happen. We\u2019re working on improving the the robustness of these features and will likely support an automatic clean option in the future.<\/p>",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-01-02T18:22:08.008Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1635963680198,
        "Question_original_content":"user pid updat hello tempfil mkdtemp creat temporari directori run dont want persist folder ton run train work fine resum run valid evalu updat run summari updat kei valu got warn path tmp tmpquafyd wasn writabl temp directori obvious file mnt projet nlp venv lib python site packag sdk intern sender line updat summari open summari path filenotfounderror errno file directori tmp tmpquafyd run file summari json doc mkdtemp directori readabl writabl searchabl creat user guess user abl write directori updat note directori differ train random init thank advanc help great dai",
        "Question_preprocessed_content":"user pid updat hello creat temporari directori run train work fine resum run valid evalu updat got obvious doc guess user abl write directori updat note directori differ train thank advanc help great dai",
        "Question_gpt_summary_original":"The user encountered a challenge when using WandB to update their runs. They created a temporary directory using tempfile.mkdtemp() for their runs, but when resuming the run to do some validation\/evaluation updates and using run.summary.update({\"key\": value}), they received a warning that the path wasn't writable, and a FileNotFoundError occurred. The user suspects that WandB is not using the user ID and thus is not able to write in the directory for updating.",
        "Question_gpt_summary":"user encount challeng updat run creat temporari directori tempfil mkdtemp run resum run valid evalu updat run summari updat kei valu receiv warn path wasn writabl filenotfounderror occur user suspect user abl write directori updat",
        "Answer_original_content":"isnt option enabl default issu awar current log run sync clean bad thing happen work improv robust featur like support automat clean option futur",
        "Answer_preprocessed_content":"isnt option enabl default issu awar current log run bad thing happen work improv robust featur like support automat clean option futur",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Comparison to DataLad and git-annex",
        "Question_body":"<p>What are the main differences between DVC and DataLad\/git-annex ( <a href=\"https:\/\/www.datalad.org\/\" rel=\"nofollow noopener\">https:\/\/www.datalad.org\/<\/a> , <a href=\"https:\/\/git-annex.branchable.com\/\" rel=\"nofollow noopener\">https:\/\/git-annex.branchable.com\/<\/a> )?  What would be reasons to use one or the other?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1536601415671,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":7.0,
        "Question_view_count":1861.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ilya\">@ilya<\/a> !<\/p>\n<p>There are few main differences that might make you choose one over another:<\/p>\n<ol>\n<li>\n<p>DataLad uses and requires git and git-annex for all of its operations. DVC generally speaking doesn\u2019t require any SCM at all for the core features, but can also be made to work with any SCM of your choice(e.g. Mercurial) by adding a simple driver to <code>dvc\/scm.py<\/code>. Also the git-annex itself has its own pros and cons and some people might prefer to avoid using it. In dvc we use our own system to store and transfer data through a variety of ways(i.e. Amazon s3, Google Cloud Storage, Microsoft Azure, SSH, HDFS, etc).<\/p>\n<\/li>\n<li>\n<p>DataLad is more focused on the data itself and provides a convenient way to discover and use datasets created by other people with a single command, while dvc is currently focused on separate projects and doesn\u2019t provide that functionality(though we are working on it).<\/p>\n<\/li>\n<li>\n<p>DataLad is using git to provide reproducibility, while dvc stores every stage of your pipeline in a separate human-readable <code>.dvc<\/code> file and provides convenient tools to manipulate the DAG(e.g. <a href=\"https:\/\/dvc.org\/doc\/get-started\/visualize\">visualization<\/a>).<\/p>\n<\/li>\n<li>\n<p>With DVC you can specify <a href=\"https:\/\/dvc.org\/doc\/user-guide\/external-dependencies\">external dependencies<\/a> and <a href=\"https:\/\/dvc.org\/doc\/user-guide\/external-outputs\">external outputs<\/a> in your pipeline stages and have them automatically tracked and cached without transfering them to your local machine. As far as I know this is not possible with DataLad and might be even impossible to implement because of the design relying on the git and git-annex.<\/p>\n<\/li>\n<\/ol>\n<p>Thanks,<br>\nRuslan<\/p>. <p>Almost 2 year later, is there anything else to add to this comparison?<\/p>. <p>The data versioning layer of DVC is still basically the same, although .dvc files in 1.0 are much simpler and easier to edit manually \u2014 e.g. you can a bunch of different files independently, and combine the resulting .dvc files into a single one later.<\/p>\n<p>But otherwise yes! DVC provides many more features now, on pipeline control (also made easier with a single dvc.yaml instead of multiple stage files in 1.0), performance, and experiment management mainly. For example params, metrics, and plots (those are all <a href=\"https:\/\/dvc.org\/doc\/command-reference\">DVC commands<\/a>).<\/p>. <p>I hope its okay for me to post this here (I\u2019m from the DataLad team). We\u2019ve been asked this question, too, and we have written a procedural comparison between the two tools.<\/p>\n<p>We\u2019ve recreated a workflow with from DVC with DataLad <a href=\"http:\/\/handbook.datalad.org\/en\/latest\/beyond_basics\/101-168-dvc.html\" rel=\"noopener nofollow ugc\">here<\/a>, and we\u2019re showcasing an ML analysis with DataLad in a DataLad-centric way <a href=\"http:\/\/handbook.datalad.org\/en\/latest\/usecases\/ml-analysis.html\" rel=\"noopener nofollow ugc\">here<\/a>.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/comparison-to-datalad-and-git-annex\/92",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-09-10T19:34:28.288Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ilya\">@ilya<\/a> !<\/p>\n<p>There are few main differences that might make you choose one over another:<\/p>\n<ol>\n<li>\n<p>DataLad uses and requires git and git-annex for all of its operations. DVC generally speaking doesn\u2019t require any SCM at all for the core features, but can also be made to work with any SCM of your choice(e.g. Mercurial) by adding a simple driver to <code>dvc\/scm.py<\/code>. Also the git-annex itself has its own pros and cons and some people might prefer to avoid using it. In dvc we use our own system to store and transfer data through a variety of ways(i.e. Amazon s3, Google Cloud Storage, Microsoft Azure, SSH, HDFS, etc).<\/p>\n<\/li>\n<li>\n<p>DataLad is more focused on the data itself and provides a convenient way to discover and use datasets created by other people with a single command, while dvc is currently focused on separate projects and doesn\u2019t provide that functionality(though we are working on it).<\/p>\n<\/li>\n<li>\n<p>DataLad is using git to provide reproducibility, while dvc stores every stage of your pipeline in a separate human-readable <code>.dvc<\/code> file and provides convenient tools to manipulate the DAG(e.g. <a href=\"https:\/\/dvc.org\/doc\/get-started\/visualize\">visualization<\/a>).<\/p>\n<\/li>\n<li>\n<p>With DVC you can specify <a href=\"https:\/\/dvc.org\/doc\/user-guide\/external-dependencies\">external dependencies<\/a> and <a href=\"https:\/\/dvc.org\/doc\/user-guide\/external-outputs\">external outputs<\/a> in your pipeline stages and have them automatically tracked and cached without transfering them to your local machine. As far as I know this is not possible with DataLad and might be even impossible to implement because of the design relying on the git and git-annex.<\/p>\n<\/li>\n<\/ol>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-06-02T05:37:26.988Z",
                "Answer_body":"<p>Almost 2 year later, is there anything else to add to this comparison?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-06-04T16:03:12.644Z",
                "Answer_body":"<p>The data versioning layer of DVC is still basically the same, although .dvc files in 1.0 are much simpler and easier to edit manually \u2014 e.g. you can a bunch of different files independently, and combine the resulting .dvc files into a single one later.<\/p>\n<p>But otherwise yes! DVC provides many more features now, on pipeline control (also made easier with a single dvc.yaml instead of multiple stage files in 1.0), performance, and experiment management mainly. For example params, metrics, and plots (those are all <a href=\"https:\/\/dvc.org\/doc\/command-reference\">DVC commands<\/a>).<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-10-05T14:53:49.015Z",
                "Answer_body":"<p>I hope its okay for me to post this here (I\u2019m from the DataLad team). We\u2019ve been asked this question, too, and we have written a procedural comparison between the two tools.<\/p>\n<p>We\u2019ve recreated a workflow with from DVC with DataLad <a href=\"http:\/\/handbook.datalad.org\/en\/latest\/beyond_basics\/101-168-dvc.html\" rel=\"noopener nofollow ugc\">here<\/a>, and we\u2019re showcasing an ML analysis with DataLad in a DataLad-centric way <a href=\"http:\/\/handbook.datalad.org\/en\/latest\/usecases\/ml-analysis.html\" rel=\"noopener nofollow ugc\">here<\/a>.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"comparison datalad git annex main differ datalad git annex http datalad org http git annex branchabl com reason us",
        "Question_preprocessed_content":"comparison datalad main differ reason us",
        "Question_gpt_summary_original":"The user is seeking to understand the main differences between DVC and DataLad\/git-annex and the reasons for using one over the other.",
        "Question_gpt_summary":"user seek understand main differ datalad git annex reason",
        "Answer_original_content":"ilya main differ choos datalad us requir git git annex oper gener speak doesnt requir scm core featur work scm choic mercuri ad simpl driver scm git annex pro con peopl prefer avoid us store transfer data varieti wai amazon googl cloud storag microsoft azur ssh hdf datalad focus data provid conveni wai discov us dataset creat peopl singl command current focus separ project doesnt provid function work datalad git provid reproduc store stage pipelin separ human readabl file provid conveni tool manipul dag visual specifi extern depend extern output pipelin stage automat track cach transfer local machin far know possibl datalad imposs implement design reli git git annex thank ruslan year later add comparison data version layer basic file simpler easier edit manual bunch differ file independ combin result file singl later ye provid featur pipelin control easier singl yaml instead multipl stage file perform experi manag mainli exampl param metric plot command hope okai post datalad team weve ask question written procedur comparison tool weve recreat workflow datalad showcas analysi datalad datalad centric wai",
        "Answer_preprocessed_content":"main differ choos datalad us requir git oper gener speak doesnt requir scm core featur work scm mercuri ad simpl driver pro con peopl prefer avoid us store transfer data varieti amazon googl cloud storag microsoft azur ssh hdf datalad focus data provid conveni wai discov us dataset creat peopl singl command current focus separ project doesnt provid function datalad git provid reproduc store stage pipelin separ file provid conveni tool manipul visual specifi extern depend extern output pipelin stage automat track cach transfer local machin far know possibl datalad imposs implement design reli git thank ruslan year later add comparison data version layer basic file simpler easier edit manual bunch differ file independ combin result file singl later ye provid featur pipelin control perform experi manag mainli exampl param metric plot hope okai post weve ask question written procedur comparison tool weve recreat workflow datalad showcas analysi datalad wai",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- DVC doesn't require any SCM for its core features, while DataLad requires git and git-annex.\n- DataLad is more focused on the data itself and provides a convenient way to discover and use datasets created by other people with a single command, while DVC is currently focused on separate projects and doesn\u2019t provide that functionality.\n- DVC stores every stage of your pipeline in a separate human-readable .dvc file and provides convenient tools to manipulate the DAG, while DataLad is using git to provide reproducibility.\n- With DVC, you can specify external dependencies and external outputs in your pipeline stages and have them automatically tracked and cached without transferring them to your local machine",
        "Answer_gpt_summary":"possibl solut mention discuss requir scm core featur datalad requir git git annex datalad focus data provid conveni wai discov us dataset creat peopl singl command current focus separ project doesnt provid function store stage pipelin separ human readabl file provid conveni tool manipul dag datalad git provid reproduc specifi extern depend extern output pipelin stage automat track cach transfer local machin"
    },
    {
        "Question_title":"SageMaker Studio projects in VpcOnly mode without internet access",
        "Question_body":"A customer is using SageMaker Studio in VpcOnly mode (VPC, protected subnets without internet access, NO NAT gateways). The all functionality is fine. However, when I try create a SageMaker projects - as described here, SageMaker Studio is unable to list the project templates (timeout and unspecified error) resulting in empty list of the available project templates.\n\nProjects are enabled for the users - as described here. The problem is with project creation.\n\nIs internet access (e.g. via NAT gateways) is needed for SageMaker projects?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615480055000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":323.0,
        "Answer_body":"Figured it out. SageMaker Studio projects need Service Catalog access and VPCE for com.amazonaws.${AWS::Region}.servicecatalog",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcyhpq1pxRTmtjkDRAh_MDA\/sage-maker-studio-projects-in-vpc-only-mode-without-internet-access",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-10T20:10:40.000Z",
                "Answer_score":0,
                "Answer_body":"Figured it out. SageMaker Studio projects need Service Catalog access and VPCE for com.amazonaws.${AWS::Region}.servicecatalog",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1618085440000,
        "Question_original_content":"studio project vpconli mode internet access custom studio vpconli mode vpc protect subnet internet access nat gatewai function fine try creat project describ studio unabl list project templat timeout unspecifi error result list avail project templat project enabl user describ problem project creation internet access nat gatewai need project",
        "Question_preprocessed_content":"studio project vpconli mode internet access custom studio vpconli mode function fine try creat project describ studio unabl list project templat result list avail project templat project enabl user describ problem project creation internet access need project",
        "Question_gpt_summary_original":"The user is facing challenges in creating SageMaker projects in VpcOnly mode without internet access. Although all other functionalities are working fine, SageMaker Studio is unable to list project templates resulting in an empty list of available project templates. The user has enabled projects for the users, but the problem persists with project creation. The user is unsure if internet access is required for SageMaker projects.",
        "Question_gpt_summary":"user face challeng creat project vpconli mode internet access function work fine studio unabl list project templat result list avail project templat user enabl project user problem persist project creation user unsur internet access requir project",
        "Answer_original_content":"figur studio project need servic catalog access vpce com amazonaw aw region servicecatalog",
        "Answer_preprocessed_content":"figur studio project need servic catalog access vpce",
        "Answer_gpt_summary_original":"Solution: The user found a solution to the challenge by enabling Service Catalog access and VPCE for com.amazonaws.${AWS::Region}.servicecatalog. This resolved the issue with SageMaker Studio not being able to list project templates and allowed for project creation in VpcOnly mode without internet access.",
        "Answer_gpt_summary":"solut user solut challeng enabl servic catalog access vpce com amazonaw aw region servicecatalog resolv issu studio abl list project templat allow project creation vpconli mode internet access"
    },
    {
        "Question_title":"How do I stop a Google Cloud's AutoML (now VertexAI) batch prediction job using the web GUI?",
        "Question_body":"<p>I started a batch prediction job in AutoML (now VertexAI) for a small csv in one of my buckets, using a classification model, then I noticed the csv had an error but was unable to find a way to cancel the job using the web GUI, it just says &quot;running&quot; but I see no &quot;stop&quot; or &quot;cancel&quot; button.<\/p>\n<p>Fortunately, it was done after 20 minutes, but I need to know how to stop a job since I will require predictions for way bigger files and can't risk having to wait until the job ends by itself. It was kind of desperating being able to watch the log throwing error after error and not being able to stop the job. I tried to delete the job but it said it can't be deleted while its running.<\/p>\n<p>I found a related question, but it was not answered, the job just finished itself after a couple of days. I can't risk that.\n<a href=\"https:\/\/stackoverflow.com\/questions\/68077606\/how-do-i-force-batch-prediction-to-stop\">How do I force &quot;batch prediction&quot; to stop?<\/a><\/p>\n<p>I will greatly appreciate any help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625608160277,
        "Question_favorite_count":null,
        "Question_last_edit_time":1625612012432,
        "Question_score":0.0,
        "Question_view_count":619.0,
        "Answer_body":"<p>Unfortunately the cancel\/stop feature is not yet available in the Vertex AI UI. As per <a href=\"https:\/\/stackoverflow.com\/questions\/68077606\/how-do-i-force-batch-prediction-to-stop\">How do I force &quot;batch prediction&quot; to stop?<\/a>, the OP sent a feedback. You can ask if there was a public issue tracker created for this so you can monitor the progress of the feature request there.<\/p>\n<p>But there is a workaround for this, just send a request <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.batchPredictionJobs\/cancel\" rel=\"nofollow noreferrer\">projects.locations.batchPredictionJobs.cancel<\/a> via REST.<\/p>\n<p>To do this you can send a request via curl. In this example the model and endpoint are located in <code>us-central1<\/code> thus the location defined in the request.<\/p>\n<p>Just supply your <code>project-id<\/code> and the <code>batch-prediction-id<\/code> on the request. To get the <code>batch-prediction-id<\/code> you can get it via UI:<\/p>\n<p>Get <code>batch-prediction-id<\/code> via UI:<\/p>\n<ul>\n<li>Open &quot;Batch Predictions&quot; tab in the Vertex AI UI<\/li>\n<li>Click on the job you want to cancel<\/li>\n<li>Job information will be displayed and the 1st entry will contain the Job ID<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/lKJdT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lKJdT.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To cancel the job send a cancel request via curl. If requests is successful, the response body is empty.<\/p>\n<pre><code>curl -X POST -H &quot;Content-Type: application\/json&quot; \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\nhttps:\/\/us-central1-aiplatform.googleapis.com\/v1\/projects\/your-project-id\/locations\/us-central1\/batchPredictionJobs\/batch-prediction-job-id:cancel\n<\/code><\/pre>\n<p>Check in Vertex AI UI if the job was canceled.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/atSqt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/atSqt.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68277691",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1625628028808,
        "Question_original_content":"stop googl cloud automl vertexai batch predict job web gui start batch predict job automl vertexai small csv bucket classif model notic csv error unabl wai cancel job web gui sai run stop cancel button fortun minut need know stop job requir predict wai bigger file risk have wait job end kind desper abl watch log throw error error abl stop job tri delet job said delet run relat question answer job finish coupl dai risk forc batch predict stop greatli appreci help",
        "Question_preprocessed_content":"stop googl cloud automl batch predict job web gui start batch predict job automl small csv bucket classif model notic csv error unabl wai cancel job web gui sai run stop cancel button fortun minut need know stop job requir predict wai bigger file risk have wait job end kind desper abl watch log throw error error abl stop job tri delet job said delet run relat question answer job finish coupl dai risk forc batch predict stop greatli appreci help",
        "Question_gpt_summary_original":"The user encountered a challenge in stopping a batch prediction job in Google Cloud's AutoML (now VertexAI) using the web GUI. The user was unable to find a way to cancel the job using the web GUI and was forced to wait until the job ended by itself. The user needs to know how to stop a job since they will require predictions for larger files and cannot risk waiting for the job to end by itself. The user tried to delete the job but was unable to do so while it was running.",
        "Question_gpt_summary":"user encount challeng stop batch predict job googl cloud automl vertexai web gui user unabl wai cancel job web gui forc wait job end user need know stop job requir predict larger file risk wait job end user tri delet job unabl run",
        "Answer_original_content":"unfortun cancel stop featur avail forc batch predict stop sent feedback ask public issu tracker creat monitor progress featur request workaround send request project locat batchpredictionjob cancel rest send request curl exampl model endpoint locat central locat defin request suppli project batch predict request batch predict batch predict open batch predict tab click job want cancel job inform displai entri contain job cancel job send cancel request curl request success respons bodi curl post content type applic json author bearer gcloud auth applic default print access token http central aiplatform googleapi com project project locat central batchpredictionjob batch predict job cancel check job cancel",
        "Answer_preprocessed_content":"unfortun featur avail forc batch predict stop sent feedback ask public issu tracker creat monitor progress featur request workaround send request rest send request curl exampl model endpoint locat locat defin request suppli request open batch predict tab click job want cancel job inform displai entri contain job cancel job send cancel request curl request success respons bodi check job cancel",
        "Answer_gpt_summary_original":"Solution: The cancel\/stop feature is not yet available in the Vertex AI UI. However, there is a workaround for this, which is to send a request via REST using the projects.locations.batchPredictionJobs.cancel API. To do this, the user can send a request via curl, supplying their project-id and the batch-prediction-id on the request. The batch-prediction-id can be obtained via the UI by opening the \"Batch Predictions\" tab, clicking on the job to be canceled, and obtaining the Job ID from the job information displayed. If the request is successful, the response body is empty.",
        "Answer_gpt_summary":"solut cancel stop featur avail workaround send request rest project locat batchpredictionjob cancel api user send request curl suppli project batch predict request batch predict obtain open batch predict tab click job cancel obtain job job inform displai request success respons bodi"
    },
    {
        "Question_title":"Training a TensorFlow model in Azure ML",
        "Question_body":"I am following the link below for training a TensorFlow model in Azure ML:\n\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/ml-frameworks\/tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow.ipynb\n\nHowever, as my training dataset is in a container named \"sample-datasets\" in ADLS Gen2, I changed the following code (in the above link) to refer to the paths in my data lake. So I replaced code A (in the link above) with code B (my code)\n\nCode A:\n\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/train-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 'train-images-idx3-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/train-labels-idx1-ubyte.gz',\nfilename=os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/t10k-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/t10k-labels-idx1-ubyte.gz',\nfilename=os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'))\n\n\n\n\nCode B:\n\nfrom azureml.core.dataset import Dataset\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/train-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 'train-images-idx3-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/train-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/t10k-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/t10k-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'))\n\nBut I receive the following error:\n\nHTTPError: HTTP Error 401: Server failed to authenticate the request. Please refer to the information in the www-authenticate header.\n\nCan you please let me know how I can train the model using my data which are stored in the data lake? More precisely, how my Python code can copy the training dataset from my data lake into data_folder?\n\nPS: Please note that I have already granted the Blob Storage data Contributor role on my data lake storage account to my Azure ML workspace as a managed identity.",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1649367124903,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"anonymous user I have not worked on ADLS scenarios with Azure ML but I have added the ADLS tag to this thread for others to chip in and add their views.\n\nBased on the documentation for ADLS REST API it supports Azure Active Directory (Azure AD), Shared Key, and shared access signature (SAS) authorization with the APIs that are available to download the files from its storage. So, I think a direct download might not work in this case without authentication.\n\nI think the easiest way to get your files locally from ADLS is to use the python SDK to authenticate using account key or AD as listed here.\n\nIf you have many files that needs to be downloaded and referenced in your ML experiments then you may also consider to use the import data module of designer for designer experiments or register them as dataset from dataset tab of ml.azure.com which can also be referenced using the Azure ML SDK.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/804968\/training-a-tensorflow-model-in-azure-ml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-08T12:21:38.973Z",
                "Answer_score":0,
                "Answer_body":"anonymous user I have not worked on ADLS scenarios with Azure ML but I have added the ADLS tag to this thread for others to chip in and add their views.\n\nBased on the documentation for ADLS REST API it supports Azure Active Directory (Azure AD), Shared Key, and shared access signature (SAS) authorization with the APIs that are available to download the files from its storage. So, I think a direct download might not work in this case without authentication.\n\nI think the easiest way to get your files locally from ADLS is to use the python SDK to authenticate using account key or AD as listed here.\n\nIf you have many files that needs to be downloaded and referenced in your ML experiments then you may also consider to use the import data module of designer for designer experiments or register them as dataset from dataset tab of ml.azure.com which can also be referenced using the Azure ML SDK.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-04-19T15:42:44.427Z",
                "Answer_score":0,
                "Answer_body":"I solved the problem by assigning an user-assigned managed identity to the target compute to access my ASDLS Gen2",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1649420498972,
        "Question_original_content":"train tensorflow model follow link train tensorflow model http github com azur machinelearningnotebook blob master us framework tensorflow train hyperparamet tune deploi tensorflow train hyperparamet tune deploi tensorflow ipynb train dataset contain name sampl dataset adl gen chang follow code link refer path data lake replac code link code code code urllib request urlretriev http azureopendatastorag blob core window net mnist train imag idx ubyt filenam path join data folder train imag idx ubyt urllib request urlretriev http azureopendatastorag blob core window net mnist train label idx ubyt filenam path join data folder train label idx ubyt urllib request urlretriev http azureopendatastorag blob core window net mnist imag idx ubyt filenam path join data folder imag idx ubyt urllib request urlretriev http azureopendatastorag blob core window net mnist label idx ubyt filenam path join data folder label idx ubyt code core dataset import dataset urllib request urlretriev http lakehousestgenrichedzon df core window net sampl dataset train imag idx ubyt filenam path join data folder train imag idx ubyt urllib request urlretriev http lakehousestgenrichedzon df core window net sampl dataset train label idx ubyt filenam path join data folder train label idx ubyt urllib request urlretriev http lakehousestgenrichedzon df core window net sampl dataset imag idx ubyt filenam path join data folder imag idx ubyt urllib request urlretriev http lakehousestgenrichedzon df core window net sampl dataset label idx ubyt filenam path join data folder label idx ubyt receiv follow error httperror http error server fail authent request refer inform authent header let know train model data store data lake precis python code copi train dataset data lake data folder note grant blob storag data contributor role data lake storag account workspac manag ident",
        "Question_preprocessed_content":"train tensorflow model follow link train tensorflow model train dataset contain name adl gen chang follow code refer path data lake replac code code code code import dataset receiv follow error httperror http error server fail authent request refer inform header let know train model data store data lake precis python code copi train dataset data lake note grant blob storag data contributor role data lake storag account workspac manag ident",
        "Question_gpt_summary_original":"The user is encountering an error while trying to train a TensorFlow model in Azure ML using their own training dataset stored in a container named \"sample-datasets\" in ADLS Gen2. They have modified the code to refer to the paths in their data lake, but they are receiving an HTTPError 401: Server failed to authenticate the request. They are seeking assistance on how to copy the training dataset from their data lake into the data_folder using Python code. The user has already granted the Blob Storage data Contributor role on their data lake storage account to their Azure ML workspace as a managed identity.",
        "Question_gpt_summary":"user encount error try train tensorflow model train dataset store contain name sampl dataset adl gen modifi code refer path data lake receiv httperror server fail authent request seek assist copi train dataset data lake data folder python code user grant blob storag data contributor role data lake storag account workspac manag ident",
        "Answer_original_content":"anonym user work adl scenario ad adl tag thread chip add view base document adl rest api support azur activ directori azur share kei share access signatur sa author api avail download file storag think direct download work case authent think easiest wai file local adl us python sdk authent account kei list file need download referenc experi consid us import data modul design design experi regist dataset dataset tab azur com referenc sdk answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"anonym user work adl scenario ad adl tag thread chip add view base document adl rest api support azur activ directori share kei share access signatur author api avail download file storag think direct download work case authent think easiest wai file local adl us python sdk authent account kei list file need download referenc experi consid us import data modul design design experi regist dataset dataset tab referenc sdk answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Use the Python SDK to authenticate using account key or Azure Active Directory (AD) to download the files from ADLS.\n2. Consider using the import data module of designer for designer experiments or register the files as a dataset from the dataset tab of ml.azure.com, which can also be referenced using the Azure ML SDK.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss us python sdk authent account kei azur activ directori download file adl consid import data modul design design experi regist file dataset dataset tab azur com referenc sdk person opinion bias includ summari"
    },
    {
        "Question_title":"Advice for versioning many many small files?",
        "Question_body":"<p>Hello! I\u2019m new to DVC <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><br>\nI was working on versioning a large dataset (totaling ~140gb) which consists of many many very small files. They are mp3 audio files (imagine for training a speech model), each for example 100-200kb (they\u2019re only 5-15 seconds each on average). This means I\u2019ve got a lot of files of course<\/p>\n<p>I ran <code>dvc push<\/code> and let it run for ~16 hours before realizing this probably isn\u2019t a great idea, since it seemed like it was nowhere near completion, and cancelled the run. I found a couple links online telling me dvc isn\u2019t ideal for many small files, but I also found a comment from a dvc maintainer on this forum saying recent changes (that post was ~apr 2020) should improve performance for dvc push on many small files \u2013 Would anyone be able to advise me? Did I possibly do something wrong, or should I be rolling these files into a tar, zip, etc and versioning that? Wouldn\u2019t it make more sense to version individual data files? (I\u2019m new to working with data too <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/upside_down_face.png?v=9\" title=\":upside_down_face:\" class=\"emoji\" alt=\":upside_down_face:\">)<\/p>\n<p>Help would be really appreciated! Thanks<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1609777134249,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":2322.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/uzair\">@uzair<\/a> !<\/p>\n<p>Could you show output for <code>dvc doctor<\/code>, please?<\/p>. <p>hey <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a> thanks for offering help! sorry for the late response, here\u2019s the output of <code>dvc doctor<\/code>:<\/p>\n<h2>DVC version: 1.9.1 (pip)<\/h2>\n<p>Platform: Python 3.8.5 on Linux-5.4.0-58-generic-x86_64-with-glibc2.29<br>\nSupports: http, https, s3, ssh<\/p>\n<p>Thank you <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/uzair\">@uzair<\/a>!<\/p>\n<p>What type of remote are you using? Did you check, by any chance, how much time would transferring your data take using other tool?<\/p>. <p>hi <a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a> !<\/p>\n<p>Honestly, I have not checked. However, according to speedtest I have 900Mbps\/112.5MBps upload speed, and so if I\u2019m not mistaken that should take 24ish minutes for just the raw data, although I understand there would be a cap when uploading to the remote probably (s3 in my case), establishing connections, etc etc.<\/p>\n<p>I found this github PR where <span class=\"mention\">@efiop<\/span> (github username, not sure who this is but they seem to be a DVC maintainer) mentioned DVC isn\u2019t ideal for many many small files, because it has to keep reestablishing a connection to the remote for each file or something? <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/497#issuecomment-370109931\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">dvc cache push is slow for many files \u00b7 Issue #497 \u00b7 iterative\/dvc \u00b7 GitHub<\/a><\/p>\n<p>So should I be rolling my files into something like a tar file and be versioning that instead of raw small audio files?<\/p>. <p><a class=\"mention\" href=\"\/u\/uzair\">@uzair<\/a><br>\nThis issue is from 2 years ago, and since then we did a fair share of optimizations, and nowadays it should not be the case. In yours, it seems to be something wrong, 140 GB is not that much, to justify 16 hours of one command execution. I\u2019ll do some scaled-down test and get back to you.<\/p>. <p><a class=\"mention\" href=\"\/u\/uzair\">@uzair<\/a><br>\nSo, I\u2019ve been playing around and it seems that indeed, a lot of small files is still painful.<br>\nI run my own tests on 140 mb dataset, 70k files:<br>\nUsing DVC with default JOBS vaue (4*cpu num) - 1100 s<br>\nplain <code>aws s3 cp --recursive<\/code> - 3000s<\/p>\n<p>Even though dvc spends a lot of time acquiring lock in first case, transfer is much faster, than if we were to reduce number of jobs.<\/p>\n<p>So my suggestion is to play around with number of <code>jobs<\/code> (<code>dvc push --jobs {X}<\/code>) - that might help to some extend. Regretfully it seems that a lot of files is still painfully slow.<br>\nSome solution might be to pack the files, but that will be at cost of cache size, when we decide to update the dataset.<\/p>\n<p><a class=\"mention\" href=\"\/u\/uzair\">@uzair<\/a> please tell me if any of this solutions could help you. You can also chime in original issue for dir optimizations and share your problem: <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1970\">https:\/\/github.com\/iterative\/dvc\/issues\/1970<\/a>. We might need to reconsider current state of optimizations and think if there is something more to be done.<\/p>. <p>I\u2019m struggling with the same issue. Pulling a directory of many files is like an order of magnitude slower than pulling a zip of the same directory. The size of the zip is basically the same as the total size of the individual files so there\u2019s not really any compression going on. It\u2019s just the overhead of doing the files one by one<\/p>. <p>For the record, replied in <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1970#issuecomment-759650962\" class=\"inline-onebox\">dvc: performance optimization for directories \u00b7 Issue #1970 \u00b7 iterative\/dvc \u00b7 GitHub<\/a><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/advice-for-versioning-many-many-small-files\/609",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-05T03:43:07.553Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/uzair\">@uzair<\/a> !<\/p>\n<p>Could you show output for <code>dvc doctor<\/code>, please?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-08T01:57:40.055Z",
                "Answer_body":"<p>hey <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a> thanks for offering help! sorry for the late response, here\u2019s the output of <code>dvc doctor<\/code>:<\/p>\n<h2>DVC version: 1.9.1 (pip)<\/h2>\n<p>Platform: Python 3.8.5 on Linux-5.4.0-58-generic-x86_64-with-glibc2.29<br>\nSupports: http, https, s3, ssh<\/p>\n<p>Thank you <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-08T16:15:32.159Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/uzair\">@uzair<\/a>!<\/p>\n<p>What type of remote are you using? Did you check, by any chance, how much time would transferring your data take using other tool?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-11T11:50:54.793Z",
                "Answer_body":"<p>hi <a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a> !<\/p>\n<p>Honestly, I have not checked. However, according to speedtest I have 900Mbps\/112.5MBps upload speed, and so if I\u2019m not mistaken that should take 24ish minutes for just the raw data, although I understand there would be a cap when uploading to the remote probably (s3 in my case), establishing connections, etc etc.<\/p>\n<p>I found this github PR where <span class=\"mention\">@efiop<\/span> (github username, not sure who this is but they seem to be a DVC maintainer) mentioned DVC isn\u2019t ideal for many many small files, because it has to keep reestablishing a connection to the remote for each file or something? <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/497#issuecomment-370109931\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">dvc cache push is slow for many files \u00b7 Issue #497 \u00b7 iterative\/dvc \u00b7 GitHub<\/a><\/p>\n<p>So should I be rolling my files into something like a tar file and be versioning that instead of raw small audio files?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-11T11:59:30.150Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/uzair\">@uzair<\/a><br>\nThis issue is from 2 years ago, and since then we did a fair share of optimizations, and nowadays it should not be the case. In yours, it seems to be something wrong, 140 GB is not that much, to justify 16 hours of one command execution. I\u2019ll do some scaled-down test and get back to you.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-12T18:00:43.362Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/uzair\">@uzair<\/a><br>\nSo, I\u2019ve been playing around and it seems that indeed, a lot of small files is still painful.<br>\nI run my own tests on 140 mb dataset, 70k files:<br>\nUsing DVC with default JOBS vaue (4*cpu num) - 1100 s<br>\nplain <code>aws s3 cp --recursive<\/code> - 3000s<\/p>\n<p>Even though dvc spends a lot of time acquiring lock in first case, transfer is much faster, than if we were to reduce number of jobs.<\/p>\n<p>So my suggestion is to play around with number of <code>jobs<\/code> (<code>dvc push --jobs {X}<\/code>) - that might help to some extend. Regretfully it seems that a lot of files is still painfully slow.<br>\nSome solution might be to pack the files, but that will be at cost of cache size, when we decide to update the dataset.<\/p>\n<p><a class=\"mention\" href=\"\/u\/uzair\">@uzair<\/a> please tell me if any of this solutions could help you. You can also chime in original issue for dir optimizations and share your problem: <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1970\">https:\/\/github.com\/iterative\/dvc\/issues\/1970<\/a>. We might need to reconsider current state of optimizations and think if there is something more to be done.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-13T17:52:16.543Z",
                "Answer_body":"<p>I\u2019m struggling with the same issue. Pulling a directory of many files is like an order of magnitude slower than pulling a zip of the same directory. The size of the zip is basically the same as the total size of the individual files so there\u2019s not really any compression going on. It\u2019s just the overhead of doing the files one by one<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-13T19:11:22.471Z",
                "Answer_body":"<p>For the record, replied in <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1970#issuecomment-759650962\" class=\"inline-onebox\">dvc: performance optimization for directories \u00b7 Issue #1970 \u00b7 iterative\/dvc \u00b7 GitHub<\/a><\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"advic version small file hello new work version larg dataset total consist small file audio file imagin train speech model exampl theyr second averag mean iv got lot file cours ran push let run hour realiz probabl isnt great idea like near complet cancel run coupl link onlin tell isnt ideal small file comment maintain forum sai recent chang post apr improv perform push small file abl advis possibl wrong roll file tar zip version wouldnt sens version individu data file new work data help appreci thank",
        "Question_preprocessed_content":"advic version small file hello new work version larg dataset consist small file audio file exampl mean iv got lot file cours ran let run hour realiz probabl isnt great idea like near complet cancel run coupl link onlin tell isnt ideal small file comment maintain forum sai recent chang improv perform push small file abl advis possibl wrong roll file tar zip version wouldnt sens version individu data file help appreci thank",
        "Question_gpt_summary_original":"The user is facing challenges in versioning a large dataset consisting of many small mp3 audio files, each ranging from 100-200kb. They attempted to use DVC to version the files but found that it was not ideal for many small files and the process was taking too long. The user is seeking advice on whether they should roll the files into a tar or zip file or continue to version individual data files.",
        "Question_gpt_summary":"user face challeng version larg dataset consist small audio file rang attempt us version file ideal small file process take long user seek advic roll file tar zip file continu version individu data file",
        "Answer_original_content":"uzair output doctor hei kuprus thank offer help sorri late respons here output doctor version pip platform python linux gener glibc support http http ssh thank uzair type remot check chanc time transfer data tool paffciu honestli check accord speedtest mbp mbp upload speed mistaken ish minut raw data understand cap upload remot probabl case establish connect github efiop github usernam sure maintain mention isnt ideal small file reestablish connect remot file cach push slow file issu iter github roll file like tar file version instead raw small audio file uzair issu year ago fair share optim nowadai case wrong justifi hour command execut ill scale test uzair iv plai lot small file pain run test dataset file default job vaue cpu num plain aw recurs spend lot time acquir lock case transfer faster reduc number job suggest plai number job push job help extend regretfulli lot file painfulli slow solut pack file cost cach size decid updat dataset uzair tell solut help chime origin issu dir optim share problem http github com iter issu need reconsid current state optim think struggl issu pull directori file like order magnitud slower pull zip directori size zip basic total size individu file there compress go overhead file record repli perform optim directori issu iter github",
        "Answer_preprocessed_content":"output hei thank offer help sorri late respons here output version platform python support http http ssh thank type remot check chanc time transfer data tool honestli check accord speedtest upload speed mistaken ish minut raw data understand cap upload remot probabl establish connect github mention isnt ideal small file reestablish connect remot file cach push slow file issu iter github roll file like tar file version instead raw small audio file issu year ago fair share optim nowadai case wrong justifi hour command execut ill test iv plai lot small file pain run test dataset file default job vaue plain spend lot time acquir lock case transfer faster reduc number job suggest plai number help extend regretfulli lot file painfulli slow solut pack file cost cach size decid updat dataset tell solut help chime origin issu dir optim share problem need reconsid current state optim think struggl issu pull directori file like order magnitud slower pull zip directori size zip basic total size individu file there compress go overhead file record repli perform optim directori issu iter github",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Play around with the number of jobs in DVC push command to improve transfer speed.\n- Pack the files into a tar or zip file, but this will increase the cache size when updating the dataset.\n- Share the problem in the original issue for directory optimizations and reconsider the current state of optimizations.\n\nNote: One user mentioned that they faced a similar issue and found that pulling a zip file of the same directory was much faster than pulling individual files. However, no solution was provided for this.",
        "Answer_gpt_summary":"possibl solut mention discuss plai number job push command improv transfer speed pack file tar zip file increas cach size updat dataset share problem origin issu directori optim reconsid current state optim note user mention face similar issu pull zip file directori faster pull individu file solut provid"
    },
    {
        "Question_title":"Save_period Not Working",
        "Question_body":"<p>I\u2019m trying to train a model, but I keep receiving an error that tells me \u201ctrain.py: error: unrecognized arguments: --save_period 1.\u201d<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755.png\" data-download-href=\"\/uploads\/short-url\/8oz1v5B3P0W95o6FWMDht4yfVLD.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_690x337.png\" alt=\"image\" data-base62-sha1=\"8oz1v5B3P0W95o6FWMDht4yfVLD\" width=\"690\" height=\"337\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_690x337.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_1035x505.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1326\u00d7648 66 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nWhat issue do I have here?  Thanks in advance.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1636684136179,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":217.0,
        "Answer_body":"<p>I think you have  a typo. According to the usage info, the argument name is <code>--save-period<\/code>  , not <code>--save_period<\/code><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/save-period-not-working\/1264",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-13T06:40:50.863Z",
                "Answer_body":"<p>I think you have  a typo. According to the usage info, the argument name is <code>--save-period<\/code>  , not <code>--save_period<\/code><\/p>",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-11-14T00:54:05.542Z",
                "Answer_body":"<p>It worked. Thanks! <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/smiley.png?v=10\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\"><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-13T00:54:51.119Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1636785650863,
        "Question_original_content":"save period work try train model receiv error tell train error unrecogn argument save period imag issu thank advanc",
        "Question_preprocessed_content":"work try train model receiv error tell error unrecogn argument imag issu thank advanc",
        "Question_gpt_summary_original":"The user is encountering an error while trying to train a model, specifically with the \"--save_period 1\" argument. They are seeking assistance in resolving the issue.",
        "Question_gpt_summary":"user encount error try train model specif save period argument seek assist resolv issu",
        "Answer_original_content":"think typo accord usag info argument save period save period",
        "Answer_preprocessed_content":"think typo accord usag info argument",
        "Answer_gpt_summary_original":"Solution: The user may have a typo in their argument name. The correct argument name is \"--save-period\" instead of \"--save_period\".",
        "Answer_gpt_summary":"solut user typo argument correct argument save period instead save period"
    },
    {
        "Question_title":"Sagemaker Endpoint returning strange error",
        "Question_body":"<p>Hey guys so recently i started working with sagemaker and I was testing autopilot and it got a fairly good accuracy and I wanted to test it on some more data so I chose the one with best ACC and created an endpoint. The problem now is that I don't know how to use the endpoit properly. I tried using AWS CLI but I keep getting the following errors:<\/p>\n<p>The command:<\/p>\n<pre><code>aws sagemaker-runtime invoke-endpoint --endpoint-name autopilottest --body 'SW0gaGFwcHk=' f\n<\/code><\/pre>\n<p>The error message:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from container-1 with message &quot;'application\/json' is an unsupported content type.&quot;. See https:\/\/eu-west-2.console.aws.amazon.com\/cloudwatch\/home?region=eu-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/autopilottest in account 288240193481 for more information.\n<\/code><\/pre>\n<p>The command:<\/p>\n<pre><code>aws sagemaker-runtime invoke-endpoint --endpoint-name autopilottest --body 'Im happy!' f\n<\/code><\/pre>\n<p>The error message:<\/p>\n<pre><code>Invalid base64: &quot;Im happy!&quot;\n<\/code><\/pre>\n<p>Endpoit configuration:\n<a href=\"https:\/\/i.stack.imgur.com\/11qAt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/11qAt.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633715769763,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":521.0,
        "Answer_body":"<p>Ended up fixing the issue by adding <code>--content-type text\/csv<\/code> and using base64 and it worked like a charm.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69499960",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1633716252927,
        "Question_original_content":"endpoint return strang error hei gui recent start work test autopilot got fairli good accuraci want test data chose best acc creat endpoint problem know us endpoit properli tri aw cli get follow error command runtim invok endpoint endpoint autopilottest bodi swgagfwchk error messag error occur modelerror call invokeendpoint oper receiv client error contain messag applic json unsupport content type http west consol aw amazon com cloudwatch home region west logeventview group aw endpoint autopilottest account inform command runtim invok endpoint endpoint autopilottest bodi happi error messag invalid base happi endpoit configur",
        "Question_preprocessed_content":"endpoint return strang error hei gui recent start work test autopilot got fairli good accuraci want test data chose best acc creat endpoint problem know us endpoit properli tri aw cli get follow error command error messag command error messag endpoit configur",
        "Question_gpt_summary_original":"The user is encountering challenges while using an AWS Sagemaker endpoint. They are trying to use AWS CLI to invoke the endpoint but are receiving errors related to unsupported content type and invalid base64. The user has provided the endpoint configuration for reference.",
        "Question_gpt_summary":"user encount challeng endpoint try us aw cli invok endpoint receiv error relat unsupport content type invalid base user provid endpoint configur refer",
        "Answer_original_content":"end fix issu ad content type text csv base work like charm",
        "Answer_preprocessed_content":"end fix issu ad base work like charm",
        "Answer_gpt_summary_original":"Solution: The user was able to fix the issue by adding the parameter \"--content-type text\/csv\" and using base64 encoding. This solution resolved the errors related to unsupported content type and invalid base64 while using AWS CLI to invoke the Sagemaker endpoint.",
        "Answer_gpt_summary":"solut user abl fix issu ad paramet content type text csv base encod solut resolv error relat unsupport content type invalid base aw cli invok endpoint"
    },
    {
        "Question_title":"Share dataset between two azureml environnement",
        "Question_body":"<p>a friend have sent me a python3 notebook with his dataset to validate his notebook.<\/p>\n\n<p>but when i try to use his dataset on my azureml workspace i have an error saying that the dataset does not exist<\/p>\n\n<p>he sent me his datset code :<\/p>\n\n<pre><code>from azureml import Workspace\n\nws = Workspace(\n    workspace_id='toto',\n    authorization_token='titi',\n    endpoint='https:\/\/studioapi.azureml.net'\n)\nds = ws.datasets['mini.csv00']\nframe = ds.to_dataframe()\n\nframe\n<\/code><\/pre>\n\n<p>when i try to use it i have a :<\/p>\n\n<pre><code>ndexError                                Traceback (most recent call last)\n&lt;ipython-input-7-5f41120e38e4&gt; in &lt;module&gt;()\n----&gt; 1 ds = ws.datasets['mini.csv00']\n      2 frame = ds.to_dataframe()\n      3 \n      4 frame\n\n\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/azureml\/__init__.py in __getitem__(self, index)\n    461                     return self._create_dataset(dataset)\n    462 \n--&gt; 463         raise IndexError('A data set named \"{}\" does not exist'.format(index))\n    464 \n    465     def add_from_dataframe(self, dataframe, data_type_id, name, description):\n\nIndexError: A data set named \"mini.csv00\" does not exist\n<\/code><\/pre>\n\n<p>error ...<\/p>\n\n<p>But when i try it on my computer jupyter it works.\nAny ideas ?<\/p>\n\n<p>Thanks and regards<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1486668512167,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":108.0,
        "Answer_body":"<p>I guess you are using Jupyter notebook on AzureML to do the experiment. In that case the <code>'mini.csv00'<\/code> should be in your experiments with <code>workspace_id='toto'<\/code>. <\/p>\n\n<p>Create a new experiment in your workspace named toto and put the dataset into it first. Then open the dataset using 'open in a new Notebook'. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ztIw0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ztIw0.png\" alt=\"enter image description here\"><\/a> <\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42145256",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1486875192387,
        "Question_original_content":"share dataset environn friend sent python notebook dataset valid notebook try us dataset workspac error sai dataset exist sent datset code import workspac workspac workspac toto author token titi endpoint http studioapi net dataset mini csv frame datafram frame try us ndexerror traceback recent dataset mini csv frame datafram frame home nbuser anaconda lib python site packag init getitem self index return self creat dataset dataset rais indexerror data set name exist format index def add datafram self datafram data type descript indexerror data set name mini csv exist error try jupyt work idea thank regard",
        "Question_preprocessed_content":"share dataset environn friend sent python notebook dataset valid notebook try us dataset workspac error sai dataset exist sent datset code try us error try jupyt work idea thank regard",
        "Question_gpt_summary_original":"The user is facing a challenge in sharing a dataset between two AzureML environments. They received a Python3 notebook with a dataset from a friend to validate the notebook. However, when they try to use the dataset on their AzureML workspace, they receive an error saying that the dataset does not exist. The user has tried using the dataset code provided by their friend, but it still results in an error. The code works on their computer's Jupyter notebook, but not on their AzureML workspace.",
        "Question_gpt_summary":"user face challeng share dataset environ receiv python notebook dataset friend valid notebook try us dataset workspac receiv error sai dataset exist user tri dataset code provid friend result error code work jupyt notebook workspac",
        "Answer_original_content":"guess jupyt notebook experi case mini csv experi workspac toto creat new experi workspac name toto dataset open dataset open new notebook",
        "Answer_preprocessed_content":"guess jupyt notebook experi case experi creat new experi workspac name toto dataset open dataset open new notebook",
        "Answer_gpt_summary_original":"Solution: The user can create a new experiment in their AzureML workspace and upload the dataset into it. Then, they can open the dataset using 'open in a new Notebook' and use the code provided by their friend to validate the notebook.",
        "Answer_gpt_summary":"solut user creat new experi workspac upload dataset open dataset open new notebook us code provid friend valid notebook"
    },
    {
        "Question_title":"how to set path of bucket in amazonsagemaker jupyter notebook?",
        "Question_body":"<p>I'm new to the aws how to set path of my bucket and access file of that bucket?<\/p>\n\n<p>Is there anything i need to change with prefix ?<\/p>\n\n<pre><code>import os\nimport boto3\nimport re\nimport copy\nimport time\nfrom time import gmtime, strftime\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\n\nregion = boto3.Session().region_name\n\nbucket='ltfs1' # Replace with your s3 bucket name\nprefix = 'sagemaker\/ltfs1' # Used as part of the path in the bucket where you store data\n# bucket_path = 'https:\/\/s3-{}.amazonaws.com\/{}'.format(region,bucket) # The URL to access the bucket\n<\/code><\/pre>\n\n<p>I'm using the above code but it's showing file not found error<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1562134154127,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":852.0,
        "Answer_body":"<p>If the file you are accessing is in the root directory of your s3 bucket, you can access the file like this:<\/p>\n\n<pre><code>import pandas as pd\n\nbucket='ltfs1'\ndata_key = 'data.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\ntraining_data = pd.read_csv(data_location)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56863907",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1562166882852,
        "Question_original_content":"set path bucket amazon jupyt notebook new aw set path bucket access file bucket need chang prefix import import boto import import copi import time time import gmtime strftime import execut role role execut role region boto session region bucket ltf replac bucket prefix ltf path bucket store data bucket path http amazonaw com format region bucket url access bucket code show file error",
        "Question_preprocessed_content":"set path bucket amazon jupyt notebook new aw set path bucket access file bucket need chang prefix code show file error",
        "Question_gpt_summary_original":"The user is facing challenges in setting the path of their bucket in Amazon SageMaker Jupyter Notebook and accessing files from that bucket. They have provided a code snippet that they are using, but it is resulting in a \"file not found\" error. The user is seeking guidance on whether they need to change anything with the prefix.",
        "Question_gpt_summary":"user face challeng set path bucket jupyt notebook access file bucket provid code snippet result file error user seek guidanc need chang prefix",
        "Answer_original_content":"file access root directori bucket access file like import panda bucket ltf data kei data csv data locat format bucket data kei train data read csv data locat",
        "Answer_preprocessed_content":"file access root directori bucket access file like",
        "Answer_gpt_summary_original":"Solution: The user can access the file in the root directory of their s3 bucket by using the code snippet provided in the discussion. They need to replace the 'ltfs1' with their own bucket name and 'data.csv' with the name of the file they want to access.",
        "Answer_gpt_summary":"solut user access file root directori bucket code snippet provid discuss need replac ltf bucket data csv file want access"
    },
    {
        "Question_title":"How to connect AMLS to ADLS Gen 2?",
        "Question_body":"<p>I would like to register a dataset from ADLS Gen2 in my Azure Machine Learning workspace (<code>azureml-core==1.12.0<\/code>). Given that service principal information is not required in the Python SDK <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py#register-azure-data-lake-gen2-workspace--datastore-name--filesystem--account-name--tenant-id-none--client-id-none--client-secret-none--resource-url-none--authority-url-none--protocol-none--endpoint-none--overwrite-false-\" rel=\"noreferrer\">documentation<\/a> for <code>.register_azure_data_lake_gen2()<\/code>, I successfully used the following code to register ADLS gen2 as a datastore:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Datastore\n\nadlsgen2_datastore_name = os.environ['adlsgen2_datastore_name']\naccount_name=os.environ['account_name'] # ADLS Gen2 account name\nfile_system=os.environ['filesystem']\n\nadlsgen2_datastore = Datastore.register_azure_data_lake_gen2(\n    workspace=ws,\n    datastore_name=adlsgen2_datastore_name,\n    account_name=account_name, \n    filesystem=file_system\n)\n<\/code><\/pre>\n<p>However, when I try to register a dataset, using<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Dataset\nadls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)\ndata = Dataset.Tabular.from_delimited_files((adls_ds, 'folder\/data.csv'))\n<\/code><\/pre>\n<p>I get an error<\/p>\n<blockquote>\n<p>Cannot load any data from the specified path. Make sure the path is accessible and contains data.\n<code>ScriptExecutionException<\/code> was caused by <code>StreamAccessException<\/code>.\nStreamAccessException was caused by AuthenticationException.\n<code>'AdlsGen2-ReadHeaders'<\/code> for '[REDACTED]' on storage failed with status code 'Forbidden' (This request is not authorized to perform this operation using this permission.), client request ID &lt;CLIENT_REQUEST_ID&gt;, request ID &lt;REQUEST_ID&gt;. Error message: [REDACTED]\n| session_id=&lt;SESSION_ID&gt;<\/p>\n<\/blockquote>\n<p>Do I need the to enable the service principal to get this to work? Using the ML Studio UI, it appears that the service principal is required even to register the datastore.<\/p>\n<p>Another issue I noticed is that AMLS is trying to access the dataset here:\n<code>https:\/\/adls_gen2_account_name.**dfs**.core.windows.net\/container\/folder\/data.csv<\/code> whereas the actual URI in ADLS Gen2 is: <code>https:\/\/adls_gen2_account_name.**blob**.core.windows.net\/container\/folder\/data.csv<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1600115991930,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1600160631356,
        "Question_score":7.0,
        "Question_view_count":3331.0,
        "Answer_body":"<p>According to this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-data-lake-storage-generation-2\" rel=\"noreferrer\">documentation<\/a>,you need to enable the service principal.<\/p>\n<p>1.you need to register your application and grant the service principal with <strong>Storage Blob Data Reader access<\/strong>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/FZl8O.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FZl8O.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>2.try this code:<\/p>\n<pre><code>adlsgen2_datastore = Datastore.register_azure_data_lake_gen2(workspace=ws,\n                                                             datastore_name=adlsgen2_datastore_name,\n                                                             account_name=account_name,\n                                                             filesystem=file_system,\n                                                             tenant_id=tenant_id,\n                                                             client_id=client_id,\n                                                             client_secret=client_secret\n                                                             )\n\nadls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)\ndataset = Dataset.Tabular.from_delimited_files((adls_ds,'sample.csv'))\nprint(dataset.to_pandas_dataframe())\n<\/code><\/pre>\n<p><strong>Result:<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/50mit.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/50mit.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1600166834147,
        "Answer_score":9.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63891547",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1600155716360,
        "Question_original_content":"connect aml adl gen like regist dataset adl gen workspac core given servic princip inform requir python sdk document regist azur data lake gen successfulli follow code regist adl gen datastor core import datastor adlsgen datastor environ adlsgen datastor account environ account adl gen account file environ filesystem adlsgen datastor datastor regist azur data lake gen workspac datastor adlsgen datastor account account filesystem file try regist dataset core import dataset adl datastor datastor adlsgen datastor data dataset tabular delimit file adl folder data csv error load data specifi path sure path access contain data scriptexecutionexcept caus streamaccessexcept streamaccessexcept caus authenticationexcept adlsgen readhead redact storag fail statu code forbidden request author perform oper permiss client request request error messag redact session need enabl servic princip work studio appear servic princip requir regist datastor issu notic aml try access dataset http adl gen account df core window net contain folder data csv actual uri adl gen http adl gen account blob core window net contain folder data csv",
        "Question_preprocessed_content":"connect aml adl gen like regist dataset adl gen workspac given servic princip inform requir python sdk document successfulli follow code regist adl gen datastor try regist dataset error load data specifi path sure path access contain data caus streamaccessexcept caus authenticationexcept storag fail statu code forbidden client request request error messag need enabl servic princip work studio appear servic princip requir regist datastor issu notic aml try access dataset actual uri adl gen",
        "Question_gpt_summary_original":"The user is facing challenges while trying to register a dataset from ADLS Gen2 in their Azure Machine Learning workspace. Although they were able to register ADLS Gen2 as a datastore without using service principal information, they encountered an error while trying to register a dataset. The error message suggests that the path is inaccessible and contains no data. The user is unsure if they need to enable the service principal to make it work. Additionally, they noticed that AMLS is trying to access the dataset using the wrong URI.",
        "Question_gpt_summary":"user face challeng try regist dataset adl gen workspac abl regist adl gen datastor servic princip inform encount error try regist dataset error messag suggest path inaccess contain data user unsur need enabl servic princip work addition notic aml try access dataset wrong uri",
        "Answer_original_content":"accord document need enabl servic princip need regist applic grant servic princip storag blob data reader access try code adlsgen datastor datastor regist azur data lake gen workspac datastor adlsgen datastor account account filesystem file tenant tenant client client client secret client secret adl datastor datastor adlsgen datastor dataset dataset tabular delimit file adl sampl csv print dataset panda datafram result",
        "Answer_preprocessed_content":"accord document need enabl servic princip need regist applic grant servic princip storag blob data reader access try code result",
        "Answer_gpt_summary_original":"Solution:\n- Enable the service principal and grant it with Storage Blob Data Reader access.\n- Use the provided code to register the ADLS Gen2 datastore and access the dataset.",
        "Answer_gpt_summary":"solut enabl servic princip grant storag blob data reader access us provid code regist adl gen datastor access dataset"
    },
    {
        "Question_title":"Does AutoML support optimizing convolutional neural network over the number of layers and pool layer parameters?",
        "Question_body":"Does AutoML support optimizing convolutional neural network over the number of layers and pool layer parameters?",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1607093437353,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"AutoML doesn't currently support CNNs publicly, it's on our roadmap and it will come with optimizations across different parameters, so stay tuned. Hope this helps.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/186789\/does-automl-support-optimizing-convolutional-neura.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-04T21:28:34.597Z",
                "Answer_score":0,
                "Answer_body":"AutoML doesn't currently support CNNs publicly, it's on our roadmap and it will come with optimizations across different parameters, so stay tuned. Hope this helps.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1607117314596,
        "Question_original_content":"automl support optim convolut neural network number layer pool layer paramet automl support optim convolut neural network number layer pool layer paramet",
        "Question_preprocessed_content":"automl support optim convolut neural network number layer pool layer paramet automl support optim convolut neural network number layer pool layer paramet",
        "Question_gpt_summary_original":"The user is seeking information on whether AutoML can optimize convolutional neural networks based on the number of layers and pool layer parameters.",
        "Question_gpt_summary":"user seek inform automl optim convolut neural network base number layer pool layer paramet",
        "Answer_original_content":"automl current support cnn publicli roadmap come optim differ paramet stai tune hope help",
        "Answer_preprocessed_content":"automl current support cnn publicli roadmap come optim differ paramet stai tune hope help",
        "Answer_gpt_summary_original":"Solution: No solution provided.",
        "Answer_gpt_summary":"solut solut provid"
    },
    {
        "Question_title":"Dvc external output add after changing files data in remote is failing",
        "Question_body":"<p>I know that, after changing data being tracked by dvc, we can use \u201cdvc add\u201d command and then \u201cdvc push\u201d to github. I can use different .dvc files to get back data using \u201cdvc pull\u201d.<\/p>\n<p>The same when am trying to do with dvc external output, it is not working,<br>\nI\u2019m using remote storage and remote cache. And adding data of remote using \u201cadd --external\u201d command.<\/p>\n<ul>\n<li>dvc add --external remote:\/\/s3remote\/wine-quality.csv # tracks data, creates a cache folder in remote<\/li>\n<li>git push<\/li>\n<\/ul>\n<p>Now, am changing data in remote place, and I want to track the news changes,<\/p>\n<ul>\n<li>dvc add --external remote:\/\/s3remote\/wine-quality.csv<br>\n[I\u2019m using custom hosted Minion s3 bucket, to make changes am deleting the data file and uploading new one with same name with changes in data]<br>\nThis is failing with following error,<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/73a7302944e800f404f5a12f8b6123e45fbaa3bd.png\" data-download-href=\"\/uploads\/short-url\/gv76HdBPPYpiEEDxtGnScWBoiO9.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/73a7302944e800f404f5a12f8b6123e45fbaa3bd.png\" alt=\"image\" data-base62-sha1=\"gv76HdBPPYpiEEDxtGnScWBoiO9\" width=\"690\" height=\"23\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/73a7302944e800f404f5a12f8b6123e45fbaa3bd_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1104\u00d738 2.62 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div>\n<\/li>\n<\/ul>\n<p>FYI,<\/p>\n<ul>\n<li>\n<strong>config file<\/strong><br>\n[cache]<br>\ns3 = s3cache<br>\n[\u2018remote \u201cs3remote\u201d\u2019]<br>\nurl = S3:\/\/datasource-bucket\/<br>\nendpointurl = <a href=\"http:\/\/localhostminio:10009\/\" rel=\"noopener nofollow ugc\">http:\/\/localhostminio:10009\/<\/a><br>\naccess_key_id = user<br>\nsecret_access_key = password<br>\nuse_ssl = false<br>\n[\u2018remote \u201cs3cache\u201d\u2019]<br>\nurl = s3:\/\/datasource-bucket\/cache\/<br>\nendpointurl = <a href=\"http:\/\/localhostminio:10009\/\" rel=\"noopener nofollow ugc\">http:\/\/localhostminio:10009\/<\/a><br>\naccess_key_id = user<br>\nsecret_access_key = password<br>\nuse_ssl = false<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1618818748017,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":440.0,
        "Answer_body":"<aside class=\"quote no-group\" data-username=\"veeresh\" data-post=\"1\" data-topic=\"731\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/v\/df705f\/40.png\" class=\"avatar\"> veeresh:<\/div>\n<blockquote>\n<p>This is failing with following error,<\/p>\n<\/blockquote>\n<\/aside>\n<p>It seems like you initially added data in a folder named <code>remoteTrack<\/code> and now you are trying to add it again from a different folder. This is trying to create a new stage but then it fails since the stage already exists. You can do either go to the <code>remoteTrack\\<\/code> folder and use the same command or specify the filename that belongs to the previous stage via <code>--file remoteTrack\\wine-quality.csv.dvc<\/code>.<\/p>. <p>Thanks <a class=\"mention\" href=\"\/u\/isidentical\">@isidentical<\/a> <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-external-output-add-after-changing-files-data-in-remote-is-failing\/731",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-19T07:58:49.322Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"veeresh\" data-post=\"1\" data-topic=\"731\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/v\/df705f\/40.png\" class=\"avatar\"> veeresh:<\/div>\n<blockquote>\n<p>This is failing with following error,<\/p>\n<\/blockquote>\n<\/aside>\n<p>It seems like you initially added data in a folder named <code>remoteTrack<\/code> and now you are trying to add it again from a different folder. This is trying to create a new stage but then it fails since the stage already exists. You can do either go to the <code>remoteTrack\\<\/code> folder and use the same command or specify the filename that belongs to the previous stage via <code>--file remoteTrack\\wine-quality.csv.dvc<\/code>.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-19T08:40:59.781Z",
                "Answer_body":"<p>Thanks <a class=\"mention\" href=\"\/u\/isidentical\">@isidentical<\/a> <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"extern output add chang file data remot fail know chang data track us add command push github us differ file data pull try extern output work remot storag remot cach ad data remot add extern command add extern remot sremot wine qualiti csv track data creat cach folder remot git push chang data remot place want track new chang add extern remot sremot wine qualiti csv custom host minion bucket chang delet data file upload new chang data fail follow error imag fyi config file cach scach remot sremot url datasourc bucket endpointurl http localhostminio access kei user secret access kei password us ssl fals remot scach url datasourc bucket cach endpointurl http localhostminio access kei user secret access kei password us ssl fals",
        "Question_preprocessed_content":"extern output add chang file data remot fail know chang data track us add command push github us differ file data pull try extern output work remot storag remot cach ad data remot add command add track data creat cach folder remot git push chang data remot place want track new chang add custom host minion bucket chang delet data file upload new chang data fail follow error imag fyi config file cach cach remot remot url endpointurl user password fals remot cach url endpointurl user password fals",
        "Question_gpt_summary_original":"The user is encountering challenges with using the \"dvc external output\" command after changing data in a remote location. They are using remote storage and cache and have tried to add data using the \"add --external\" command, but it is failing with an error. The user has provided their config file for reference.",
        "Question_gpt_summary":"user encount challeng extern output command chang data remot locat remot storag cach tri add data add extern command fail error user provid config file refer",
        "Answer_original_content":"veeresh fail follow error like initi ad data folder name remotetrack try add differ folder try creat new stage fail stage exist remotetrack folder us command specifi filenam belong previou stage file remotetrack wine qualiti csv thank isident",
        "Answer_preprocessed_content":"veeresh fail follow error like initi ad data folder name try add differ folder try creat new stage fail stage exist folder us command specifi filenam belong previou stage thank",
        "Answer_gpt_summary_original":"Solution:\n- The user can either go to the original folder where the data was added and use the same command, or specify the filename that belongs to the previous stage via \"--file\" option.",
        "Answer_gpt_summary":"solut user origin folder data ad us command specifi filenam belong previou stage file option"
    },
    {
        "Question_title":"MLflow Error while deploying the Model to local REST server",
        "Question_body":"<blockquote>\n  <p><strong>System Details:<\/strong><\/p>\n  \n  <p>Operating System: Ubuntu 19.04<\/p>\n  \n  <p>Anaconda version: 2019.03<\/p>\n  \n  <p>Python version: 3.7.3<\/p>\n  \n  <p>mlflow version: 1.0.0<\/p>\n<\/blockquote>\n\n<p><strong>Steps to Reproduce:<\/strong> <a href=\"https:\/\/mlflow.org\/docs\/latest\/tutorial.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tutorial.html<\/a><\/p>\n\n<p><strong>Error at line\/command:<\/strong> <code>mlflow models serve -m [path_to_model] -p 1234<\/code><\/p>\n\n<p><strong>Error:<\/strong>\nCommand 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1>&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1<\/p>\n\n<p><strong>Terminal Log:<\/strong><\/p>\n\n<pre><code>(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# mlflow models serve -m $(pwd) -p 1234\n2019\/06\/18 16:15:16 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n2019\/06\/18 16:15:17 INFO mlflow.pyfunc.backend: === Running command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app'\nbash: activate: No such file or directory\nTraceback (most recent call last):\n  File \"\/root\/anaconda3\/envs\/mlflow\/bin\/mlflow\", line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/models\/cli.py\", line 43, in serve\n    host=host)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 76, in serve\n    command_env=command_env)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 147, in _execute_in_conda_env\n    command, rc\nException: Command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1\n(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# \n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1560855399150,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1840.0,
        "Answer_body":"<p>Following the steps mentioned in the GitHub Issue <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">1507<\/a> (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/1507<\/a>) I was able to resolve this issue.<\/p>\n\n<p>In reference to this post, the \"<strong>anaconda\/bin\/<\/strong>\" directory is never added to the list of environment variables i.e. PATH variable. In order to resolve this issue, add the \"<strong>else<\/strong>\" part of conda initialize code block from ~\/.bashrc file to your PATH variable.<\/p>\n\n<pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('\/home\/atulk\/anaconda3\/bin\/conda' 'shell.bash' 'hook' 2&gt; \/dev\/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\" ]; then\n        . \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\"\n    else\n        export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n<\/code><\/pre>\n\n<p>In this case, I added <strong>export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"<\/strong> to the PATH variable. However, this is just a temporary fix until the issue is fixed in the project.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56647549",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1561730574528,
        "Question_original_content":"error deploi model local rest server detail oper ubuntu anaconda version python version version step reproduc http org doc latest tutori html error line command model serv path model error command sourc activ cceeebbfbdcebbdb gunicorn timeout pyfunc score server wsgi app return non zero return code return code termin log root user home user exampl sklearn elasticnet wine mlrun eddddffabdbed artifact model model serv pwd info model cli select backend flavor python function info pyfunc backend run command sourc activ cceeebbfbdcebbdb gunicorn timeout pyfunc score server wsgi app bash activ file directori traceback recent file root anaconda env bin line sy exit cli file root anaconda env lib python site packag click core line return self main arg kwarg file root anaconda env lib python site packag click core line main self invok ctx file root anaconda env lib python site packag click core line invok return process result sub ctx command invok sub ctx file root anaconda env lib python site packag click core line invok return process result sub ctx command invok sub ctx file root anaconda env lib python site packag click core line invok return ctx invok self callback ctx param file root anaconda env lib python site packag click core line invok return callback arg kwarg file root anaconda env lib python site packag model cli line serv host host file root anaconda env lib python site packag pyfunc backend line serv command env command env file root anaconda env lib python site packag pyfunc backend line execut conda env command except command sourc activ cceeebbfbdcebbdb gunicorn timeout pyfunc score server wsgi app return non zero return code return code root user home user exampl sklearn elasticnet wine mlrun eddddffabdbed artifact model",
        "Question_preprocessed_content":"error deploi model local rest server detail oper ubuntu anaconda version python version version step reproduc error error command sourc activ bfb dce gunicorn return non zero return code return code termin log",
        "Question_gpt_summary_original":"The user encountered an error while deploying a model to a local REST server using MLflow. The error occurred at the command \"mlflow models serve -m [path_to_model] -p 1234\" and returned a non-zero return code. The terminal log shows that the error was caused by a missing file or directory for the command \"source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1>&2 && gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app\".",
        "Question_gpt_summary":"user encount error deploi model local rest server error occur command model serv path model return non zero return code termin log show error caus miss file directori command sourc activ cceeebbfbdcebbdb gunicorn timeout pyfunc score server wsgi app",
        "Answer_original_content":"follow step mention github issu http github com issu abl resolv issu refer post anaconda bin directori ad list environ variabl path variabl order resolv issu add conda initi code block bashrc file path variabl conda initi content block manag conda init conda setup home atulk anaconda bin conda shell bash hook dev null eval conda setup home atulk anaconda profil conda home atulk anaconda profil conda export path home atulk anaconda bin path unset conda setup conda initi case ad export path home atulk anaconda bin path path variabl temporari fix issu fix project",
        "Answer_preprocessed_content":"follow step mention github issu abl resolv issu refer post directori ad list environ variabl path variabl order resolv issu add conda initi code block file path variabl case ad export path variabl temporari fix issu fix project",
        "Answer_gpt_summary_original":"Solution: The user was able to resolve the issue by adding the \"else\" part of conda initialize code block from ~\/.bashrc file to the PATH variable. The solution involved adding \"export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"\" to the PATH variable. However, this is just a temporary fix until the issue is fixed in the project.",
        "Answer_gpt_summary":"solut user abl resolv issu ad conda initi code block bashrc file path variabl solut involv ad export path home atulk anaconda bin path path variabl temporari fix issu fix project"
    },
    {
        "Question_title":"Azure Data Factory : How to pass DataPath as a parameter to Azure ML Pipeline activity?",
        "Question_body":"Hello All,\n\nHow to pass a Datapath as a parameter in Azure ML Pipeline activity?\n\nMore details here : Have opened an issue here : https:\/\/github.com\/Azure\/Azure-DataFactory\/issues\/216\n\n\n\n\n\nThanks.",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1599771191990,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Thanks @SriramNarayanan-6939 for your patience!\n\nI discussed with the Product team and they confirmed that there is no datatype supported for \"DataPath\" parameter today in Azure Data Factory(ADF). However, there is a feature already raised for the same and work is in progress for it.\n\nI would recommend you also to submit an idea in feedback forum. The ideas in this forum are closely monitored by data factory product team and will prioritize implementing them in future releases.\n\nSorry for the inconvenience!",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/91785\/azure-data-factory-how-to-pass-datapath-as-a-param.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-09-25T08:43:19.257Z",
                "Answer_score":1,
                "Answer_body":"Thanks @SriramNarayanan-6939 for your patience!\n\nI discussed with the Product team and they confirmed that there is no datatype supported for \"DataPath\" parameter today in Azure Data Factory(ADF). However, there is a feature already raised for the same and work is in progress for it.\n\nI would recommend you also to submit an idea in feedback forum. The ideas in this forum are closely monitored by data factory product team and will prioritize implementing them in future releases.\n\nSorry for the inconvenience!",
                "Answer_comment_count":8,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1601023399256,
        "Question_original_content":"azur data factori pass datapath paramet pipelin activ hello pass datapath paramet pipelin activ detail open issu http github com azur azur datafactori issu thank",
        "Question_preprocessed_content":"azur data factori pass datapath paramet pipelin activ hello pass datapath paramet pipelin activ detail open issu thank",
        "Question_gpt_summary_original":"The user is facing a challenge in passing a Datapath as a parameter to Azure ML Pipeline activity and has opened an issue on GitHub seeking a solution.",
        "Question_gpt_summary":"user face challeng pass datapath paramet pipelin activ open issu github seek solut",
        "Answer_original_content":"thank sriramnarayanan patienc discuss product team confirm datatyp support datapath paramet todai azur data factori adf featur rais work progress recommend submit idea feedback forum idea forum close monitor data factori product team priorit implement futur releas sorri inconveni",
        "Answer_preprocessed_content":"thank patienc discuss product team confirm datatyp support datapath paramet todai azur data factori featur rais work progress recommend submit idea feedback forum idea forum close monitor data factori product team priorit implement futur releas sorri inconveni",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion. The product team confirmed that there is no datatype supported for \"DataPath\" parameter in Azure Data Factory, but there is a feature already raised for the same and work is in progress for it. The user is recommended to submit an idea in the feedback forum.",
        "Answer_gpt_summary":"solut solut provid discuss product team confirm datatyp support datapath paramet azur data factori featur rais work progress user recommend submit idea feedback forum"
    },
    {
        "Question_title":"'Enter Data' as list instead of list of lists in Azure ML Web Service",
        "Question_body":"<p>In Azure ML, I want to enter data to a model through a published Web Service. \nThe way to tell this to the Web Service, as far as I can tell, it to have an 'Enter Data' box coming into the same input as the Web service. <\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/m1x5x.png\" alt=\"enter image description here\"><\/p>\n\n<p>You can then set you data format in the 'Enter Data' properties:<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/VQJ9V.png\" alt=\"enter image description here\"><\/p>\n\n<p>I want that list to be an arbitrary-length array of samples. This works if your input is:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"samples\"\n      ],\n      \"Values\": [\n        [\n          1\n        ],\n        [\n          2\n        ],\n        [\n          3\n        ],\n        [\n          4\n        ],\n        [\n          5\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>This is ok, but ideally it would be easier, and (more importantly) more network-efficient, if I could send them as:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"samples\"\n      ],\n      \"Values\": [\n        [\n          1,2,3,4,5\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>Is there a correct syntax to implement this? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1437737046243,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":174.0,
        "Answer_body":"<p>I have worked internally to request a confirmation of your concern - <\/p>\n\n<blockquote>\n  <p>'Enter Data' as list instead of list of lists in Azure ML Web Service<\/p>\n<\/blockquote>\n\n<p>but you expected feature is not available today in Azure ML Studio (The reason behind is Azure ML has to be able to read the input data as a tabular format, rows and columns). Such being the case, I would like to suggest you to submit a new feature request via below option:<\/p>\n\n<p>On Azure ML Studio -> the upper right corner, there is a smiley face, please click that and send the feedback.<\/p>\n\n<p>Should you have any further concerns, please feel free to let me know.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/31609319",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1438871325836,
        "Question_original_content":"enter data list instead list list web servic want enter data model publish web servic wai tell web servic far tell enter data box come input web servic set data format enter data properti want list arbitrari length arrai sampl work input input input columnnam sampl valu globalparamet ideal easier importantli network effici send input input columnnam sampl valu globalparamet correct syntax implement",
        "Question_preprocessed_content":"enter data list instead list list web servic want enter data model publish web servic wai tell web servic far tell enter data box come input web servic set data format enter data properti want list arrai sampl work input ideal easier send correct syntax implement",
        "Question_gpt_summary_original":"The user is facing a challenge in entering data to a model through a published Web Service in Azure ML. They want to send an arbitrary-length array of samples as a list instead of a list of lists, which would be more network-efficient. The user is seeking guidance on the correct syntax to implement this.",
        "Question_gpt_summary":"user face challeng enter data model publish web servic want send arbitrari length arrai sampl list instead list list network effici user seek guidanc correct syntax implement",
        "Answer_original_content":"work intern request confirm concern enter data list instead list list web servic expect featur avail todai studio reason abl read input data tabular format row column case like suggest submit new featur request option studio upper right corner smilei face click send feedback concern feel free let know",
        "Answer_preprocessed_content":"work intern request confirm concern enter data list instead list list web servic expect featur avail todai studio case like suggest submit new featur request option studio upper right corner smilei face click send feedback concern feel free let know",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion. The user is advised to submit a new feature request via Azure ML Studio's feedback option.",
        "Answer_gpt_summary":"solut solut provid discuss user advis submit new featur request studio feedback option"
    },
    {
        "Question_title":"Mlflow Serve functionality",
        "Question_body":"Hi,\n\nIs there any API that allows us to serve model using remote system?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559881963000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":21.0,
        "Answer_body":"I would take a look at the built-in model deployment tools for SageMaker, AzureML, and Spark:\u00a0https:\/\/www.mlflow.org\/docs\/latest\/models.html#built-in-deployment-tools\n\n\nOn Fri, Jun 7, 2019 at 1:32 AM Shevy Mittal <shevy....@gslab.com> wrote:\n\nHi,\n\nIs there any API that allows us to serve model using remote system?\n\n\nConfidentiality Notice and Disclaimer: This email (including any attachments) contains information that may be confidential, privileged and\/or copyrighted. If you are not the intended recipient, please notify the sender immediately and destroy this email. Any unauthorized use of the contents of this email in any manner whatsoever, is strictly prohibited. If improper activity is suspected, all available information may be used by the sender for possible disciplinary action, prosecution, civil claim or any remedy or lawful purpose. Email transmission cannot be guaranteed to be secure or error-free, as information could be intercepted, lost, arrive late, or contain viruses. The sender is not liable whatsoever for damage resulting from the opening of this message and\/or the use of the information contained in this message and\/or attachments. Expressions in this email cannot be treated as opined by the sender company management \u2013 they are solely expressed by the sender unless authorized.\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/30a5371f-cec0-4f37-abc9-b50dce0ad1ea%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/How0fUco2FE",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-06-07T11:23:13",
                "Answer_body":"I would take a look at the built-in model deployment tools for SageMaker, AzureML, and Spark:\u00a0https:\/\/www.mlflow.org\/docs\/latest\/models.html#built-in-deployment-tools\n\n\nOn Fri, Jun 7, 2019 at 1:32 AM Shevy Mittal <shevy....@gslab.com> wrote:\n\nHi,\n\nIs there any API that allows us to serve model using remote system?\n\n\nConfidentiality Notice and Disclaimer: This email (including any attachments) contains information that may be confidential, privileged and\/or copyrighted. If you are not the intended recipient, please notify the sender immediately and destroy this email. Any unauthorized use of the contents of this email in any manner whatsoever, is strictly prohibited. If improper activity is suspected, all available information may be used by the sender for possible disciplinary action, prosecution, civil claim or any remedy or lawful purpose. Email transmission cannot be guaranteed to be secure or error-free, as information could be intercepted, lost, arrive late, or contain viruses. The sender is not liable whatsoever for damage resulting from the opening of this message and\/or the use of the information contained in this message and\/or attachments. Expressions in this email cannot be treated as opined by the sender company management \u2013 they are solely expressed by the sender unless authorized.\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/30a5371f-cec0-4f37-abc9-b50dce0ad1ea%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"serv function api allow serv model remot",
        "Question_preprocessed_content":"serv function api allow serv model remot",
        "Question_gpt_summary_original":"The user is seeking information about an API that can be used to serve a model using a remote system.",
        "Question_gpt_summary":"user seek inform api serv model remot",
        "Answer_original_content":"look built model deploy tool spark http org doc latest model html built deploy tool fri jun shevi mittal wrote api allow serv model remot confidenti notic disclaim email includ attach contain inform confidenti privileg copyright intend recipi notifi sender immedi destroi email unauthor us content email manner whatsoev strictli prohibit improp activ suspect avail inform sender possibl disciplinari action prosecut civil claim remedi law purpos email transmiss guarante secur error free inform intercept lost arriv late contain virus sender liabl whatsoev damag result open messag us inform contain messag attach express email treat opin sender compani manag sole express sender author receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user cec abc bdceadea googlegroup com option visit http group googl com optout",
        "Answer_preprocessed_content":"look model deploy tool fri jun shevi mittal wrote api allow serv model remot confidenti notic disclaim email contain inform confidenti privileg copyright intend recipi notifi sender immedi destroi email unauthor us content email manner whatsoev strictli prohibit improp activ suspect avail inform sender possibl disciplinari action prosecut civil claim remedi law purpos email transmiss guarante secur inform intercept lost arriv late contain virus sender liabl whatsoev damag result open messag us inform contain messag attach express email treat opin sender compani manag sole express sender author receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit",
        "Answer_gpt_summary_original":"Solution: The discussion suggests taking a look at the built-in model deployment tools for SageMaker, AzureML, and Spark. No other solutions are mentioned.",
        "Answer_gpt_summary":"solut discuss suggest take look built model deploy tool spark solut mention"
    },
    {
        "Question_title":"Sagemaker XGBoost Hyperparameter Tuning Error",
        "Question_body":"<p>I am new to Sagemaker and trying to set up a hyperparameter tuning job for xgboost algorithm in Sagemaker. I have very imbalanced data (98% majority class, 2% minority\u00a0class) and would like to use the\u00a0&quot;scale_pos_weight&quot; parameter but the below error happens.<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: The hyperparameter tuning job that you requested has the following untunable hyperparameters: [scale_pos_weight]. For the algorithm, ---------------.us-east-1.amazonaws.com\/xgboost:1, you can tune only [colsample_bytree, lambda, eta, max_depth, alpha, num_round, colsample_bylevel, subsample, min_child_weight, max_delta_step, gamma]. Delete untunable hyperparameters.\u00a0\u00a0\n<\/code><\/pre>\n<p>I have upgraded the sagemaker package, restarted my kernel (I am using juptyer notebook), and instance but the problem still exists.<\/p>\n<p>Does anyone have any ideas why this error happens and how I can fix it? I appreciate the help.<\/p>\n<p>\u00a0\nHere is my code that I followed from an example in AWS.\u00a0<\/p>\n<pre><code>sess = sagemaker.Session()\ncontainer = get_image_uri(region, 'xgboost', '1.0-1')\n\nxgb = sagemaker.estimator.Estimator(container,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 role, \n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 train_instance_count=1, \n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 train_instance_type='ml.m4.4xlarge',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output_path='s3:\/\/{}\/{}\/output'.format(bucket, prefix),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sagemaker_session=sess)\n\nxgb.set_hyperparameters(eval_metric='auc',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 objective='binary:logistic',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 num_round=100,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rate_drop=0.3,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 tweedie_variance_power=1.4)\n\nhyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'min_child_weight': ContinuousParameter(1, 10),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'scale_pos_weight' : ContinuousParameter(700, 800),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'alpha': ContinuousParameter(0, 2),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'max_depth': IntegerParameter(1, 10),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'colsample_bytree' : ContinuousParameter(0.1, 0.9)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\nobjective_metric_name = 'validation:auc'\n\ntuner = HyperparameterTuner(xgb,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 objective_metric_name,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hyperparameter_ranges,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 max_jobs=10,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 max_parallel_jobs=2)\n\ns3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train'.format(bucket, prefix), content_type='csv')\ns3_input_validation = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/validation\/'.format(bucket, prefix), content_type='csv')\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603939946643,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":827.0,
        "Answer_body":"<p>Based on the Sagemaker developer documentation, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html<\/a>, the hyperparameter <code>scale_pos_weight<\/code> is NOT tunable. The only parameters that you can tune are given in the link.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64584295",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1603941358963,
        "Question_original_content":"xgboost hyperparamet tune error new try set hyperparamet tune job xgboost algorithm imbalanc data major class minorityclass like us scale po weight paramet error happen clienterror error occur validationexcept call createhyperparametertuningjob oper hyperparamet tune job request follow untun hyperparamet scale po weight algorithm east amazonaw com xgboost tune colsampl bytre lambda eta max depth alpha num round colsampl bylevel subsampl min child weight max delta step gamma delet untun hyperparamet upgrad packag restart kernel juptyer notebook instanc problem exist idea error happen fix appreci help code follow exampl aw sess session contain imag uri region xgboost xgb estim estim contain role train instanc count train instanc type xlarg output path output format bucket prefix session sess xgb set hyperparamet eval metric auc object binari logist num round rate drop tweedi varianc power hyperparamet rang eta continuousparamet min child weight continuousparamet scale po weight continuousparamet alpha continuousparamet max depth integerparamet colsampl bytre continuousparamet object metric valid auc tuner hyperparametertun xgb object metric hyperparamet rang max job max parallel job input train input data train format bucket prefix content type csv input valid input data valid format bucket prefix content type csv tuner fit train input train valid input valid includ cl metadata fals",
        "Question_preprocessed_content":"xgboost hyperparamet tune error new try set hyperparamet tune job xgboost algorithm imbalanc data like us paramet error happen upgrad packag restart kernel instanc problem exist idea error happen fix appreci help code follow exampl aw",
        "Question_gpt_summary_original":"The user is encountering an error while setting up a hyperparameter tuning job for the xgboost algorithm in Sagemaker. The error is related to the \"scale_pos_weight\" parameter, which cannot be tuned for the algorithm. The user has tried upgrading the Sagemaker package and restarting the kernel and instance, but the problem persists. The user is seeking help to understand why the error is happening and how to fix it.",
        "Question_gpt_summary":"user encount error set hyperparamet tune job xgboost algorithm error relat scale po weight paramet tune algorithm user tri upgrad packag restart kernel instanc problem persist user seek help understand error happen fix",
        "Answer_original_content":"base develop document http doc aw amazon com latest xgboost tune html hyperparamet scale po weight tunabl paramet tune given link",
        "Answer_preprocessed_content":"base develop document hyperparamet tunabl paramet tune given link",
        "Answer_gpt_summary_original":"Solution: The hyperparameter \"scale_pos_weight\" is not tunable for the xgboost algorithm in Sagemaker, according to the Sagemaker developer documentation. Therefore, there is no solution to fix the error related to this parameter.",
        "Answer_gpt_summary":"solut hyperparamet scale po weight tunabl xgboost algorithm accord develop document solut fix error relat paramet"
    },
    {
        "Question_title":"Part of speech tagging and entity recognition - python",
        "Question_body":"<p>I want to perform part of speech tagging and entity recognition in python similar to Maxent_POS_Tag_Annotator and Maxent_Entity_Annotator functions of openNLP in R.  I would prefer a code in python which takes input as textual sentence and gives output as different features- like number of \"CC\", number of \"CD\", number of \"DT\" etc.. CC, CD, DT are POS tags as used in Penn Treebank. So there should be 36 columns\/features for POS tagging corresponding to 36 POS tags as in <a href=\"http:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html\" rel=\"nofollow\">Penn Treebank POS<\/a>. I want to implement this on Azure ML \"Execute Python Script\" module and Azure ML supports python 2.7.7. I heard nltk in python may does the job, but I am a beginner on python. Any help would be appreciated. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1441535794287,
        "Question_favorite_count":null,
        "Question_last_edit_time":1441776651843,
        "Question_score":0.0,
        "Question_view_count":1014.0,
        "Answer_body":"<p>Take a look at <a href=\"http:\/\/www.nltk.org\/book\/ch05.html\" rel=\"nofollow\">NTLK book<\/a>, Categorizing and Tagging Words section.<\/p>\n\n<p>Simple example, it uses the Penn Treebank tagset:<\/p>\n\n<pre><code>from nltk.tag import pos_tag\nfrom nltk.tokenize import word_tokenize\npos_tag(word_tokenize(\"John's big idea isn't all that bad.\")) \n\n[('John', 'NNP'),\n(\"'s\", 'POS'),\n ('big', 'JJ'),\n ('idea', 'NN'),\n ('is', 'VBZ'),\n (\"n't\", 'RB'),\n ('all', 'DT'),\n ('that', 'DT'),\n ('bad', 'JJ'),\n ('.', '.')]\n<\/code><\/pre>\n\n<p>Then you can use<\/p>\n\n<pre><code>from collections import defaultdict\ncounts = defaultdict(int)\nfor (word, tag) in pos_tag(word_tokenize(\"John's big idea isn't all that bad.\")):\n    counts[tag] += 1\n<\/code><\/pre>\n\n<p>to get frequencies:<\/p>\n\n<pre><code>defaultdict(&lt;type 'int'&gt;, {'JJ': 2, 'NN': 1, 'POS': 1, '.': 1, 'RB': 1, 'VBZ': 1, 'DT': 2, 'NNP': 1})\n<\/code><\/pre>",
        "Answer_comment_count":7.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32422626",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1441539548048,
        "Question_original_content":"speech tag entiti recognit python want perform speech tag entiti recognit python similar maxent po tag annot maxent entiti annot function opennlp prefer code python take input textual sentenc give output differ featur like number number number po tag penn treebank column featur po tag correspond po tag penn treebank po want implement execut python script modul support python heard nltk python job beginn python help appreci",
        "Question_preprocessed_content":"speech tag entiti recognit python want perform speech tag entiti recognit python similar function opennlp prefer code python take input textual sentenc give output differ featur like number number number po tag penn treebank po tag correspond po tag penn treebank po want implement execut python script modul support python heard nltk python job beginn python help appreci",
        "Question_gpt_summary_original":"The user is facing a challenge in performing part of speech tagging and entity recognition in Python, similar to the Maxent_POS_Tag_Annotator and Maxent_Entity_Annotator functions of openNLP in R. They require a code in Python that takes input as a textual sentence and gives output as different features, such as the number of \"CC\", \"CD\", \"DT\", etc. The user wants to implement this on Azure ML \"Execute Python Script\" module, which supports Python 2.7.7. Although they have heard that nltk in Python may do the job, they are a beginner in Python and would appreciate any help.",
        "Question_gpt_summary":"user face challeng perform speech tag entiti recognit python similar maxent po tag annot maxent entiti annot function opennlp requir code python take input textual sentenc give output differ featur number user want implement execut python script modul support python heard nltk python job beginn python appreci help",
        "Answer_original_content":"look ntlk book categor tag word section simpl exampl us penn treebank tagset nltk tag import po tag nltk token import word token po tag word token john big idea isn bad john nnp po big idea vbz bad us collect import defaultdict count defaultdict int word tag po tag word token john big idea isn bad count tag frequenc defaultdict po vbz nnp",
        "Answer_preprocessed_content":"look ntlk book categor tag word section simpl exampl us penn treebank tagset us frequenc",
        "Answer_gpt_summary_original":"Solution:\nThe discussion suggests using the NLTK library in Python to perform part of speech tagging and entity recognition. The code provided in the discussion takes input as a textual sentence and gives output as different features, such as the number of \"CC\", \"CD\", \"DT\", etc. The NLTK library can be used to achieve this. The code provided in the discussion can be used to get the frequencies of different parts of speech in a sentence.",
        "Answer_gpt_summary":"solut discuss suggest nltk librari python perform speech tag entiti recognit code provid discuss take input textual sentenc give output differ featur number nltk librari achiev code provid discuss frequenc differ part speech sentenc"
    },
    {
        "Question_title":"VertexAI Pipelines: The provided location ID doesn't match the endpoint",
        "Question_body":"<p>I have successfully run the following pipeline that creates a dataset, trains a model, and deploys to an endpoint using VertexAIs pipeline tool when everything is based in us-central1. Now, when I change the region to europe-west2, I get the following error:<\/p>\n<pre><code>debug_error_string = &quot;{&quot;created&quot;:&quot;@1647430410.324290053&quot;,&quot;description&quot;:&quot;Error received from peer\nipv4:172.217.169.74:443&quot;,&quot;file&quot;:&quot;src\/core\/lib\/surface\/call.cc&quot;,&quot;file_line&quot;:1066,\n&quot;grpc_message&quot;:&quot;List of found errors:\\t1.Field: name; Message: \nThe provided location ID doesn't match the endpoint. The valid location ID is `us-central1`.\\t&quot;,&quot;grpc_status&quot;:3}&quot;\n<\/code><\/pre>\n<p>This error occurs after the dataset is created in europe-west2, and before the model starts to train. Here is my code:<\/p>\n<pre><code>#import libraries\nfrom typing import NamedTuple\nimport kfp\nfrom kfp import dsl\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import (Artifact, Input, InputPath, Model, Output, \n                        OutputPath, ClassificationMetrics, Metrics, component)\nfrom kfp.v2.components.types.artifact_types import Dataset\nfrom kfp.v2.google.client import AIPlatformClient\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom google.api_core.exceptions import NotFound\n\n@kfp.dsl.pipeline(name=f&quot;lookalike-model-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = f&quot;bq:\/\/{PROJECT_ID}.{DATASET_ID}.{TABLE_NAME}&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;europe-west2&quot;,\n    api_endpoint: str = &quot;europe-west2-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auPrc&quot;: 0.5}',\n):\n            \n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project,\n        display_name=display_name, \n        bq_source=bq_source,\n        location = gcp_region\n    )\n\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        location=gcp_region,\n        predefined_split_column_name=&quot;set&quot;,\n        column_transformations=[\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;agentId&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;postcode&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;isMobile&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;gender&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;timeOfDay&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;set&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;sale&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;sale&quot;,\n    )\n\ncompiler.Compiler().compile(\n    pipeline_func=pipeline, package_path=&quot;tab_classif_pipeline.json&quot;\n)\n\nml_pipeline_job = aiplatform.PipelineJob(\n    display_name=f&quot;{MODEL_PREFIX}_training&quot;,\n    template_path=&quot;tab_classif_pipeline.json&quot;,\n    pipeline_root=PIPELINE_ROOT,\n    parameter_values={&quot;project&quot;: PROJECT_ID, &quot;display_name&quot;: DISPLAY_NAME},\n    enable_caching=True,\n    location=&quot;europe-west2&quot;\n)\nml_pipeline_job.submit()\n<\/code><\/pre>\n<p>As previously mentioned, the dataset gets created so I suspect that the issue must lie in <code>training_op = gcc_aip.AutoMLTabularTrainingJobRunOp<\/code><\/p>\n<p>I tried providing another endpoint: <code>eu-aiplatform.googleapis.com<\/code> which yielded the following error:<\/p>\n<pre><code>google.api_core.exceptions.InvalidArgument: 400 List of found errors: 1.Field: name; Message: \nThe provided location ID doesn't match the endpoint. The valid location ID is `us-central1`.\n\nFail to send metric: [rpc error: code = PermissionDenied desc = Permission monitoring.metricDescriptors.create \ndenied (or the resource may not exist).; rpc error: code = PermissionDenied desc = Permission monitoring.timeSeries.create\n denied (or the resource may not exist).]\n<\/code><\/pre>\n<p>I understand that I am not passing api-endpoint to any of the methods above, but I thought I'd highlight that the error changed slightly.<\/p>\n<p>Does anyone know what the issue may be? Or how I can run <code>gcc_aip.AutoMLTabularTrainingJobRunOp<\/code> in europe-west2 (or EU in general)?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1647433181040,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":220.0,
        "Answer_body":"<p>Try Updating the pipeline component using the command:<\/p>\n<p><code>pip3 install --force-reinstall google_cloud_pipeline_components==0.1.3<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71496966",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1649155832383,
        "Question_original_content":"vertexai pipelin provid locat match endpoint successfulli run follow pipelin creat dataset train model deploi endpoint vertexai pipelin tool base central chang region europ west follow error debug error string creat descript error receiv peer ipv file src core lib surfac file line grpc messag list error field messag provid locat match endpoint valid locat central grpc statu error occur dataset creat europ west model start train code import librari type import namedtupl import kfp kfp import dsl kfp import compil kfp dsl import artifact input inputpath model output outputpath classificationmetr metric compon kfp compon type artifact type import dataset kfp googl client import aiplatformcli googl cloud import aiplatform googl cloud pipelin compon import aiplatform gcc aip googl api core except import notfound kfp dsl pipelin lookalik model train pipelin root pipelin root def pipelin sourc str project dataset tabl displai str displai project str project gcp region str europ west api endpoint str europ west aiplatform googleapi com threshold dict str str auprc dataset creat gcc aip tabulardatasetcreateop project project displai displai sourc sourc locat gcp region train gcc aip automltabulartrainingjobrunop project project displai displai optim predict type classif budget milli node hour locat gcp region predefin split column set column transform categor column agentid categor column postcod categor column ismobil categor column gender categor column timeofdai categor column set categor column sale dataset dataset creat output dataset target column sale compil compil compil pipelin func pipelin packag path tab classif pipelin json pipelin job aiplatform pipelinejob displai model prefix train templat path tab classif pipelin json pipelin root pipelin root paramet valu project project displai displai enabl cach true locat europ west pipelin job submit previous mention dataset get creat suspect issu lie train gcc aip automltabulartrainingjobrunop tri provid endpoint aiplatform googleapi com yield follow error googl api core except invalidargu list error field messag provid locat match endpoint valid locat central fail send metric rpc error code permissiondeni desc permiss monitor metricdescriptor creat deni resourc exist rpc error code permissiondeni desc permiss monitor timeseri creat deni resourc exist understand pass api endpoint method thought highlight error chang slightli know issu run gcc aip automltabulartrainingjobrunop europ west gener thank",
        "Question_preprocessed_content":"vertexai pipelin provid locat match endpoint successfulli run follow pipelin creat dataset train model deploi endpoint vertexai pipelin tool base chang region follow error error occur dataset creat model start train code previous mention dataset get creat suspect issu lie tri provid endpoint yield follow error understand pass method thought highlight error chang slightli know issu run thank",
        "Question_gpt_summary_original":"The user encountered an error when trying to run a pipeline that creates a dataset, trains a model, and deploys to an endpoint using VertexAI's pipeline tool. The error occurred when the user changed the region to europe-west2, and the error message indicated that the provided location ID did not match the endpoint. The user tried providing another endpoint, but the error persisted. The user is seeking assistance in resolving the issue and running the pipeline in europe-west2 or EU in general.",
        "Question_gpt_summary":"user encount error try run pipelin creat dataset train model deploi endpoint vertexai pipelin tool error occur user chang region europ west error messag indic provid locat match endpoint user tri provid endpoint error persist user seek assist resolv issu run pipelin europ west gener",
        "Answer_original_content":"try updat pipelin compon command pip instal forc reinstal googl cloud pipelin compon",
        "Answer_preprocessed_content":"try updat pipelin compon command",
        "Answer_gpt_summary_original":"Solution: The discussion suggests updating the pipeline component using the command \"pip3 install --force-reinstall google_cloud_pipeline_components==0.1.3\" to resolve the error encountered when running the pipeline in europe-west2 or EU in general.",
        "Answer_gpt_summary":"solut discuss suggest updat pipelin compon command pip instal forc reinstal googl cloud pipelin compon resolv error encount run pipelin europ west gener"
    },
    {
        "Question_title":"Deep learning training on Azure steps and tutorial",
        "Question_body":"Hi MSFT Community,\n\nI followed this guide to set up a GPU: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/dsvm-ubuntu-intro\n\nVM: Standard NC12_Promo, 12 vCPUs, 112 Gib RAM\nOperating System: Linux\nOffer: Ubuntu-1804\n\nI am ready to start deep learning training but I am confused about what to do next. I am doing a medical image classification project. I have 1 millions images store in Azure blob now. Do I need to download them to my VM in order to train? Or is it a better way to access image efficiently?\n\nWhat are some good tutorials to set up the experiments? I've read a lot of documentation but still confused.\n\nThank you very much!\nBest Regards,\nClaire",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1605040090287,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@gecheng-2063\ncheck on the below AI training modules.\nhttps:\/\/aischool.microsoft.com\/en-us\/services\/learning-paths\n\nAI Lab\nhttps:\/\/www.microsoft.com\/en-us\/ai\/ai-lab-projects\n\nAI module gallery\nhttps:\/\/gallery.azure.ai\/browse\n\n\n\n\nPlease don\u2019t forget to \"Accept the answer\" and \u201cup-vote\u201d wherever the information provided helps you, this can be beneficial to other community members.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/158168\/deep-learning-training-on-azure-steps-and-tutorial.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-11T01:10:40.68Z",
                "Answer_score":2,
                "Answer_body":"@gecheng-2063\ncheck on the below AI training modules.\nhttps:\/\/aischool.microsoft.com\/en-us\/services\/learning-paths\n\nAI Lab\nhttps:\/\/www.microsoft.com\/en-us\/ai\/ai-lab-projects\n\nAI module gallery\nhttps:\/\/gallery.azure.ai\/browse\n\n\n\n\nPlease don\u2019t forget to \"Accept the answer\" and \u201cup-vote\u201d wherever the information provided helps you, this can be beneficial to other community members.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1605057040680,
        "Question_original_content":"deep learn train azur step tutori msft commun follow guid set gpu http doc microsoft com azur machin learn data scienc virtual machin dsvm ubuntu intro standard promo vcpu gib ram oper linux offer ubuntu readi start deep learn train confus medic imag classif project million imag store azur blob need download order train better wai access imag effici good tutori set experi read lot document confus thank best regard clair",
        "Question_preprocessed_content":"deep learn train azur step tutori msft commun follow guid set gpu standard vcpu gib ram oper linux offer readi start deep learn train confus medic imag classif project million imag store azur blob need download order train better wai access imag effici good tutori set experi read lot document confus thank best regard clair",
        "Question_gpt_summary_original":"The user is facing challenges in setting up deep learning training on Azure for a medical image classification project. They are unsure about whether they need to download 1 million images stored in Azure blob to their VM for training and are looking for tutorials to help them set up the experiments.",
        "Question_gpt_summary":"user face challeng set deep learn train azur medic imag classif project unsur need download million imag store azur blob train look tutori help set experi",
        "Answer_original_content":"gecheng check train modul http aischool microsoft com servic learn path lab http microsoft com lab project modul galleri http galleri azur brows dont forget accept answer vote inform provid help benefici commun member",
        "Answer_preprocessed_content":"check train modul lab modul galleri dont forget accept answer inform provid help benefici commun member",
        "Answer_gpt_summary_original":"Solution: The discussion provides links to AI training modules, AI Lab projects, and AI module gallery on Microsoft Azure. These resources can help the user set up deep learning training for their medical image classification project. No specific solution is mentioned regarding whether the user needs to download 1 million images stored in Azure blob to their VM for training.",
        "Answer_gpt_summary":"solut discuss provid link train modul lab project modul galleri microsoft azur resourc help user set deep learn train medic imag classif project specif solut mention user need download million imag store azur blob train"
    },
    {
        "Question_title":"Pathway for code free predictive modeling",
        "Question_body":"I am looking at Azure&amp;#39;s training modules and it states I can learn no-code models with Azure, but it also tells me I should know python. I&amp;#39;m a little confused at where I should spend time training in most efficient pathway. My goal is to just do predictive modeling within Azure. I have technical\/IT literacy however coding is at a basic level.\n\nIdeally id like some sort of Certification, if possible from just &amp;#34;Create no-code predictive models with Azure Machine Learning&amp;#34;\n\nIs &amp;#34;Microsoft Certified: Azure Data Scientist Associate&amp;#34; going to require a lot of pre work on python\/torch\/tensor? I&amp;#39;d ideally like Azure to be my entry.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592869306553,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Thanks for reaching out. Azure machine learning has a drag and drop interface (Designer) that supports code free predictive modeling. Create no-code predictive models with Azure Machine Learning training modules is a great starting point and provides a pathway for Azure Data Scientist Associate certification. However, you also need programming experience and familiarity with various data science processes\/principles to be successful on the certification exam.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/38841\/pathway-for-code-free-predictive-modeling.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-23T15:06:48.467Z",
                "Answer_score":2,
                "Answer_body":"Thanks for reaching out. Azure machine learning has a drag and drop interface (Designer) that supports code free predictive modeling. Create no-code predictive models with Azure Machine Learning training modules is a great starting point and provides a pathway for Azure Data Scientist Associate certification. However, you also need programming experience and familiarity with various data science processes\/principles to be successful on the certification exam.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":2.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1592924808467,
        "Question_original_content":"pathwai code free predict model look azur train modul state learn code model azur tell know python littl confus spend time train effici pathwai goal predict model azur technic literaci code basic level ideal like sort certif possibl creat code predict model microsoft certifi azur data scientist associ go requir lot pre work python torch tensor ideal like azur entri",
        "Question_preprocessed_content":"pathwai code free predict model look azur train modul state learn model azur tell know python littl confus spend time train effici pathwai goal predict model azur literaci code basic level ideal like sort certif possibl creat predict model microsoft certifi azur data scientist associ go requir lot pre work ideal like azur entri",
        "Question_gpt_summary_original":"The user is confused about the most efficient pathway to learn code-free predictive modeling with Azure, as they have basic coding skills and want to avoid extensive pre-work on Python\/torch\/tensor. They are also interested in obtaining a certification, specifically for creating no-code predictive models with Azure Machine Learning. They are unsure if the Microsoft Certified: Azure Data Scientist Associate certification will require extensive knowledge of Python\/torch\/tensor.",
        "Question_gpt_summary":"user confus effici pathwai learn code free predict model azur basic code skill want avoid extens pre work python torch tensor interest obtain certif specif creat code predict model unsur microsoft certifi azur data scientist associ certif requir extens knowledg python torch tensor",
        "Answer_original_content":"thank reach drag drop interfac design support code free predict model creat code predict model train modul great start point provid pathwai azur data scientist associ certif need program experi familiar data scienc process principl success certif exam",
        "Answer_preprocessed_content":"thank reach drag drop interfac support code free predict model creat predict model train modul great start point provid pathwai azur data scientist associ certif need program experi familiar data scienc success certif exam",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that the user can start with the Create no-code predictive models with Azure Machine Learning training modules to learn code-free predictive modeling with Azure. The modules provide a pathway for the Microsoft Certified: Azure Data Scientist Associate certification. However, the user also needs programming experience and familiarity with various data science processes\/principles to be successful on the certification exam.",
        "Answer_gpt_summary":"solut discuss suggest user start creat code predict model train modul learn code free predict model azur modul provid pathwai microsoft certifi azur data scientist associ certif user need program experi familiar data scienc process principl success certif exam"
    },
    {
        "Question_title":"How to change metrics computation and recompute",
        "Question_body":"<p>Hi all,<br>\nI trained several models and computed a simple metrics summary for each of them. Now, those models (and their corresponding metrics.json files) are versioned by dvc and git and there is a git tag associated with each model.<br>\nHaving the models trained and stored, I would like to enrich the metrics summary and go back and recompute it for each model \u2013 and store it in git as with the simple summary. This means the last part of the pipeline, say compute_metrics.py, will be changed and I would like to run dvc repro again. Is there a simple way of recomputing the metrics for all models (that is for all tags)?<\/p>\n<p>Thank you,<br>\nMichal<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1575979302220,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":420.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/michal.najman\">@michal.najman<\/a><br>\nSo what you are looking for is probably <code>dvc repro<\/code><br>\n<a href=\"https:\/\/dvc.org\/doc\/command-reference\/repro#repro\">docs<\/a><br>\nIf you have only modified the code responsible for metrics calculation, dvc should detect that and recalculate only metrics. If you want to be sure, you can explicitly tell dvc to reproduce metrics stage only with <code>--single-item<\/code> (eg <code>dvc repro --single-item metrics_calc.dvc<\/code>) param. I am afraid that currently, we do not support <code>repro<\/code> for multiple git revisions, which would probably have to be done manually. Maybe you want to create feature request on GitHub, with a short description how would you expect that to work?<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-change-metrics-computation-and-recompute\/278",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-12-10T17:46:20.045Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/michal.najman\">@michal.najman<\/a><br>\nSo what you are looking for is probably <code>dvc repro<\/code><br>\n<a href=\"https:\/\/dvc.org\/doc\/command-reference\/repro#repro\">docs<\/a><br>\nIf you have only modified the code responsible for metrics calculation, dvc should detect that and recalculate only metrics. If you want to be sure, you can explicitly tell dvc to reproduce metrics stage only with <code>--single-item<\/code> (eg <code>dvc repro --single-item metrics_calc.dvc<\/code>) param. I am afraid that currently, we do not support <code>repro<\/code> for multiple git revisions, which would probably have to be done manually. Maybe you want to create feature request on GitHub, with a short description how would you expect that to work?<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"chang metric comput recomput train model comput simpl metric summari model correspond metric json file version git git tag associ model have model train store like enrich metric summari recomput model store git simpl summari mean pipelin comput metric chang like run repro simpl wai recomput metric model tag thank michal",
        "Question_preprocessed_content":"chang metric comput recomput train model comput simpl metric summari model version git git tag associ model have model train store like enrich metric summari recomput model store git simpl summari mean pipelin chang like run repro simpl wai recomput metric model thank michal",
        "Question_gpt_summary_original":"The user has trained several models and computed a simple metrics summary for each of them. The models and their corresponding metrics.json files are versioned by dvc and git, and there is a git tag associated with each model. The user wants to enrich the metrics summary and recompute it for each model, which requires changing the last part of the pipeline and running dvc repro again. The user is seeking a simple way to recomputed the metrics for all models (that is for all tags).",
        "Question_gpt_summary":"user train model comput simpl metric summari model correspond metric json file version git git tag associ model user want enrich metric summari recomput model requir chang pipelin run repro user seek simpl wai recomput metric model tag",
        "Answer_original_content":"michal najman look probabl repro doc modifi code respons metric calcul detect recalcul metric want sure explicitli tell reproduc metric stage singl item repro singl item metric calc param afraid current support repro multipl git revis probabl manual mayb want creat featur request github short descript expect work",
        "Answer_preprocessed_content":"look probabl doc modifi code respons metric calcul detect recalcul metric want sure explicitli tell reproduc metric stage param afraid current support multipl git revis probabl manual mayb want creat featur request github short descript expect work",
        "Answer_gpt_summary_original":"Solutions provided: \n- The user can use `dvc repro` to recompute the metrics for all models. If only the code responsible for metrics calculation has been modified, dvc should detect that and recalculate only metrics. \n- The user can explicitly tell dvc to reproduce metrics stage only with `--single-item` parameter. \n- Currently, `repro` for multiple git revisions is not supported, and it would have to be done manually. The user is advised to create a feature request on GitHub with a short description of how they would expect it to work.",
        "Answer_gpt_summary":"solut provid user us repro recomput metric model code respons metric calcul modifi detect recalcul metric user explicitli tell reproduc metric stage singl item paramet current repro multipl git revis support manual user advis creat featur request github short descript expect work"
    },
    {
        "Question_title":"Support for Oracle dialect in SqlAlchemyStore",
        "Question_body":"Hello:\n\n\nIs anyone working on allowing Oracle databases as an option for backend storage?\u00a0 I made the necessary modifications and have it working in my local development environment, but it would need a little more work to ensure that the other dialects don't break with the changes.\u00a0 Is it worth proceeding, or is it something that someone else is already working on?\n\n\n\nThanks",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1560524168000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":14.0,
        "Answer_body":"Hi Mike,\n\n\nThat sounds great! If there are changes expect, would you mind writing up a short design doc. You can find several RFC examples for MLflow. If you could outline different options and test cases needed before implementing or making a pull request, we can help review that.\n\n\nYou could write the RFC directly as a github issue or create a google doc and share for comments.\n\n\nThanks again for checking and looking forward to seeing this contribution.\n\n\n\nMani Parkhe\n\nma...@databricks.com\n\n\u00a0\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/b155ea33-8b8c-4e09-b421-668584467581%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/EC5HClf5fAE",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-06-14T20:53:20",
                "Answer_body":"Hi Mike,\n\n\nThat sounds great! If there are changes expect, would you mind writing up a short design doc. You can find several RFC examples for MLflow. If you could outline different options and test cases needed before implementing or making a pull request, we can help review that.\n\n\nYou could write the RFC directly as a github issue or create a google doc and share for comments.\n\n\nThanks again for checking and looking forward to seeing this contribution.\n\n\n\nMani Parkhe\n\nma...@databricks.com\n\n\u00a0\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/b155ea33-8b8c-4e09-b421-668584467581%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"support oracl dialect sqlalchemystor hello work allow oracl databas option backend storag necessari modif work local develop environ need littl work ensur dialect break chang worth proceed work thank",
        "Question_preprocessed_content":"support oracl dialect sqlalchemystor hello work allow oracl databas option backend storag necessari modif work local develop environ need littl work ensur dialect break chang worth proceed work thank",
        "Question_gpt_summary_original":"The user is seeking support for Oracle databases as a backend storage option in SqlAlchemyStore. They have made modifications to make it work in their local development environment but are unsure if it's worth proceeding as it may require more work to ensure other dialects don't break with the changes.",
        "Question_gpt_summary":"user seek support oracl databas backend storag option sqlalchemystor modif work local develop environ unsur worth proceed requir work ensur dialect break chang",
        "Answer_original_content":"mike sound great chang expect mind write short design doc rfc exampl outlin differ option test case need implement make pull request help review write rfc directli github issu creat googl doc share comment thank check look forward see contribut mani parkh databrick com receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user bea googlegroup com option visit http group googl com optout",
        "Answer_preprocessed_content":"mike sound great chang expect mind write short design doc rfc exampl outlin differ option test case need implement make pull request help review write rfc directli github issu creat googl doc share comment thank check look forward see contribut mani parkh receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut mention discuss"
    },
    {
        "Question_title":"ModuleNotFoundError: No module named 'azureml'",
        "Question_body":"Hi,\n\nI am doing the Challenge. https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/intro-to-azure-machine-learning-service\/\n\n\n\n\nPlease see what I have installed:\n\npip install azureml-sdk\n\nI am getting the following messages at the end:\n\n\n\n\nERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n\n\nWe recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n\n\njupyterlab 2.2.9 requires jupyterlab-server<2.0,>=1.1.5, which is not installed.\nSuccessfully installed applicationinsights-0.11.9 azure-identity-1.4.1 azureml-automl-core-1.19.0 azureml-dataprep-2.6.3 azureml-dataprep-native-26.0.0 azureml-dataprep-rslex-1.4.0 azureml-dataset-runtime-1.19.0.post1 azureml-pipeline-1.19.0 azureml-pipeline-core-1.19.0 azureml-pipeline-steps-1.19.0 azureml-sdk-1.19.0 azureml-telemetry-1.19.0 azureml-train-1.19.0 azureml-train-automl-client-1.19.0 azureml-train-core-1.19.0 azureml-train-restclients-hyperdrive-1.19.0 distro-1.5.0 dotnetcore2-2.1.20 fusepy-3.0.1 msal-1.8.0 msal-extensions-0.2.2 numpy-1.19.3 portalocker-1.7.1 pyarrow-1.0.1 pywin32-227\n\n\n\n\nNow I am trying to start up and type the following in .py file in Visual Studio Code\n\nfrom azureml.core import Workspace\n\nThis is the error message I am getting:\n\nFile \"c:\/Users\/User\/OneDrive\/Desktop\/New folder\/Build AI Solution\/automl_python.py\", line 1, in <module>\nfrom azureml.core import Workspace\nModuleNotFoundError: No module named 'azureml'\n\n\n\n\nPlease could you help me?\n\nthanks,\n\nNaveen",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1609112074090,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"This is now solved. Thanks!",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/211503\/modulenotfounderror-no-module-named-39azureml39.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-15T23:26:14.427Z",
                "Answer_score":28,
                "Answer_body":"This is now solved. Thanks!",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2020-12-29T06:50:34.837Z",
                "Answer_score":1,
                "Answer_body":"@Naveen-5899 From the above error it looks like the package did not install successfully. Could you please try to downgrade jupyterlab to a version >=1.1.5 but <2.0 and try to re-install the azureml SDK? Currently you are using jupyterlab 2.2.9\n\nA more detailed procedure to install the SDK is available directly in the documentation.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1610753174427,
        "Question_original_content":"modulenotfounderror modul name challeng http doc microsoft com learn modul intro azur machin learn servic instal pip instal sdk get follow messag end error octob experi error instal updat packag pip chang wai resolv depend conflict recommend us us featur resolv test packag new resolv default jupyterlab requir jupyterlab server instal successfulli instal applicationinsight azur ident automl core dataprep dataprep nativ dataprep rslex dataset runtim post pipelin pipelin core pipelin step sdk telemetri train train automl client train core train restclient hyperdr distro dotnetcor fusepi msal msal extens numpi portalock pyarrow pywin try start type follow file visual studio code core import workspac error messag get file user user onedr desktop new folder build solut automl python line core import workspac modulenotfounderror modul name help thank naveen",
        "Question_preprocessed_content":"modulenotfounderror modul name challeng instal pip instal sdk get follow messag end error octob experi error instal updat packag pip chang wai resolv depend conflict recommend us test packag new resolv default jupyterlab requir instal successfulli instal try start type follow file visual studio code core import workspac error messag get file line core import workspac modulenotfounderror modul name help thank naveen",
        "Question_gpt_summary_original":"The user is encountering a challenge with the installation of the Azure Machine Learning SDK. They have installed the SDK using pip, but are receiving an error message when trying to import the 'azureml' module in their Python code. The error message indicates that the module is not found, suggesting that the installation may not have been successful.",
        "Question_gpt_summary":"user encount challeng instal sdk instal sdk pip receiv error messag try import modul python code error messag indic modul suggest instal success",
        "Answer_original_content":"solv thank",
        "Answer_preprocessed_content":"solv thank",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut mention discuss"
    },
    {
        "Question_title":"Sagemaker endpoint(with VPC) url accessible from internet",
        "Question_body":"<p>I have created a model with VPC, Private subnet, and appropriate security group. The endpoint URL can, however, be reached through the internet though failing due to the lack of security token<\/p>\n\n<p>Things I need clarification on now are<\/p>\n\n<ol>\n<li>Is there a way to avoid the URL being accessible from the internet<\/li>\n<li>Are we not charged for requests failed on AUTH(like for API Gateway)<\/li>\n<li>Does that make our deployment vulnerable to any attacks<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1550066008687,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":679.0,
        "Answer_body":"<p>You are not hitting your endpoint, but the endpoint of AWS SageMaker runtime. This endpoint is checking all the permissions to access your hosted model, and only if the credentials and requirements are met, the request is forwarded to your instances and models. <\/p>\n\n<p>Therefore, you can't prevent this URL from being accessible from the Internet, but at the same time, you don't need to protect it or pay for it. AWS has a high level of security on these endpoints, and I don't think that you have a more secure way to protect these endpoints. <\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54671841",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1550250221603,
        "Question_original_content":"endpoint vpc url access internet creat model vpc privat subnet appropri secur group endpoint url reach internet fail lack secur token thing need clarif wai avoid url access internet charg request fail auth like api gatewai deploy vulner attack",
        "Question_preprocessed_content":"endpoint url access internet creat model vpc privat subnet appropri secur group endpoint url reach internet fail lack secur token thing need clarif wai avoid url access internet charg request fail auth deploy vulner attack",
        "Question_gpt_summary_original":"The user has encountered challenges with their Sagemaker endpoint with VPC, as the URL can be accessed through the internet without a security token. They are seeking clarification on how to prevent internet access to the URL, whether failed AUTH requests are charged, and if their deployment is vulnerable to attacks.",
        "Question_gpt_summary":"user encount challeng endpoint vpc url access internet secur token seek clarif prevent internet access url fail auth request charg deploy vulner attack",
        "Answer_original_content":"hit endpoint endpoint runtim endpoint check permiss access host model credenti requir met request forward instanc model prevent url access internet time need protect pai aw high level secur endpoint think secur wai protect endpoint",
        "Answer_preprocessed_content":"hit endpoint endpoint runtim endpoint check permiss access host model credenti requir met request forward instanc model prevent url access internet time need protect pai aw high level secur endpoint think secur wai protect endpoint",
        "Answer_gpt_summary_original":"Solutions provided: \n- The endpoint of AWS SageMaker runtime checks all permissions to access the hosted model, and only if the credentials and requirements are met, the request is forwarded to instances and models. \n- AWS has a high level of security on these endpoints, and there is no need to protect or pay for them. \n\nNo solutions provided for preventing internet access to the URL or if failed AUTH requests are charged.",
        "Answer_gpt_summary":"solut provid endpoint runtim check permiss access host model credenti requir met request forward instanc model aw high level secur endpoint need protect pai solut provid prevent internet access url fail auth request charg"
    },
    {
        "Question_title":"Azure Auto ML JobConfigurationMaxSizeExceeded error when using a cluster",
        "Question_body":"<p>I am running into the following error when I try to run Automated ML through the studio on a GPU compute cluster:<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/uLyxr.png\" alt=\"Azure ML error message\" \/><\/p>\n<blockquote>\n<p>Error: AzureMLCompute job failed. JobConfigurationMaxSizeExceeded: The\nspecified job configuration exceeds the max allowed size of 32768\ncharacters. Please reduce the size of the job's command line arguments\nand environment settings<\/p>\n<\/blockquote>\n<p>The attempted run is on a registered tabulated dataset in filestore and is a simple regression case. Strangely, it works just fine with the CPU compute instance I use for my other pipelines. I have been able to run it a few times using that and wanted to upgrade to a cluster only to be hit by this error. I found online that it could be a case of having the following setting: AZUREML_COMPUTE_USE_COMMON_RUNTIME:false; but I am not sure where to put this in when just running from the web studio.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1638986274657,
        "Question_favorite_count":null,
        "Question_last_edit_time":1641204588636,
        "Question_score":5.0,
        "Question_view_count":171.0,
        "Answer_body":"<p>It looks like the bug was fixed. I just ran it on a cluster without changing any of the parameters. Thank you Yutong for the help!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70279636",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1639585163672,
        "Question_original_content":"azur auto jobconfigurationmaxsizeexceed error cluster run follow error try run autom studio gpu comput cluster error comput job fail jobconfigurationmaxsizeexceed specifi job configur exce max allow size charact reduc size job command line argument environ set attempt run regist tabul dataset filestor simpl regress case strang work fine cpu comput instanc us pipelin abl run time want upgrad cluster hit error onlin case have follow set comput us common runtim fals sure run web studio",
        "Question_preprocessed_content":"azur auto jobconfigurationmaxsizeexceed error cluster run follow error try run autom studio gpu comput cluster error comput job fail jobconfigurationmaxsizeexceed specifi job configur exce max allow size charact reduc size job command line argument environ set attempt run regist tabul dataset filestor simpl regress case strang work fine cpu comput instanc us pipelin abl run time want upgrad cluster hit error onlin case have follow set sure run web studio",
        "Question_gpt_summary_original":"The user encountered an error while running Automated ML through the studio on a GPU compute cluster. The error message indicates that the job configuration exceeds the maximum allowed size of 32768 characters. The user has tried to run the same job on a CPU compute instance without any issues. The user is unsure where to put the setting \"AZUREML_COMPUTE_USE_COMMON_RUNTIME:false\" when running from the web studio.",
        "Question_gpt_summary":"user encount error run autom studio gpu comput cluster error messag indic job configur exce maximum allow size charact user tri run job cpu comput instanc issu user unsur set comput us common runtim fals run web studio",
        "Answer_original_content":"look like bug fix ran cluster chang paramet thank yutong help",
        "Answer_preprocessed_content":"look like bug fix ran cluster chang paramet thank yutong help",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion as the user reported that the issue was resolved without making any changes to the parameters.",
        "Answer_gpt_summary":"solut mention discuss user report issu resolv make chang paramet"
    },
    {
        "Question_title":"How to Find Azure ML Experiment based on Deployed Web Service",
        "Question_body":"<p>I have a list of ML experiments which I have created in Azure Machine Learning Studio.  I have deployed them as web services (the new version, not classic).  <\/p>\n\n<p>How can I go into Azure Machine Learning Web Services, click on a web service (which was deployed from an experiment), then navigate back to the experiment \/ predictive model which feeds it?<\/p>\n\n<p>The only link I can find between the two is by updating the web service from the predictive experiment, which then confirms what the web service is. I can see that the \"ExperimentId\" is a GUID in the URL when in the experiment and the web service, so hopefully this is possible.<\/p>\n\n<p>My reasoning is that relying on matching naming conventions, etc., to select the appropriate model to update is subject to human error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1489415750577,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":7.0,
        "Question_view_count":224.0,
        "Answer_body":"<p>The <em>new<\/em> web service does not store any information about the experiment or workspace that was deployed (not all <em>new<\/em> web services are deployed from an experiment).<\/p>\n\n<p>Here are the options available to track the relationship between the experiment and a <em>new<\/em> web service.<\/p>\n\n<h2>last deployment<\/h2>\n\n<p>However, the experiment keeps track of the <strong>last<\/strong> <em>new<\/em> web service that was deployed from the experiment. each deployment to a <em>new<\/em> web service overwrites this value.<\/p>\n\n<p>The value is stored in the experiment graph. One way to get the graph is to use the powershell module <a href=\"http:\/\/aka.ms\/amlps\" rel=\"nofollow noreferrer\">amlps<\/a><\/p>\n\n<p><code>Export-AmlExperimentGraph -ExperimentId &lt;Experiment Id&gt; -OutputFile e.json<\/code><\/p>\n\n<p><strong>e.json<\/strong><\/p>\n\n<pre><code>{\n\"ExperimentId\":\"&lt;Experiment Id&gt;\",\n\/\/ . . .\n\"WebService\":{\n\/\/ . . .\n\"ArmWebServiceId\":\"&lt;Arm Id&gt;\"\n},\n\/\/ . . . \n}\n<\/code><\/pre>\n\n<h2>azure resource tags<\/h2>\n\n<p>The tags feature for Azure resources is supported by the <em>new<\/em> web services. Setting a <code>tag<\/code> on the web service programmatically, with powershell or via the azure portal UX can be used to store a reference to the experiment on the <em>new<\/em> web service.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42766263",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1490283526688,
        "Question_original_content":"experi base deploi web servic list experi creat studio deploi web servic new version classic web servic click web servic deploi experi navig experi predict model feed link updat web servic predict experi confirm web servic experimentid guid url experi web servic hopefulli possibl reason reli match name convent select appropri model updat subject human error",
        "Question_preprocessed_content":"experi base deploi web servic list experi creat studio deploi web servic web servic click web servic navig experi predict model feed link updat web servic predict experi confirm web servic experimentid guid url experi web servic hopefulli possibl reason reli match name convent select appropri model updat subject human error",
        "Question_gpt_summary_original":"The user is facing a challenge in finding the Azure ML experiment that feeds a deployed web service. They are unable to navigate back to the experiment from the web service and are relying on matching naming conventions, which is subject to human error. The user is looking for a way to link the web service to the experiment using the ExperimentId GUID in the URL.",
        "Question_gpt_summary":"user face challeng find experi feed deploi web servic unabl navig experi web servic reli match name convent subject human error user look wai link web servic experi experimentid guid url",
        "Answer_original_content":"new web servic store inform experi workspac deploi new web servic deploi experi option avail track relationship experi new web servic deploy experi keep track new web servic deploi experi deploy new web servic overwrit valu valu store experi graph wai graph us powershel modul amlp export amlexperimentgraph experimentid outputfil json json experimentid webservic armwebserviceid azur resourc tag tag featur azur resourc support new web servic set tag web servic programmat powershel azur portal store refer experi new web servic",
        "Answer_preprocessed_content":"new web servic store inform experi workspac deploi option avail track relationship experi new web servic deploy experi keep track new web servic deploi experi deploy new web servic overwrit valu valu store experi graph wai graph us powershel modul amlp azur resourc tag tag featur azur resourc support new web servic set web servic programmat powershel azur portal store refer experi new web servic",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. The experiment keeps track of the last new web service that was deployed from the experiment. The value is stored in the experiment graph, which can be obtained using the powershell module amlps.\n2. The tags feature for Azure resources is supported by the new web services. Setting a tag on the web service programmatically, with powershell or via the azure portal UX can be used to store a reference to the experiment on the new web service.",
        "Answer_gpt_summary":"possibl solut mention discuss experi keep track new web servic deploi experi valu store experi graph obtain powershel modul amlp tag featur azur resourc support new web servic set tag web servic programmat powershel azur portal store refer experi new web servic"
    },
    {
        "Question_title":"User Docker Hub registry containers in AWS Sagemaker",
        "Question_body":"<p>Is there any way to load containers stored in docker hub registry in Amazon Sagemaker.\nAccording to some documentation, it should be possible, but I have not been able to find any relevan example or guide for it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1571665775480,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":135.0,
        "Answer_body":"<p>While you can use any registry when working with Docker on a SageMaker notebook, as of this writing other SageMaker components presently only support images from Amazon ECR repositories.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58487710",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1582242007587,
        "Question_original_content":"user docker hub registri contain wai load contain store docker hub registri accord document possibl abl relevan exampl guid",
        "Question_preprocessed_content":"user docker hub registri contain wai load contain store docker hub registri accord document possibl abl relevan exampl guid",
        "Question_gpt_summary_original":"The user is facing challenges in loading containers stored in Docker Hub registry in Amazon Sagemaker and has not been able to find any relevant example or guide for it.",
        "Question_gpt_summary":"user face challeng load contain store docker hub registri abl relev exampl guid",
        "Answer_original_content":"us registri work docker notebook write compon present support imag amazon ecr repositori",
        "Answer_preprocessed_content":"us registri work docker notebook write compon present support imag amazon ecr repositori",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion.",
        "Answer_gpt_summary":"solut solut provid discuss"
    },
    {
        "Question_title":"Basic DVC workflow",
        "Question_body":"<p>I\u2019m a new DVC user and I have a question about the basic model management workflow.<\/p>\n<p>I\u2019ve configured a repo for DVC, I ran a test experiment by creating a feature branch. I ran a sweep and selected the <code>best<\/code> model, i.e.<\/p>\n<pre><code class=\"lang-auto\">dvc exp run -S 'train.batch_size=16,32,64,128' --queue\ndvc queue start\ndvc exp apply ex1\ngit add .\ngit commit -m 'My Experiment'\n<\/code><\/pre>\n<p>I have a question about how to merge this back to main correctly, the process I\u2019ve been following is:<\/p>\n<ul>\n<li>switch to <code>main<\/code>\n<\/li>\n<li>merge the feature branch to <code>main<\/code>\n<\/li>\n<li><code>dvc pull<\/code><\/li>\n<\/ul>\n<p>The last step seems to be important - if I don\u2019t do the the vscode source control sidebar shows uncommitted DVC tracked model files. Is this the correct workflow (I\u2019m about to learn GTO but I wanted to ensure that I can correctly manage the state of my <code>main<\/code> branch first).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1674171670865,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":44.0,
        "Answer_body":"<p>Hey, <a class=\"mention\" href=\"\/u\/david.waterworth\">@david.waterworth<\/a> !<\/p>\n<p>TL;DR: I think this link is worth checking: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/apply#example-make-an-experiment-persistent\"> Example: Make an experiment persistent<\/a><\/p>\n<p>A few comments on the workflow and suggestion to try:<\/p>\n<ul>\n<li>I think you don\u2019t need a branch for this. If you want at the end an experiment as part of the main branch (e.g. a new commit as a result of running a sweep), I would just run experiments in main. If you run them in the queue they don\u2019t affect the workspace, etc.<\/li>\n<li>When you are done, you could do <a href=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/apply\"><code>dvc exp apply<\/code><\/a>, <a href=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/branch\"><code>dvc exp branch<\/code><\/a> to \u201cmaterialize\u201d an experiment as a commit in <code>main<\/code> or as a new branch.<\/li>\n<\/ul>\n<p>In VS Code you could use context menu to apply an experiment or do a branch.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/52c529bd868aaf30f61c55c5dc982fdd7cd5e0b7.png\" data-download-href=\"\/uploads\/short-url\/bOdzux5ZfcxTuYLwa5BMMHGZzV5.png?dl=1\" title=\"Screen Shot 2023-01-19 at 3.55.39 PM\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/52c529bd868aaf30f61c55c5dc982fdd7cd5e0b7_2_690x409.png\" alt=\"Screen Shot 2023-01-19 at 3.55.39 PM\" data-base62-sha1=\"bOdzux5ZfcxTuYLwa5BMMHGZzV5\" width=\"690\" height=\"409\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/52c529bd868aaf30f61c55c5dc982fdd7cd5e0b7_2_690x409.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/52c529bd868aaf30f61c55c5dc982fdd7cd5e0b7_2_1035x613.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/52c529bd868aaf30f61c55c5dc982fdd7cd5e0b7_2_1380x818.png 2x\" data-dominant-color=\"282828\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2023-01-19 at 3.55.39 PM<\/span><span class=\"informations\">1562\u00d7926 105 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Let us know if that solves the issue.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/basic-dvc-workflow\/1467",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-19T23:56:24.314Z",
                "Answer_body":"<p>Hey, <a class=\"mention\" href=\"\/u\/david.waterworth\">@david.waterworth<\/a> !<\/p>\n<p>TL;DR: I think this link is worth checking: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/apply#example-make-an-experiment-persistent\"> Example: Make an experiment persistent<\/a><\/p>\n<p>A few comments on the workflow and suggestion to try:<\/p>\n<ul>\n<li>I think you don\u2019t need a branch for this. If you want at the end an experiment as part of the main branch (e.g. a new commit as a result of running a sweep), I would just run experiments in main. If you run them in the queue they don\u2019t affect the workspace, etc.<\/li>\n<li>When you are done, you could do <a href=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/apply\"><code>dvc exp apply<\/code><\/a>, <a href=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/branch\"><code>dvc exp branch<\/code><\/a> to \u201cmaterialize\u201d an experiment as a commit in <code>main<\/code> or as a new branch.<\/li>\n<\/ul>\n<p>In VS Code you could use context menu to apply an experiment or do a branch.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/52c529bd868aaf30f61c55c5dc982fdd7cd5e0b7.png\" data-download-href=\"\/uploads\/short-url\/bOdzux5ZfcxTuYLwa5BMMHGZzV5.png?dl=1\" title=\"Screen Shot 2023-01-19 at 3.55.39 PM\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/52c529bd868aaf30f61c55c5dc982fdd7cd5e0b7_2_690x409.png\" alt=\"Screen Shot 2023-01-19 at 3.55.39 PM\" data-base62-sha1=\"bOdzux5ZfcxTuYLwa5BMMHGZzV5\" width=\"690\" height=\"409\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/52c529bd868aaf30f61c55c5dc982fdd7cd5e0b7_2_690x409.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/52c529bd868aaf30f61c55c5dc982fdd7cd5e0b7_2_1035x613.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/52c529bd868aaf30f61c55c5dc982fdd7cd5e0b7_2_1380x818.png 2x\" data-dominant-color=\"282828\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2023-01-19 at 3.55.39 PM<\/span><span class=\"informations\">1562\u00d7926 105 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Let us know if that solves the issue.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"basic workflow new user question basic model manag workflow iv configur repo ran test experi creat featur branch ran sweep select best model exp run train batch size queue queue start exp appli git add git commit experi question merg main correctli process iv follow switch main merg featur branch main pull step import dont vscode sourc control sidebar show uncommit track model file correct workflow learn gto want ensur correctli manag state main branch",
        "Question_preprocessed_content":"basic workflow new user question basic model manag workflow iv configur repo ran test experi creat featur branch ran sweep select model question merg main correctli process iv follow switch merg featur branch step import dont vscode sourc control sidebar show uncommit track model file correct workflow",
        "Question_gpt_summary_original":"The user is a new DVC user who is facing challenges in merging a feature branch back to the main branch. They are unsure about the correct workflow and are concerned about managing the state of their main branch. The user has been following a process of switching to main, merging the feature branch to main, and then performing a dvc pull, which seems to be important to avoid uncommitted DVC tracked model files.",
        "Question_gpt_summary":"user new user face challeng merg featur branch main branch unsur correct workflow concern manag state main branch user follow process switch main merg featur branch main perform pull import avoid uncommit track model file",
        "Answer_original_content":"hei david waterworth think link worth check exampl experi persist comment workflow suggest try think dont need branch want end experi main branch new commit result run sweep run experi main run queue dont affect workspac exp appli exp branch materi experi commit main new branch code us context menu appli experi branch screen shot let know solv issu",
        "Answer_preprocessed_content":"hei think link worth check exampl experi persist comment workflow suggest try think dont need branch want end experi main branch run experi main run queue dont affect workspac materi experi commit new branch code us context menu appli experi branch screen shot let know solv issu",
        "Answer_gpt_summary_original":"Solutions provided:\n- The user doesn't need a branch for this. They can run experiments in the main branch and use `dvc exp apply` or `dvc exp branch` to materialize an experiment as a commit in the main branch or as a new branch.\n- In VS Code, the user can use the context menu to apply an experiment or do a branch.\n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"solut provid user need branch run experi main branch us exp appli exp branch materi experi commit main branch new branch code user us context menu appli experi branch person opinion bias includ respons"
    },
    {
        "Question_title":"Load Azure ML experiment run information from datastore",
        "Question_body":"<p>I have lots of run files created by running PyTorch estimator\/ ScriptRunStep experiments that are saved in azureml blob storage container. Previously, I'd been viewing these runs in the Experiments tab of the ml.azure.com portal and associating tags to these runs to categorise and load the desired models.<\/p>\n<p>However, a coworker recently deleted my workspace. I created a new one which is connected to the previously-existing blob container, the run files therefore still exist and can be accessed on this new workspace, but they no longer show up in the Experiment viewer on ml.azure.com. Neither can I see the tags I'd associated to the runs.<\/p>\n<p><strong>Is there any way to load these old run files into the Experiment viewer or is it only possible to view runs created inside the current workspace?<\/strong><\/p>\n<p>Sample scriptrunconfig code:<\/p>\n<pre><code>data_ref = DataReference(datastore=ds,\n                         data_reference_name=&quot;&lt;name&gt;&quot;,        \n                         path_on_datastore = &quot;&lt;path&gt;&quot;)\nargs = ['--data_dir',   str(data_ref),     \n        '--num_epochs', 30,     \n        '--lr',         0.01,          \n        '--classifier', 'int_ext' ]  \n\nsrc = ScriptRunConfig(source_directory='.',                       \n                      arguments=args,                      \n                      compute_target = compute_target,                       \n                      environment = env,                       \n                      script='train.py') \nsrc.run_config.data_references = {data_ref.data_reference_name: \n                                  data_ref.to_config()} \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1616682318310,
        "Question_favorite_count":null,
        "Question_last_edit_time":1617188538763,
        "Question_score":1.0,
        "Question_view_count":164.0,
        "Answer_body":"<p>Sorry for your loss! First, I'd make absolutely sure that you can't recover the deleted workspace. Definitely worthwhile to open an priority support ticket with Azure.<\/p>\n<p>Another thing you might try is:<\/p>\n<ol>\n<li>create a new workspace (which will create a new storage account for you for the new workspace's logs)<\/li>\n<li>copy your old workspace's data into the new workspace's storage account.<\/li>\n<\/ol>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66801546",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1616813814780,
        "Question_original_content":"load experi run inform datastor lot run file creat run pytorch estim scriptrunstep experi save blob storag contain previous view run experi tab azur com portal associ tag run categoris load desir model cowork recent delet workspac creat new connect previous exist blob contain run file exist access new workspac longer experi viewer azur com tag associ run wai load old run file experi viewer possibl view run creat insid current workspac sampl scriptrunconfig code data ref datarefer datastor data refer path datastor arg data dir str data ref num epoch classifi int ext src scriptrunconfig sourc directori argument arg comput target comput target environ env script train src run config data refer data ref data refer data ref config",
        "Question_preprocessed_content":"load experi run inform datastor lot run file creat run pytorch estim scriptrunstep experi save blob storag contain previous view run experi tab portal associ tag run categoris load desir model cowork recent delet workspac creat new connect blob contain run file exist access new workspac longer experi viewer tag associ run wai load old run file experi viewer possibl view run creat insid current workspac sampl scriptrunconfig code",
        "Question_gpt_summary_original":"The user has encountered a challenge in loading old run files created by PyTorch estimator\/ScriptRunStep experiments from Azure ML blob storage container into the Experiment viewer on ml.azure.com after their workspace was deleted by a coworker. The user is seeking a solution to load these old run files into the Experiment viewer or if it is only possible to view runs created inside the current workspace.",
        "Question_gpt_summary":"user encount challeng load old run file creat pytorch estim scriptrunstep experi blob storag contain experi viewer azur com workspac delet cowork user seek solut load old run file experi viewer possibl view run creat insid current workspac",
        "Answer_original_content":"sorri loss absolut sure recov delet workspac definit worthwhil open prioriti support ticket azur thing try creat new workspac creat new storag account new workspac log copi old workspac data new workspac storag account",
        "Answer_preprocessed_content":"sorri loss absolut sure recov delet workspac definit worthwhil open prioriti support ticket azur thing try creat new workspac copi old workspac data new workspac storag account",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are: \n- Trying to recover the deleted workspace by opening a priority support ticket with Azure.\n- Creating a new workspace and copying the old workspace's data into the new workspace's storage account.",
        "Answer_gpt_summary":"possibl solut mention discuss try recov delet workspac open prioriti support ticket azur creat new workspac copi old workspac data new workspac storag account"
    },
    {
        "Question_title":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Question_body":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590501108000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":130.0,
        "Answer_body":"Amazon SageMaker Model Monitor currently supports only endpoints that host a single model and does not support monitoring multi-model endpoints. For information on using multi-model endpoints, see Host Multiple Models with Multi-Model Endpoints . https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor.html",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq2z-BEt7TnmZ8vFYs-Hu7g\/does-sage-maker-multi-model-endpoint-support-sage-maker-model-monitor",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-05-26T13:58:28.000Z",
                "Answer_score":0,
                "Answer_body":"Amazon SageMaker Model Monitor currently supports only endpoints that host a single model and does not support monitoring multi-model endpoints. For information on using multi-model endpoints, see Host Multiple Models with Multi-Model Endpoints . https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor.html",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1590501508000,
        "Question_original_content":"multi model endpoint support model monitor multi model endpoint support model monitor",
        "Question_preprocessed_content":"endpoint support model monitor endpoint support model monitor",
        "Question_gpt_summary_original":"The user is seeking information on whether SageMaker Multi-Model Endpoint supports SageMaker Model Monitor. No specific challenges are mentioned in the given text.",
        "Question_gpt_summary":"user seek inform multi model endpoint support model monitor specif challeng mention given text",
        "Answer_original_content":"model monitor current support endpoint host singl model support monitor multi model endpoint inform multi model endpoint host multipl model multi model endpoint http doc aw amazon com latest model monitor html",
        "Answer_preprocessed_content":"model monitor current support endpoint host singl model support monitor endpoint inform endpoint host multipl model endpoint",
        "Answer_gpt_summary_original":"Solution: The discussion mentions that currently, SageMaker Model Monitor does not support monitoring multi-model endpoints. However, the user can refer to the documentation on Host Multiple Models with Multi-Model Endpoints for information on using multi-model endpoints. No other solutions are provided in the given text.",
        "Answer_gpt_summary":"solut discuss mention current model monitor support monitor multi model endpoint user refer document host multipl model multi model endpoint inform multi model endpoint solut provid given text"
    },
    {
        "Question_title":"Does Sagemaker pass any data other than the model itself between training and prediction steps?",
        "Question_body":"<p>I'm building a Scikit-learn model on Sagemaker.<\/p>\n\n<p>I'd like to reference the data used in training in my <code>predict_fn<\/code>. (Instead of the indices returned from NNS, I'd like to return the names and data of each neighbor.)<\/p>\n\n<p>I know this can be done by writing\/reading from S3, as in <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/<\/a> , but was wondering if there were more elegant solutions.<\/p>\n\n<p>Are there other ways to make the data used in the training job available to the prediction function?<\/p>\n\n<p>Edit: Using the advice from the accepted solution I was able to pass data as a dict.<\/p>\n\n<pre><code>model = nn.fit(train_data)\n\nmodel_dict = {\n   \"model\": model,\n   \"reference\": train_data\n}\n\njoblib.dump(model_dict, path)\n<\/code><\/pre>\n\n<p>predict_fn:<\/p>\n\n<pre><code>def predict_fn(input_data, model_dict):\n   model = model_dict[\"model\"]\n   reference = model_dict[\"reference\"]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1581546836597,
        "Question_favorite_count":null,
        "Question_last_edit_time":1582837925156,
        "Question_score":0.0,
        "Question_view_count":140.0,
        "Answer_body":"<p>you can bring to the endpoint instance (either in the <code>model.tar.gz<\/code> or via later download) a file storing the mapping between indexes and record names; this way you can translate from neighbor IDs to record names on the fly in the <code>predict_fn<\/code> or in the <code>output_fn<\/code>. For giant indexes this mapping (along with other metadata) can be in an external database too (eg dynamoDB, redis)<\/p>\n\n<p>the link you attach (SageMaker Batch Transform) is quite a different concept; it's for instantiating ephemeral fleet of machine(s) to run a one-time prediction task with input data in S3 and results written to s3. You question seem to refer to the alternative, permanent, real-time endpoint deployment mode.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60197897",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1581723595003,
        "Question_original_content":"pass data model train predict step build scikit learn model like refer data train predict instead indic return nn like return name data neighbor know write read http aw amazon com blog machin learn associ predict result input data amazon batch transform wonder eleg solut wai data train job avail predict function edit advic accept solut abl pass data dict model fit train data model dict model model refer train data joblib dump model dict path predict def predict input data model dict model model dict model refer model dict refer",
        "Question_preprocessed_content":"pass data model train predict step build model like refer data train know wonder eleg solut wai data train job avail predict function edit advic accept solut abl pass data dict",
        "Question_gpt_summary_original":"The user is building a Scikit-learn model on Sagemaker and wants to reference the data used in training in their predict function. They are looking for ways to make the data used in the training job available to the prediction function other than writing\/reading from S3. The user was able to pass data as a dictionary using the advice from the accepted solution.",
        "Question_gpt_summary":"user build scikit learn model want refer data train predict function look wai data train job avail predict function write read user abl pass data dictionari advic accept solut",
        "Answer_original_content":"bring endpoint instanc model tar later download file store map index record name wai translat neighbor id record name fly predict output giant index map metadata extern databas dynamodb redi link attach batch transform differ concept instanti ephemer fleet machin run time predict task input data result written question refer altern perman real time endpoint deploy mode",
        "Answer_preprocessed_content":"bring endpoint instanc file store map index record name wai translat neighbor id record name fly giant index map extern databas link attach differ concept instanti ephemer fleet machin run predict task input data result written question refer altern perman endpoint deploy mode",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to bring a file storing the mapping between indexes and record names to the endpoint instance, either in the model.tar.gz or via later download. This way, the user can translate from neighbor IDs to record names on the fly in the predict_fn or in the output_fn. For large indexes, this mapping (along with other metadata) can be stored in an external database such as DynamoDB or Redis. However, no solution was provided for making the data used in the training job available to the prediction function other than writing\/reading from S3.",
        "Answer_gpt_summary":"solut possibl solut mention discuss bring file store map index record name endpoint instanc model tar later download wai user translat neighbor id record name fly predict output larg index map metadata store extern databas dynamodb redi solut provid make data train job avail predict function write read"
    },
    {
        "Question_title":"How can I pass parameters to a Vertex AI Platform Pipeline?",
        "Question_body":"<p>I have created a Vertex AI pipeline similar to <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/google_cloud_pipeline_components_automl_images.ipynb\" rel=\"nofollow noreferrer\">this.<\/a><\/p>\n<p>Now the pipeline has reference to a csv file. So if this csv file changes the pipeline needs to be recreated.<\/p>\n<p>Is there any way to pass a new csv as a parameter to the pipeline when it is re-run? That is without recreating the pipeline using the notebook?<\/p>\n<p>If not, is there a best practice way of auto updating the dataset, model and deployment?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641570150640,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":611.0,
        "Answer_body":"<p>Have a look to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/build-pipeline\" rel=\"nofollow noreferrer\">that documentation<\/a>.<\/p>\n<p>You can define your pipeline like that<\/p>\n<pre><code>...\n# Define the workflow of the pipeline.\n@kfp.dsl.pipeline(\n    name=&quot;automl-image-training-v2&quot;,\n    pipeline_root=pipeline_root_path)\ndef pipeline(project_id: str):\n...\n<\/code><\/pre>\n<p>(you have something very similar in your notebook sample)<\/p>\n<p>Then, when you invoke your pipeline, you can pass some parameter<\/p>\n<pre><code>import google.cloud.aiplatform as aip\n\njob = aip.PipelineJob(\n    display_name=&quot;automl-image-training-v2&quot;,\n    template_path=&quot;image_classif_pipeline.json&quot;,\n    pipeline_root=pipeline_root_path,\n    parameter_values={\n        'project_id': project_id\n    }\n)\n\njob.submit()\n<\/code><\/pre>\n<p>You can see the <code>project_id<\/code> a dict parameter in the parameter values, and in parameter of your pipeline function.<\/p>\n<p>Do the same for your CSV file name!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70623713",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1641588471208,
        "Question_original_content":"pass paramet platform pipelin creat pipelin similar pipelin refer csv file csv file chang pipelin need recreat wai pass new csv paramet pipelin run recreat pipelin notebook best practic wai auto updat dataset model deploy",
        "Question_preprocessed_content":"pass paramet platform pipelin creat pipelin similar pipelin refer csv file csv file chang pipelin need recreat wai pass new csv paramet pipelin recreat pipelin notebook best practic wai auto updat dataset model deploy",
        "Question_gpt_summary_original":"The user is facing challenges in passing parameters to a Vertex AI Platform Pipeline. Specifically, they are looking for a way to pass a new csv file as a parameter to the pipeline when it is re-run without having to recreate the pipeline using the notebook. Additionally, they are seeking best practices for auto-updating the dataset, model, and deployment.",
        "Question_gpt_summary":"user face challeng pass paramet platform pipelin specif look wai pass new csv file paramet pipelin run have recreat pipelin notebook addition seek best practic auto updat dataset model deploy",
        "Answer_original_content":"look document defin pipelin like defin workflow pipelin kfp dsl pipelin automl imag train pipelin root pipelin root path def pipelin project str similar notebook sampl invok pipelin pass paramet import googl cloud aiplatform aip job aip pipelinejob displai automl imag train templat path imag classif pipelin json pipelin root pipelin root path paramet valu project project job submit project dict paramet paramet valu paramet pipelin function csv file",
        "Answer_preprocessed_content":"look document defin pipelin like similar notebook sampl invok pipelin pass paramet dict paramet paramet valu paramet pipelin function csv file",
        "Answer_gpt_summary_original":"Solution:\n- Define the pipeline workflow using the Vertex AI Platform Pipeline.\n- Pass the new CSV file as a parameter to the pipeline when it is re-run using the `parameter_values` parameter in the `aip.PipelineJob` function.\n- Define the CSV file name as a parameter in the pipeline function.",
        "Answer_gpt_summary":"solut defin pipelin workflow platform pipelin pass new csv file paramet pipelin run paramet valu paramet aip pipelinejob function defin csv file paramet pipelin function"
    },
    {
        "Question_title":"How to pass environment variables to gcloud beta ai custom-jobs create with custom container (Vertex AI)",
        "Question_body":"<p>I'm running custom training jobs in google's Vertex AI. A simple <code>gcloud<\/code> command to execute a custom job would use something like the following syntax (complete documentation for the command can be seen <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/beta\/ai\/custom-jobs\/create#--config\" rel=\"nofollow noreferrer\">here<\/a>):<\/p>\n<pre><code>gcloud beta ai custom-jobs create --region=us-central1 \\\n--display-name=test \\\n--config=config.yaml\n<\/code><\/pre>\n<p>In the <code>config.yaml<\/code> file, it is possible to specify the machine and accelerator (GPU) types, etc., and in my case, point to a custom container living in the Google Artifact Registry that executes the training code (specified in the <code>imageUri<\/code> part of the <code>containerSpec<\/code>). An example config file may look like this:<\/p>\n<pre><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n    acceleratorType: NVIDIA_TESLA_P100\n    acceleratorCount: 2\n  replicaCount: 1\n  containerSpec:\n    imageUri: {URI_FOR_CUSTOM_CONATINER}\n    args:\n    - {ARGS TO PASS TO CONTAINER ENTRYPOINT COMMAND}\n<\/code><\/pre>\n<p>The code we're running needs some runtime environment variables (that need to be secure) passed to the container. In the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">API documentation<\/a> for the <code>containerSpec<\/code>, it says it is possible to set environment variables as follows:<\/p>\n<pre><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n    acceleratorType: NVIDIA_TESLA_P100\n    acceleratorCount: 2\n  replicaCount: 1\n  containerSpec:\n    imageUri: {URI_FOR_CUSTOM_CONATINER}\n    args:\n    - {ARGS TO PASS TO CONTAINER ENTRYPOINT COMMAND}\n    env:\n    - name: SECRET_ONE\n      value: $SECRET_ONE\n    - name: SECRET_TWO\n      value: $SECRET_TWO\n<\/code><\/pre>\n<p>When I try and add the <code>env<\/code> flag to the <code>containerSpec<\/code>, I get an error saying it's not part of the container spec:<\/p>\n<pre><code>ERROR: (gcloud.beta.ai.custom-jobs.create) INVALID_ARGUMENT: Invalid JSON payload received. Unknown name &quot;env&quot; at 'custom_job.job_spec.worker_pool_specs[0].container_spec': Cannot find field.\n- '@type': type.googleapis.com\/google.rpc.BadRequest\n  fieldViolations:\n  - description: &quot;Invalid JSON payload received. Unknown name \\&quot;env\\&quot; at 'custom_job.job_spec.worker_pool_specs[0].container_spec':\\\n      \\ Cannot find field.&quot;\n    field: custom_job.job_spec.worker_pool_specs[0].container_spec\n<\/code><\/pre>\n<p>Any idea how to securely set runtime environment variables in Vertex AI custom jobs using custom containers?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1632409414673,
        "Question_favorite_count":null,
        "Question_last_edit_time":1632410531832,
        "Question_score":3.0,
        "Question_view_count":1033.0,
        "Answer_body":"<p>There are two versions of the REST API - \u201c<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">v1<\/a>\u201d and \u201c<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1beta1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">v1beta1<\/a>\u201d where &quot;v1beta1&quot; does not have the <code>env<\/code> option in <code>ContainerSpec<\/code> but &quot;v1&quot; does. The <code>gcloud ai custom-jobs create<\/code> command without the <code>beta<\/code> parameter doesn\u2019t throw the error as it uses version \u201cv1\u201d to make the API calls.<\/p>\n<p>The environment variables from the yaml file can be passed to the custom container in the following way:<\/p>\n<p>This is the docker file of the sample custom training application I used to test the requirement. Please refer to this <a href=\"https:\/\/codelabs.developers.google.com\/vertex_custom_training_prediction\" rel=\"nofollow noreferrer\">codelab<\/a> for more information about the training application.<\/p>\n<pre class=\"lang-docker prettyprint-override\"><code>FROM gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3\nWORKDIR \/root\n\nWORKDIR \/\n\n# Copies the trainer code to the docker image.\nCOPY trainer \/trainer\n\n\n# Copies the bash script to the docker image.\nCOPY commands.sh \/scripts\/commands.sh\n\n# Bash command to make the script file an executable\nRUN [&quot;chmod&quot;, &quot;+x&quot;, &quot;\/scripts\/commands.sh&quot;]\n\n\n# Command to execute the file\nENTRYPOINT [&quot;\/scripts\/commands.sh&quot;]\n\n# Sets up the entry point to invoke the trainer.\n# ENTRYPOINT &quot;python&quot; &quot;-m&quot; $SECRET_TWO \u21d2 To use the environment variable  \n# directly in the docker ENTRYPOINT. In case you are not using a bash script, \n# the trainer can be invoked directly from the docker ENTRYPOINT.\n<\/code><\/pre>\n<br \/>\n<p>Below is the <code>commands.sh<\/code> file used in the docker container to test whether the environment variables are passed to the container.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>#!\/bin\/bash\nmkdir \/root\/.ssh\necho $SECRET_ONE\npython -m $SECRET_TWO\n<\/code><\/pre>\n<br \/>\n<p>The example <code>config.yaml<\/code> file<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n  replicaCount: 1\n  containerSpec:\n    imageUri: gcr.io\/infosys-kabilan\/mpg:v1\n    env:\n    - name: SECRET_ONE\n      value: &quot;Passing the environment variables&quot;\n    - name: SECRET_TWO\n      value: &quot;trainer.train&quot;\n<\/code><\/pre>\n<p>As the next step, I built and pushed the container to Google Container Repository. Now, the <code>gcloud ai custom-jobs create --region=us-central1  --display-name=test --config=config.yaml<\/code> can be run to create the custom training job and the output of the <code>commands.sh<\/code> file can be seen in the job logs as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qRHV7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qRHV7.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69302528",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1632987814700,
        "Question_original_content":"pass environ variabl gcloud beta custom job creat custom contain run custom train job googl simpl gcloud command execut custom job us like follow syntax complet document command seen gcloud beta custom job creat region central displai test config config yaml config yaml file possibl specifi machin acceler gpu type case point custom contain live googl artifact registri execut train code specifi imageuri containerspec exampl config file look like config yaml workerpoolspec machinespec machinetyp highmem acceleratortyp nvidia tesla acceleratorcount replicacount containerspec imageuri uri custom conatin arg arg pass contain entrypoint command code run need runtim environ variabl need secur pass contain api document containerspec sai possibl set environ variabl follow config yaml workerpoolspec machinespec machinetyp highmem acceleratortyp nvidia tesla acceleratorcount replicacount containerspec imageuri uri custom conatin arg arg pass contain entrypoint command env secret valu secret secret valu secret try add env flag containerspec error sai contain spec error gcloud beta custom job creat invalid argument invalid json payload receiv unknown env custom job job spec worker pool spec contain spec field type type googleapi com googl rpc badrequest fieldviol descript invalid json payload receiv unknown env custom job job spec worker pool spec contain spec field field custom job job spec worker pool spec contain spec idea secur set runtim environ variabl custom job custom contain",
        "Question_preprocessed_content":"pass environ variabl gcloud beta creat custom contain run custom train job googl simpl command execut custom job us like follow syntax file possibl specifi machin acceler type case point custom contain live googl artifact registri execut train code exampl config file look like code run need runtim environ variabl pass contain api document sai possibl set environ variabl follow try add flag error sai contain spec idea secur set runtim environ variabl custom job custom contain",
        "Question_gpt_summary_original":"The user is facing a challenge in passing secure runtime environment variables to a custom container in Google's Vertex AI while running custom training jobs. The API documentation suggests using the \"env\" flag in the containerSpec, but the user is receiving an error message stating that the flag is not part of the container spec. The user is seeking a solution to securely set runtime environment variables in Vertex AI custom jobs using custom containers.",
        "Question_gpt_summary":"user face challeng pass secur runtim environ variabl custom contain googl run custom train job api document suggest env flag containerspec user receiv error messag state flag contain spec user seek solut secur set runtim environ variabl custom job custom contain",
        "Answer_original_content":"version rest api vbeta vbeta env option containerspec gcloud custom job creat command beta paramet doesnt throw error us version api call environ variabl yaml file pass custom contain follow wai docker file sampl custom train applic test requir refer codelab inform train applic gcr deeplearn platform releas cpu workdir root workdir copi trainer code docker imag copi trainer trainer copi bash script docker imag copi command script command bash command script file execut run chmod script command command execut file entrypoint script command set entri point invok trainer entrypoint python secret us environ variabl directli docker entrypoint case bash script trainer invok directli docker entrypoint command file docker contain test environ variabl pass contain bin bash mkdir root ssh echo secret python secret exampl config yaml file config yaml workerpoolspec machinespec machinetyp highmem replicacount containerspec imageuri gcr infosi kabilan mpg env secret valu pass environ variabl secret valu trainer train step built push contain googl contain repositori gcloud custom job creat region central displai test config config yaml run creat custom train job output command file seen job log shown",
        "Answer_preprocessed_content":"version rest api beta beta option command paramet doesnt throw error us version api call environ variabl yaml file pass custom contain follow wai docker file sampl custom train applic test requir refer codelab inform train applic file docker contain test environ variabl pass contain exampl file step built push contain googl contain repositori run creat custom train job output file seen job log shown",
        "Answer_gpt_summary_original":"Solution:\n- There are two versions of the REST API - \u201cv1\u201d and \u201cv1beta1\u201d where \"v1beta1\" does not have the `env` option in `ContainerSpec` but \"v1\" does. The `gcloud ai custom-jobs create` command without the `beta` parameter doesn\u2019t throw the error as it uses version \u201cv1\u201d to make the API calls.\n- The environment variables from the yaml file can be passed to the custom container by using the `env` flag in the `ContainerSpec` of version \"v1\" of the REST API.\n- An example `config.yaml` file and `commands.sh` file are provided in the discussion to demonstrate how to pass environment",
        "Answer_gpt_summary":"solut version rest api vbeta vbeta env option containerspec gcloud custom job creat command beta paramet doesnt throw error us version api call environ variabl yaml file pass custom contain env flag containerspec version rest api exampl config yaml file command file provid discuss demonstr pass environ"
    },
    {
        "Question_title":"Tracking resources used by VertexAI pipeline",
        "Question_body":"<p>Is it possible to track the resources consumed by a VertexAI pipeline run, similar to how it is possible to do for Dataflow where it shows a live graph of how many nodes are currently running to execute the pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628219232657,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":272.0,
        "Answer_body":"<p>Vertex AI Pipeline provides a feature for <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/visualize-pipeline\" rel=\"nofollow noreferrer\">Visualizing and analyzing<\/a> the pipeline results.<\/p>\n<p>This feature can be used to check the resource utilization once the Pipeline is deployed.<\/p>\n<p><strong>steps:<\/strong><\/p>\n<pre><code>Go to vertex AI pipeline-&gt;\n         Select a pipeline-&gt;\n               pipeline step-&gt;\n                     view job(from Pipeline run analysis pane)\n<\/code><\/pre>\n<p>In the View Job pane we can check for the resources utilized i.e machine types,machine count,CPU utilization graph for the pipeline step and we can view the logs too.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BTMXZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BTMXZ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Utilizations:<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8WQRc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8WQRc.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As per this <a href=\"https:\/\/cloud.google.com\/monitoring\/api\/metrics_gcp#gcp-aiplatform\" rel=\"nofollow noreferrer\">document<\/a>, metrics from the Vertex AI like CPU utilization, CPU load are in the <a href=\"https:\/\/cloud.google.com\/products\/#product-launch-stages\" rel=\"nofollow noreferrer\">Beta<\/a> launch stage. However, you can examine the metrics like CPU utilization from Cloud Monitoring by referring to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/monitoring-metrics\" rel=\"nofollow noreferrer\">document<\/a> and also find the below snap for more reference.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yyvnd.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yyvnd.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For changing the timeline of the graph you have to select the <strong>custom<\/strong> option in <strong>metrics explorer<\/strong> and provide the date and time for the duration that you want to view as shown in the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6ZUgE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6ZUgE.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1628692036932,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68675615",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1628260970907,
        "Question_original_content":"track resourc vertexai pipelin possibl track resourc consum vertexai pipelin run similar possibl dataflow show live graph node current run execut pipelin",
        "Question_preprocessed_content":"track resourc vertexai pipelin possibl track resourc consum vertexai pipelin run similar possibl dataflow show live graph node current run execut pipelin",
        "Question_gpt_summary_original":"The user is facing a challenge in tracking the resources used by a VertexAI pipeline run and is seeking a solution similar to the live graph feature available in Dataflow.",
        "Question_gpt_summary":"user face challeng track resourc vertexai pipelin run seek solut similar live graph featur avail dataflow",
        "Answer_original_content":"pipelin provid featur visual analyz pipelin result featur check resourc util pipelin deploi step pipelin select pipelin pipelin step view job pipelin run analysi pane view job pane check resourc util machin type machin count cpu util graph pipelin step view log util document metric like cpu util cpu load beta launch stage examin metric like cpu util cloud monitor refer document snap refer chang timelin graph select custom option metric explor provid date time durat want view shown screenshot",
        "Answer_preprocessed_content":"pipelin provid featur visual analyz pipelin result featur check resourc util pipelin deploi step view job pane check resourc util machin type machin count cpu util graph pipelin step view log util document metric like cpu util cpu load beta launch stage examin metric like cpu util cloud monitor refer document snap refer chang timelin graph select custom option metric explor provid date time durat want view shown screenshot",
        "Answer_gpt_summary_original":"Solution:\n- Vertex AI Pipeline provides a feature for visualizing and analyzing the pipeline results, which can be used to check the resource utilization once the Pipeline is deployed.\n- In the View Job pane, users can check for the resources utilized, such as machine types, machine count, and CPU utilization graph for the pipeline step, and view the logs too.\n- Metrics from the Vertex AI like CPU utilization, CPU load are in the Beta launch stage, but users can examine the metrics like CPU utilization from Cloud Monitoring by referring to the provided document.\n- Users can change the timeline of the graph by selecting the custom option in metrics explorer and providing the date and time for the duration that they want to view.",
        "Answer_gpt_summary":"solut pipelin provid featur visual analyz pipelin result check resourc util pipelin deploi view job pane user check resourc util machin type machin count cpu util graph pipelin step view log metric like cpu util cpu load beta launch stage user examin metric like cpu util cloud monitor refer provid document user chang timelin graph select custom option metric explor provid date time durat want view"
    },
    {
        "Question_title":"Tensorflow error. TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn",
        "Question_body":"<p>I am trying to run this on Amazon Sagemaker but I am getting this error while when I try to run it on my local machine, it works very fine.<\/p>\n<p>this is my code:<\/p>\n<pre><code>import tensorflow as tf\n\nimport IPython.display as display\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12,12)\nmpl.rcParams['axes.grid'] = False\n\nimport numpy as np\nimport PIL.Image\nimport time\nimport functools\n    \ndef tensor_to_image(tensor):\n  tensor = tensor*255\n  tensor = np.array(tensor, dtype=np.uint8)\n  if np.ndim(tensor)&gt;3:\n    assert tensor.shape[0] == 1\n    tensor = tensor[0]\n  return PIL.Image.fromarray(tensor)\n\ncontent_path = tf.keras.utils.get_file('YellowLabradorLooking_nw4.jpg', 'https:\/\/example.com\/IMG_20200216_163015.jpg')\n\n\nstyle_path = tf.keras.utils.get_file('kandinsky3.jpg','https:\/\/example.com\/download+(2).png')\n\n\ndef load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n\n\ndef imshow(image, title=None):\n  if len(image.shape) &gt; 3:\n    image = tf.squeeze(image, axis=0)\n\n  plt.imshow(image)\n  if title:\n    plt.title(title)\n\n\ncontent_image = load_img(content_path)\nstyle_image = load_img(style_path)\n\nplt.subplot(1, 2, 1)\nimshow(content_image, 'Content Image')\n\nplt.subplot(1, 2, 2)\nimshow(style_image, 'Style Image')\n\nimport tensorflow_hub as hub\nhub_module = hub.load('https:\/\/tfhub.dev\/google\/magenta\/arbitrary-image-stylization-v1-256\/1')\nstylized_image = hub_module(tf.constant(content_image), tf.constant(style_image))[0]\ntensor_to_image(stylized_image)\n\n\nfile_name = 'stylized-image5.png'\ntensor_to_image(stylized_image).save(file_name)\n<\/code><\/pre>\n<p>This is the exact error I get:<\/p>\n<pre><code>---------------------------------------------------------------------------\n<\/code><\/pre>\n<p>TypeError                                 Traceback (most recent call last)<\/p>\n<pre><code>&lt;ipython-input-24-c47a4db4880c&gt; in &lt;module&gt;()\n     53 \n     54 \n---&gt; 55 content_image = load_img(content_path)\n     56 style_image = load_img(style_path)\n     57 \n<\/code><\/pre>\n<p> in load_img(path_to_img)<\/p>\n<pre><code>     34 \n     35     shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n---&gt; 36     long_dim = max(shape)\n     37     scale = max_dim \/ long_dim\n     38 \n<\/code><\/pre>\n<p>~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/tensorflow\/python\/framework\/ops.py in <strong>iter<\/strong>(self)<\/p>\n<pre><code>    475     if not context.executing_eagerly():\n    476       raise TypeError(\n--&gt; 477           &quot;Tensor objects are only iterable when eager execution is &quot;\n    478           &quot;enabled. To iterate over this tensor use tf.map_fn.&quot;)\n    479     shape = self._shape_tuple()\n<\/code><\/pre>\n<p>TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1594076057097,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1530.0,
        "Answer_body":"<p>Your error is being raised in this function <code>load_img<\/code>:<\/p>\n<pre><code>def load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n<\/code><\/pre>\n<p>Specifically, this line:<\/p>\n<pre><code>    long_dim = max(shape)\n<\/code><\/pre>\n<p>You are passing a tensor to the <a href=\"https:\/\/docs.python.org\/3\/library\/functions.html#max\" rel=\"nofollow noreferrer\">built-in Python max function<\/a> in graph execution mode. You can only iterate through tensors in eager-execution mode. You probably want to use <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/math\/reduce_max\" rel=\"nofollow noreferrer\">tf.reduce_max<\/a> instead:<\/p>\n<pre><code>    long_dim = tf.reduce_max(shape)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1594159994640,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62765658",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1594129639003,
        "Question_original_content":"tensorflow error typeerror tensor object iter eager execut enabl iter tensor us map try run get error try run local machin work fine code import tensorflow import ipython displai displai import matplotlib pyplot plt import matplotlib mpl mpl rcparam figur figsiz mpl rcparam ax grid fals import numpi import pil imag import time import functool def tensor imag tensor tensor tensor tensor arrai tensor dtype uint ndim tensor assert tensor shape tensor tensor return pil imag fromarrai tensor content path kera util file yellowlabradorlook jpg http exampl com img jpg style path kera util file kandinski jpg http exampl com download png def load img path img max dim img read file path img img imag decod imag img channel img imag convert imag dtype img float shape cast shape img float long dim max shape scale max dim long dim new shape cast shape scale int img imag resiz img new shape img img newaxi return img def imshow imag titl len imag shape imag squeez imag axi plt imshow imag titl plt titl titl content imag load img content path style imag load img style path plt subplot imshow content imag content imag plt subplot imshow style imag style imag import tensorflow hub hub hub modul hub load http tfhub dev googl magenta arbitrari imag styliz styliz imag hub modul constant content imag constant style imag tensor imag styliz imag file styliz imag png tensor imag styliz imag save file exact error typeerror traceback recent content imag load img content path style imag load img style path load img path img shape cast shape img float long dim max shape scale max dim long dim anaconda env amazonei tensorflow lib python site packag tensorflow python framework op iter self context execut eagerli rais typeerror tensor object iter eager execut enabl iter tensor us map shape self shape tupl typeerror tensor object iter eager execut enabl iter tensor us map",
        "Question_preprocessed_content":"tensorflow error typeerror tensor object iter eager execut enabl iter tensor us try run get error try run local machin work fine code exact error typeerror traceback iter typeerror tensor object iter eager execut enabl iter tensor us",
        "Question_gpt_summary_original":"The user is encountering a Tensorflow error while trying to run a code on Amazon Sagemaker. The error message states that Tensor objects are only iterable when eager execution is enabled and suggests using tf.map_fn to iterate over the tensor. The code works fine on the user's local machine.",
        "Question_gpt_summary":"user encount tensorflow error try run code error messag state tensor object iter eager execut enabl suggest map iter tensor code work fine user local machin",
        "Answer_original_content":"error rais function load img def load img path img max dim img read file path img img imag decod imag img channel img imag convert imag dtype img float shape cast shape img float long dim max shape scale max dim long dim new shape cast shape scale int img imag resiz img new shape img img newaxi return img specif line long dim max shape pass tensor built python max function graph execut mode iter tensor eager execut mode probabl want us reduc max instead long dim reduc max shape",
        "Answer_preprocessed_content":"error rais function specif line pass tensor python max function graph execut mode iter tensor mode probabl want us instead",
        "Answer_gpt_summary_original":"Solution:\n- Replace the line `long_dim = max(shape)` with `long_dim = tf.reduce_max(shape)` to avoid passing a tensor to the built-in Python max function in graph execution mode.",
        "Answer_gpt_summary":"solut replac line long dim max shape long dim reduc max shape avoid pass tensor built python max function graph execut mode"
    },
    {
        "Question_title":"Share notebooks across Azure Machine learning service notebook VM",
        "Question_body":"<p>Is there any way to share a Azure notebook across multiple users who use different notebook VMs? It seems the VMs itself is not shareable across users. <\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1560331782567,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":1224.0,
        "Answer_body":"<p>Notebook VMs has own Jupyter environment and we don't need to use notebooks.azure.com. The former can be used in enterprise scenarios within the team to share the resources, and the latter is open, similar to google colab. When each user login to his notebook VM, there is a top level folder with his\/her alias and under that all notebooks are stored. this is stored in an Azure storage and each user's notebook VM will mount same storage. Hence If I want to view other person \\'s notebook, I need to navigate to his alias in the Jupyter nb in my nbvm<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56558892",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1562565689110,
        "Question_original_content":"share notebook servic notebook wai share azur notebook multipl user us differ notebook vm vm shareabl user",
        "Question_preprocessed_content":"share notebook servic notebook wai share azur notebook multipl user us differ notebook vm vm shareabl user",
        "Question_gpt_summary_original":"The user is facing a challenge in sharing an Azure notebook across multiple users who use different notebook VMs, as the VMs are not shareable across users.",
        "Question_gpt_summary":"user face challeng share azur notebook multipl user us differ notebook vm vm shareabl user",
        "Answer_original_content":"notebook vm jupyt environ need us notebook azur com enterpris scenario team share resourc open similar googl colab user login notebook level folder alia notebook store store azur storag user notebook mount storag want view person notebook need navig alia jupyt nbvm",
        "Answer_preprocessed_content":"notebook vm jupyt environ need us enterpris scenario team share resourc open similar googl colab user login notebook level folder alia notebook store store azur storag user notebook mount storag want view person notebook need navig alia jupyt nbvm",
        "Answer_gpt_summary_original":"Solution: The user can use Notebook VMs instead of notebooks.azure.com to share resources within the team. Each user can log in to their own notebook VM, which has a top-level folder with their alias, and all notebooks are stored under that folder. This is stored in an Azure storage, and each user's notebook VM will mount the same storage. To view another person's notebook, the user needs to navigate to their alias in the Jupyter nb in their own notebook VM.",
        "Answer_gpt_summary":"solut user us notebook vm instead notebook azur com share resourc team user log notebook level folder alia notebook store folder store azur storag user notebook mount storag view person notebook user need navig alia jupyt notebook"
    },
    {
        "Question_title":"Dataset + Preprocessed Text : Parameter \"Stopwords columns\" value should be less than or equal to parameter \"1\" value. . ( Error 0007 )",
        "Question_body":"I encounter the following error :\n\nParameter \"Stopwords columns\" value should be less than or equal to parameter \"1\" value. . ( Error 0007 )\nwhen building a simple pipeline :\n\nwith a .csv Dataset followed by a \"Preprocessed Text\".\n\nNo parameter 'Stopwords columns' is available in the \"Preprocessed Text\" properties !!!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1612874807950,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Solved.\n\nThere must be only one connection (left: Dataset) and not 2 connections (left : Dataset + right : Stopwords) from the \"Dataset\" to the \"Preprocessed Text\"",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/265397\/dataset-preprocessed-test-parameter-34stopwords-co.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-09T13:42:08.547Z",
                "Answer_score":0,
                "Answer_body":"Solved.\n\nThere must be only one connection (left: Dataset) and not 2 connections (left : Dataset + right : Stopwords) from the \"Dataset\" to the \"Preprocessed Text\"",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1612878128547,
        "Question_original_content":"dataset preprocess text paramet stopword column valu equal paramet valu error encount follow error paramet stopword column valu equal paramet valu error build simpl pipelin csv dataset follow preprocess text paramet stopword column avail preprocess text properti",
        "Question_preprocessed_content":"dataset preprocess text paramet stopword column valu equal paramet valu encount follow error paramet stopword column valu equal paramet valu build simpl pipelin csv dataset follow preprocess text paramet stopword column avail preprocess text properti",
        "Question_gpt_summary_original":"The user encountered an error (Error 0007) while building a pipeline with a .csv dataset and \"Preprocessed Text\". The error message stated that the value of the \"Stopwords columns\" parameter should be less than or equal to the value of the \"1\" parameter. However, the user could not find the \"Stopwords columns\" parameter in the \"Preprocessed Text\" properties.",
        "Question_gpt_summary":"user encount error error build pipelin csv dataset preprocess text error messag state valu stopword column paramet equal valu paramet user stopword column paramet preprocess text properti",
        "Answer_original_content":"solv connect left dataset connect left dataset right stopword dataset preprocess text",
        "Answer_preprocessed_content":"solv connect connect dataset preprocess text",
        "Answer_gpt_summary_original":"Solution: The user was able to solve the issue by ensuring that there was only one connection from the \"Dataset\" to the \"Preprocessed Text\" and not two connections (one from the \"Dataset\" and one from the \"Stopwords\").",
        "Answer_gpt_summary":"solut user abl solv issu ensur connect dataset preprocess text connect dataset stopword"
    },
    {
        "Question_title":"How to make parameters available to SageMaker Tensorflow Endpoint",
        "Question_body":"<p>I'm looking to make some hyper parameters available to the serving endpoint in SageMaker. The training instances is given access to input parameters using hyperparameters in:<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='autocat.py',\n                       role=role,\n                       output_path=params['output_path'],\n                       code_location=params['code_location'],\n                       train_instance_count=1,\n                       train_instance_type='ml.c4.xlarge',\n                       training_steps=10000,\n                       evaluation_steps=None,\n                       hyperparameters=params)\n<\/code><\/pre>\n\n<p>However, when the endpoint is deployed, there is no way to pass in parameters that are used to control the data processing in the <code>input_fn(serialized_input, content_type)<\/code> function.<\/p>\n\n<p>What would be the best way to pass parameters to the serving instance?? Is the <code>source_dir<\/code> parameter defined in the <code>sagemaker.tensorflow.TensorFlow<\/code> class copied to the serving instance? If so, I could use a config.yml or similar.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1521754623920,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1521754933880,
        "Question_score":1.0,
        "Question_view_count":1426.0,
        "Answer_body":"<p>Ah i have had a similar problem to you where I needed to download something off S3 to use in the input_fn for inference. In my case it was a dictionary.<\/p>\n\n<p>Three options:<\/p>\n\n<ol>\n<li>use your config.yml approach, and download and import the s3 file from within your entrypoint file before any function declarations. This would make it available to the input_fn <\/li>\n<li>Keep using the hyperparameter approach, download and import the vectorizer in <code>serving_input_fn<\/code> and make it available via a global variable so that <code>input_fn<\/code> has access to it.<\/li>\n<li>Download the file from s3 before training and include it in the source_dir directly.<\/li>\n<\/ol>\n\n<p>Option 3 would only work if you didnt need to make changes to the vectorizer seperately after initial training.<\/p>\n\n<p>Whatever you do, don't download the file directly in input_fn. I made that mistake and the performance is terrible as each invoking of the endpoint would result in the s3 file being downloaded.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49438903",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1523591814356,
        "Question_original_content":"paramet avail tensorflow endpoint look hyper paramet avail serv endpoint train instanc given access input paramet hyperparamet estim tensorflow entri point autocat role role output path param output path code locat param code locat train instanc count train instanc type xlarg train step evalu step hyperparamet param endpoint deploi wai pass paramet control data process input serial input content type function best wai pass paramet serv instanc sourc dir paramet defin tensorflow tensorflow class copi serv instanc us config yml similar",
        "Question_preprocessed_content":"paramet avail tensorflow endpoint look hyper paramet avail serv endpoint train instanc given access input paramet hyperparamet endpoint deploi wai pass paramet control data process function best wai pass paramet serv instanc paramet defin class copi serv instanc us similar",
        "Question_gpt_summary_original":"The user is facing a challenge in making hyperparameters available to the serving endpoint in SageMaker Tensorflow. While the training instances have access to input parameters using hyperparameters, there is no way to pass in parameters to control data processing in the input function when the endpoint is deployed. The user is seeking advice on the best way to pass parameters to the serving instance.",
        "Question_gpt_summary":"user face challeng make hyperparamet avail serv endpoint tensorflow train instanc access input paramet hyperparamet wai pass paramet control data process input function endpoint deploi user seek advic best wai pass paramet serv instanc",
        "Answer_original_content":"similar problem need download us input infer case dictionari option us config yml approach download import file entrypoint file function declar avail input hyperparamet approach download import vector serv input avail global variabl input access download file train includ sourc dir directli option work didnt need chang vector seper initi train download file directli input mistak perform terribl invok endpoint result file download",
        "Answer_preprocessed_content":"similar problem need download us infer case dictionari option us approach download import file entrypoint file function declar avail hyperparamet approach download import vector avail global variabl access download file train includ directli option work didnt need chang vector seper initi train download file directli mistak perform terribl invok endpoint result file download",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Use the config.yml approach, download and import the S3 file from within the entrypoint file before any function declarations. This would make it available to the input_fn.\n2. Keep using the hyperparameter approach, download and import the vectorizer in serving_input_fn and make it available via a global variable so that input_fn has access to it.\n3. Download the file from S3 before training and include it in the source_dir directly.\n\nIt is also mentioned that option 3 would only work if the vectorizer does not need to be changed separately after initial training. Additionally, it is advised not to download the file directly in input_fn as it would result in poor performance.",
        "Answer_gpt_summary":"possibl solut mention discuss us config yml approach download import file entrypoint file function declar avail input hyperparamet approach download import vector serv input avail global variabl input access download file train includ sourc dir directli mention option work vector need chang separ initi train addition advis download file directli input result poor perform"
    },
    {
        "Question_title":"Import ML Model from ADLS to Azure ML using Databricks",
        "Question_body":"Hi,\nI have stored some ml model in my ADLS and I want to register the model to Azure ML using databricks.\nTried to use the following codes to register my ml model but keep encountering an error that the path cannot be found.\n\nimport urllib.request\nfrom azureml.core.model import Model\n\nRegister a model\n\n\n\nmodel = Model.register(model_path = 'dbfs:\/mnt\/machinelearning\/classifier.joblib',\nmodel_name = \"pretrained-classifier\",\ndescription = \"Pretrained Classifier\",\nworkspace=ws)",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1642414997297,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@Yuzu-9670 Using the databricks file path for registering a model is not supported. When using the model.register() you need to download the model locally and then use the path of the model or the folder in which the model is present to register the same.\n\n\n\n\nmodel_path\n\n\nThe path on the local file system where the model assets are located. This can be a direct pointer to a single file or folder. If pointing to a folder, the child_paths parameter can be used to specify individual files to bundle together as the Model object, as opposed to using the entire contents of the folder.\n\nThis sample notebook should help you with using the method.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/697789\/import-ml-model-from-adls-to-azure-ml-using-databr.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-17T16:51:50.977Z",
                "Answer_score":1,
                "Answer_body":"@Yuzu-9670 Using the databricks file path for registering a model is not supported. When using the model.register() you need to download the model locally and then use the path of the model or the folder in which the model is present to register the same.\n\n\n\n\nmodel_path\n\n\nThe path on the local file system where the model assets are located. This can be a direct pointer to a single file or folder. If pointing to a folder, the child_paths parameter can be used to specify individual files to bundle together as the Model object, as opposed to using the entire contents of the folder.\n\nThis sample notebook should help you with using the method.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-01-18T06:21:46.863Z",
                "Answer_score":0,
                "Answer_body":"Hi @romungi-MSFT,\nThank you for your comment!\nI have shifted my ml model to a repo folder and it works now.\nThank you!",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1642438310976,
        "Question_original_content":"import model adl databrick store model adl want regist model databrick tri us follow code regist model encount error path import urllib request core model import model regist model model model regist model path dbf mnt machinelearn classifi joblib model pretrain classifi descript pretrain classifi workspac",
        "Question_preprocessed_content":"import model adl databrick store model adl want regist model databrick tri us follow code regist model encount error path import import model regist model model descript pretrain classifi workspac",
        "Question_gpt_summary_original":"The user is facing challenges in registering an ML model from ADLS to Azure ML using Databricks. Despite using the provided codes, the user keeps encountering an error that the path cannot be found.",
        "Question_gpt_summary":"user face challeng regist model adl databrick despit provid code user keep encount error path",
        "Answer_original_content":"yuzu databrick file path regist model support model regist need download model local us path model folder model present regist model path path local file model asset locat direct pointer singl file folder point folder child path paramet specifi individu file bundl model object oppos entir content folder sampl notebook help method answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"databrick file path regist model support need download model local us path model folder model present regist path local file model asset locat direct pointer singl file folder point folder paramet specifi individu file bundl model object oppos entir content folder sampl notebook help method answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"Solution: The user should download the ML model locally and use the path of the model or the folder in which the model is present to register it using the model.register() method. Using the databricks file path for registering a model is not supported. The model_path parameter can be used to specify the path on the local file system where the model assets are located. If pointing to a folder, the child_paths parameter can be used to specify individual files to bundle together as the Model object. A sample notebook is provided to help with using the method. No personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"solut user download model local us path model folder model present regist model regist method databrick file path regist model support model path paramet specifi path local file model asset locat point folder child path paramet specifi individu file bundl model object sampl notebook provid help method person opinion bias includ summari"
    },
    {
        "Question_title":"Why does Azure ML Studio (classic) take additional time to execute Python Scripts?",
        "Question_body":"<p>I have been working with ML Studio (classic) and facing a problem with &quot;Execute Python&quot; scripts. I have noticed that it takes additional time to perform some internal tasks after which it starts executing the actual Python code in ML Studio. This delay has caused an increased time of 40-60 seconds per module which is aggregating and causing a delay of 400-500 seconds per execution when consumed through Batch Execution System or on running the experiments manually. (I've multiple Modules of &quot;Execute Python&quot; scripts)<\/p>\n<p>For instance - If I run a code in my local system, suppose it takes 2-3 seconds. The same would consume 50-60 seconds in Azure ML Studio.<\/p>\n<p>Can you please help understand the reason behind this or any optimization that can be done?<\/p>\n<p>Regards,\nAnant<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593694819477,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":166.0,
        "Answer_body":"<p>The known limitations of Machine Learning Studio (classic) are:<\/p>\n<p>The Python runtime is sandboxed and does not allow access to the network or to the local file system in a persistent manner.<\/p>\n<p>All files saved locally are isolated and deleted once the module finishes. The Python code cannot access most directories on the machine it runs on, the exception being the current directory and its subdirectories.<\/p>\n<p>When you provide a zipped file as a resource, the files are copied from your workspace to the experiment execution space, unpacked, and then used. Copying and unpacking resources can consume memory.<\/p>\n<p>The module can output a single data frame. It's not possible to return arbitrary Python objects such as trained models directly back to the Studio (classic) runtime. However, you can write objects to storage or to the workspace. Another option is to use pickle to serialize multiple objects into a byte array and then return the array inside a data frame.<\/p>\n<p>Hope this helps!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62696966",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1593695267950,
        "Question_original_content":"studio classic addit time execut python script work studio classic face problem execut python script notic take addit time perform intern task start execut actual python code studio delai caus increas time second modul aggreg caus delai second execut consum batch execut run experi manual multipl modul execut python script instanc run code local suppos take second consum second studio help understand reason optim regard anant",
        "Question_preprocessed_content":"studio addit time execut python script work studio face problem execut python script notic take addit time perform intern task start execut actual python code studio delai caus increas time second modul aggreg caus delai second execut consum batch execut run experi manual instanc run code local suppos take second consum second studio help understand reason optim regard anant",
        "Question_gpt_summary_original":"The user is facing a challenge with Azure ML Studio (classic) where it takes additional time to execute Python scripts due to internal tasks, causing a delay of 40-60 seconds per module and 400-500 seconds per execution. The user is seeking help to understand the reason behind this delay and any optimization that can be done.",
        "Question_gpt_summary":"user face challeng studio classic take addit time execut python script intern task caus delai second modul second execut user seek help understand reason delai optim",
        "Answer_original_content":"known limit machin learn studio classic python runtim sandbox allow access network local file persist manner file save local isol delet modul finish python code access directori machin run except current directori subdirectori provid zip file resourc file copi workspac experi execut space unpack copi unpack resourc consum memori modul output singl data frame possibl return arbitrari python object train model directli studio classic runtim write object storag workspac option us pickl serial multipl object byte arrai return arrai insid data frame hope help",
        "Answer_preprocessed_content":"known limit machin learn studio python runtim sandbox allow access network local file persist manner file save local isol delet modul finish python code access directori machin run except current directori subdirectori provid zip file resourc file copi workspac experi execut space unpack copi unpack resourc consum memori modul output singl data frame possibl return arbitrari python object train model directli studio runtim write object storag workspac option us pickl serial multipl object byte arrai return arrai insid data frame hope help",
        "Answer_gpt_summary_original":"No solutions are provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"How to execute multiple rows in web service Azure Machine Learning Studio",
        "Question_body":"<p>I create a model in Azure ML studio. \nI deployed the web service.<\/p>\n\n<p>Now, I know how to check one record at a time, but how can I load a csv file and made the algorithm go through all records ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/1tHuM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1tHuM.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If I click on Batch Execution - it will ask me to create an account for Azure storage. <\/p>\n\n<p>Is any way to execute multiple records from csv file without creating any other accounts?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/90zP7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/90zP7.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1515448065933,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":204.0,
        "Answer_body":"<p>Yes, there is a way and it is simple. What you need is an excel add-in. You need not create any other account.<\/p>\n\n<p>You can either read <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/excel-add-in-for-web-services\" rel=\"nofollow noreferrer\">Excel Add-in for Azure Machine Learning web services doc<\/a> or you can watch <a href=\"https:\/\/www.youtube.com\/watch?v=ju1CzDjiOMQ\" rel=\"nofollow noreferrer\">Azure ML Excel Add-in video<\/a>. <\/p>\n\n<p>If you search for <a href=\"https:\/\/www.google.co.in\/search?q=excel%20add%20in%20for%20azure%20ml&amp;client=firefox-b-ab&amp;dcr=0&amp;source=lnms&amp;tbm=vid&amp;sa=X&amp;ved=0ahUKEwinqP3a_67ZAhXBr48KHdiYAXUQ_AUICigB&amp;biw=1280&amp;bih=616\" rel=\"nofollow noreferrer\">videos on excel add in for azure ml<\/a>, you get other useful videos too. <\/p>\n\n<p>I hope this is the solution you are looking for.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48158545",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1518941429767,
        "Question_original_content":"execut multipl row web servic studio creat model studio deploi web servic know check record time load csv file algorithm record click batch execut ask creat account azur storag wai execut multipl record csv file creat account",
        "Question_preprocessed_content":"execut multipl row web servic studio creat model studio deploi web servic know check record time load csv file algorithm record click batch execut ask creat account azur storag wai execut multipl record csv file creat account",
        "Question_gpt_summary_original":"The user is facing challenges in executing multiple rows in a web service created in Azure Machine Learning Studio. They are able to check one record at a time but are unsure how to load a CSV file and make the algorithm go through all records. Clicking on Batch Execution prompts the user to create an account for Azure storage, and they are looking for a way to execute multiple records from a CSV file without creating any other accounts.",
        "Question_gpt_summary":"user face challeng execut multipl row web servic creat studio abl check record time unsur load csv file algorithm record click batch execut prompt user creat account azur storag look wai execut multipl record csv file creat account",
        "Answer_original_content":"ye wai simpl need excel add need creat account read excel add web servic doc watch excel add video search video excel add us video hope solut look",
        "Answer_preprocessed_content":"ye wai simpl need excel need creat account read excel web servic doc watch excel video search video excel add us video hope solut look",
        "Answer_gpt_summary_original":"Solution: The user can use an Excel add-in to execute multiple records from a CSV file without creating any other accounts. They can refer to the documentation or watch a video tutorial on how to use the add-in.",
        "Answer_gpt_summary":"solut user us excel add execut multipl record csv file creat account refer document watch video tutori us add"
    },
    {
        "Question_title":"How to retrieve the model signature from the MLflow Model Registry",
        "Question_body":"<p>I have registered a scikit learn model on my MLflow Tracking server, and I am loading it with <code>sklearn.load_model(model_uri)<\/code>.<\/p>\n<p>Now, I would like to access the signature of the model so I can get a list of the model's required inputs\/features so I can retrieve them from my feature store by name. I can't seem to find any utility or method in the <code>mlflow<\/code> API or the <code>MLFlowClient<\/code> API that will let me access a signature or inputs\/outputs attribute, even though I can see a list of inputs and outputs under each version of the model in the UI.<\/p>\n<p>I know that I can find the input sample and the model configuration in the model's artifacts, but that would require me actually downloading the artifacts and loading them manually in my script. I don't need to avoid that, but I am surprised that I can't just return the signature as a dictionary the same way I can return a run's parameters or metrics.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643655088920,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":904.0,
        "Answer_body":"<p>The way to access the model's signature without downloading the MLModel file is under the loaded model. And then you'll access the model's attributes, such as its signature or even other Pyfunc-defined methods.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmodel = mlflow.pyfunc.load_model(&quot;runs:\/&lt;run_id&gt;\/model&quot;)\nprint(model._model_meta._signature)\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70931309",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1645469817663,
        "Question_original_content":"retriev model signatur model registri regist scikit learn model track server load sklearn load model model uri like access signatur model list model requir input featur retriev featur store util method api client api let access signatur input output attribut list input output version model know input sampl model configur model artifact requir actual download artifact load manual script need avoid surpris return signatur dictionari wai return run paramet metric",
        "Question_preprocessed_content":"retriev model signatur model registri regist scikit learn model track server load like access signatur model list model requir retriev featur store util method api api let access signatur attribut list input output version model know input sampl model configur model artifact requir actual download artifact load manual script need avoid surpris return signatur dictionari wai return run paramet metric",
        "Question_gpt_summary_original":"The user is facing a challenge in retrieving the model signature from the MLflow Model Registry. They have registered a scikit learn model on their MLflow Tracking server and loaded it, but they cannot find any utility or method in the mlflow API or the MLFlowClient API that will let them access a signature or inputs\/outputs attribute. Although they can see a list of inputs and outputs under each version of the model in the UI, they need to access the signature to get a list of the model's required inputs\/features so they can retrieve them from their feature store by name. The user knows that they can find the input sample and the model configuration in the model's artifacts, but that would require them to download the artifacts and load them manually in their script.",
        "Question_gpt_summary":"user face challeng retriev model signatur model registri regist scikit learn model track server load util method api client api let access signatur input output attribut list input output version model need access signatur list model requir input featur retriev featur store user know input sampl model configur model artifact requir download artifact load manual script",
        "Answer_original_content":"wai access model signatur download mlmodel file load model access model attribut signatur pyfunc defin method import model pyfunc load model run model print model model meta signatur",
        "Answer_preprocessed_content":"wai access model signatur download mlmodel file load model access model attribut signatur method",
        "Answer_gpt_summary_original":"Solution: The user can access the model's signature by loading the model using the mlflow.pyfunc.load_model() method and then accessing the model's attributes, such as its signature or other Pyfunc-defined methods. They can print the model's signature using the following code: \n\n```\nimport mlflow\n\nmodel = mlflow.pyfunc.load_model(\"runs:\/<run_id>\/model\")\nprint(model._model_meta._signature)\n```\n\nNo other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut user access model signatur load model pyfunc load model method access model attribut signatur pyfunc defin method print model signatur follow code import model pyfunc load model run model print model model meta signatur solut mention discuss"
    },
    {
        "Question_title":"How to get the version of gremlin python client on AWS SageMaker",
        "Question_body":"<p>What is the command to check the version of Gremlin Python client running on a AWS Sagemaker jupyter notebook? I would like to run the command on the jupyter notebook cell.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610823303577,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":198.0,
        "Answer_body":"<p>From a notebook cell you should be able to just ask Pip which version is being used<\/p>\n<pre><code>!pip list | grep gremlinpython\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65753455",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1610839775480,
        "Question_original_content":"version gremlin python client command check version gremlin python client run jupyt notebook like run command jupyt notebook cell",
        "Question_preprocessed_content":"version gremlin python client command check version gremlin python client run jupyt notebook like run command jupyt notebook cell",
        "Question_gpt_summary_original":"The user is seeking assistance in finding the command to check the version of Gremlin Python client running on an AWS Sagemaker Jupyter notebook.",
        "Question_gpt_summary":"user seek assist find command check version gremlin python client run jupyt notebook",
        "Answer_original_content":"notebook cell abl ask pip version pip list grep gremlinpython",
        "Answer_preprocessed_content":"notebook cell abl ask pip version",
        "Answer_gpt_summary_original":"Solution: The solution provided in the discussion is to use the command \"!pip list | grep gremlinpython\" in a notebook cell to check the version of Gremlin Python client running on an AWS Sagemaker Jupyter notebook.",
        "Answer_gpt_summary":"solut solut provid discuss us command pip list grep gremlinpython notebook cell check version gremlin python client run jupyt notebook"
    },
    {
        "Question_title":"AML - Web service TimeoutError",
        "Question_body":"<p>We created a webservice endpoint and tested it with the following code, and also with POSTMAN.<\/p>\n\n<p>We deployed the service to an AKS in the same resource group and subscription as the AML resource.<\/p>\n\n<p><strong>UPDATE: the attached AKS had a custom networking configuration and rejected external connections.<\/strong><\/p>\n\n<pre><code>import numpy\nimport os, json, datetime, sys\nfrom operator import attrgetter\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.image import Image\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.authentication import AzureCliAuthentication\n\ncli_auth = AzureCliAuthentication()\n# Get workspace\nws = Workspace.from_config(auth=cli_auth)\n\n# Get the AKS Details\ntry:\n    with open(\"..\/aml_config\/aks_webservice.json\") as f:\n        config = json.load(f)\nexcept:\n    print(\"No new model, thus no deployment on AKS\")\n    # raise Exception('No new model to register as production model perform better')\n    sys.exit(0)\n\nservice_name = config[\"aks_service_name\"]\n# Get the hosted web service\nservice = Webservice(workspace=ws, name=service_name)\n\n# Input for Model with all features\ninput_j = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]]\nprint(input_j)\ntest_sample = json.dumps({\"data\": input_j})\ntest_sample = bytes(test_sample, encoding=\"utf8\")\ntry:\n    prediction = service.run(input_data=test_sample)\n    print(prediction)\nexcept Exception as e:\n    result = str(e)\n    print(result)\n    raise Exception(\"AKS service is not working as expected\")\n<\/code><\/pre>\n\n<p>In AML Studio, the deployment state is \"Healthy\".<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RTB10.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RTB10.png\" alt=\"Endpoint attributes\"><\/a><\/p>\n\n<p>We get the following error when testing:<\/p>\n\n<pre><code>Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'\n<\/code><\/pre>\n\n<p><strong>Log just after deploying the AKS Webservice <a href=\"http:\/\/t.ly\/t79b\" rel=\"nofollow noreferrer\">here<\/a>.<\/strong><\/p>\n\n<p><strong>Log after running the test script <a href=\"http:\/\/t.ly\/79k5\" rel=\"nofollow noreferrer\">here<\/a>.<\/strong><\/p>\n\n<p>How can we know what is causing this problem and fix it?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1592508291480,
        "Question_favorite_count":null,
        "Question_last_edit_time":1592590061567,
        "Question_score":2.0,
        "Question_view_count":332.0,
        "Answer_body":"<p>We checked the AKS networking configuration and realized it has an Azure CNI profile.<\/p>\n\n<p>In order to test the webservice we need to do it from inside the created virtual network.\nIt worked well!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62457880",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1592590202852,
        "Question_original_content":"aml web servic timeouterror creat webservic endpoint test follow code postman deploi servic ak resourc group subscript aml resourc updat attach ak custom network configur reject extern connect import numpi import json datetim sy oper import attrgett core import workspac core model import model core imag import imag core webservic import webservic core authent import azurecliauthent cli auth azurecliauthent workspac workspac config auth cli auth ak detail try open aml config ak webservic json config json load print new model deploy ak rais except new model regist product model perform better sy exit servic config ak servic host web servic servic webservic workspac servic input model featur input print input test sampl json dump data input test sampl byte test sampl encod utf try predict servic run input data test sampl print predict except result str print result rais except ak servic work expect aml studio deploy state healthi follow error test fail establish new connect winerror connect attempt fail connect parti properli respond period time establish connect fail connect host fail respond log deploi ak webservic log run test script know caus problem fix",
        "Question_preprocessed_content":"aml web servic timeouterror creat webservic endpoint test follow code postman deploi servic ak resourc group subscript aml resourc updat attach ak custom network configur reject extern connect aml studio deploy state healthi follow error test log deploi ak webservic log run test script know caus problem fix",
        "Question_gpt_summary_original":"The user encountered a TimeoutError when testing a webservice endpoint that was deployed to an AKS in the same resource group and subscription as the AML resource. The deployment state in AML Studio is \"Healthy\", but the AKS had a custom networking configuration and rejected external connections. The user is seeking assistance in identifying and resolving the cause of the problem.",
        "Question_gpt_summary":"user encount timeouterror test webservic endpoint deploi ak resourc group subscript aml resourc deploy state aml studio healthi ak custom network configur reject extern connect user seek assist identifi resolv caus problem",
        "Answer_original_content":"check ak network configur realiz azur cni profil order test webservic need insid creat virtual network work",
        "Answer_preprocessed_content":"check ak network configur realiz azur cni profil order test webservic need insid creat virtual network work",
        "Answer_gpt_summary_original":"Solution: The solution mentioned in the discussion was to test the webservice from inside the virtual network created for the AKS with Azure CNI profile. This resolved the TimeoutError issue.",
        "Answer_gpt_summary":"solut solut mention discuss test webservic insid virtual network creat ak azur cni profil resolv timeouterror issu"
    },
    {
        "Question_title":"What would be the best ML method for this use case?",
        "Question_body":"<p>Within AzureML, I have a CSV file which contains <code>2 columns<\/code> of data with <code>thousands of rows<\/code>. I'm looking to run this entire file as training, and find a pattern between these 2 sets of numbers, for example:<\/p>\n\n<pre><code>x -&gt; y\n\n... 10k x\n<\/code><\/pre>\n\n<p>And after all that training, I'd want to give this one line as the score model, so It'd look like:\nx -> ? (Predict answer from training)\n-- Note, the question mark here wouldn't need to be an exact match, as long as it is somewhat around what that actual number would turn out to be like.<\/p>\n\n<p>Is their a ML method (Inside <code>Azure ML<\/code>) that does such thing? Any points would be great.<\/p>\n\n<p>tl;dr: <code>Finding any type of pattern between 2 numbers (w\/ intense training).<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1441616495073,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":32.0,
        "Answer_body":"<p>Read about <code>linear regression<\/code>. This is answer for your question. And here is the link to Azure ML tutorial <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-create-experiment\/\" rel=\"nofollow\">link<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32434805",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1448906765136,
        "Question_original_content":"best method us case csv file contain column data thousand row look run entir file train pattern set number exampl train want line score model look like predict answer train note question mark wouldn need exact match long somewhat actual number turn like method insid thing point great find type pattern number intens train",
        "Question_preprocessed_content":"best method us case csv file contain data look run entir file train pattern set number exampl train want line score model look like note question mark wouldn need exact match long somewhat actual number turn like method thing point great",
        "Question_gpt_summary_original":"The user is looking for a machine learning method within AzureML to find a pattern between two sets of numbers in a CSV file with thousands of rows. They want to use the entire file for training and then predict the answer for a single line. The user is seeking advice on the best ML method to accomplish this task.",
        "Question_gpt_summary":"user look machin learn method pattern set number csv file thousand row want us entir file train predict answer singl line user seek advic best method accomplish task",
        "Answer_original_content":"read linear regress answer question link tutori link",
        "Answer_preprocessed_content":"read answer question link tutori link",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to use linear regression as the machine learning method within AzureML to find a pattern between two sets of numbers in a CSV file with thousands of rows. The user is also provided with a link to an Azure ML tutorial for further guidance.",
        "Answer_gpt_summary":"solut solut suggest discuss us linear regress machin learn method pattern set number csv file thousand row user provid link tutori guidanc"
    },
    {
        "Question_title":"Amazon Sage Maker: How to authenticate AWS SageMaker End-Point Request",
        "Question_body":"<p>I have an aws sagemaker end-point which need to be called from .Net core client, I have used the AWS SDK that deals with SageMaker and provided the required credentials however, always it keeps saying : <\/p>\n\n<p>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.<\/p>\n\n<p>var requestBody = \"{'url':'\"+\"<a href=\"https:\/\/cdn.pixabay.com\/photo\/2018\/05\/28\/22\/11\/message-in-a-bottle-3437294_960_720.jpg\" rel=\"nofollow noreferrer\">https:\/\/cdn.pixabay.com\/photo\/2018\/05\/28\/22\/11\/message-in-a-bottle-3437294_960_720.jpg<\/a>\" + \"'}\";<\/p>\n\n<pre><code>        var request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest()\n        {\n            EndpointName = \"CG-model-v1-endpoint\",\n            ContentType = \"application\/json;utf-8\",\n            Body = new MemoryStream(Encoding.ASCII.GetBytes(JsonConvert.SerializeObject(requestBody)))\n\n        };\n\n\n        var awsClient = new AmazonSageMakerRuntimeClient(awsAccessKeyId: \"XXXX\", awsSecretAccessKey: \"XXX\", region: RegionEndpoint.EUCentral1);\n\n        try\n        {\n            var resposnse = await awsClient.InvokeEndpointAsync(request);\n\n        }\n        catch (Exception ex)\n        {\n\n            return ApiResponse&lt;bool&gt;.Create(false);\n        }\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1563803017410,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":222.0,
        "Answer_body":"<p>I found the error , it was simply because of the request content-type,it had to be application\/json instead of application\/json;utf-8<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57147396",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1563872105436,
        "Question_original_content":"amazon sage maker authent end point request end point need call net core client aw sdk deal provid requir credenti keep sai request signatur calcul match signatur provid check aw secret access kei sign method consult servic document detail var requestbodi url http cdn pixabai com photo messag bottl jpg var request new amazon runtim model invokeendpointrequest endpointnam model endpoint contenttyp applic json utf bodi new memorystream encod ascii getbyt jsonconvert serializeobject requestbodi var awsclient new amazonruntimecli awsaccesskeyid awssecretaccesskei region regionendpoint eucentr try var resposns await awsclient invokeendpointasync request catch except return apirespons creat fals",
        "Question_preprocessed_content":"amazon sage maker authent request need call net core client aw sdk deal provid requir credenti keep sai request signatur calcul match signatur provid check aw secret access kei sign method consult servic document detail var requestbodi",
        "Question_gpt_summary_original":"The user is facing challenges in authenticating AWS SageMaker End-Point requests from a .Net core client. Despite providing the required credentials, the user is receiving an error message indicating that the request signature does not match the provided signature. The user has shared a code snippet that includes the request body and the AWS SDK used to deal with SageMaker.",
        "Question_gpt_summary":"user face challeng authent end point request net core client despit provid requir credenti user receiv error messag indic request signatur match provid signatur user share code snippet includ request bodi aw sdk deal",
        "Answer_original_content":"error simpli request content type applic json instead applic json utf",
        "Answer_preprocessed_content":"error simpli request instead",
        "Answer_gpt_summary_original":"Solution: The user found the solution to the challenge by changing the request content-type from \"application\/json;utf-8\" to \"application\/json\".",
        "Answer_gpt_summary":"solut user solut challeng chang request content type applic json utf applic json"
    },
    {
        "Question_title":"Deploying the sagemaker endpoint created as a service",
        "Question_body":"<p>I have trained a credit-fraud data set on AWS Sagemaker and created an endpoint of the model. Suppose I want to provide it as a service to my friend. He has some credit data and wanted to know whether the transaction is fraud or not. He wishes to use my endpoint. How do I share it?<\/p>\n\n<ol>\n<li>Should I share my ARN for endpoint? I don't think its the right way. without a common account he won't be able to use it.<\/li>\n<li>Or is there another way<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573479066647,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":226.0,
        "Answer_body":"<p>To share your model as an endpoint, you should use lambda and API Gateway to create your API.<\/p>\n\n<ol>\n<li>Create an API gateway that triggers a Lambda with the HTTP POST method;<\/li>\n<li>your lambda should instantiate the SageMaker endpoint, get the requested parameter in the event, call the SageMaker endpoint and return the predicted value. you can also create a DynamoDB to store commonly requested parameters with their answers;<\/li>\n<li>Send the API Gateway Endpoint to your friend.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/qLss4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qLss4.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1573654268900,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58802366",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1573653626700,
        "Question_original_content":"deploi endpoint creat servic train credit fraud data set creat endpoint model suppos want provid servic friend credit data want know transact fraud wish us endpoint share share arn endpoint think right wai common account won abl us wai",
        "Question_preprocessed_content":"deploi endpoint creat servic train data set creat endpoint model suppos want provid servic friend credit data want know transact fraud wish us endpoint share share arn endpoint think right wai common account won abl us wai",
        "Question_gpt_summary_original":"The user has encountered challenges in sharing their AWS Sagemaker endpoint, which they created after training a credit-fraud dataset, with a friend who wants to use it to determine whether a transaction is fraudulent or not. The user is unsure whether sharing their ARN for the endpoint is the right way, as their friend does not have a common account and may not be able to use it. The user is seeking alternative ways to share the endpoint.",
        "Question_gpt_summary":"user encount challeng share endpoint creat train credit fraud dataset friend want us determin transact fraudul user unsur share arn endpoint right wai friend common account abl us user seek altern wai share endpoint",
        "Answer_original_content":"share model endpoint us lambda api gatewai creat api creat api gatewai trigger lambda http post method lambda instanti endpoint request paramet event endpoint return predict valu creat dynamodb store commonli request paramet answer send api gatewai endpoint friend",
        "Answer_preprocessed_content":"share model endpoint us lambda api gatewai creat api creat api gatewai trigger lambda http post method lambda instanti endpoint request paramet event endpoint return predict valu creat dynamodb store commonli request paramet answer send api gatewai endpoint friend",
        "Answer_gpt_summary_original":"Solution: The user can create an API gateway that triggers a Lambda with the HTTP POST method. The Lambda should instantiate the SageMaker endpoint, get the requested parameter in the event, call the SageMaker endpoint and return the predicted value. The user can also create a DynamoDB to store commonly requested parameters with their answers. Finally, the user can send the API Gateway Endpoint to their friend.",
        "Answer_gpt_summary":"solut user creat api gatewai trigger lambda http post method lambda instanti endpoint request paramet event endpoint return predict valu user creat dynamodb store commonli request paramet answer final user send api gatewai endpoint friend"
    },
    {
        "Question_title":"ModuleNotFoundError while using AzureML pipeline with yml file based RunConfiguration and environment.yml",
        "Question_body":"<p>I am running into a ModuleNotFoundError for pandas while using the following code to orchestrate my Azure Machine Learning Pipeline:<\/p>\n<pre><code># Loading run config\nprint(&quot;Loading run config&quot;)\ntask_1_run_config = RunConfiguration.load(\n    os.path.join(WORKING_DIR + '\/pipeline\/task_runconfigs\/T01_Test_Task.yml')\n    ) \n\ntask_1_script_run_config = ScriptRunConfig(\n    source_directory=os.path.join(WORKING_DIR + '\/pipeline\/task_scripts'),\n    run_config=task_1_run_config    \n)\n\ntask_1_py_script_step = PythonScriptStep(\n    name='Task_1_Step',\n    script_name=task_1_script_run_config.script,\n    source_directory=task_1_script_run_config.source_directory,\n    compute_target=compute_target\n)\n\npipeline_run_config = Pipeline(workspace=workspace, steps=[task_1_py_script_step])#, task_2])\n\npipeline_run = Experiment(workspace, 'Test_Run_New_Pipeline').submit(pipeline_run_config)\npipeline_run.wait_for_completion()\n<\/code><\/pre>\n<p>The environment.yml<\/p>\n<pre><code>name: phinmo_pipeline_env\ndependencies:\n- python=3.8\n- pip:\n  - pandas\n  - azureml-core==1.43.0\n  - azureml-sdk\n  - scipy\n  - scikit-learn\n  - numpy\n  - pyyaml==6.0\n  - datetime\n  - azure\nchannels:\n  - conda-forge\n<\/code><\/pre>\n<p>The loaded RunConfiguration in T01_Test_Task.yml looks like this:<\/p>\n<pre><code># The script to run.\nscript: T01_Test_Task.py\n# The arguments to the script file.\narguments: [\n  &quot;--test&quot;, False,\n  &quot;--date&quot;, &quot;2022-07-26&quot;\n]\n# The name of the compute target to use for this run.\ncompute_target: phinmo-compute-cluster\n# Framework to execute inside. Allowed values are &quot;Python&quot;, &quot;PySpark&quot;, &quot;CNTK&quot;, &quot;TensorFlow&quot;, and &quot;PyTorch&quot;.\nframework: Python\n# Maximum allowed duration for the run.\nmaxRunDurationSeconds: 6000\n# Number of nodes to use for running job.\nnodeCount: 1\n\n#Environment details.\nenvironment:\n  # Environment name\n  name: phinmo_pipeline_env\n  # Environment version\n  version:\n  # Environment variables set for the run.\n  #environmentVariables:\n  #  EXAMPLE_ENV_VAR: EXAMPLE_VALUE\n  # Python details\n  python:\n    # user_managed_dependencies=True indicates that the environmentwill be user managed. False indicates that AzureML willmanage the user environment.\n    userManagedDependencies: false\n    # The python interpreter path\n    interpreterPath: python\n    # Path to the conda dependencies file to use for this run. If a project\n    # contains multiple programs with different sets of dependencies, it may be\n    # convenient to manage those environments with separate files.\n    condaDependenciesFile: environment.yml\n    # The base conda environment used for incremental environment creation.\n    baseCondaEnvironment: AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\n  # Docker details\n  \n# History details.\nhistory:\n  # Enable history tracking -- this allows status, logs, metrics, and outputs\n  # to be collected for a run.\n  outputCollection: true\n  # Whether to take snapshots for history.\n  snapshotProject: true\n  # Directories to sync with FileWatcher.\n  directoriesToWatch:\n  - logs\n# data reference configuration details\ndataReferences: {}\n# The configuration details for data.\ndata: {}\n# Project share datastore reference.\nsourceDirectoryDataStore:\n<\/code><\/pre>\n<p>I already tried a few things like overwriting the environment attribute in the RunConfiguration object with a environment.python.conda_dependencies object or assigning a version number to pandas in the environment.yml, changing the location of the environment.yml. But I am at a loss at what else to try. the T01_Test_Task.py runs without issues on its own. But putting it into a pipeline just does not seem to work.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1658922798813,
        "Question_favorite_count":null,
        "Question_last_edit_time":1658924360076,
        "Question_score":0.0,
        "Question_view_count":51.0,
        "Answer_body":"<p>Okay I found the issue.\nI am unnecessarily using the ScriptRunConfig which overwrites the assigned environment with some default azureml environment. I was able to see that only in the Task description in the Azure Machine Learning Studio UI.<\/p>\n<p>I was able to just remove that part and now it works:<\/p>\n<pre><code>task_1_run_config = RunConfiguration.load(\n    os.path.join(WORKING_DIR + '\/pipeline\/task_runconfigs\/T01_Test_Task.yml')\n    ) \ntask_1_py_script_step = PythonScriptStep(\n    name='Task_1_Step',\n    script_name='T01_Test_Task.py',\n    source_directory=os.path.join(WORKING_DIR + '\/pipeline\/task_scripts'),\n    runconfig=task_1_run_config, \n    compute_target=compute_target\n)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73137433",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1658937635368,
        "Question_original_content":"modulenotfounderror pipelin yml file base runconfigur environ yml run modulenotfounderror panda follow code orchestr pipelin load run config print load run config task run config runconfigur load path join work dir pipelin task runconfig test task yml task script run config scriptrunconfig sourc directori path join work dir pipelin task script run config task run config task script step pythonscriptstep task step script task script run config script sourc directori task script run config sourc directori comput target comput target pipelin run config pipelin workspac workspac step task script step task pipelin run experi workspac test run new pipelin submit pipelin run config pipelin run wait complet environ yml phinmo pipelin env depend python pip panda core sdk scipi scikit learn numpi pyyaml datetim azur channel conda forg load runconfigur test task yml look like script run script test task argument script file argument test fals date comput target us run comput target phinmo comput cluster framework execut insid allow valu python pyspark cntk tensorflow pytorch framework python maximum allow durat run maxrundurationsecond number node us run job nodecount environ detail environ environ phinmo pipelin env environ version version environ variabl set run environmentvari exampl env var exampl valu python detail python user manag depend true indic environmentwil user manag fals indic willmanag user environ usermanageddepend fals python interpret path interpreterpath python path conda depend file us run project contain multipl program differ set depend conveni manag environ separ file condadependenciesfil environ yml base conda environ increment environ creation basecondaenviron sklearn ubuntu cpu docker detail histori detail histori enabl histori track allow statu log metric output collect run outputcollect true snapshot histori snapshotproject true directori sync filewatch directoriestowatch log data refer configur detail datarefer configur detail data data project share datastor refer sourcedirectorydatastor tri thing like overwrit environ attribut runconfigur object environ python conda depend object assign version number panda environ yml chang locat environ yml loss try test task run issu put pipelin work",
        "Question_preprocessed_content":"modulenotfounderror pipelin yml file base runconfigur run modulenotfounderror panda follow code orchestr pipelin load runconfigur look like tri thing like overwrit environ attribut runconfigur object object assign version number panda chang locat loss try run issu put pipelin work",
        "Question_gpt_summary_original":"The user is encountering a ModuleNotFoundError for pandas while using Azure Machine Learning Pipeline with a yml file based RunConfiguration and environment.yml. The T01_Test_Task.py runs without issues on its own, but putting it into a pipeline does not seem to work. The user has tried overwriting the environment attribute in the RunConfiguration object with a environment.python.conda_dependencies object or assigning a version number to pandas in the environment.yml, changing the location of the environment.yml, but is still unable to resolve the issue.",
        "Question_gpt_summary":"user encount modulenotfounderror panda pipelin yml file base runconfigur environ yml test task run issu put pipelin work user tri overwrit environ attribut runconfigur object environ python conda depend object assign version number panda environ yml chang locat environ yml unabl resolv issu",
        "Answer_original_content":"okai issu unnecessarili scriptrunconfig overwrit assign environ default environ abl task descript studio abl remov work task run config runconfigur load path join work dir pipelin task runconfig test task yml task script step pythonscriptstep task step script test task sourc directori path join work dir pipelin task script runconfig task run config comput target comput target",
        "Answer_preprocessed_content":"okai issu unnecessarili scriptrunconfig overwrit assign environ default environ abl task descript studio abl remov work",
        "Answer_gpt_summary_original":"Solution: The user found that the issue was caused by using the ScriptRunConfig which overwrites the assigned environment with some default azureml environment. The user was able to resolve the issue by removing that part and using the correct RunConfiguration object.",
        "Answer_gpt_summary":"solut user issu caus scriptrunconfig overwrit assign environ default environ user abl resolv issu remov correct runconfigur object"
    },
    {
        "Question_title":"Move a .dvc stage without re-run",
        "Question_body":"<p>I just ran a <code>dvc run ...<\/code> but forgot to specify the <code>-f<\/code> flag so it created a <code>.dvc<\/code> file in an unintended location. Is there a smart way of moving it without re-running the stage? This method should take care of location and relative paths <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1589374711255,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":445.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/drorata\">@drorata<\/a>,<\/p>\n<p>Unfortunatelly it seems you need to just move the DVC-file and manually change the paths in it to make them relative to the new location. Or you can change them into absolute paths.<\/p>\n<p>If you think you may be moving stage files in the future, maybe consider specifying the dependencies and outputs as absolute paths when you use <code>dvc run<\/code>.<\/p>\n<p>It would be great to have this as a feature request though, would you like to open an issue in out Git repo?<\/p>\n<p>Thanks<\/p>. <p>Done! <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/3795\" rel=\"nofollow noopener\">https:\/\/github.com\/iterative\/dvc\/issues\/3795<\/a><\/p>. <p><a class=\"mention\" href=\"\/u\/drorata\">@drorata<\/a> have you had chance to try the pre-release 1.0a? It has changed a lot the way pipelines are organized - it\u2019s a single file now that is intended to be human readable\/editable. Would be really great if you give it a try and let us know if solves the problem with renaming\/moving things.<\/p>. <p><a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a> as I mentioned in the ticket, I didn\u2019t have the chance to put in action the changes introduced  in 1.0a. Most of my time with DVC is currently invested in a single project where I\u2019m worried about backward incompatibilities.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/move-a-dvc-stage-without-re-run\/384",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-05-13T15:12:12.312Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/drorata\">@drorata<\/a>,<\/p>\n<p>Unfortunatelly it seems you need to just move the DVC-file and manually change the paths in it to make them relative to the new location. Or you can change them into absolute paths.<\/p>\n<p>If you think you may be moving stage files in the future, maybe consider specifying the dependencies and outputs as absolute paths when you use <code>dvc run<\/code>.<\/p>\n<p>It would be great to have this as a feature request though, would you like to open an issue in out Git repo?<\/p>\n<p>Thanks<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-13T17:36:12.540Z",
                "Answer_body":"<p>Done! <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/3795\" rel=\"nofollow noopener\">https:\/\/github.com\/iterative\/dvc\/issues\/3795<\/a><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-13T18:50:45.040Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/drorata\">@drorata<\/a> have you had chance to try the pre-release 1.0a? It has changed a lot the way pipelines are organized - it\u2019s a single file now that is intended to be human readable\/editable. Would be really great if you give it a try and let us know if solves the problem with renaming\/moving things.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-13T19:03:45.378Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a> as I mentioned in the ticket, I didn\u2019t have the chance to put in action the changes introduced  in 1.0a. Most of my time with DVC is currently invested in a single project where I\u2019m worried about backward incompatibilities.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"stage run ran run forgot specifi flag creat file unintend locat smart wai move run stage method care locat rel path",
        "Question_preprocessed_content":"stage ran forgot specifi flag creat file unintend locat smart wai move stage method care locat rel path",
        "Question_gpt_summary_original":"The user forgot to specify the -f flag while running a dvc command, resulting in the creation of a .dvc file in an unintended location. The user is seeking a way to move the file without re-running the stage, while ensuring that the location and relative paths are taken care of.",
        "Question_gpt_summary":"user forgot specifi flag run command result creation file unintend locat user seek wai file run stage ensur locat rel path taken care",
        "Answer_original_content":"drorata unfortunatelli need file manual chang path rel new locat chang absolut path think move stage file futur mayb consid specifi depend output absolut path us run great featur request like open issu git repo thank http github com iter issu drorata chanc try pre releas chang lot wai pipelin organ singl file intend human readabl edit great try let know solv problem renam move thing shcheklein mention ticket didnt chanc action chang introduc time current invest singl project worri backward incompat",
        "Answer_preprocessed_content":"unfortunatelli need file manual chang path rel new locat chang absolut path think move stage file futur mayb consid specifi depend output absolut path us great featur request like open issu git repo thank chanc try chang lot wai pipelin organ singl file intend human great try let know solv problem thing mention ticket didnt chanc action chang introduc time current invest singl project worri backward incompat",
        "Answer_gpt_summary_original":"Solutions provided:\n- Manually change the paths in the .dvc file to make them relative to the new location or change them into absolute paths.\n- Consider specifying dependencies and outputs as absolute paths when using \"dvc run\" to avoid similar issues in the future.\n- Open an issue in the Git repo to request this as a feature.\n- Try the pre-release 1.0a, which has changed the way pipelines are organized and is intended to be human-readable\/editable, to see if it solves the problem with renaming\/moving things.\n\nNo personal opinions or biases were included in the summary.",
        "Answer_gpt_summary":"solut provid manual chang path file rel new locat chang absolut path consid specifi depend output absolut path run avoid similar issu futur open issu git repo request featur try pre releas chang wai pipelin organ intend human readabl edit solv problem renam move thing person opinion bias includ summari"
    },
    {
        "Question_title":"How can I generate prediction intervals for Azure AutoML timeseries forecasts?",
        "Question_body":"<p>Is it possible to generate prediction intervals for time series forecasts when using a Azure AutoML trained models? Could we get the training errors out of the process and use them for bootstrapping?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618193869380,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":205.0,
        "Answer_body":"<p>You can generate forecast quantiles. See the following notebook for more details: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-forecast-function\/auto-ml-forecasting-function.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-forecast-function\/auto-ml-forecasting-function.ipynb<\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67051900",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1618942132740,
        "Question_original_content":"gener predict interv azur automl timeseri forecast possibl gener predict interv time seri forecast azur automl train model train error process us bootstrap",
        "Question_preprocessed_content":"gener predict interv azur automl timeseri forecast possibl gener predict interv time seri forecast azur automl train model train error process us bootstrap",
        "Question_gpt_summary_original":"The user is facing challenges in generating prediction intervals for time series forecasts using Azure AutoML trained models. They are unsure if it is possible to obtain training errors for bootstrapping purposes.",
        "Question_gpt_summary":"user face challeng gener predict interv time seri forecast azur automl train model unsur possibl obtain train error bootstrap purpos",
        "Answer_original_content":"gener forecast quantil follow notebook detail http github com azur machinelearningnotebook blob master us autom machin learn forecast forecast function auto forecast function ipynb",
        "Answer_preprocessed_content":"gener forecast quantil follow notebook detail",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the challenge by suggesting the user to generate forecast quantiles. The user can refer to the provided notebook for more details on how to generate forecast quantiles.",
        "Answer_gpt_summary":"solut discuss provid solut challeng suggest user gener forecast quantil user refer provid notebook detail gener forecast quantil"
    },
    {
        "Question_title":"What's the difference between regular and ml AWS EC2 instances?",
        "Question_body":"<p>I'm experimenting with <a href=\"https:\/\/aws.amazon.com\/sagemaker\/\" rel=\"nofollow noreferrer\">AWS Sagemaker<\/a> using a Free Tier account. According to the <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">Sagemaker pricing<\/a>, I can use 50 hours of m4.xlarge and m5.xlarge instances for training in the free tier. (I am safely within the two-month limit.) But when I attempt to train an algorithm with the XGBoost container using m5.xlarge, I get the error shown below the code.<\/p>\n<p>Are the ml-type and non-ml-type instances the same with just a fancy prefix for those that one would use with Sagemaker or are they entirely different? The <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/\" rel=\"nofollow noreferrer\">EC2 page<\/a> doesn't even list the ml instances.<\/p>\n<pre><code>sess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(container,\n                                    role, \n                                    instance_count=1, \n                                    instance_type='m5.xlarge',\n                                    output_path=output_location,\n                                    sagemaker_session=sess)\n<\/code><\/pre>\n<blockquote>\n<p>ClientError: An error occurred (ValidationException) when calling the\nCreateTrainingJob operation: 1 validation error detected: Value\n'm5.xlarge' at 'resourceConfig.instanceType' failed to satisfy\nconstraint: Member must satisfy enum value set: [ml.p2.xlarge,\nml.m5.4xlarge, ml.m4.16xlarge, ml.p4d.24xlarge, ml.c5n.xlarge,\nml.p3.16xlarge, ml.m5.large, ml.p2.16xlarge, ml.c4.2xlarge,\nml.c5.2xlarge, ml.c4.4xlarge, ml.c5.4xlarge, ml.c5n.18xlarge,\nml.g4dn.xlarge, ml.g4dn.12xlarge, ml.c4.8xlarge, ml.g4dn.2xlarge,\nml.c5.9xlarge, ml.g4dn.4xlarge, ml.c5.xlarge, ml.g4dn.16xlarge,\nml.c4.xlarge, ml.g4dn.8xlarge, ml.c5n.2xlarge, ml.c5n.4xlarge,\nml.c5.18xlarge, ml.p3dn.24xlarge, ml.p3.2xlarge, ml.m5.xlarge,\nml.m4.10xlarge, ml.c5n.9xlarge, ml.m5.12xlarge, ml.m4.xlarge,\nml.m5.24xlarge, ml.m4.2xlarge, ml.p2.8xlarge, ml.m5.2xlarge,\nml.p3.8xlarge, ml.m4.4xlarge]<\/p>\n<\/blockquote>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1607865181387,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1608039265823,
        "Question_score":5.0,
        "Question_view_count":7690.0,
        "Answer_body":"<p>The instances with the <code>ml<\/code> prefix are instance classes specifically for use in Sagemaker.<\/p>\n<p>In addition to being used within the Sagemaker service, the instance will be running an AMI with all the necessary libraries and packages such as Jupyter.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_last_edit_time":null,
        "Answer_score":12.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65276017",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1607865492452,
        "Question_original_content":"differ regular aw instanc experi free tier account accord price us hour xlarg xlarg instanc train free tier safe month limit attempt train algorithm xgboost contain xlarg error shown code type non type instanc fanci prefix us entir differ page list instanc sess session xgb estim estim contain role instanc count instanc type xlarg output path output locat session sess clienterror error occur validationexcept call createtrainingjob oper valid error detect valu xlarg resourceconfig instancetyp fail satisfi constraint member satisfi enum valu set xlarg xlarg xlarg xlarg xlarg xlarg larg xlarg xlarg xlarg xlarg xlarg xlarg gdn xlarg gdn xlarg xlarg gdn xlarg xlarg gdn xlarg xlarg gdn xlarg xlarg gdn xlarg xlarg xlarg xlarg pdn xlarg xlarg xlarg xlarg xlarg xlarg xlarg xlarg xlarg xlarg xlarg xlarg xlarg",
        "Question_preprocessed_content":"differ regular aw instanc experi free tier account accord price us hour instanc train free tier attempt train algorithm xgboost contain error shown code instanc fanci prefix us entir differ page list instanc clienterror error occur call createtrainingjob oper valid error detect valu fail satisfi constraint member satisfi enum valu set",
        "Question_gpt_summary_original":"The user is encountering an error when attempting to train an algorithm with the XGBoost container using m5.xlarge instance type in AWS Sagemaker. The user is questioning whether ml-type and non-ml-type instances are the same or entirely different, as the EC2 page does not list the ml instances.",
        "Question_gpt_summary":"user encount error attempt train algorithm xgboost contain xlarg instanc type user question type non type instanc entir differ page list instanc",
        "Answer_original_content":"instanc prefix instanc class specif us addit servic instanc run ami necessari librari packag jupyt",
        "Answer_preprocessed_content":"instanc prefix instanc class specif us addit servic instanc run ami necessari librari packag jupyt",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the user's question by stating that instances with the \"ml\" prefix are instance classes specifically designed for use in Sagemaker. These instances come with all the necessary libraries and packages such as Jupyter, making them suitable for machine learning tasks.",
        "Answer_gpt_summary":"solut discuss provid solut user question state instanc prefix instanc class specif design us instanc come necessari librari packag jupyt make suitabl machin learn task"
    },
    {
        "Question_title":"Azure Python SDK & Machine Learning Studio Web Service Batch Execution Snippet: TypeError",
        "Question_body":"<p><strong>First Issue resolved, please read scroll down to EDIT2<\/strong><\/p>\n\n<p>I'm trying to access a Web Service deployed via Azure Machine Learning Studio, using the Batch Execution-Sample Code for Python on the bottom of below page:<\/p>\n\n<p><a href=\"https:\/\/studio.azureml.net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs<\/a><\/p>\n\n<p>I have already fixed an Issue according to this question (replaced BlobService by BlobBlockService and so on):<\/p>\n\n<p><a href=\"https:\/\/studio.azureml.net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs<\/a><\/p>\n\n<p>And I also have entered the API-Key, Container-Name, URL, account_key and account_name according to the instructions.<\/p>\n\n<p>However it seems that today the Code Snippet is even more outdated than it was back then because I receive a different error now:<\/p>\n\n<pre><code>File \"C:\/Users\/Alex\/Desktop\/scripts\/BatchExecution.py\", line 80, in uploadFileToBlob\n    blob_service = asb.BlockBlobService(account_name=storage_account_name, account_key=storage_account_key)\n\n  File \"C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\azure\\storage\\blob\\blockblobservice.py\", line 145, in __init__\n\n  File \"C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\azure\\storage\\blob\\baseblobservice.py\", line 205, in __init__\n\nTypeError: get_service_parameters() got an unexpected keyword argument 'token_credential'\n<\/code><\/pre>\n\n<p>I also noticed, that when installing the Azure SDK for Python via pip, I get the following warnings in the end of the process (installation is successful however):<\/p>\n\n<pre><code>azure-storage-queue 1.3.0 has requirement azure-storage-common&lt;1.4.0,&gt;=1.3.0, but you'll have azure-storage-common 1.1.0 which is incompatible.\n\nazure-storage-file 1.3.0 has requirement azure-storage-common&lt;1.4.0,&gt;=1.3.0, but you'll have azure-storage-common 1.1.0 which is incompatible.\n\nazure-storage-blob 1.3.0 has requirement azure-storage-common&lt;1.4.0,&gt;=1.3.0, but you'll have azure-storage-common 1.1.0 which is incompatible.\n<\/code><\/pre>\n\n<p>I can't find anything about all this in the latest documentation for the Python SDK (the word 'token_credential' is not even contained):<\/p>\n\n<p><a href=\"https:\/\/media.readthedocs.org\/pdf\/azure-storage\/latest\/azure-storage.pdf\" rel=\"nofollow noreferrer\">https:\/\/media.readthedocs.org\/pdf\/azure-storage\/latest\/azure-storage.pdf<\/a><\/p>\n\n<p>Does anyone have a clue what's going wrong during the installation or why the type-error with the 'token_credential' pops up during execution?<\/p>\n\n<p>Or does anyone know how I can install the necessary version of azure-storage-common or azure-storage-blob?<\/p>\n\n<p>EDIT: Here's a my code (however not-reproducible because I changed the keys before posting)<\/p>\n\n<pre><code># How this works:\n#\n# 1. Assume the input is present in a local file (if the web service accepts input)\n# 2. Upload the file to an Azure blob - you\"d need an Azure storage account\n# 3. Call BES to process the data in the blob. \n# 4. The results get written to another Azure blob.\n\n# 5. Download the output blob to a local file\n#\n# Note: You may need to download\/install the Azure SDK for Python.\n# See: http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/python-how-to-install\/\n\nimport urllib\n# If you are using Python 3+, import urllib instead of urllib2\n\nimport json\nimport time\nimport azure.storage.blob as asb          # replaces BlobService by BlobBlockService\n\n\ndef printHttpError(httpError):\n    print(\"The request failed with status code: \" + str(httpError.code))\n\n    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n    print(httpError.info())\n\n    print(json.loads(httpError.read()))\n    return\n\n\ndef saveBlobToFile(blobUrl, resultsLabel):\n    output_file = \"myresults.csv\" # Replace this with the location you would like to use for your output file\n    print(\"Reading the result from \" + blobUrl)\n    try:\n        # If you are using Python 3+, replace urllib2 with urllib.request in the following code\n        response = urllib.request.urlopen(blobUrl)\n    except urllib.request.HTTPError:\n        printHttpError(urllib.HTTPError)\n        return\n\n    with open(output_file, \"w+\") as f:\n        f.write(response.read())\n    print(resultsLabel + \" have been written to the file \" + output_file)\n    return\n\n\ndef processResults(result):\n\n\n    first = True\n    results = result[\"Results\"]\n    for outputName in results:\n        result_blob_location = results[outputName]\n        sas_token = result_blob_location[\"SasBlobToken\"]\n        base_url = result_blob_location[\"BaseLocation\"]\n        relative_url = result_blob_location[\"RelativeLocation\"]\n\n        print(\"The results for \" + outputName + \" are available at the following Azure Storage location:\")\n        print(\"BaseLocation: \" + base_url)\n        print(\"RelativeLocation: \" + relative_url)\n        print(\"SasBlobToken: \" + sas_token)\n\n\n        if (first):\n            first = False\n            url3 = base_url + relative_url + sas_token\n            saveBlobToFile(url3, \"The results for \" + outputName)\n    return\n\n\n\ndef uploadFileToBlob(input_file, input_blob_name, storage_container_name, storage_account_name, storage_account_key):\n    blob_service = asb.BlockBlobService(account_name=storage_account_name, account_key=storage_account_key)\n\n    print(\"Uploading the input to blob storage...\")\n    data_to_upload = open(input_file, \"r\").read()\n    blob_service.put_blob(storage_container_name, input_blob_name, data_to_upload, x_ms_blob_type=\"BlockBlob\")\n\ndef invokeBatchExecutionService():\n    storage_account_name = \"storage1\" # Replace this with your Azure Storage Account name\n    storage_account_key = \"kOveEtQMoP5zbUGfFR47\" # Replace this with your Azure Storage Key\n    storage_container_name = \"input\" # Replace this with your Azure Storage Container name\n    connection_string = \"DefaultEndpointsProtocol=https;AccountName=\" + storage_account_name + \";AccountKey=\" + storage_account_key #\"DefaultEndpointsProtocol=https;AccountName=mayatostorage1;AccountKey=aOYA2P5VQPR3ZQCl+aWhcGhDRJhsR225teGGBKtfXWwb2fNEo0CrhlwGWdfbYiBTTXPHYoKZyMaKuEAU8A\/Fzw==;EndpointSuffix=core.windows.net\"\n    api_key = \"5wUaln7n99rt9k+enRLG2OrhSsr9VLeoCfh0q3mfYo27hfTCh32f10PsRjJtuA==\" # Replace this with the API key for the web service\n    url = \"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/306bc1f050\/services\/61670382104542bc9533a920830b263c\/jobs\" #\"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/services\/61670382104542bc9533a920830b263c\/jobs\/job_id\/start?api-version=2.0\"\n\n\n\n    uploadFileToBlob(r\"C:\\Users\\Alex\\Desktop\\16_da.csv\", # Replace this with the location of your input file\n                     \"input1datablob.csv\", # Replace this with the name you would like to use for your Azure blob; this needs to have the same extension as the input file \n                     storage_container_name, storage_account_name, storage_account_key)\n\n    payload =  {\n\n        \"Inputs\": {\n\n            \"input1\": { \"ConnectionString\": connection_string, \"RelativeLocation\": \"\/\" + storage_container_name + \"\/input1datablob.csv\" },\n        },     \n\n        \"Outputs\": {\n\n            \"output1\": { \"ConnectionString\": connection_string, \"RelativeLocation\": \"\/\" + storage_container_name + \"\/output1results.csv\" },\n        },\n        \"GlobalParameters\": {\n}\n    }\n\n    body = str.encode(json.dumps(payload))\n    headers = { \"Content-Type\":\"application\/json\", \"Authorization\":(\"Bearer \" + api_key)}\n    print(\"Submitting the job...\")\n\n    # If you are using Python 3+, replace urllib2 with urllib.request in the following code\n\n    # submit the job\n    req = urllib.request.Request(url + \"?api-version=2.0\", body, headers)\n    try:\n        response = urllib.request.urlopen(req)\n    except urllib.request.HTTPError:\n        printHttpError(urllib.HTTPError)\n        return\n\n    result = response.read()\n    job_id = result[1:-1] # remove the enclosing double-quotes\n    print(\"Job ID: \" + job_id)\n\n\n    # If you are using Python 3+, replace urllib2 with urllib.request in the following code\n    # start the job\n    print(\"Starting the job...\")\n    req = urllib.request.Request(url + \"\/\" + job_id + \"\/start?api-version=2.0\", \"\", headers)\n    try:\n        response = urllib.request.urlopen(req)\n    except urllib.request.HTTPError:\n        printHttpError(urllib.HTTPError)\n        return\n\n    url2 = url + \"\/\" + job_id + \"?api-version=2.0\"\n\n    while True:\n        print(\"Checking the job status...\")\n        # If you are using Python 3+, replace urllib2 with urllib.request in the follwing code\n        req = urllib.request.Request(url2, headers = { \"Authorization\":(\"Bearer \" + api_key) })\n\n        try:\n            response = urllib.request.urlopen(req)\n        except urllib.request.HTTPError:\n            printHttpError(urllib.HTTPError)\n            return    \n\n        result = json.loads(response.read())\n        status = result[\"StatusCode\"]\n        if (status == 0 or status == \"NotStarted\"):\n            print(\"Job \" + job_id + \" not yet started...\")\n        elif (status == 1 or status == \"Running\"):\n            print(\"Job \" + job_id + \" running...\")\n        elif (status == 2 or status == \"Failed\"):\n            print(\"Job \" + job_id + \" failed!\")\n            print(\"Error details: \" + result[\"Details\"])\n            break\n        elif (status == 3 or status == \"Cancelled\"):\n            print(\"Job \" + job_id + \" cancelled!\")\n            break\n        elif (status == 4 or status == \"Finished\"):\n            print(\"Job \" + job_id + \" finished!\")\n\n            processResults(result)\n            break\n        time.sleep(1) # wait one second\n    return\n\ninvokeBatchExecutionService()\n<\/code><\/pre>\n\n<p>EDIT 2: The above issue has been resolved thanks to jon and the csv gets uploaded in blob storage.<\/p>\n\n<p>However now there is an HTTPError, when the job gets submitted in Line 130:<\/p>\n\n<pre><code>   raise HTTPError(req.full_url, code, msg, hdrs, fp)  HTTPError: Bad Request\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1530205297260,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1530279219952,
        "Question_score":0.0,
        "Question_view_count":499.0,
        "Answer_body":"<p>I think the code they give may be pretty old at this point.<\/p>\n\n<p>The <a href=\"https:\/\/pypi.org\/project\/azure-storage-blob\/#history\" rel=\"nofollow noreferrer\">latest version<\/a> of <code>azure.storage.blob<\/code> is 1.3. So perhaps a <code>pip install azure.storage.blob --update<\/code> or simply uninstalling and reinstalling would help.<\/p>\n\n<p>Once you got the latest version, try using the <code>create_blob_from_text<\/code> method to load the file to your storage container.<\/p>\n\n<pre><code>from azure.storage.blob import BlockBlobService\n\nblobService = BlockBlobService(account_name=\"accountName\", account_key=\"accountKey)\n\nblobService.create_blob_from_text(\"containerName\", \"fileName\", csv_file)\n<\/code><\/pre>\n\n<p>Hope that works to help lead you down the right path, but if not we can work through it. :)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51088145",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1530266278528,
        "Question_original_content":"azur python sdk machin learn studio web servic batch execut snippet typeerror issu resolv read scroll edit try access web servic deploi studio batch execut sampl code python page http studio net apihelp workspac bcfbacdbadbccccab webservic eeddecaeabfdcd endpoint bcabc job fix issu accord question replac blobservic blobblockservic http studio net apihelp workspac bcfbacdbadbccccab webservic eeddecaeabfdcd endpoint bcabc job enter api kei contain url account kei account accord instruct todai code snippet outdat receiv differ error file user alex desktop script batchexecut line uploadfiletoblob blob servic asb blockblobservic account storag account account kei storag account kei file user alex anaconda lib site packag azur storag blob blockblobservic line init file user alex anaconda lib site packag azur storag blob baseblobservic line init typeerror servic paramet got unexpect keyword argument token credenti notic instal azur sdk python pip follow warn end process instal success azur storag queue requir azur storag common azur storag common incompat azur storag file requir azur storag common azur storag common incompat azur storag blob requir azur storag common azur storag common incompat latest document python sdk word token credenti contain http media readthedoc org pdf azur storag latest azur storag pdf clue go wrong instal type error token credenti pop execut know instal necessari version azur storag common azur storag blob edit code reproduc chang kei post work assum input present local file web servic accept input upload file azur blob need azur storag account be process data blob result written azur blob download output blob local file note need download instal azur sdk python http azur microsoft com document articl python instal import urllib python import urllib instead urllib import json import time import azur storag blob asb replac blobservic blobblockservic def printhttperror httperror print request fail statu code str httperror code print header includ requert timestamp us debug failur print httperror info print json load httperror read return def saveblobtofil bloburl resultslabel output file myresult csv replac locat like us output file print read result bloburl try python replac urllib urllib request follow code respons urllib request urlopen bloburl urllib request httperror printhttperror urllib httperror return open output file write respons read print resultslabel written file output file return def processresult result true result result result outputnam result result blob locat result outputnam sa token result blob locat sasblobtoken base url result blob locat baseloc rel url result blob locat relativeloc print result outputnam avail follow azur storag locat print baseloc base url print relativeloc rel url print sasblobtoken sa token fals url base url rel url sa token saveblobtofil url result outputnam return def uploadfiletoblob input file input blob storag contain storag account storag account kei blob servic asb blockblobservic account storag account account kei storag account kei print upload input blob storag data upload open input file read blob servic blob storag contain input blob data upload blob type blockblob def invokebatchexecutionservic storag account storag replac azur storag account storag account kei koveetqmopzbugffr replac azur storag kei storag contain input replac azur storag contain connect string defaultendpointsprotocol http accountnam storag account accountkei storag account kei defaultendpointsprotocol http accountnam mayatostorag accountkei aoyapvqprzqcl awhcghdrjhsrteggbktfxwwbfneocrhlwgwdfbyibttxphyokzymakueaua fzw endpointsuffix core window net api kei wualnnrtk enrlgorhssrvleocfhqmfyohftchfpsrjjtua replac api kei web servic url http ussouthcentr servic net workspac bcf servic bcabc job http ussouthcentr servic net workspac bcfbacdbadbccccab servic bcabc job job start api version uploadfiletoblob user alex desktop csv replac locat input file inputdatablob csv replac like us azur blob need extens input file storag contain storag account storag account kei payload input input connectionstr connect string relativeloc storag contain inputdatablob csv output output connectionstr connect string relativeloc storag contain outputresult csv globalparamet bodi str encod json dump payload header content type applic json author bearer api kei print submit job python replac urllib urllib request follow code submit job req urllib request request url api version bodi header try respons urllib request urlopen req urllib request httperror printhttperror urllib httperror return result respons read job result remov enclos doubl quot print job job python replac urllib urllib request follow code start job print start job req urllib request request url job start api version header try respons urllib request urlopen req urllib request httperror printhttperror urllib httperror return url url job api version true print check job statu python replac urllib urllib request follw code req urllib request request url header author bearer api kei try respons urllib request urlopen req urllib request httperror printhttperror urllib httperror return result json load respons read statu result statuscod statu statu notstart print job job start elif statu statu run print job job run elif statu statu fail print job job fail print error detail result detail break elif statu statu cancel print job job cancel break elif statu statu finish print job job finish processresult result break time sleep wait second return invokebatchexecutionservic edit issu resolv thank jon csv get upload blob storag httperror job get submit line rais httperror req url code msg hdr httperror bad request",
        "Question_preprocessed_content":"azur python sdk machin learn studio web servic batch execut snippet typeerror issu resolv read scroll edit try access web servic deploi studio batch code python page fix issu accord question enter url accord instruct todai code snippet outdat receiv differ error notic instal azur sdk python pip follow warn end process latest document python sdk clue go wrong instal pop execut know instal necessari version edit code edit issu resolv thank jon csv get upload blob storag httperror job get submit line",
        "Question_gpt_summary_original":"The user is encountering challenges while trying to access a Web Service deployed via Azure Machine Learning Studio using the Batch Execution-Sample Code for Python. The user has already fixed one issue but is now facing a different error related to the 'token_credential'. The user is also receiving warnings during the installation of the Azure SDK for Python. The user is seeking help to resolve the installation issue and the error related to 'token_credential'. Additionally, the user is facing an HTTPError when the job gets submitted.",
        "Question_gpt_summary":"user encount challeng try access web servic deploi studio batch execut sampl code python user fix issu face differ error relat token credenti user receiv warn instal azur sdk python user seek help resolv instal issu error relat token credenti addition user face httperror job get submit",
        "Answer_original_content":"think code pretti old point latest version azur storag blob pip instal azur storag blob updat simpli uninstal reinstal help got latest version try creat blob text method load file storag contain azur storag blob import blockblobservic blobservic blockblobservic account accountnam account kei accountkei blobservic creat blob text containernam filenam csv file hope work help lead right path work",
        "Answer_preprocessed_content":"think code pretti old point latest version simpli uninstal reinstal help got latest version try method load file storag contain hope work help lead right path work",
        "Answer_gpt_summary_original":"Solutions provided:\n- Update the version of 'azure.storage.blob' using 'pip install azure.storage.blob --update' or uninstalling and reinstalling it.\n- Use the 'create_blob_from_text' method to load the file to the storage container.",
        "Answer_gpt_summary":"solut provid updat version azur storag blob pip instal azur storag blob updat uninstal reinstal us creat blob text method load file storag contain"
    },
    {
        "Question_title":"R web scraping in Azure ML errors out",
        "Question_body":"<p>I have written a script in RStudio (running R 3.5.2) that scrapes data from a particular website. The script reaches out to a website, uses download.file to pull the underlying code, and uses tags to extract the desired data.<\/p>\n\n<p>The script runs without error in RStudio, but when I try to run the code in the \"Execute R Script\" node in Azure ML it throws a 0063 error saying that it \"cannot reach URL \". The code runs perfectly up until it tries to reach out to the URL. (see code below)<\/p>\n\n<p>I have tried switching the R version in Azure ML--neither of the 3 options work.<\/p>\n\n<pre class=\"lang-r prettyprint-override\"><code>for(a in 1:length(job_url)) {\n     download.file(url, destfile = filename, quiet=TRUE)\n      ...\n}\n<\/code><\/pre>\n\n<p>I expect the script to run the same in RStudio and Azure ML. Any ideas how to get this script to run in Azure ML the same way it runs in RStudio?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1556739448177,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":84.0,
        "Answer_body":"<blockquote>\n  <p>For security reasons, all networking from or to R code in Execute R Script modules is blocked by Azure.<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/execute-r-script#networking\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/execute-r-script#networking<\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55941720",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1556746082267,
        "Question_original_content":"web scrape error written script rstudio run scrape data particular websit script reach websit us download file pull underli code us tag extract desir data script run error rstudio try run code execut script node throw error sai reach url code run perfectli tri reach url code tri switch version option work length job url download file url destfil filenam quiet true expect script run rstudio idea script run wai run rstudio",
        "Question_preprocessed_content":"web scrape error written script rstudio scrape data particular websit script reach websit us pull underli code us tag extract desir data script run error rstudio try run code execut script node throw error sai reach url code run perfectli tri reach url tri switch version option work expect script run rstudio idea script run wai run rstudio",
        "Question_gpt_summary_original":"The user has encountered challenges while trying to run a web scraping script in Azure ML. The script runs without errors in RStudio but throws a 0063 error in Azure ML, indicating that it cannot reach the URL. The user has tried switching the R version in Azure ML, but none of the options work. The user is seeking help to get the script to run in Azure ML the same way it runs in RStudio.",
        "Question_gpt_summary":"user encount challeng try run web scrape script script run error rstudio throw error indic reach url user tri switch version option work user seek help script run wai run rstudio",
        "Answer_original_content":"secur reason network code execut script modul block azur http doc microsoft com azur machin learn studio modul refer execut script network",
        "Answer_preprocessed_content":"secur reason network code execut script modul block azur",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion. The user encountered a challenge due to the security restrictions in Azure ML that block networking from or to R code in Execute R Script modules.",
        "Answer_gpt_summary":"solut solut provid discuss user encount challeng secur restrict block network code execut script modul"
    },
    {
        "Question_title":"Pull data from Gdrive in GH actions?",
        "Question_body":"<p>Hi, I was wondering how I have to setup the github actions yaml file, so that it pulls the data and the model from google drive? Maybe I need to make the Gdrive folder public, or add my Gdrive credentials? Right now, the sanity-check fails using the workflow file from the video.<\/p>\n<p>test.yaml:<\/p>\n<pre><code class=\"lang-auto\">name: auto-testing\non: [push]\njobs:\n  run:\n    runs-on: [ubuntu-latest]\n    container: docker:\/\/dvcorg\/cml-py3:latest\n    steps:\n      - uses: actions\/checkout@v2\n      - name: sanity-check\n        env:\n          repo_token: ${{ secrets.GITHUB_TOKEN }}\n        run: |\n          # Your ML workflow goes here\n          pip install -r requirements.txt\n          python test.py\n<\/code><\/pre>",
        "Question_answer_count":10,
        "Question_comment_count":0,
        "Question_creation_time":1657891978346,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":228.0,
        "Answer_body":"<aside class=\"quote no-group\" data-username=\"rmbzmb\" data-post=\"1\" data-topic=\"1246\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/r\/ed8c4c\/40.png\" class=\"avatar\"> rmbzmb:<\/div>\n<blockquote>\n<p>Hi, I was wondering how I have to setup the github actions yaml file, so that it pulls the data and the model from google drive? Maybe I need to make the Gdrive folder public, or add my Gdrive credentials? Right now, the sanity-check fails using the workflow file from the video.<\/p>\n<\/blockquote>\n<\/aside>\n<p>If you want to combine DVC with Github actions to achieve some CI automation, maybe you should take a look at our another product <a href=\"https:\/\/cml.dev\/\" rel=\"noopener nofollow ugc\">CML<\/a>. In its <a href=\"https:\/\/github.com\/iterative\/cml#using-cml-with-dvc\" rel=\"noopener nofollow ugc\">documents<\/a> it provides some info on how to setup this.<\/p>\n<blockquote>\n<p>env:<br>\nGDRIVE_CREDENTIALS_DATA: ${{ secrets.GDRIVE_CREDENTIALS_DATA }}<\/p>\n<\/blockquote>. <p><a href=\"https:\/\/help.talend.com\/r\/en-US\/7.2\/google-drive\/how-to-access-google-drive-using-client-secret-json-file-the\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https:\/\/help.talend.com\/r\/en-US\/7.2\/google-drive\/how-to-access-google-drive-using-client-secret-json-file-the<\/a><\/p>\n<ul>\n<li>First, you have to enable the Google Drive API.<\/li>\n<li>Then add a service account and create credentials as a JSON file.<\/li>\n<li>That file can then be placed in the <code>.dvc\/tmp\/<\/code> folder.<\/li>\n<li>Then you need to modify the storage:<\/li>\n<\/ul>\n<pre><code class=\"lang-auto\">dvc remote modify storage --local gdrive_user_credentials_file .dvc\/tmp\/gdrive-credentials.json\n<\/code><\/pre>\n<p>However, in my case, that led to the following error:<\/p>\n<pre><code class=\"lang-auto\">dvc pull\nERROR: configuration error - GDrive remote auth failed with credentials in '...\/.dvc\/tmp\/gdrive-credentials.json'.\nBackup first, remove or fix them, and run again.\nIt should do auth again and refresh the credentials.\n\nDetails:: '_module'\nERROR: GDrive remote auth failed with credentials in '...\/.dvc\/tmp\/gdrive-credentials.json'.\nBackup first, remove or fix them, and run again.\nIt should do auth again and refresh the credentials.\n\nDetails:\nLearn more about configuration settings at &lt;https:\/\/man.dvc.org\/remote\/modify&gt;.\n<\/code><\/pre>. <p>Excuse me, does this credentials file works on a local computer?<\/p>. <p>Hi, Could you please try to test it locally if it fails because Github Action or Because of credential problem?<\/p>. <p><a class=\"mention\" href=\"\/u\/rmbzmb\">@rmbzmb<\/a> if you are using a service account you need to follow the instruction here:<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#using-service-accounts\">\n  <header class=\"source\">\n      <img src=\"https:\/\/dvc.org\/favicon-32x32.png?v=dfbc4a93a926127fc4495e9d640409f8\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#using-service-accounts\" target=\"_blank\" rel=\"noopener\">Data Version Control \u00b7 DVC<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/388;\"><img src=\"https:\/\/dvc.org\/social-share.png\" class=\"thumbnail\" width=\"690\" height=\"388\"><\/div>\n\n<h3><a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#using-service-accounts\" target=\"_blank\" rel=\"noopener\">Setup Google Drive Remote<\/a><\/h3>\n\n  <p>Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>Namely, you would not use the <code>gdrive_user_credentials_file<\/code>, instead you should specify:<\/p>\n<pre><code class=\"lang-auto\">dvc remote modify myremote gdrive_use_service_account true\ndvc remote modify myremote --local \\\n              gdrive_service_account_json_file_path path\/to\/file.json\n<\/code><\/pre>\n<p>On CI you can then set the <code>GDRIVE_CREDENTIALS_DATA<\/code> to the content of the JSON file with the service account credentials.<\/p>\n<p>Please let us know if this still doesn\u2019t work.<\/p>. <p>That means, I have to use the json file (or the content) on both the local version and the GitHub version? I cannot not use the json file locally and <code>GDRIVE_CREDENTIALS_DATA<\/code> on GitHub at the same time?<\/p>\n<p>I tried it now. Locally it the json file works, on GitHub, I get the same error message as before.<br>\nIs there an example yaml file available somewhere?<\/p>\n<pre><code class=\"lang-auto\">name: auto-testing\non: [push]\njobs:\n  run:\n    runs-on: [ubuntu-latest]\n    container: docker:\/\/dvcorg\/cml-py3:latest\n    steps:\n      - uses: actions\/checkout@v2\n      - name: sanity-check\n        env:\n          repo_token: ${{ secrets.GITHUB_TOKEN }}\n          GDRIVE_CREDENTIALS_DATA : ${{ secrets.GDRIVE_CREDENTIALS_DATA }}          \n\n        run: |\n          # Your ML workflow goes here\n          pip install -r requirements.txt\n          dvc pull data\n          dvc repro\n<\/code><\/pre>\n<p>The data pull still fails with the same error message.<\/p>\n<pre><code class=\"lang-auto\">WARNING: You are using pip version 21.1; however, version 21.3.1 is available.\nYou should consider upgrading via the '\/usr\/bin\/python3 -m pip install --upgrade pip' command.\n\/usr\/local\/lib\/python3.6\/dist-packages\/pycaret\/loggers\/mlflow_logger.py:14: FutureWarning: MLflow support for Python 3.6 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.6 workflows that use MLflow will continue to work without modification, but Python 3.6 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3.7 or newer.\nPycaret: 2.3.10\n  import mlflow\nTraceback (most recent call last):\n  File \"src\/test.py\", line 55, in &lt;module&gt;\n    (x_train, y_train), (x_test, y_test)  = mdl.load_data()\n  File \"src\/test.py\", line 44, in load_data\n    x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n  File \"src\/test.py\", line 22, in read_images_labels\n    with open(labels_filepath, 'rb') as file:\nFileNotFoundError: [Errno 2] No such file or directory: 'data\/MINST\/train\/train-labels-idx1-ubyte'\nError: Process completed with exit code 1.\n\n<\/code><\/pre>\n<p>Is it enough to pull the parent directory of all the data files, or do I need to pull them all individually specifying all the file names?<\/p>. <aside class=\"quote no-group\" data-username=\"rmbzmb\" data-post=\"9\" data-topic=\"1246\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/r\/ed8c4c\/40.png\" class=\"avatar\"> rmbzmb:<\/div>\n<blockquote>\n<p>I tried it now. Locally it the json file works, on GitHub, I get the same error message as before.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Hi <a class=\"mention\" href=\"\/u\/rmbzmb\">@rmbzmb<\/a> You don\u2019t need to use a json file, you just need to copy the contents of json file and set it to the <code>GDRIVE_CREDENTIALS_DATA<\/code> on Github.<\/p>. <p>Yes, I am using the JSON file only on my local computer and on a remote workstation. On GitHub I copy-pasted the content of the JSON file into a GitHub secret named <code>GDRIVE_CREDENTIALS_DATA<\/code>. But it is still not working.<\/p>\n<pre><code class=\"lang-auto\">    ...\n    GDRIVE_CREDENTIALS_DATA : ${{ secrets.GDRIVE_CREDENTIALS_DATA }}\n    ....\n<\/code><\/pre>. <p>It seems dvc on GitHub is ignoring <code>GDRIVE_CREDENTIALS_DATA<\/code>, even though it is set.<\/p>\n<blockquote>\n<p><a href=\"https:\/\/github.com\/sorenwacker\/dvc-test-project-01\/runs\/7390516277?check_suite_focus=true#step:4:46\" rel=\"noopener nofollow ugc\">45<\/a>ERROR: failed to pull data from the cloud - To use service account, set <code>gdrive_service_account_json_file_path<\/code>, and optionally<code>gdrive_service_account_user_email<\/code> in DVC config<\/p>\n<\/blockquote>. <p>Seems there is an error in dvc currently: <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/7949\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">gdrive: raises unexpected error - name: drive version: v2 (again) \u00b7 Issue #7949 \u00b7 iterative\/dvc \u00b7 GitHub<\/a><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/pull-data-from-gdrive-in-gh-actions\/1246",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-16T03:55:07.806Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"rmbzmb\" data-post=\"1\" data-topic=\"1246\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/r\/ed8c4c\/40.png\" class=\"avatar\"> rmbzmb:<\/div>\n<blockquote>\n<p>Hi, I was wondering how I have to setup the github actions yaml file, so that it pulls the data and the model from google drive? Maybe I need to make the Gdrive folder public, or add my Gdrive credentials? Right now, the sanity-check fails using the workflow file from the video.<\/p>\n<\/blockquote>\n<\/aside>\n<p>If you want to combine DVC with Github actions to achieve some CI automation, maybe you should take a look at our another product <a href=\"https:\/\/cml.dev\/\" rel=\"noopener nofollow ugc\">CML<\/a>. In its <a href=\"https:\/\/github.com\/iterative\/cml#using-cml-with-dvc\" rel=\"noopener nofollow ugc\">documents<\/a> it provides some info on how to setup this.<\/p>\n<blockquote>\n<p>env:<br>\nGDRIVE_CREDENTIALS_DATA: ${{ secrets.GDRIVE_CREDENTIALS_DATA }}<\/p>\n<\/blockquote>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-16T08:23:30.675Z",
                "Answer_body":"<p><a href=\"https:\/\/help.talend.com\/r\/en-US\/7.2\/google-drive\/how-to-access-google-drive-using-client-secret-json-file-the\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https:\/\/help.talend.com\/r\/en-US\/7.2\/google-drive\/how-to-access-google-drive-using-client-secret-json-file-the<\/a><\/p>\n<ul>\n<li>First, you have to enable the Google Drive API.<\/li>\n<li>Then add a service account and create credentials as a JSON file.<\/li>\n<li>That file can then be placed in the <code>.dvc\/tmp\/<\/code> folder.<\/li>\n<li>Then you need to modify the storage:<\/li>\n<\/ul>\n<pre><code class=\"lang-auto\">dvc remote modify storage --local gdrive_user_credentials_file .dvc\/tmp\/gdrive-credentials.json\n<\/code><\/pre>\n<p>However, in my case, that led to the following error:<\/p>\n<pre><code class=\"lang-auto\">dvc pull\nERROR: configuration error - GDrive remote auth failed with credentials in '...\/.dvc\/tmp\/gdrive-credentials.json'.\nBackup first, remove or fix them, and run again.\nIt should do auth again and refresh the credentials.\n\nDetails:: '_module'\nERROR: GDrive remote auth failed with credentials in '...\/.dvc\/tmp\/gdrive-credentials.json'.\nBackup first, remove or fix them, and run again.\nIt should do auth again and refresh the credentials.\n\nDetails:\nLearn more about configuration settings at &lt;https:\/\/man.dvc.org\/remote\/modify&gt;.\n<\/code><\/pre>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-17T02:45:37.868Z",
                "Answer_body":"<p>Excuse me, does this credentials file works on a local computer?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T04:58:29.997Z",
                "Answer_body":"<p>Hi, Could you please try to test it locally if it fails because Github Action or Because of credential problem?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T05:04:18.014Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/rmbzmb\">@rmbzmb<\/a> if you are using a service account you need to follow the instruction here:<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#using-service-accounts\">\n  <header class=\"source\">\n      <img src=\"https:\/\/dvc.org\/favicon-32x32.png?v=dfbc4a93a926127fc4495e9d640409f8\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#using-service-accounts\" target=\"_blank\" rel=\"noopener\">Data Version Control \u00b7 DVC<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/388;\"><img src=\"https:\/\/dvc.org\/social-share.png\" class=\"thumbnail\" width=\"690\" height=\"388\"><\/div>\n\n<h3><a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#using-service-accounts\" target=\"_blank\" rel=\"noopener\">Setup Google Drive Remote<\/a><\/h3>\n\n  <p>Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>Namely, you would not use the <code>gdrive_user_credentials_file<\/code>, instead you should specify:<\/p>\n<pre><code class=\"lang-auto\">dvc remote modify myremote gdrive_use_service_account true\ndvc remote modify myremote --local \\\n              gdrive_service_account_json_file_path path\/to\/file.json\n<\/code><\/pre>\n<p>On CI you can then set the <code>GDRIVE_CREDENTIALS_DATA<\/code> to the content of the JSON file with the service account credentials.<\/p>\n<p>Please let us know if this still doesn\u2019t work.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T08:42:14.661Z",
                "Answer_body":"<p>That means, I have to use the json file (or the content) on both the local version and the GitHub version? I cannot not use the json file locally and <code>GDRIVE_CREDENTIALS_DATA<\/code> on GitHub at the same time?<\/p>\n<p>I tried it now. Locally it the json file works, on GitHub, I get the same error message as before.<br>\nIs there an example yaml file available somewhere?<\/p>\n<pre><code class=\"lang-auto\">name: auto-testing\non: [push]\njobs:\n  run:\n    runs-on: [ubuntu-latest]\n    container: docker:\/\/dvcorg\/cml-py3:latest\n    steps:\n      - uses: actions\/checkout@v2\n      - name: sanity-check\n        env:\n          repo_token: ${{ secrets.GITHUB_TOKEN }}\n          GDRIVE_CREDENTIALS_DATA : ${{ secrets.GDRIVE_CREDENTIALS_DATA }}          \n\n        run: |\n          # Your ML workflow goes here\n          pip install -r requirements.txt\n          dvc pull data\n          dvc repro\n<\/code><\/pre>\n<p>The data pull still fails with the same error message.<\/p>\n<pre><code class=\"lang-auto\">WARNING: You are using pip version 21.1; however, version 21.3.1 is available.\nYou should consider upgrading via the '\/usr\/bin\/python3 -m pip install --upgrade pip' command.\n\/usr\/local\/lib\/python3.6\/dist-packages\/pycaret\/loggers\/mlflow_logger.py:14: FutureWarning: MLflow support for Python 3.6 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.6 workflows that use MLflow will continue to work without modification, but Python 3.6 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3.7 or newer.\nPycaret: 2.3.10\n  import mlflow\nTraceback (most recent call last):\n  File \"src\/test.py\", line 55, in &lt;module&gt;\n    (x_train, y_train), (x_test, y_test)  = mdl.load_data()\n  File \"src\/test.py\", line 44, in load_data\n    x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n  File \"src\/test.py\", line 22, in read_images_labels\n    with open(labels_filepath, 'rb') as file:\nFileNotFoundError: [Errno 2] No such file or directory: 'data\/MINST\/train\/train-labels-idx1-ubyte'\nError: Process completed with exit code 1.\n\n<\/code><\/pre>\n<p>Is it enough to pull the parent directory of all the data files, or do I need to pull them all individually specifying all the file names?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T10:58:15.266Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"rmbzmb\" data-post=\"9\" data-topic=\"1246\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/r\/ed8c4c\/40.png\" class=\"avatar\"> rmbzmb:<\/div>\n<blockquote>\n<p>I tried it now. Locally it the json file works, on GitHub, I get the same error message as before.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Hi <a class=\"mention\" href=\"\/u\/rmbzmb\">@rmbzmb<\/a> You don\u2019t need to use a json file, you just need to copy the contents of json file and set it to the <code>GDRIVE_CREDENTIALS_DATA<\/code> on Github.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T11:26:26.514Z",
                "Answer_body":"<p>Yes, I am using the JSON file only on my local computer and on a remote workstation. On GitHub I copy-pasted the content of the JSON file into a GitHub secret named <code>GDRIVE_CREDENTIALS_DATA<\/code>. But it is still not working.<\/p>\n<pre><code class=\"lang-auto\">    ...\n    GDRIVE_CREDENTIALS_DATA : ${{ secrets.GDRIVE_CREDENTIALS_DATA }}\n    ....\n<\/code><\/pre>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T13:40:29.695Z",
                "Answer_body":"<p>It seems dvc on GitHub is ignoring <code>GDRIVE_CREDENTIALS_DATA<\/code>, even though it is set.<\/p>\n<blockquote>\n<p><a href=\"https:\/\/github.com\/sorenwacker\/dvc-test-project-01\/runs\/7390516277?check_suite_focus=true#step:4:46\" rel=\"noopener nofollow ugc\">45<\/a>ERROR: failed to pull data from the cloud - To use service account, set <code>gdrive_service_account_json_file_path<\/code>, and optionally<code>gdrive_service_account_user_email<\/code> in DVC config<\/p>\n<\/blockquote>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-19T10:19:12.228Z",
                "Answer_body":"<p>Seems there is an error in dvc currently: <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/7949\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">gdrive: raises unexpected error - name: drive version: v2 (again) \u00b7 Issue #7949 \u00b7 iterative\/dvc \u00b7 GitHub<\/a><\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"pull data gdrive action wonder setup github action yaml file pull data model googl drive mayb need gdrive folder public add gdrive credenti right saniti check fail workflow file video test yaml auto test push job run run ubuntu latest contain docker org cml latest step us action checkout saniti check env repo token secret github token run workflow goe pip instal requir txt python test",
        "Question_preprocessed_content":"pull data gdrive action wonder setup github action yaml file pull data model googl drive mayb need gdrive folder public add gdrive credenti right fail workflow file video",
        "Question_gpt_summary_original":"The user is facing challenges in setting up a Github actions yaml file to pull data and a model from Google Drive. They are unsure whether they need to make the Gdrive folder public or add their Gdrive credentials. The sanity-check is failing using the workflow file from the video.",
        "Question_gpt_summary":"user face challeng set github action yaml file pull data model googl drive unsur need gdrive folder public add gdrive credenti saniti check fail workflow file video",
        "Answer_original_content":"rmbzmb wonder setup github action yaml file pull data model googl drive mayb need gdrive folder public add gdrive credenti right saniti check fail workflow file video want combin github action achiev autom mayb look product cml document provid info setup env gdrive credenti data secret gdrive credenti data http help talend com googl drive access googl drive client secret json file enabl googl drive api add servic account creat credenti json file file place tmp folder need modifi storag remot modifi storag local gdrive user credenti file tmp gdrive credenti json case led follow error pull error configur error gdrive remot auth fail credenti tmp gdrive credenti json backup remov fix run auth refresh credenti detail modul error gdrive remot auth fail credenti tmp gdrive credenti json backup remov fix run auth refresh credenti detail learn configur set excus credenti file work local try test local fail github action credenti problem rmbzmb servic account need follow instruct data version control setup googl drive remot open sourc version control data scienc machin learn project git like experi organ data model experi us gdrive user credenti file instead specifi remot modifi myremot gdrive us servic account true remot modifi myremot local gdrive servic account json file path path file json set gdrive credenti data content json file servic account credenti let know doesnt work mean us json file content local version github version us json file local gdrive credenti data github time tri local json file work github error messag exampl yaml file avail auto test push job run run ubuntu latest contain docker org cml latest step us action checkout saniti check env repo token secret github token gdrive credenti data secret gdrive credenti data run workflow goe pip instal requir txt pull data repro data pull fail error messag warn pip version version avail consid upgrad usr bin python pip instal upgrad pip command usr local lib python dist packag pycaret logger logger futurewarn support python deprec drop upcom releas point exist python workflow us continu work modif python user longer access latest featur bugfix recommend upgrad python newer pycaret import traceback recent file src test line train train test test mdl load data file src test line load data train train self read imag label self train imag filepath self train label filepath file src test line read imag label open label filepath file filenotfounderror errno file directori data minst train train label idx ubyt error process complet exit code pull parent directori data file need pull individu specifi file name rmbzmb tri local json file work github error messag rmbzmb dont need us json file need copi content json file set gdrive credenti data github ye json file local remot workstat github copi past content json file github secret name gdrive credenti data work gdrive credenti data secret gdrive credenti data github ignor gdrive credenti data set error fail pull data cloud us servic account set gdrive servic account json file path optionallygdr servic account user email config error current gdrive rais unexpect error drive version issu iter github",
        "Answer_preprocessed_content":"rmbzmb wonder setup github action yaml file pull data model googl drive mayb need gdrive folder public add gdrive credenti right fail workflow file video want combin github action achiev autom mayb look product cml document provid info setup env enabl googl drive api add servic account creat credenti json file file place folder need modifi storag case led follow error excus credenti file work local try test local fail github action credenti problem servic account need follow instruct data version control setup googl drive remot version control data scienc machin learn project experi organ data model experi us instead specifi set content json file servic account credenti let know doesnt work mean us json file local version github version us json file local github time tri local json file work github error messag exampl yaml file avail data pull fail error messag pull parent directori data file need pull individu specifi file name rmbzmb tri local json file work github error messag dont need us json file need copi content json file set github ye json file local remot workstat github content json file github secret name work github ignor set error fail pull data cloud us servic account set option config error current gdrive rais unexpect error drive version issu iter github",
        "Answer_gpt_summary_original":"Solutions provided in the discussion are:\n\n- Use CML instead of Github actions to achieve CI automation.\n- Enable the Google Drive API, add a service account, create credentials as a JSON file, place the file in the .dvc\/tmp\/ folder, and modify the storage. On CI, set the GDRIVE_CREDENTIALS_DATA to the content of the JSON file with the service account credentials.\n- Copy the contents of the JSON file and set it to the GDRIVE_CREDENTIALS_DATA on Github.\n- Set gdrive_service_account_json_file_path and optionally gdrive_service_account_user_email in DVC config to use a service account. \n\nHowever, the user is still facing issues with the data pull failing with an error message",
        "Answer_gpt_summary":"solut provid discuss us cml instead github action achiev autom enabl googl drive api add servic account creat credenti json file place file tmp folder modifi storag set gdrive credenti data content json file servic account credenti copi content json file set gdrive credenti data github set gdrive servic account json file path option gdrive servic account user email config us servic account user face issu data pull fail error messag"
    },
    {
        "Question_title":"Vertex AI create endpoint error - FAILED_PRECONDITION: Project xxxxxxxx is not active.",
        "Question_body":"Hi, I'm stuck at following error message when I try to create vertex-ai endpoint from workbench notebook.  I have enabled aiplatform.googleapis.com.Command:\ngcloud ai endpoints create \\\n--project=XXXXX\n--region=us-central1 \\\n--display-name=ld-test-resnet-classifierUsing endpoint [https:\/\/us-central1-aiplatform.googleapis.com\/]\nERROR: (gcloud.ai.endpoints.create) FAILED_PRECONDITION: Project XXXXXXXXXX is not active.Please suggest what am I missing.   ",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661561100000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":144.0,
        "Answer_body":"Hi,\nThe issue is resolved.\nAt least one model has to be uploaded first to model registry for this command to work.\nThe official documentation titled \"Deploy a model using the Vertex AI API\" - implies deploy a model uploaded to model registry\".\n\nThanks for the views.\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-create-endpoint-error-FAILED-PRECONDITION-Project\/td-p\/460565\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-27T04:38:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hi,\nThe issue is resolved.\nAt least one model has to be uploaded first to model registry for this command to work.\nThe official documentation titled \"Deploy a model using the Vertex AI API\" - implies deploy a model uploaded to model registry\".\n\nThanks for the views.\n\nView solution in original post"
            },
            {
                "Answer_creation_time":"2022-08-27T04:38:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hi,\nThe issue is resolved.\nAt least one model has to be uploaded first to model registry for this command to work.\nThe official documentation titled \"Deploy a model using the Vertex AI API\" - implies deploy a model uploaded to model registry\".\n\nThanks for the views."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1661575080000,
        "Question_original_content":"creat endpoint error fail precondit project activ stuck follow error messag try creat vertex endpoint workbench notebook enabl aiplatform googleapi com command gcloud endpoint creat project region central displai test resnet classifierus endpoint http central aiplatform googleapi com error gcloud endpoint creat fail precondit project activ suggest miss",
        "Question_preprocessed_content":"creat endpoint error project activ stuck follow error messag try creat endpoint workbench notebook enabl gcloud endpoint creat project endpoint error project suggest miss",
        "Question_gpt_summary_original":"The user encountered an error message when trying to create a Vertex AI endpoint from a workbench notebook. The error message stated that the project was not active, resulting in a FAILED_PRECONDITION error. The user is seeking assistance in resolving the issue.",
        "Question_gpt_summary":"user encount error messag try creat endpoint workbench notebook error messag state project activ result fail precondit error user seek assist resolv issu",
        "Answer_original_content":"issu resolv model upload model registri command work offici document titl deploi model api impli deploi model upload model registri thank view view solut origin post",
        "Answer_preprocessed_content":"issu resolv model upload model registri command work offici document titl deploi model api impli deploi model upload model registri thank view view solut origin post",
        "Answer_gpt_summary_original":"Solution: The issue was resolved by uploading at least one model to the model registry before creating a Vertex AI endpoint. The official documentation titled \"Deploy a model using the Vertex AI API\" implies that a model uploaded to the model registry is required for this command to work.",
        "Answer_gpt_summary":"solut issu resolv upload model model registri creat endpoint offici document titl deploi model api impli model upload model registri requir command work"
    },
    {
        "Question_title":"How do I give Vertex AI pipeline component permissions?",
        "Question_body":"<p>In a Vertex AI pipeline component,I try:<\/p>\n<pre><code>def my_comp(project_id: str, location: str, endpoint_id: str, endpoint: Output[Artifact]):\n    import google.cloud.aiplatform as aip\n    endpoints = aip.Endpoint.list()\n...\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code>'aiplatform.endpoints.list' denied on resource '\/\/aiplatform.googleapis.com\/projects\/...\n<\/code><\/pre>\n<p>My service account has owner permissions, and it works outside of the component. What do I need to do?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1662721061203,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":62.0,
        "Answer_body":"<p>This permission denied on resource issue can be resolved by using import statement:<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73661090",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1663223342092,
        "Question_original_content":"pipelin compon permiss pipelin compon try def comp project str locat str endpoint str endpoint output artifact import googl cloud aiplatform aip endpoint aip endpoint list give aiplatform endpoint list deni resourc aiplatform googleapi com project servic account owner permiss work outsid compon need",
        "Question_preprocessed_content":"pipelin compon permiss pipelin compon try give servic account owner permiss work outsid compon need",
        "Question_gpt_summary_original":"The user is encountering a challenge in giving Vertex AI pipeline component permissions. They are receiving an error message stating that 'aiplatform.endpoints.list' is denied on a resource, despite having owner permissions on their service account. The user is seeking advice on how to resolve this issue.",
        "Question_gpt_summary":"user encount challeng give pipelin compon permiss receiv error messag state aiplatform endpoint list deni resourc despit have owner permiss servic account user seek advic resolv issu",
        "Answer_original_content":"permiss deni resourc issu resolv import statement googl cloud import aiplatform aiplatform",
        "Answer_preprocessed_content":"permiss deni resourc issu resolv import statement",
        "Answer_gpt_summary_original":"Solution: The user can resolve the permission denied on resource issue by using the import statement \"from google.cloud import aiplatform_v1 as aiplatform\".",
        "Answer_gpt_summary":"solut user resolv permiss deni resourc issu import statement googl cloud import aiplatform aiplatform"
    },
    {
        "Question_title":"Facing issues in gitlab-runner for ci-cml and face issue to use AWS-s3 bucket for dvc in gitlab",
        "Question_body":"<p>I am working on Mlops. so train.py is my script in which i am using dvc for data versioning and train the model by using s3 bucket. this all thing i push on gitlab and I have to create ci-cml pipeline, but not able to use s3 bucket to use dvc data and facing error when try run .gitlab-runner so find error in gitlab-runner.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1662585071904,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":86.0,
        "Answer_body":"<p>I am working on Mlops. so train.py is my script in which i am using dvc for data versioning and train the model by using s3 bucket. this all thing i push on gitlab and I have to create ci-cml pipeline, but not able to use s3 bucket to use dvc data and facing error when try run .gitlab-runner so find error in gitlab-runner.<\/p>. <p>What is the error message that you get?<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/facing-issues-in-gitlab-runner-for-ci-cml-and-face-issue-to-use-aws-s3-bucket-for-dvc-in-gitlab\/1332",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-07T21:13:25.421Z",
                "Answer_body":"<p>I am working on Mlops. so train.py is my script in which i am using dvc for data versioning and train the model by using s3 bucket. this all thing i push on gitlab and I have to create ci-cml pipeline, but not able to use s3 bucket to use dvc data and facing error when try run .gitlab-runner so find error in gitlab-runner.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-13T07:43:25.229Z",
                "Answer_body":"<p>What is the error message that you get?<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"face issu gitlab runner cml face issu us aw bucket gitlab work mlop train script data version train model bucket thing push gitlab creat cml pipelin abl us bucket us data face error try run gitlab runner error gitlab runner",
        "Question_preprocessed_content":"face issu face issu us bucket gitlab work mlop script data version train model bucket thing push gitlab creat pipelin abl us bucket us data face error try run error",
        "Question_gpt_summary_original":"The user is facing challenges in using gitlab-runner for ci-cml and is unable to use AWS-s3 bucket for dvc in gitlab. They are working on Mlops and using dvc for data versioning and training the model by using s3 bucket. However, they are encountering errors when trying to run .gitlab-runner and are unable to use the s3 bucket to access dvc data.",
        "Question_gpt_summary":"user face challeng gitlab runner cml unabl us aw bucket gitlab work mlop data version train model bucket encount error try run gitlab runner unabl us bucket access data",
        "Answer_original_content":"work mlop train script data version train model bucket thing push gitlab creat cml pipelin abl us bucket us data face error try run gitlab runner error gitlab runner error messag",
        "Answer_preprocessed_content":"work mlop script data version train model bucket thing push gitlab creat pipelin abl us bucket us data face error try run error error messag",
        "Answer_gpt_summary_original":"Solution: No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut solut provid discuss"
    },
    {
        "Question_title":"SageMaker deploy custom script",
        "Question_body":"<p>I'm pretty new to SageMaker, so I'm sorry if I miss something obvious.<\/p>\n\n<p>I've trained a DL model which uses frames from a video to make a prediction. The current script, that runs in the SageMaker jupyter-notebook, takes a video URL as an input and uses an FFMPEG subprocess pipe to extract the frames and predict them afterwards. This works fine, but now I want to start that script from Lambda.<\/p>\n\n<p>As far as I understood, I could deploy my model with sagemaker and make predictions for every single frame from Lambda, unfortunately this is not an option, as ffprobe, ffmpeg and numpy are too large to fit into the limited lambda space.<\/p>\n\n<p>tl;dr: Is it possible to run my custom script (ffmpeg frame extraction + tensorflow model prediction) as an endpoint in SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1580859395073,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":386.0,
        "Answer_body":"<p>Sagemaker allows you to use a custom Docker image (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms.html\" rel=\"nofollow noreferrer\">AWS document<\/a>)<\/p>\n\n<blockquote>\n  <p>Build your own custom container image: If there is no pre-built Amazon\n  SageMaker container image that you can use or modify for an advanced\n  scenario, you can package your own script or algorithm to use with\n  Amazon SageMaker.You can use any programming language or framework to\n  develop your container<\/p>\n<\/blockquote>\n\n<ul>\n<li>Create a docker image with your code (FFmpeg, TensorFlow)<\/li>\n<li>Testing the docker container locally<\/li>\n<li>Deploying the image on Amazon ECR (Elastic Container Repository)<\/li>\n<li>Create a SageMaker model and point to the image<\/li>\n<\/ul>\n\n<p>For details, you can learn more from <a href=\"https:\/\/towardsdatascience.com\/brewing-up-custom-ml-models-on-aws-sagemaker-e09b64627722\" rel=\"nofollow noreferrer\">this tutorial<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60067075",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1580863000328,
        "Question_original_content":"deploi custom script pretti new sorri miss obviou train model us frame video predict current script run jupyt notebook take video url input us ffmpeg subprocess pipe extract frame predict work fine want start script lambda far understood deploi model predict singl frame lambda unfortun option ffprobe ffmpeg numpi larg fit limit lambda space possibl run custom script ffmpeg frame extract tensorflow model predict endpoint",
        "Question_preprocessed_content":"deploi custom script pretti new sorri miss obviou train model us frame video predict current script run take video url input us ffmpeg subprocess pipe extract frame predict work fine want start script lambda far understood deploi model predict singl frame lambda unfortun option ffprobe ffmpeg numpi larg fit limit lambda space possibl run custom script endpoint",
        "Question_gpt_summary_original":"The user has encountered a challenge in deploying a custom script for a DL model in SageMaker. They want to run the script from Lambda, but the required tools such as ffprobe, ffmpeg, and numpy are too large to fit into the limited Lambda space. The user is seeking a solution to run their custom script as an endpoint in SageMaker.",
        "Question_gpt_summary":"user encount challeng deploi custom script model want run script lambda requir tool ffprobe ffmpeg numpi larg fit limit lambda space user seek solut run custom script endpoint",
        "Answer_original_content":"allow us custom docker imag aw document build custom contain imag pre built amazon contain imag us modifi advanc scenario packag script algorithm us us program languag framework develop contain creat docker imag code ffmpeg tensorflow test docker contain local deploi imag amazon ecr elast contain repositori creat model point imag detail learn tutori",
        "Answer_preprocessed_content":"allow us custom docker imag build custom contain imag amazon contain imag us modifi advanc scenario packag script algorithm us us program languag framework develop contain creat docker imag code test docker contain local deploi imag amazon ecr creat model point imag detail learn tutori",
        "Answer_gpt_summary_original":"Solution: The user can create a custom Docker image with their code and required tools such as FFmpeg, TensorFlow, and numpy. They can test the Docker container locally and deploy the image on Amazon ECR (Elastic Container Repository). Then, they can create a SageMaker model and point to the image. A tutorial is provided for more details.",
        "Answer_gpt_summary":"solut user creat custom docker imag code requir tool ffmpeg tensorflow numpi test docker contain local deploi imag amazon ecr elast contain repositori creat model point imag tutori provid detail"
    },
    {
        "Question_title":"Dvc non bare remote",
        "Question_body":"<p>Hi,<br>\nlet\u2019s say I want to use dvc with azure blob storage. Is there a way that I can use the data from the blob storage directly in other azure projects? Normally a blob storage just gets mounted into another resource. With dvc this seems a problem, because files on the remote are stored differently than locally. Is there a way around that? With git for example it is possible to push into a non bare repository. Is there a similar option for dvc?<br>\nThanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1632379867905,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":204.0,
        "Answer_body":"<blockquote>\n<p>Is there a way that I can use the data from the blob storage directly in other azure projects?<\/p>\n<\/blockquote>\n<p>The data is there as is, but just formatted in a different convention to allow deduplication and simplify the process for the DVC, so you can\u2019t technically access it is as if it was present in a workspace.<\/p>\n<blockquote>\n<p>Is there a way around that?<\/p>\n<\/blockquote>\n<p>External outputs should help you on this point, this way the data is in your remote as if it were locally and DVC tracks the data on your remote not the data in your local workspace. This way, you should be able to get around this problem. See <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\">https:\/\/dvc.org\/doc\/user-guide\/managing-external-data<\/a> for more details.<\/p>. <p>You can also access the data through the Python API: <a href=\"https:\/\/dvc.org\/doc\/api-reference\">https:\/\/dvc.org\/doc\/api-reference<\/a>. You can get the URL of the data or read directly into your Python program. Alternatively, you can use the command line to download the data anywhere: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\">https:\/\/dvc.org\/doc\/command-reference\/get<\/a>.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-non-bare-remote\/901",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-23T08:00:51.805Z",
                "Answer_body":"<blockquote>\n<p>Is there a way that I can use the data from the blob storage directly in other azure projects?<\/p>\n<\/blockquote>\n<p>The data is there as is, but just formatted in a different convention to allow deduplication and simplify the process for the DVC, so you can\u2019t technically access it is as if it was present in a workspace.<\/p>\n<blockquote>\n<p>Is there a way around that?<\/p>\n<\/blockquote>\n<p>External outputs should help you on this point, this way the data is in your remote as if it were locally and DVC tracks the data on your remote not the data in your local workspace. This way, you should be able to get around this problem. See <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\">https:\/\/dvc.org\/doc\/user-guide\/managing-external-data<\/a> for more details.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-23T12:49:23.681Z",
                "Answer_body":"<p>You can also access the data through the Python API: <a href=\"https:\/\/dvc.org\/doc\/api-reference\">https:\/\/dvc.org\/doc\/api-reference<\/a>. You can get the URL of the data or read directly into your Python program. Alternatively, you can use the command line to download the data anywhere: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\">https:\/\/dvc.org\/doc\/command-reference\/get<\/a>.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"non bare remot let want us azur blob storag wai us data blob storag directli azur project normal blob storag get mount resourc problem file remot store differ local wai git exampl possibl push non bare repositori similar option thank",
        "Question_preprocessed_content":"non bare remot let want us azur blob storag wai us data blob storag directli azur project normal blob storag get mount resourc problem file remot store differ local wai git exampl possibl push non bare repositori similar option thank",
        "Question_gpt_summary_original":"The user is facing a challenge in using dvc with Azure Blob Storage as files on the remote are stored differently than locally, making it difficult to use the data from the blob storage directly in other Azure projects. The user is seeking a solution to this problem and is wondering if there is a way around it, similar to the option available in git to push into a non-bare repository.",
        "Question_gpt_summary":"user face challeng azur blob storag file remot store differ local make difficult us data blob storag directli azur project user seek solut problem wonder wai similar option avail git push non bare repositori",
        "Answer_original_content":"wai us data blob storag directli azur project data format differ convent allow dedupl simplifi process technic access present workspac wai extern output help point wai data remot local track data remot data local workspac wai abl problem http org doc user guid manag extern data detail access data python api http org doc api refer url data read directli python program altern us command line download data http org doc command refer",
        "Answer_preprocessed_content":"wai us data blob storag directli azur project data format differ convent allow dedupl simplifi process technic access present workspac wai extern output help point wai data remot local track data remot data local workspac wai abl problem detail access data python api url data read directli python program altern us command line download data",
        "Answer_gpt_summary_original":"Solution:\n- External outputs can be used to manage the data on the remote as if it were locally, allowing DVC to track the data on the remote instead of the data in the local workspace.\n- The data can also be accessed through the Python API or downloaded using the command line.",
        "Answer_gpt_summary":"solut extern output manag data remot local allow track data remot instead data local workspac data access python api download command line"
    },
    {
        "Question_title":"Vertex AI scheduled notebooks doesn't recognize existence of folders",
        "Question_body":"<p>I have a managed jupyter notebook in Vertex AI that I want to schedule. The notebook works just fine as long as I start it manually, but as soon as it is scheduled, it fails. There are in fact many things that go wrong when scheduled, some of them are fixable. Before explaining what my trouble is, let me first give some details of the context.<\/p>\n<p>The notebook gathers information from an API for several stores and saves the data in different folders before processing it, saving csv-files to store-specific folders and to bigquery. So, in the location of the notebook, I have:<\/p>\n<ul>\n<li>The notebook<\/li>\n<li>Functions needed for the handling of data (as *.py files)<\/li>\n<li>A series of folders, some of which have subfolders which also have subfolders<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xhOWs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xhOWs.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>When I execute this manually, no problem. Everything works well and all files end up exactly where they should, as well as in different bigQuery tables.<\/p>\n<p>However, when scheduling the execution of the notebook, everything goes wrong. First, the files *.py cannot be read (as <code>import<\/code>). No problem, I added the functions in the notebook.<\/p>\n<p>Now, the following error is where I am at a loss, because I have no idea why it does work or how to fix it. The code that leads to the error is the following:<\/p>\n<pre><code>internal = &quot;https:\/\/api.************************&quot;\n\ndf_descriptions = [] \n\nstoress = internal\nresponse_stores = requests.get(storess,auth = HTTPBasicAuth(userInternal, keyInternal))\npathlib.Path(&quot;stores\/request_1.json&quot;).write_bytes(response_stores.content)\n\nfilepath = &quot;stores&quot;\n\nfiles = os.listdir(filepath)\n\nfor file in files:\n    with open(filepath + &quot;\/&quot;+file) as json_string:\n        jsonstr = json.load(json_string)\n        information = pd.json_normalize(jsonstr)\n    df_descriptions.append(information)\n\nStoreINFO = pd.concat(df_descriptions)\nStoreINFO = StoreINFO.dropna()\nStoreINFO = StoreINFO[StoreINFO['storeIdMappings'].map(lambda d: len(d)) &gt; 0]\n\ncloud_store_ids = list(set(StoreINFO.cloudStoreId))\n\nLastWeek = datetime.date.today()- timedelta(days=2)\nLastWeek =np.datetime64(LastWeek)\n<\/code><\/pre>\n<p>and the error reported is:<\/p>\n<pre><code>FileNotFoundError                         Traceback (most recent call last)\n\/tmp\/ipykernel_165\/2970402631.py in &lt;module&gt;\n      5 storess = internal\n      6 response_stores = requests.get(storess,auth = HTTPBasicAuth(userInternal, keyInternal))\n----&gt; 7 pathlib.Path(&quot;stores\/request_1.json&quot;).write_bytes(response_stores.content)\n      8 \n      9 filepath = &quot;stores&quot;\n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in write_bytes(self, data)\n   1228         # type-check for the buffer interface before truncating the file\n   1229         view = memoryview(data)\n-&gt; 1230         with self.open(mode='wb') as f:\n   1231             return f.write(view)\n   1232 \n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in open(self, mode, buffering, encoding, errors, newline)\n   1206             self._raise_closed()\n   1207         return io.open(self, mode, buffering, encoding, errors, newline,\n-&gt; 1208                        opener=self._opener)\n   1209 \n   1210     def read_bytes(self):\n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in _opener(self, name, flags, mode)\n   1061     def _opener(self, name, flags, mode=0o666):\n   1062         # A stub for the opener argument to built-in open()\n-&gt; 1063         return self._accessor.open(self, flags, mode)\n   1064 \n   1065     def _raw_open(self, flags, mode=0o777):\n\nFileNotFoundError: [Errno 2] No such file or directory: 'stores\/request_1.json'\n<\/code><\/pre>\n<p>I would gladly find another way to do this, for instance by using GCS buckets, but my issue is the existence of sub-folders. There are many stores and I do not wish to do this operation manually because some retailers for which I am doing this have over 1000 stores. My python code generates all these folders and as I understand it, this is not feasible in GCS.<\/p>\n<p>How can I solve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1650711286300,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1650713970783,
        "Question_score":0.0,
        "Question_view_count":247.0,
        "Answer_body":"<p>GCS uses a flat namespace, so folders don't actually exist, but can be simulated as given in this <a href=\"https:\/\/cloud.google.com\/storage\/docs\/folders\" rel=\"nofollow noreferrer\">documentation<\/a>.For your requirement, you can either use absolute path (starting with &quot;\/&quot; -- not relative) or create the &quot;stores&quot; <a href=\"https:\/\/docs.python.org\/3\/library\/pathlib.html#pathlib.Path.mkdir\" rel=\"nofollow noreferrer\">directory<\/a> (with &quot;mkdir&quot;). For more information you can check this <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/schedule-and-execute-notebooks-with-vertex-ai-workbench\" rel=\"nofollow noreferrer\">blog<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1651402328352,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71979012",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1651402011487,
        "Question_original_content":"schedul notebook recogn exist folder manag jupyt notebook want schedul notebook work fine long start manual soon schedul fail fact thing wrong schedul fixabl explain troubl let detail context notebook gather inform api store save data differ folder process save csv file store specif folder bigqueri locat notebook notebook function need handl data file seri folder subfold subfold execut manual problem work file end exactli differ bigqueri tabl schedul execut notebook goe wrong file read import problem ad function notebook follow error loss idea work fix code lead error follow intern http api descript storess intern respons store request storess auth httpbasicauth userintern keyintern pathlib path store request json write byte respons store content filepath store file listdir filepath file file open filepath file json string jsonstr json load json string inform json normal jsonstr descript append inform storeinfo concat descript storeinfo storeinfo dropna storeinfo storeinfo storeinfo storeidmap map lambda len cloud store id list set storeinfo cloudstoreid lastweek datetim date todai timedelta dai lastweek datetim lastweek error report filenotfounderror traceback recent tmp ipykernel storess intern respons store request storess auth httpbasicauth userintern keyintern pathlib path store request json write byte respons store content filepath store opt conda lib python pathlib write byte self data type check buffer interfac truncat file view memoryview data self open mode return write view opt conda lib python pathlib open self mode buffer encod error newlin self rais close return open self mode buffer encod error newlin open self open def read byte self opt conda lib python pathlib open self flag mode def open self flag mode stub open argument built open return self accessor open self flag mode def raw open self flag mode filenotfounderror errno file directori store request json gladli wai instanc gc bucket issu exist sub folder store wish oper manual retail store python code gener folder understand feasibl gc solv issu",
        "Question_preprocessed_content":"schedul notebook recogn exist folder manag jupyt notebook want schedul notebook work fine long start manual soon schedul fail fact thing wrong schedul fixabl explain troubl let detail context notebook gather inform api store save data differ folder process save folder bigqueri locat notebook notebook function need handl data seri folder subfold subfold execut manual problem work file end exactli differ bigqueri tabl schedul execut notebook goe wrong file read problem ad function notebook follow error loss idea work fix code lead error follow error report gladli wai instanc gc bucket issu exist store wish oper manual retail store python code gener folder understand feasibl gc solv issu",
        "Question_gpt_summary_original":"The user is facing challenges with scheduling a managed Jupyter notebook in Vertex AI. The notebook gathers information from an API for several stores and saves the data in different folders before processing it, saving csv-files to store-specific folders and to bigquery. When scheduling the execution of the notebook, the files *.py cannot be read, and the code that leads to the error is related to the existence of sub-folders. The user is looking for a solution to this issue.",
        "Question_gpt_summary":"user face challeng schedul manag jupyt notebook notebook gather inform api store save data differ folder process save csv file store specif folder bigqueri schedul execut notebook file read code lead error relat exist sub folder user look solut issu",
        "Answer_original_content":"gc us flat namespac folder actual exist simul given document requir us absolut path start rel creat store directori mkdir inform check blog",
        "Answer_preprocessed_content":"gc us flat namespac folder actual exist simul given requir us absolut path creat store directori inform check blog",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are to use absolute path or create the \"stores\" directory with \"mkdir\" to simulate the existence of sub-folders in GCS. The user can refer to the documentation and blog provided for more information.",
        "Answer_gpt_summary":"possibl solut mention discuss us absolut path creat store directori mkdir simul exist sub folder gc user refer document blog provid inform"
    },
    {
        "Question_title":"Unable to parametrize ML pipeline endpoint name - Azure Data Factory",
        "Question_body":"<p>Sorry for long post, I need to explain it properly for people to undertsand.<\/p>\n<p>I have a pipeline in datafctory that triggers a published AML endpoint:\n<a href=\"https:\/\/i.stack.imgur.com\/mKIeU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mKIeU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I am trying to parametrize this ADF pipeline so that I can deploy to test and prod, but on test and prod the aml endpoints are different.<\/p>\n<p>Therefore, I have tried to edit the <strong>parameter configuration<\/strong> in ADF as shows here:\n<a href=\"https:\/\/i.stack.imgur.com\/c4g7x.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/c4g7x.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Here in the section <code>Microsoft.DataFactory\/factories\/pipelines<\/code> I add <code>&quot;*&quot;:&quot;=&quot;<\/code> so that all the pipeline parameters are parametrized:<\/p>\n<pre><code> &quot;Microsoft.DataFactory\/factories\/pipelines&quot;: {\n        &quot;*&quot;: &quot;=&quot;\n    }\n<\/code><\/pre>\n<p>After this I export the template to see which parameters are there in json, there are lot of them but I do not see any paramter that has aml endpoint name as value, but I see the endpint ID is parametrized.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/7WRUL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7WRUL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>My question is: Is it possible to parametrize the AML endpoint by name? So that, when deploying ADF to test I can just provide the AML endpoint name and it can pick the id automatically:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/0Fu1g.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0Fu1g.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661258450327,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":67.0,
        "Answer_body":"<p>i faced the similar issue when deploying adf pipelines with ml between environments. Unfortunately, As of now, adf parameter file do not have ml pipeline name as parameter value. only turn around solution is modifiying the parameter file(json) file with aligns with your pipeline design. For example, i am triggering ml pipeline endpoint inside foreach activity--&gt;if condition--&gt;ml pipeline<\/p>\n<p>Here is my parameter file values:<\/p>\n<pre><code>&quot;Microsoft.DataFactory\/factories\/pipelines&quot;: {\n    &quot;properties&quot;: {\n        &quot;activities&quot;: [\n            {\n                &quot;typeProperties&quot;: {\n                    &quot;mlPipelineEndpointId&quot;: &quot;=&quot;,\n                    &quot;url&quot;: {\n                        &quot;value&quot;: &quot;=&quot;\n                    },\n                    &quot;ifFalseActivities&quot;: [\n                        {\n                            &quot;typeProperties&quot;: {\n                                &quot;mlPipelineEndpointId&quot;: &quot;=&quot;\n                            }\n                        }\n                    ],\n                    &quot;ifTrueActivities&quot;: [\n                        {\n                            &quot;typeProperties&quot;: {\n                                &quot;mlPipelineEndpointId&quot;: &quot;=&quot;\n                            }\n                        }\n                    ],\n                    &quot;activities&quot;: [\n                        {\n                            &quot;typeProperties&quot;: {\n                                &quot;mlPipelineEndpointId&quot;: &quot;=&quot;,\n                                &quot;ifFalseActivities&quot;: [\n                                    {\n                                        &quot;typeProperties&quot;: {\n                                            &quot;mlPipelineEndpointId&quot;: &quot;=&quot;,\n                                            &quot;url&quot;: &quot;=&quot;\n                                        }\n                                    }\n                                ],\n                                &quot;ifTrueActivities&quot;: [\n                                    {\n                                        &quot;typeProperties&quot;: {\n                                            &quot;mlPipelineEndpointId&quot;: &quot;=&quot;,\n                                            &quot;url&quot;: &quot;=&quot;\n                                        }\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    }\n}\n<\/code><\/pre>\n<p>after you export the ARM template, the json file has records for your ml endpoints<\/p>\n<pre><code>&quot;ADFPIPELINE_NAME_properties_1_typeProperties_1_typeProperties_0_typeProperties_mlPipelineEndpointId&quot;: {\n        &quot;value&quot;: &quot;445xxxxx-xxxx-xxxxx-xxxxx&quot;\n<\/code><\/pre>\n<p>it is lot of manual effort to maintain if design is frequently changing so far worked for me. Hope this answers your question.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73458933",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1663188036943,
        "Question_original_content":"unabl parametr pipelin endpoint azur data factori sorri long post need explain properli peopl undertsand pipelin datafctori trigger publish aml endpoint try parametr adf pipelin deploi test prod test prod aml endpoint differ tri edit paramet configur adf show section microsoft datafactori factori pipelin add pipelin paramet parametr microsoft datafactori factori pipelin export templat paramet json lot paramt aml endpoint valu endpint parametr question possibl parametr aml endpoint deploi adf test provid aml endpoint pick automat",
        "Question_preprocessed_content":"unabl parametr pipelin endpoint azur data factori sorri long post need explain properli peopl undertsand pipelin datafctori trigger publish aml endpoint try parametr adf pipelin deploi test prod test prod aml endpoint differ tri edit paramet configur adf show section add pipelin paramet parametr export templat paramet json lot paramt aml endpoint valu endpint parametr question possibl parametr aml endpoint deploi adf test provid aml endpoint pick automat",
        "Question_gpt_summary_original":"The user is facing a challenge in parametrizing the name of an Azure Machine Learning (AML) endpoint in an Azure Data Factory (ADF) pipeline. They have tried to edit the parameter configuration in ADF to parametrize all pipeline parameters, but they cannot find any parameter that has the AML endpoint name as a value. They are asking if it is possible to parametrize the AML endpoint by name so that they can provide the endpoint name when deploying ADF to test and it can pick the ID automatically.",
        "Question_gpt_summary":"user face challeng parametr aml endpoint azur data factori adf pipelin tri edit paramet configur adf parametr pipelin paramet paramet aml endpoint valu ask possibl parametr aml endpoint provid endpoint deploi adf test pick automat",
        "Answer_original_content":"face similar issu deploi adf pipelin environ unfortun adf paramet file pipelin paramet valu turn solut modifii paramet file json file align pipelin design exampl trigger pipelin endpoint insid foreach activ condit pipelin paramet file valu microsoft datafactori factori pipelin properti activ typeproperti mlpipelineendpointid url valu iffalseact typeproperti mlpipelineendpointid iftrueact typeproperti mlpipelineendpointid activ typeproperti mlpipelineendpointid iffalseact typeproperti mlpipelineendpointid url iftrueact typeproperti mlpipelineendpointid url export arm templat json file record endpoint adfpipelin properti typeproperti typeproperti typeproperti mlpipelineendpointid valu lot manual effort maintain design frequent chang far work hope answer question",
        "Answer_preprocessed_content":"face similar issu deploi adf pipelin environ unfortun adf paramet file pipelin paramet valu turn solut modifii paramet file file align pipelin design exampl trigger pipelin endpoint insid foreach pipelin paramet file valu export arm templat json file record endpoint lot manual effort maintain design frequent chang far work hope answer question",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that currently, there is no direct way to parametrize the name of an Azure Machine Learning (AML) endpoint in an Azure Data Factory (ADF) pipeline. However, the user can modify the parameter file (JSON) to align with their pipeline design and include the AML endpoint ID. The user can export the ARM template, and the JSON file will have records for their AML endpoints. This solution requires manual effort and may not be suitable if the pipeline design frequently changes.",
        "Answer_gpt_summary":"solut discuss suggest current direct wai parametr aml endpoint azur data factori adf pipelin user modifi paramet file json align pipelin design includ aml endpoint user export arm templat json file record aml endpoint solut requir manual effort suitabl pipelin design frequent chang"
    },
    {
        "Question_title":"How to deploy AWS using CDK, sagemaker?",
        "Question_body":"<p>I want to use this <a href=\"https:\/\/github.com\/amogh147\/binance_takeHome_gemini_amogh\" rel=\"nofollow noreferrer\">repo<\/a> and I have created and activated a virtualenv and installed the required dependencies.<\/p>\n<p>I get an error when I run pytest.<\/p>\n<p>And under the file binance_cdk\/app.py it describes the following tasks:<\/p>\n<h1>App (PSVM method) entry point of the program.<\/h1>\n<h1>Note:<\/h1>\n<p>Steps tp setup CDK:<\/p>\n<ol>\n<li>install npm<\/li>\n<li>cdk -init (creates an empty project)<\/li>\n<li>Add in your infrastructure code.<\/li>\n<li>Run CDK synth<\/li>\n<li>CDK bootstrap &lt;aws_account&gt;\/<\/li>\n<li>Run CDK deploy ---&gt; This creates a cloudformation .yml file and the aws resources will be created as per the mentioned stack.<\/li>\n<\/ol>\n<p>I'm stuck on step 3, what do I add in this infrastructure code, and if I want to use this on amazon sagemaker which I am not familiar with, do I even bother doing this on my local terminal, or do I do the whole process regardless on sagemaker?<\/p>\n<p>Thank you in advance for your time and answers !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655801685383,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":100.0,
        "Answer_body":"<p>The infrastructure code is the Python code that you want to write for the resources you want to provision with SageMaker. In the example you provided for example the infra code they have is creating a Lambda function. You can do this locally on your machine, the question is what do you want to achieve with SageMaker? If you want to create an endpoint then following the CDK Python docs with SageMaker to identify the steps for creating an endpoint. Here's two guides, the first is an introduction to the AWS CDK and getting started. The second is an example of using the CDK with SageMaker to create an endpoint for  inference.<\/p>\n<p>CDK Python Starter: <a href=\"https:\/\/towardsdatascience.com\/build-your-first-aws-cdk-project-18b1fee2ed2d\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/build-your-first-aws-cdk-project-18b1fee2ed2d<\/a>\nCDK SageMaker Example: <a href=\"https:\/\/github.com\/philschmid\/cdk-samples\/tree\/master\/sagemaker-serverless-huggingface-endpoint\" rel=\"nofollow noreferrer\">https:\/\/github.com\/philschmid\/cdk-samples\/tree\/master\/sagemaker-serverless-huggingface-endpoint<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72697889",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1655849768796,
        "Question_original_content":"deploi aw cdk want us repo creat activ virtualenv instal requir depend error run pytest file binanc cdk app describ follow task app psvm method entri point program note step setup cdk instal npm cdk init creat project add infrastructur code run cdk synth cdk bootstrap run cdk deploi creat cloudform yml file aw resourc creat mention stack stuck step add infrastructur code want us familiar bother local termin process regardless thank advanc time answer",
        "Question_preprocessed_content":"deploi aw cdk want us repo creat activ virtualenv instal requir depend error run pytest file describ follow task app entri point program note step setup cdk instal npm cdk init add infrastructur code run cdk synth cdk bootstrap run cdk deploi creat cloudform yml file aw resourc creat mention stack stuck step add infrastructur code want us familiar bother local termin process regardless thank advanc time answer",
        "Question_gpt_summary_original":"The user is encountering challenges in deploying AWS using CDK and Sagemaker. They have installed the required dependencies and activated a virtualenv but are getting an error when running pytest. The user is stuck on step 3 of setting up CDK and is unsure of what to add in the infrastructure code. They are also unsure if they should do the whole process on Sagemaker or on their local terminal.",
        "Question_gpt_summary":"user encount challeng deploi aw cdk instal requir depend activ virtualenv get error run pytest user stuck step set cdk unsur add infrastructur code unsur process local termin",
        "Answer_original_content":"infrastructur code python code want write resourc want provis exampl provid exampl infra code creat lambda function local machin question want achiev want creat endpoint follow cdk python doc identifi step creat endpoint guid introduct aw cdk get start second exampl cdk creat endpoint infer cdk python starter http towardsdatasci com build aw cdk project bfeeedd cdk exampl http github com philschmid cdk sampl tree master serverless huggingfac endpoint",
        "Answer_preprocessed_content":"infrastructur code python code want write resourc want provis exampl provid exampl infra code creat lambda function local machin question want achiev want creat endpoint follow cdk python doc identifi step creat endpoint guid introduct aw cdk get start second exampl cdk creat endpoint infer cdk python starter cdk exampl",
        "Answer_gpt_summary_original":"Solutions provided in the discussion include creating the infrastructure code locally on the user's machine and following the CDK Python docs with SageMaker to identify the steps for creating an endpoint. Two guides were also provided for reference: CDK Python Starter and CDK SageMaker Example. No personal opinions or biases were included in the summary.",
        "Answer_gpt_summary":"solut provid discuss includ creat infrastructur code local user machin follow cdk python doc identifi step creat endpoint guid provid refer cdk python starter cdk exampl person opinion bias includ summari"
    },
    {
        "Question_title":"Azure ML: Include additional files during model deployment",
        "Question_body":"<p>In my AML pipeline, I've got a model built and deployed to the AciWebservice. I now have a need to include some additional data that would be used by score.py. This data is in json format (~1mb) and is specific to the model that's built. To accomplish this, I was thinking of sticking this file in blob store and updating some \"placholder\" vars in the score.py during deployment, but it seems hacky. <\/p>\n\n<p>Here are some options I was contemplating but wasn't sure on the practicality<\/p>\n\n<p><strong>Option 1:<\/strong>\nIs it possible to include this file, during the model deployment itself so that it's part of the docker image? <\/p>\n\n<p><strong>Option 2:<\/strong>\nAnother possibility I was contemplating, would it be possible to include this json data part of the Model artifacts?<\/p>\n\n<p><strong>Option 3:<\/strong>\nHow about registering it as a dataset and pull that in the score file?<\/p>\n\n<p>What is the recommended way to deploy dependent files in a model deployment scenario?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1589475408817,
        "Question_favorite_count":null,
        "Question_last_edit_time":1589479621848,
        "Question_score":3.0,
        "Question_view_count":866.0,
        "Answer_body":"<p>There are few ways to accomplish this:<\/p>\n\n<ol>\n<li><p>Put the additional file in the same folder as your model file, and <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none--sample-input-dataset-none--sample-output-dataset-none--resource-configuration-none-\" rel=\"nofollow noreferrer\">register<\/a> the whole folder as the model. In this approach the file is stored alongside the model.<\/p><\/li>\n<li><p>Put the file in a local folder, and specify that folder as source_directory in <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py\" rel=\"nofollow noreferrer\">InferenceConfig<\/a>. In this approach the file is re-uploaded every time you deploy a new endpoint.<\/p><\/li>\n<li><p>Use custom base image in InferenceConfig to bake the file into Docker image itself.<\/p><\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61803031",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1589484511663,
        "Question_original_content":"includ addit file model deploy aml pipelin got model built deploi aciwebservic need includ addit data score data json format specif model built accomplish think stick file blob store updat plachold var score deploy hacki option contempl wasn sure practic option possibl includ file model deploy docker imag option possibl contempl possibl includ json data model artifact option regist dataset pull score file recommend wai deploi depend file model deploy scenario",
        "Question_preprocessed_content":"includ addit file model deploy aml pipelin got model built deploi aciwebservic need includ addit data data json format specif model built accomplish think stick file blob store updat plachold var deploy hacki option contempl wasn sure practic option possibl includ file model deploy docker imag option possibl contempl possibl includ json data model artifact option regist dataset pull score file recommend wai deploi depend file model deploy scenario",
        "Question_gpt_summary_original":"The user has encountered a challenge in including additional data in json format (~1mb) that would be used by score.py during model deployment in Azure ML pipeline. The user is considering different options such as including the file during model deployment, including the json data as part of the model artifacts, or registering it as a dataset and pulling it in the score file. The user is seeking recommendations on the best way to deploy dependent files in a model deployment scenario.",
        "Question_gpt_summary":"user encount challeng includ addit data json format score model deploy pipelin user consid differ option includ file model deploy includ json data model artifact regist dataset pull score file user seek recommend best wai deploi depend file model deploy scenario",
        "Answer_original_content":"wai accomplish addit file folder model file regist folder model approach file store alongsid model file local folder specifi folder sourc directori inferenceconfig approach file upload time deploi new endpoint us custom base imag inferenceconfig bake file docker imag",
        "Answer_preprocessed_content":"wai accomplish addit file folder model file regist folder model approach file store alongsid model file local folder specifi folder inferenceconfig approach file time deploi new endpoint us custom base imag inferenceconfig bake file docker imag",
        "Answer_gpt_summary_original":"The discussion provides three possible solutions to the challenge of including additional data in json format during model deployment in Azure ML pipeline. The first solution is to put the additional file in the same folder as the model file and register the whole folder as the model. The second solution is to put the file in a local folder and specify that folder as source_directory in InferenceConfig. The third solution is to use a custom base image in InferenceConfig to bake the file into the Docker image itself.",
        "Answer_gpt_summary":"discuss provid possibl solut challeng includ addit data json format model deploy pipelin solut addit file folder model file regist folder model second solut file local folder specifi folder sourc directori inferenceconfig solut us custom base imag inferenceconfig bake file docker imag"
    },
    {
        "Question_title":"Failed to pull existing files from SSH DVC Remote",
        "Question_body":"<p>After running <code>dvc push data.csv<\/code> (to ssh-remote), when i try to dvc-pull the same file on another machine from the same remote, it won't get pulled. Below are the logs and the error:<\/p>\n<pre><code>2021-01-21 22:17:26,643 DEBUG: checking if 'data.csv'('HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)') has changed.\n2021-01-21 22:17:26,643 DEBUG: 'data.csv' doesn't exist.\n2021-01-21 22:17:26,644 WARNING: Cache 'HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)' not found. File 'data.csv' won't be created.\n2021-01-21 22:17:26,644 DEBUG: cache '\/usr\/src\/bohr\/.dvc\/cache\/27\/9936268f488e1e613f81a537f29055' expected 'HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)' actual 'None'\n...\n2021-01-21 22:17:26,660 ERROR: failed to pull data from the cloud - Checkout failed for following targets:\ndata.csv\n<\/code><\/pre>\n<p>However, the file is present on the remote:<\/p>\n<pre><code>$ ls -la ~\/.dvcstorage\/bohr\/27\/9936268f488e1e613f81a537f29055\n-rw-rw-r-- 1 hbabii hbabii 1458311 Jan 22 00:19 \/home\/hbabii\/.dvcstorage\/bohr\/27\/9936268f488e1e613f81a537f29055\n<\/code><\/pre>\n<p>I double-checked that I am pulling from and pushing to the same remote. I am using DVC v1.11.11.<\/p>\n<p>Could you please give me any hints on what could be wrong?<\/p>\n<p>Cheers, Hlib<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1611327752360,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":1715.0,
        "Answer_body":"<p>In the end, the problem was that I indeed was pulling from the wrong remote (I had multiple remotes, their configuration was tricky, and local configurations differed on different machines).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65847574",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1616082756536,
        "Question_original_content":"fail pull exist file ssh remot run push data csv ssh remot try pull file machin remot won pull log error debug check data csv hashinfo valu feefaf dir info size nfile chang debug data csv exist warn cach hashinfo valu feefaf dir info size nfile file data csv won creat debug cach usr src bohr cach feefaf expect hashinfo valu feefaf dir info size nfile actual error fail pull data cloud checkout fail follow target data csv file present remot storag bohr feefaf hbabii hbabii jan home hbabii storag bohr feefaf doubl check pull push remot hint wrong cheer hlib",
        "Question_preprocessed_content":"fail pull exist file ssh remot run try pull file machin remot won pull log error file present remot pull push remot hint wrong cheer hlib",
        "Question_gpt_summary_original":"The user is facing a challenge in pulling an existing file from an SSH DVC remote. Despite pushing the file successfully, the file won't get pulled on another machine from the same remote. The logs show that the cache is not found, and an error message appears, indicating that the checkout failed for the target file. However, the file is present on the remote, and the user has confirmed that they are pulling from and pushing to the same remote. The user is seeking hints on what could be wrong.",
        "Question_gpt_summary":"user face challeng pull exist file ssh remot despit push file successfulli file won pull machin remot log cach error messag appear indic checkout fail target file file present remot user confirm pull push remot user seek hint wrong",
        "Answer_original_content":"end problem pull wrong remot multipl remot configur tricki local configur differ differ machin",
        "Answer_preprocessed_content":"end problem pull wrong remot",
        "Answer_gpt_summary_original":"Solution: The user found out that they were pulling from the wrong remote, and the configuration was tricky, with local configurations differing on different machines.",
        "Answer_gpt_summary":"solut user pull wrong remot configur tricki local configur differ differ machin"
    },
    {
        "Question_title":"Create a stage where the output is a directory",
        "Question_body":"<p>I have a process that has a dependency on <code>input.txt<\/code>, and its output is a directory with files of the form <code>outputs\/*.txt<\/code>.<\/p>\n<p>What is the correct way to specify such a stage? I see from <a href=\"https:\/\/dvc.org\/doc\/use-cases\/versioning-data-and-model-files\/tutorial#second-model-version\" rel=\"noopener nofollow ugc\">this tutorial<\/a> that you can <code>dvc add<\/code> an entire directory. But is it possible to set this directory as the output from a <code>dvc run<\/code> stage?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651250034111,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":147.0,
        "Answer_body":"<p>It looks like DVC supports this feature transparently, without any extra configuration. I was able to use <code>-o<\/code> with a directory and it \u201cjust worked\u201d. Nice!<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/create-a-stage-where-the-output-is-a-directory\/1182",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-29T16:46:46.103Z",
                "Answer_body":"<p>It looks like DVC supports this feature transparently, without any extra configuration. I was able to use <code>-o<\/code> with a directory and it \u201cjust worked\u201d. Nice!<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"creat stage output directori process depend input txt output directori file form output txt correct wai specifi stage tutori add entir directori possibl set directori output run stage",
        "Question_preprocessed_content":"creat stage output directori process depend output directori file form correct wai specifi stage tutori entir directori possibl set directori output stage",
        "Question_gpt_summary_original":"The user is facing a challenge in specifying a stage where the output is a directory with files of a specific format. They are unsure if it is possible to set this directory as the output from a \"dvc run\" stage and are seeking guidance on the correct way to do so.",
        "Question_gpt_summary":"user face challeng specifi stage output directori file specif format unsur possibl set directori output run stage seek guidanc correct wai",
        "Answer_original_content":"look like support featur transpar extra configur abl us directori work nice",
        "Answer_preprocessed_content":"look like support featur transpar extra configur abl us directori work nice",
        "Answer_gpt_summary_original":"Solution: The discussion mentions that DVC supports setting a directory as the output from a \"dvc run\" stage using the \"-o\" flag without any extra configuration.",
        "Answer_gpt_summary":"solut discuss mention support set directori output run stage flag extra configur"
    },
    {
        "Question_title":"Confirming endpoints were deleted in SageMaker notebook",
        "Question_body":"<p>I'm just testing out AWS Sagemaker notebook and created an endpoint using a partial script below:<\/p>\n\n<pre><code>endpoint_name = 'engine' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nendpoint_config_name = 'engine_config' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nmodel_name = 'engine_model' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n\nwhile status=='Creating':\n    time.sleep(60)\n    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n    status = resp['EndpointStatus']\n    print(\"Status: \" + status)\n\n<\/code><\/pre>\n\n<p>I'm trying to remove that endpoint by using:\nsm_client.delete_endpoint(EndpointName=endpoint_name)<\/p>\n\n<p>However, it didn't work because I naively used timestamps for the endpoint_name and I didn't remember them. The original variable values were overriden when I re-run the code. As a result, I can't delete the existing endpoint.\nI went to the Sagemaker management dashboard --> inference --> endpoints, but it's empty. I don't even know if I'm currently having any active endpoints or not. Please advise how to delete my endpoint in this case. Thank you in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1574199115877,
        "Question_favorite_count":null,
        "Question_last_edit_time":1574275275427,
        "Question_score":1.0,
        "Question_view_count":661.0,
        "Answer_body":"<p><strong>If there are no endpoints active under the \"Endpoints\" tab in the SageMaker service console, then you will not be incurring any charges for inference or endpoint infrastructure.<\/strong><\/p>\n\n<p>If this is the case, your Endpoints tab should look like the following:\n<a href=\"https:\/\/i.stack.imgur.com\/5QvEn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5QvEn.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Endpoint Configurations<\/strong>, on the other hand, involve the metadata necessary for an endpoint deployment. This is just the metadata, and are stored (without cost) in your account, visible in the console under the \"Endpoint Configurations\" tab. You do not need to remove these configurations when tearing down an endpoint.<\/p>\n\n<p><strong>Important note:<\/strong> Double check that you are checking in the console for the <em>region you would have deployed to<\/em>. For example, if you ran the notebook and deployed an endpoint in <code>us-east-1<\/code>, but check the SageMaker console for <code>us-west-2<\/code>, it would not be displaying endpoints from the other region.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58943117",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1574200291200,
        "Question_original_content":"confirm endpoint delet notebook test notebook creat endpoint partial script endpoint engin strftime gmtime endpoint config engin config strftime gmtime model engin model strftime gmtime statu creat time sleep resp client endpoint endpointnam endpoint statu resp endpointstatu print statu statu try remov endpoint client delet endpoint endpointnam endpoint work naiv timestamp endpoint rememb origin variabl valu overriden run code result delet exist endpoint went manag dashboard infer endpoint know current have activ endpoint advis delet endpoint case thank advanc",
        "Question_preprocessed_content":"confirm endpoint delet notebook test notebook creat endpoint partial script try remov endpoint work naiv timestamp rememb origin variabl valu overriden code result delet exist endpoint went manag dashboard infer endpoint know current have activ endpoint advis delet endpoint case thank advanc",
        "Question_gpt_summary_original":"The user created an endpoint in AWS SageMaker notebook using a script with timestamp-based variable names. When attempting to delete the endpoint using the endpoint name, the user realized they did not remember the original variable values and could not delete the existing endpoint. The user checked the SageMaker management dashboard but found it empty and is unsure if they have any active endpoints. The user is seeking advice on how to delete the endpoint in this case.",
        "Question_gpt_summary":"user creat endpoint notebook script timestamp base variabl name attempt delet endpoint endpoint user realiz rememb origin variabl valu delet exist endpoint user check manag dashboard unsur activ endpoint user seek advic delet endpoint case",
        "Answer_original_content":"endpoint activ endpoint tab servic consol incur charg infer endpoint infrastructur case endpoint tab look like follow endpoint configur hand involv metadata necessari endpoint deploy metadata store cost account visibl consol endpoint configur tab need remov configur tear endpoint import note doubl check check consol region deploi exampl ran notebook deploi endpoint east check consol west displai endpoint region",
        "Answer_preprocessed_content":"endpoint activ endpoint tab servic consol incur charg infer endpoint infrastructur case endpoint tab look like follow endpoint configur hand involv metadata necessari endpoint deploy metadata store account visibl consol endpoint configur tab need remov configur tear endpoint import note doubl check check consol region deploi exampl ran notebook deploi endpoint check consol displai endpoint region",
        "Answer_gpt_summary_original":"Solution:\n- If there are no endpoints active under the \"Endpoints\" tab in the SageMaker service console, then the user will not be incurring any charges for inference or endpoint infrastructure.\n- Endpoint Configurations involve the metadata necessary for an endpoint deployment and are stored (without cost) in the user's account, visible in the console under the \"Endpoint Configurations\" tab. The user does not need to remove these configurations when tearing down an endpoint.\n- Double check that the user is checking in the console for the region they would have deployed to.",
        "Answer_gpt_summary":"solut endpoint activ endpoint tab servic consol user incur charg infer endpoint infrastructur endpoint configur involv metadata necessari endpoint deploy store cost user account visibl consol endpoint configur tab user need remov configur tear endpoint doubl check user check consol region deploi"
    },
    {
        "Question_title":"Can't approximate simple multiplication function in neural network with 1 hidden layer",
        "Question_body":"<p>I just wanted to test how good can neural network approximate multiplication function (regression task). \nI am using Azure Machine Learning Studio. I have 6500 samples, 1 hidden layer\n(I have tested 5 \/30 \/100 neurons per hidden layer), no normalization. And default parameters \n<a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn906030.aspx\" rel=\"nofollow\">Learning rate - 0.005, Number of learning iterations - 200, The initial learning weigh - 0.1,\n The momentum - 0 [description]<\/a>. I got extremely bad accuracy, close to 0.\n<em>At the same time boosted Decision forest regression shows very good approximation.<\/em><\/p>\n\n<p>What am I doing wrong? This task should be very easy for NN.<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1464596393980,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":1491916922983,
        "Question_score":3.0,
        "Question_view_count":4342.0,
        "Answer_body":"<p>Big multiplication function gradient forces the net probably almost immediately into some horrifying state where all its hidden nodes have zero gradient.\nWe can use two approaches:<\/p>\n\n<p>1) Devide by constant. We are just deviding everything before the learning and multiply after.<\/p>\n\n<p>2) Make log-normalization. It makes multiplication into addition:<\/p>\n\n<pre><code>m = x*y =&gt; ln(m) = ln(x) + ln(y).\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1465820459196,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37520849",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1465277784230,
        "Question_original_content":"approxim simpl multipl function neural network hidden layer want test good neural network approxim multipl function regress task studio sampl hidden layer test neuron hidden layer normal default paramet learn rate number learn iter initi learn weigh momentum descript got extrem bad accuraci close time boost decis forest regress show good approxim wrong task easi",
        "Question_preprocessed_content":"approxim simpl multipl function neural network hidden layer want test good neural network approxim multipl function studio sampl hidden layer test neuron hidden layer normal default paramet learn rate number learn iter initi learn weigh momentum got extrem bad accuraci close time boost decis forest regress show good approxim wrong task easi",
        "Question_gpt_summary_original":"The user is facing challenges in approximating a simple multiplication function in a neural network with 1 hidden layer using Azure Machine Learning Studio. Despite having 6500 samples and testing with 5\/30\/100 neurons per hidden layer, the user is getting extremely bad accuracy close to 0. The user is wondering what they are doing wrong as this task should be easy for a neural network.",
        "Question_gpt_summary":"user face challeng approxim simpl multipl function neural network hidden layer studio despit have sampl test neuron hidden layer user get extrem bad accuraci close user wonder wrong task easi neural network",
        "Answer_original_content":"big multipl function gradient forc net probabl immedi horrifi state hidden node zero gradient us approach devid constant devid learn multipli log normal make multipl addit",
        "Answer_preprocessed_content":"big multipl function gradient forc net probabl immedi horrifi state hidden node zero gradient us approach devid constant devid learn multipli make multipl addit",
        "Answer_gpt_summary_original":"Solutions provided in the discussion are:\n\n1) Divide by constant: The user can divide everything before learning and multiply after learning.\n\n2) Make log-normalization: The user can use log-normalization to convert multiplication into addition.",
        "Answer_gpt_summary":"solut provid discuss divid constant user divid learn multipli learn log normal user us log normal convert multipl addit"
    },
    {
        "Question_title":"Is there a workaround to make opencensus work with MLFlow?",
        "Question_body":"<p>I'm not able to import mlflow after having launched a log with opencensus Azure.\nThe MLFlow import runs forever.<\/p>\n<p>My environment is the following:<\/p>\n<ul>\n<li>Python 3.7<\/li>\n<li>opencensus-ext-azure 1.0.7<\/li>\n<li>opencensus-ext-logging 0.1.0<\/li>\n<li>mlflow 1.15.0<\/li>\n<\/ul>\n<p>Here is the code to repoduce the bug:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import logging\n\nfrom opencensus.ext.azure.log_exporter import AzureLogHandler\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(AzureLogHandler(connection_string='InstrumentationKey=&lt;your-key&gt;'))\nlogger.warning('Hello, World!')\n\nimport mlflow\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619011270043,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":41.0,
        "Answer_body":"<p>I found a workaround, not the cleanest one though.<\/p>\n<p>I import mlflow at the beginning even if it's not useful this way:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\nimport logging\n\nfrom opencensus.ext.azure.log_exporter import AzureLogHandler\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(AzureLogHandler(connection_string='InstrumentationKey=&lt;your-key&gt;'))\nlogger.warning('Hello, World!')\n\nimport mlflow\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67196775",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1619077377892,
        "Question_original_content":"workaround opencensu work abl import have launch log opencensu azur import run forev environ follow python opencensu ext azur opencensu ext log code repoduc bug import log opencensu ext azur log export import azureloghandl logger log getlogg logger addhandl azureloghandl connect string instrumentationkei logger warn hello world import",
        "Question_preprocessed_content":"workaround opencensu work abl import have launch log opencensu azur import run forev environ follow python code repoduc bug",
        "Question_gpt_summary_original":"The user is facing a challenge in importing MLFlow after launching a log with opencensus Azure. The MLFlow import runs indefinitely, and the user has provided the environment details and code to reproduce the issue.",
        "Question_gpt_summary":"user face challeng import launch log opencensu azur import run indefinit user provid environ detail code reproduc issu",
        "Answer_original_content":"workaround cleanest import begin us wai import import log opencensu ext azur log export import azureloghandl logger log getlogg logger addhandl azureloghandl connect string instrumentationkei logger warn hello world import",
        "Answer_preprocessed_content":"workaround cleanest import begin us wai",
        "Answer_gpt_summary_original":"Solution: One possible workaround mentioned in the discussion is to import MLFlow at the beginning of the code, even if it's not useful in that way. This can be done by adding the line \"import mlflow\" before the logger code. However, it is mentioned that this is not the cleanest solution.",
        "Answer_gpt_summary":"solut possibl workaround mention discuss import begin code us wai ad line import logger code mention cleanest solut"
    },
    {
        "Question_title":"Convert an instance of xgboost.Booster into a model that implements the scikit-learn API",
        "Question_body":"<p>I am trying to use <code>mlflow<\/code> to save a model and then load it later to make predictions.<\/p>\n<p>I'm using a <code>xgboost.XGBRegressor<\/code> model and its sklearn functions <code>.predict()<\/code> and <code>.predict_proba()<\/code> to make predictions but it turns out that <code>mlflow<\/code> doesn't support models that implements the sklearn API, so when loading the model later from mlflow, mlflow returns an instance of <code>xgboost.Booster<\/code>, and it doesn't implements the <code>.predict()<\/code> or <code>.predict_proba()<\/code> functions.<\/p>\n<p>Is there a way to convert a <code>xgboost.Booster<\/code> back into a <code>xgboost.sklearn.XGBRegressor<\/code> object that implements the sklearn API functions?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1600550858547,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":1317.0,
        "Answer_body":"<p>Have you tried wrapping up your model in custom class, logging and loading it using <code>mlflow.pyfunc.PythonModel<\/code>?\nI put up a simple example and upon loading back the model it correctly shows <code>&lt;class 'xgboost.sklearn.XGBRegressor'&gt;<\/code> as a type.<\/p>\n<p>Example:<\/p>\n<pre><code>import xgboost as xgb\nxg_reg = xgb.XGBRegressor(...)\n\nclass CustomModel(mlflow.pyfunc.PythonModel):\n    def __init__(self, xgbRegressor):\n        self.xgbRegressor = xgbRegressor\n\n    def predict(self, context, input_data):\n        print(type(self.xgbRegressor))\n        \n        return self.xgbRegressor.predict(input_data)\n\n# Log model to local directory\nwith mlflow.start_run():\n     custom_model = CustomModel(xg_reg)\n     mlflow.pyfunc.log_model(&quot;custome_model&quot;, python_model=custom_model)\n\n\n# Load model back\nfrom mlflow.pyfunc import load_model\nmodel = load_model(&quot;\/mlruns\/0\/..\/artifacts\/custome_model&quot;)\nmodel.predict(X_test)\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>&lt;class 'xgboost.sklearn.XGBRegressor'&gt;\n[ 9.107417 ]\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63973530",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1600607182583,
        "Question_original_content":"convert instanc xgboost booster model implement scikit learn api try us save model load later predict xgboost xgbregressor model sklearn function predict predict proba predict turn support model implement sklearn api load model later return instanc xgboost booster implement predict predict proba function wai convert xgboost booster xgboost sklearn xgbregressor object implement sklearn api function",
        "Question_preprocessed_content":"convert instanc model implement api try us save model load later predict model sklearn function predict turn support model implement sklearn api load model later return instanc implement function wai convert object implement sklearn api function",
        "Question_gpt_summary_original":"The user is facing a challenge in converting an instance of xgboost.Booster into a model that implements the scikit-learn API. They are using mlflow to save and load the model, but mlflow does not support models that implement the sklearn API. As a result, when loading the model from mlflow, it returns an instance of xgboost.Booster, which does not implement the .predict() or .predict_proba() functions. The user is seeking a way to convert xgboost.Booster back into a xgboost.sklearn.XGBRegressor object that implements the sklearn API functions.",
        "Question_gpt_summary":"user face challeng convert instanc xgboost booster model implement scikit learn api save load model support model implement sklearn api result load model return instanc xgboost booster implement predict predict proba function user seek wai convert xgboost booster xgboost sklearn xgbregressor object implement sklearn api function",
        "Answer_original_content":"tri wrap model custom class log load pyfunc pythonmodel simpl exampl load model correctli show type exampl import xgboost xgb reg xgb xgbregressor class custommodel pyfunc pythonmodel def init self xgbregressor self xgbregressor xgbregressor def predict self context input data print type self xgbregressor return self xgbregressor predict input data log model local directori start run custom model custommodel reg pyfunc log model custom model python model custom model load model pyfunc import load model model load model mlrun artifact custom model model predict test output",
        "Answer_preprocessed_content":"tri wrap model custom class log load simpl exampl load model correctli show type exampl output",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to wrap the xgboost.Booster model in a custom class and log and load it using mlflow.pyfunc.PythonModel. The custom class should implement the .predict() function and take the xgboost.Booster model as an argument. This way, when loading the model from mlflow, it will return an instance of xgboost.sklearn.XGBRegressor that implements the sklearn API functions. A code example is provided to demonstrate this solution.",
        "Answer_gpt_summary":"solut possibl solut mention discuss wrap xgboost booster model custom class log load pyfunc pythonmodel custom class implement predict function xgboost booster model argument wai load model return instanc xgboost sklearn xgbregressor implement sklearn api function code exampl provid demonstr solut"
    },
    {
        "Question_title":"How to avoid error \"conda --version: conda not found\" in az ml run --submit-script command?",
        "Question_body":"<p>I would like to run a test script on an existing compute instance of Azure using the Azure Machine Learning extension to the Azure CLI:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>az ml run submit-script test.py --target compute-instance-test --experiment-name test_example --resource-group ex-test-rg\n<\/code><\/pre>\n<p>I get a Service Error with the following error message:<\/p>\n<pre><code>Unable to run conda package manager. AzureML uses conda to provision python\\nenvironments from a dependency specification. To manage the python environment\\nmanually instead, set userManagedDependencies to True in the python environment\\nconfiguration. To use system managed python environments, install conda from:\\nhttps:\/\/conda.io\/miniconda.html\n<\/code><\/pre>\n<p>But when I connect to the compute instance through the Azure portal and select the default Python kernel, <code>conda --version<\/code> prints 4.5.12. So conda is effectively already installed on the compute instance. This is why I do not understand the error message.<\/p>\n<p>Further information on the azure versions:<\/p>\n<pre><code>  &quot;azure-cli&quot;: &quot;2.12.1&quot;,\n  &quot;azure-cli-core&quot;: &quot;2.12.1&quot;,\n  &quot;azure-cli-telemetry&quot;: &quot;1.0.6&quot;,\n  &quot;extensions&quot;: {\n    &quot;azure-cli-ml&quot;: &quot;1.15.0&quot;\n  }\n<\/code><\/pre>\n<p>The image I use is:<\/p>\n<pre><code>mcr.microsoft.com\/azure-cli:latest\n<\/code><\/pre>\n<p>Can somebody please explain as to why I am getting this error and help me resolve the error? Thank you!<\/p>\n<p>EDIT: I tried to update the environment in which the <code>az ml run<\/code>-command is run.\nEssentially this is my GitLab job. The installation of miniconda is a bit complicated as the azure-cli uses an alpine Linux image (reference: <a href=\"https:\/\/stackoverflow.com\/questions\/47177538\/installing-miniconda-on-alpine-linux-fails\">Installing miniconda on alpine linux fails<\/a>). I replaced some names with ... and cut out some irrelevant pieces of code.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>test:\n  image: 'mcr.microsoft.com\/azure-cli:latest'\n  script:\n    - echo &quot;Download conda&quot;\n    - apk --update add bash curl wget ca-certificates libstdc++ glib\n    - wget -q -O \/etc\/apk\/keys\/sgerrand.rsa.pub https:\/\/raw.githubusercontent.com\/sgerrand\/alpine-pkg-node-bower\/master\/sgerrand.rsa.pub\n    - curl -L &quot;https:\/\/github.com\/sgerrand\/alpine-pkg-glibc\/releases\/download\/2.23-r3\/glibc-2.23-r3.apk&quot; -o glibc.apk\n    - apk del libc6-compat\n    - apk add glibc.apk\n    - curl -L &quot;https:\/\/github.com\/sgerrand\/alpine-pkg-glibc\/releases\/download\/2.23-r3\/glibc-bin-2.23-r3.apk&quot; -o glibc-bin.apk \n    - apk add glibc-bin.apk \n    - curl -L &quot;https:\/\/github.com\/andyshinn\/alpine-pkg-glibc\/releases\/download\/2.25-r0\/glibc-i18n-2.25-r0.apk&quot; -o glibc-i18n.apk\n    - apk add --allow-untrusted glibc-i18n.apk \n    - \/usr\/glibc-compat\/bin\/localedef -i en_US -f UTF-8 en_US.UTF-8 \n    - \/usr\/glibc-compat\/sbin\/ldconfig \/lib \/usr\/glibc\/usr\/lib\n    - rm -rf glibc*apk \/var\/cache\/apk\/*\n    - echo &quot;yes&quot; | curl -sSL https:\/\/repo.continuum.io\/miniconda\/Miniconda3-latest-Linux-x86_64.sh -o miniconda.sh\n    - echo &quot;Install conda&quot;\n    - (echo -e &quot;\\n&quot;; echo &quot;yes&quot;; echo -e &quot;\\n&quot;; echo &quot;yes&quot;) | bash -bfp miniconda.sh\n    - echo &quot;Installing Azure Machine Learning Extension&quot;\n    - az extension add -n azure-cli-ml\n    - echo &quot;Azure Login&quot;\n    - az login\n    - az account set --subscription ...\n    - az configure --defaults group=...\n    - az ml folder attach -w ... \n    - az ml run submit-script test.py --target ... --experiment-name hello_world --resource-group ...\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":6,
        "Question_creation_time":1601897074350,
        "Question_favorite_count":null,
        "Question_last_edit_time":1601970763932,
        "Question_score":1.0,
        "Question_view_count":1089.0,
        "Answer_body":"<p>One needs to pass the <code>--workspace-name<\/code> argument to be able to run it on Azure's compute target and not on the local compute target:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>az ml run submit-script test.py --target compute-instance-test --experiment-name test_example --resource-group ex-test-rg --workspace-name test-ws\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64207678",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1603466200947,
        "Question_original_content":"avoid error conda version conda run submit script command like run test script exist comput instanc azur extens azur cli run submit script test target comput instanc test experi test exampl resourc group test servic error follow error messag unabl run conda packag manag us conda provis python nenviron depend specif manag python environ nmanual instead set usermanageddepend true python environ nconfigur us manag python environ instal conda nhttp conda miniconda html connect comput instanc azur portal select default python kernel conda version print conda effect instal comput instanc understand error messag inform azur version azur cli azur cli core azur cli telemetri extens azur cli imag us mcr microsoft com azur cli latest somebodi explain get error help resolv error thank edit tri updat environ run command run essenti gitlab job instal miniconda bit complic azur cli us alpin linux imag refer instal miniconda alpin linux fail replac name cut irrelev piec code test imag mcr microsoft com azur cli latest script echo download conda apk updat add bash curl wget certif libstdc glib wget apk kei sgerrand rsa pub http raw githubusercont com sgerrand alpin pkg node bower master sgerrand rsa pub curl http github com sgerrand alpin pkg glibc releas download glibc apk glibc apk apk del libc compat apk add glibc apk curl http github com sgerrand alpin pkg glibc releas download glibc bin apk glibc bin apk apk add glibc bin apk curl http github com andyshinn alpin pkg glibc releas download glibc apk glibc apk apk add allow untrust glibc apk usr glibc compat bin localedef utf utf usr glibc compat sbin ldconfig lib usr glibc usr lib glibc apk var cach apk echo ye curl ssl http repo continuum miniconda miniconda latest linux miniconda echo instal conda echo echo ye echo echo ye bash bfp miniconda echo instal extens extens add azur cli echo azur login login account set subscript configur default group folder attach run submit script test target experi hello world resourc group",
        "Question_preprocessed_content":"avoid error conda conda run command like run test script exist comput instanc azur extens azur cli servic error follow error messag connect comput instanc azur portal select default python kernel print conda effect instal comput instanc understand error messag inform azur version imag us somebodi explain get error help resolv error thank edit tri updat environ command run essenti gitlab job instal miniconda bit complic us alpin linux imag replac name cut irrelev piec code",
        "Question_gpt_summary_original":"The user is encountering an error message when trying to run a test script on an existing compute instance of Azure using the Azure Machine Learning extension to the Azure CLI. The error message states that the conda package manager is unable to run, even though conda is already installed on the compute instance. The user has tried to update the environment in which the az ml run-command is run, but the installation of miniconda is complicated as the azure-cli uses an alpine Linux image. The user is seeking help to resolve the error.",
        "Question_gpt_summary":"user encount error messag try run test script exist comput instanc azur extens azur cli error messag state conda packag manag unabl run conda instal comput instanc user tri updat environ run command run instal miniconda complic azur cli us alpin linux imag user seek help resolv error",
        "Answer_original_content":"need pass workspac argument abl run azur comput target local comput target run submit script test target comput instanc test experi test exampl resourc group test workspac test",
        "Answer_preprocessed_content":"need pass argument abl run azur comput target local comput target",
        "Answer_gpt_summary_original":"Solution: The user needs to pass the `--workspace-name` argument to be able to run the test script on Azure's compute target and not on the local compute target. The correct command is `az ml run submit-script test.py --target compute-instance-test --experiment-name test_example --resource-group ex-test-rg --workspace-name test-ws`.",
        "Answer_gpt_summary":"solut user need pass workspac argument abl run test script azur comput target local comput target correct command run submit script test target comput instanc test experi test exampl resourc group test workspac test"
    },
    {
        "Question_title":"How can I print the Canonical String which aws-requests-auth sends?",
        "Question_body":"<p>I want to have a lambda calling a Sagemaker instance in another region. If both are in the same region, everything works fine. If they are not, I get the following error:<\/p>\n\n<pre><code>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\n\nThe Canonical String for this request should have been\n'POST\n\/endpoints\/foo-endpoint\/invocations\n\nhost:runtime.sagemaker.us-east-1.amazonaws.com\nx-amz-date:20180406T082536Z\n\nhost;x-amz-date\n1234567890foobarfoobarfoobarboofoobarfoobarfoobarfoobarfoobarfoo'\n\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20180406T082536Z\n20180406\/us-east-1\/sagemaker\/aws4_request\n987654321abcdeffoobarfoobarfoobarfoobarfoobarfoobarfoobarfoobarf'\n<\/code><\/pre>\n\n<p>I use <a href=\"https:\/\/github.com\/DavidMuller\/aws-requests-auth\" rel=\"nofollow noreferrer\"><code>aws-requests-auth<\/code><\/a> (0.4.1) with boto3 (1.5.15 - updating to 1.7.1 didn't change anything, <a href=\"https:\/\/github.com\/boto\/boto3\/blob\/develop\/CHANGELOG.rst\" rel=\"nofollow noreferrer\">changelog<\/a>) like this:<\/p>\n\n<pre><code>import requests\nfrom aws_requests_auth.aws_auth import AWSRequestsAuth\nauth = AWSRequestsAuth(aws_access_key=config['AWS']['ACCESS_KEY'],\n                       aws_secret_access_key=(\n                           config['AWS']['SECRET_ACCESS_KEY']),\n                       aws_host=config['AWS']['HOST'],\n                       aws_region=config['AWS']['REGION'],\n                       aws_service=config['AWS']['SERVICE'])\n\npayload = {'foo': 'bar'}\nresponse = requests.post(post_url,\n                         data=json.dumps(payload),\n                         headers={'content-type': 'application\/json'},\n                         auth=auth)\n<\/code><\/pre>\n\n<p>printing <code>auth<\/code> only gives <code>&lt;aws_requests_auth.aws_auth.AWSRequestsAuth object at 0x7f9d00c98390&gt;<\/code>.<\/p>\n\n<p>Is there a way to print the \"Canonical String\" mentioned in the error message?<\/p>\n\n<p>(Any other ideas how to fix this are appreciated as well)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1523005580303,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1531211963023,
        "Question_score":3.0,
        "Question_view_count":827.0,
        "Answer_body":"<p>A work-around for the asked question:<\/p>\n\n<pre><code>req = requests.request('POST', 'http:\/\/httpbin.org\/get')\nreq.body = b''\nreq.method = ''\nprint(auth.get_aws_request_headers(req,\n                                   aws_access_key=auth.aws_access_key,\n                                   aws_secret_access_key=auth.aws_secret_access_key,\n                                   aws_token=auth.aws_token))\n<\/code><\/pre>\n\n<p>The problem is not solved, though. And now I wonder what the first argument of <code>auth.get_aws_request_headers<\/code> is.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49689216",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1523012141480,
        "Question_original_content":"print canon string aw request auth send want lambda call instanc region region work fine follow error request signatur calcul match signatur provid check aw secret access kei sign method consult servic document detail canon string request post endpoint foo endpoint invoc host runtim east amazonaw com amz date host amz date foobarfoobarfoobarboofoobarfoobarfoobarfoobarfoobarfoo string sign aw hmac sha east aw request abcdeffoobarfoobarfoobarfoobarfoobarfoobarfoobarfoobarf us aw request auth boto updat chang changelog like import request aw request auth aw auth import awsrequestsauth auth awsrequestsauth aw access kei config aw access kei aw secret access kei config aw secret access kei aw host config aw host aw region config aw region aw servic config aw servic payload foo bar respons request post post url data json dump payload header content type applic json auth auth print auth give wai print canon string mention error messag idea fix appreci",
        "Question_preprocessed_content":"print canon string send want lambda call instanc region region work fine follow error us boto like print give wai print canon string mention error messag idea fix appreci",
        "Question_gpt_summary_original":"The user is encountering an error when trying to call a Sagemaker instance in another region using a lambda. The error message indicates that the request signature does not match the signature provided and provides a Canonical String and String-to-Sign that should have been used. The user is using aws-requests-auth and boto3 to make the request and is looking for a way to print the Canonical String mentioned in the error message.",
        "Question_gpt_summary":"user encount error try instanc region lambda error messag indic request signatur match signatur provid provid canon string string sign user aw request auth boto request look wai print canon string mention error messag",
        "Answer_original_content":"work ask question req request request post http httpbin org req bodi req method print auth aw request header req aw access kei auth aw access kei aw secret access kei auth aw secret access kei aw token auth aw token problem solv wonder argument auth aw request header",
        "Answer_preprocessed_content":"ask question problem solv wonder argument",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion.",
        "Answer_gpt_summary":"solut solut provid discuss"
    },
    {
        "Question_title":"What are the differences between AWS sagemaker and sagemaker_pyspark?",
        "Question_body":"<p>I'm currently running a quick Machine Learning proof of concept on AWS with SageMaker, and I've come across two libraries: <code>sagemaker<\/code> and <code>sagemaker_pyspark<\/code>. I would like to work with distributed data. My questions are:<\/p>\n<ol>\n<li><p>Is using <code>sagemaker<\/code> the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented <code>sagemaker_pyspark<\/code>? Based on this assumption, I do not understand what it would offer regarding using <code>scikit-learn<\/code> on a SageMaker notebook (in terms of computing capabilities).<\/p>\n<\/li>\n<li><p>Is it normal for something like <code>model = xgboost_estimator.fit(training_data)<\/code> to take 4 minutes to run with <code>sagemaker_pyspark<\/code> for a small set of test data? I see that what it does below is to train the model and also create an Endpoint to be able to offer its predictive services, and I assume that this endpoint is deployed on an EC2 instance that is created and started at the moment. Correct me if I'm wrong. I assume this from how the estimator is defined:<\/p>\n<\/li>\n<\/ol>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator\n\n\nxgboost_estimator = XGBoostSageMakerEstimator (\n    trainingInstanceType = &quot;ml.m4.xlarge&quot;,\n    trainingInstanceCount = 1,\n    endpointInstanceType = &quot;ml.m4.xlarge&quot;,\n    endpointInitialInstanceCount = 1,\n    sagemakerRole = IAMRole(get_execution_role())\n)\n\nxgboost_estimator.setNumRound(1)\n<\/code><\/pre>\n<p>If so, is there a way to reuse the same endpoint with different training jobs so that I don't have to wait for a new endpoint to be created each time?<\/p>\n<ol start=\"3\">\n<li><p>Does <code>sagemaker_pyspark<\/code> support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/p>\n<\/li>\n<li><p>Do you know if <code>sagemaker_pyspark<\/code> can perform hyperparameter optimization? From what I see, <code>sagemaker<\/code> offers the <code>HyperparameterTuner<\/code> class, but I can't find anything like it in <code>sagemaker_pyspark<\/code>. I suppose it is a more recent library and there is still a lot of functionality to implement.<\/p>\n<\/li>\n<li><p>I am a bit confused about the concept of <code>entry_point<\/code> and <code>container<\/code>\/<code>image_name<\/code> (both possible input arguments for the <code>Estimator<\/code> object from the <code>sagemaker<\/code> library): can you deploy models with and without containers? why would you use model containers? Do you always need to define the model externally with the <code>entry_point<\/code> script? It is also confusing that the class <code>AlgorithmEstimator<\/code> allows the input argument <code>algorithm_arn<\/code>; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/li>\n<li><p>I see the <code>sagemaker<\/code> library offers SageMaker Pipelines, which seem to be very handy for deploying properly structured ML workflows. However, I don't think this is available with <code>sagemaker_pyspark<\/code>, so in that case, I would rather create my workflows with a combination of Step Functions (to orchestrate the entire thing), Glue processes (for ETL, preprocessing and feature\/target engineering) and SageMaker processes using <code>sagemaker_pyspark<\/code>.<\/p>\n<\/li>\n<li><p>I also found out that <code>sagemaker<\/code> has the <code>sagemaker.sparkml.model.SparkMLModel<\/code> object. What is the difference between this and what <code>sagemaker_pyspark<\/code> offers?<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616764992750,
        "Question_favorite_count":null,
        "Question_last_edit_time":1618398204670,
        "Question_score":1.0,
        "Question_view_count":242.0,
        "Answer_body":"<p><code>sagemaker<\/code> is the SageMaker Python SDK. It calls SageMaker-related AWS service APIs on your behalf. You don't need to use it, but it can make life easier<\/p>\n<blockquote>\n<ol>\n<li>Is using sagemaker the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented sagemaker_pyspark?<\/li>\n<\/ol>\n<\/blockquote>\n<p>No. You can run distributed training jobs using <code>sagemaker<\/code> (see <code>instance_count<\/code> parameter)<\/p>\n<p><code>sagemaker_pyspark<\/code> facilitates calling SageMaker-related AWS service APIs from Spark. Use it if you want to use SageMaker services from Spark<\/p>\n<blockquote>\n<ol start=\"2\">\n<li>Is it normal for something like model = xgboost_estimator.fit(training_data) to take 4 minutes to run with sagemaker_pyspark for a small set of test data?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, it takes a few minutes for an EC2 instance to spin-up. Use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a> if you want to iterate more quickly locally. Note: Local Mode won't work with SageMaker built-in algorithms, but you can prototype with (non AWS) XGBoost\/SciKit-Learn<\/p>\n<blockquote>\n<ol start=\"3\">\n<li>Does sagemaker_pyspark support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, but you'd probably want to extend <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/api.html#sagemakerestimator\" rel=\"nofollow noreferrer\">SageMakerEstimator<\/a>. Here you can provide the <code>trainingImage<\/code> URI<\/p>\n<blockquote>\n<ol start=\"4\">\n<li>Do you know if sagemaker_pyspark can perform hyperparameter optimization?<\/li>\n<\/ol>\n<\/blockquote>\n<p>It does not appear so. It'd probably be easier just to do this from SageMaker itself though<\/p>\n<blockquote>\n<p>can you deploy models with and without containers?<\/p>\n<\/blockquote>\n<p>You can certainly host your own models any way you want. But if you want to use SageMaker model inference hosting, then containers are required<\/p>\n<blockquote>\n<p>why would you use model containers?<\/p>\n<\/blockquote>\n<blockquote>\n<p>Do you always need to define the model externally with the entry_point script?<\/p>\n<\/blockquote>\n<p>The whole Docker thing makes bundling dependencies easier, and also makes things language\/runtime-neutral. SageMaker doesn't care if your algorithm is in Python or Java or Fortran. But it needs to know how to &quot;run&quot; it, so you tell it a working directory and a command to run. This is the entry point<\/p>\n<blockquote>\n<p>It is also confusing that the class AlgorithmEstimator allows the input argument algorithm_arn; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/blockquote>\n<p>Please clarify which &quot;three&quot; you are referring to<\/p>\n<p>6 is not a question, so no answer required :)<\/p>\n<blockquote>\n<ol start=\"7\">\n<li>What is the difference between this and what sagemaker_pyspark offers?<\/li>\n<\/ol>\n<\/blockquote>\n<p>sagemaker_pyspark lets you call SageMaker services from Spark, whereas SparkML Serving lets you use Spark ML services from SageMaker<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1634230982412,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66817781",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1617119678127,
        "Question_original_content":"differ pyspark current run quick machin learn proof concept aw come librari pyspark like work distribut data question equival run train job take advantag distribut comput capabl aw assum implement pyspark base assumpt understand offer scikit learn notebook term comput capabl normal like model xgboost estim fit train data minut run pyspark small set test data train model creat endpoint abl offer predict servic assum endpoint deploi instanc creat start moment correct wrong assum estim defin import execut role pyspark algorithm import xgboostestim xgboost estim xgboostestim traininginstancetyp xlarg traininginstancecount endpointinstancetyp xlarg endpointinitialinstancecount role iamrol execut role xgboost estim setnumround wai reus endpoint differ train job wait new endpoint creat time pyspark support custom algorithm allow us predefin on librari know pyspark perform hyperparamet optim offer hyperparametertun class like pyspark suppos recent librari lot function implement bit confus concept entri point contain imag possibl input argument estim object librari deploi model contain us model contain need defin model extern entri point script confus class algorithmestim allow input argument algorithm arn differ wai pass model input better librari offer pipelin handi deploi properli structur workflow think avail pyspark case creat workflow combin step function orchestr entir thing glue process etl preprocess featur target engin process pyspark sparkml model sparkmlmodel object differ pyspark offer",
        "Question_preprocessed_content":"differ current run quick machin learn proof concept aw come librari like work distribut data question equival run train job take advantag distribut comput capabl aw assum implement base assumpt understand offer notebook normal like minut run small set test data train model creat endpoint abl offer predict servic assum endpoint deploi instanc creat start moment correct wrong assum estim defin wai reus endpoint differ train job wait new endpoint creat time support custom algorithm allow us predefin on librari know perform hyperparamet optim offer class like suppos recent librari lot function implement bit confus concept deploi model contain us model contain need defin model extern script confus class allow input argument differ wai pass model input better librari offer pipelin handi deploi properli structur workflow think avail case creat workflow combin step function glue process process object differ offer",
        "Question_gpt_summary_original":"The user is encountering several challenges while working with AWS SageMaker. They are confused about the differences between the sagemaker and sagemaker_pyspark libraries, and whether sagemaker_pyspark offers distributed computing capabilities. They are also experiencing slow training times and wondering if there is a way to reuse the same endpoint with different training jobs. The user is unsure if sagemaker_pyspark supports custom algorithms and hyperparameter optimization. They are also confused about the concept of entry_point and container\/image_name and why model containers are used. Finally, the user is considering using a combination of Step Functions, Glue processes, and sagemaker_pyspark for their ML workflows and is curious about the difference between sagemaker.sparkml.model.SparkMLModel and sagemaker_pyspark.",
        "Question_gpt_summary":"user encount challeng work confus differ pyspark librari pyspark offer distribut comput capabl experienc slow train time wonder wai reus endpoint differ train job user unsur pyspark support custom algorithm hyperparamet optim confus concept entri point contain imag model contain final user consid combin step function glue process pyspark workflow curiou differ sparkml model sparkmlmodel pyspark",
        "Answer_original_content":"python sdk call relat aw servic api behalf need us life easier equival run train job take advantag distribut comput capabl aw assum implement pyspark run distribut train job instanc count paramet pyspark facilit call relat aw servic api spark us want us servic spark normal like model xgboost estim fit train data minut run pyspark small set test data ye take minut instanc spin us local mode want iter quickli local note local mode won work built algorithm prototyp non aw xgboost scikit learn pyspark support custom algorithm allow us predefin on librari ye probabl want extend estim provid trainingimag uri know pyspark perform hyperparamet optim appear probabl easier deploi model contain certainli host model wai want want us model infer host contain requir us model contain need defin model extern entri point script docker thing make bundl depend easier make thing languag runtim neutral care algorithm python java fortran need know run tell work directori command run entri point confus class algorithmestim allow input argument algorithm arn differ wai pass model input better clarifi refer question answer requir differ pyspark offer pyspark let servic spark sparkml serv let us spark servic",
        "Answer_preprocessed_content":"python sdk call relat aw servic api behalf need us life easier equival run train job take advantag distribut comput capabl aw assum implement run distribut train job facilit call relat aw servic api spark us want us servic spark normal like model minut run small set test data ye take minut instanc us local mode want iter quickli local note local mode won work algorithm prototyp support custom algorithm allow us predefin on librari ye probabl want extend estim provid uri know perform hyperparamet optim appear probabl easier deploi model contain certainli host model wai want want us model infer host contain requir us model contain need defin model extern script docker thing make bundl depend easier make thing care algorithm python java fortran need know run tell work directori command run entri point confus class algorithmestim allow input argument differ wai pass model input better clarifi refer question answer requir differ offer let servic spark sparkml serv let us spark servic",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. The user can run distributed training jobs using sagemaker by specifying the instance_count parameter.\n2. To iterate more quickly locally, the user can use Local Mode, which won't work with SageMaker built-in algorithms but can be used with XGBoost\/SciKit-Learn.\n3. sagemaker_pyspark supports custom algorithms, but the user would need to extend SageMakerEstimator and provide the trainingImage URI.\n4. sagemaker_pyspark does not support hyperparameter optimization, but it can be done from SageMaker itself.\n5. Containers are required if the user wants to use SageMaker model inference hosting.\n6. Model containers make bundling dependencies easier and make",
        "Answer_gpt_summary":"possibl solut mention discuss user run distribut train job specifi instanc count paramet iter quickli local user us local mode won work built algorithm xgboost scikit learn pyspark support custom algorithm user need extend estim provid trainingimag uri pyspark support hyperparamet optim contain requir user want us model infer host model contain bundl depend easier"
    },
    {
        "Question_title":"Consume Microsoft Cluster API using PowerBI",
        "Question_body":"<p>Thanks for getting back to me.<\/p>\n\n<p>Basically I subscribed to a Cluster API service (cortana analytics). This is the sample application as per Microsoft Machine Learning site<\/p>\n\n<p><a href=\"http:\/\/microsoftazuremachinelearning.azurewebsites.net\/ClusterModel.aspx\" rel=\"nofollow\">http:\/\/microsoftazuremachinelearning.azurewebsites.net\/ClusterModel.aspx<\/a><\/p>\n\n<p>As you could see there are 2 arguments to be passed on<\/p>\n\n<p>Input<\/p>\n\n<p>K<\/p>\n\n<p>Where input could be 10;5;2,18;1;6,7;5;5,22;3;4,12;2;1,10;3;4 (each row is separated by semi colon)<\/p>\n\n<p>And K is cluster number: 5 (for example)<\/p>\n\n<p>So to consume this API I use PowerBI Edit Query, <\/p>\n\n<p>So go to Get Data > More > Azure > Microsoft Data MarketPlace, I can see the list of APIs I subscribed to, one of them is the one I referred to in the link above.<\/p>\n\n<p>So I load that as Function lets called it \"Score\"<\/p>\n\n<p>Then I got energy table which I loaded in from a csv file, I want to cluster energy consumption into 5 clusters.<\/p>\n\n<p>So my data layout is<\/p>\n\n<p>Year   Energy<\/p>\n\n<p>2001   6.28213<\/p>\n\n<p>2002  14.12845<\/p>\n\n<p>2003   5.55851<\/p>\n\n<p>and so on, lets say I got 100 rows of the data.<\/p>\n\n<p>So I tried to pass \"6.28213;14.12845;5.55851\", \"5\" to Score function but I dont know how to <\/p>\n\n<ol>\n<li><p>Convert my table into records<\/p><\/li>\n<li><p>pass 2 argument records and constant value 5 as K.<\/p><\/li>\n<\/ol>\n\n<p>Hope this makes sense.<\/p>\n\n<p>Please help! :)<\/p>\n\n<p>Thank you in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1476795601683,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":137.0,
        "Answer_body":"<p>To convert a column of numbers into a semicolon delimited text, do this to your table:<\/p>\n\n<ol>\n<li>Convert your Energy column is type text.<\/li>\n<li>Add <code>[Energy]<\/code> after the name of your table, which gives you a list of the numbers.<\/li>\n<li>Use <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/mt253358.aspx\" rel=\"nofollow\"><code>Text.Combine<\/code><\/a> to turn the list into a text value seperated by <code>;<\/code><\/li>\n<\/ol>\n\n<p>Here's a mashup that does that:<\/p>\n\n<pre><code>let\n    Source = Table.FromRows(Json.Document(Binary.Decompress(Binary.FromText(\"NcjBCQAgDAPAXfKWYqKR7iLdfw1F8J63N9Q70bBCKQ5Ue6VbnEHl9L9xz2GniaoD\", BinaryEncoding.Base64), Compression.Deflate)), let _t = ((type text) meta [Serialized.Text = true]) in type table [Year = _t, Energy = _t]),\n    #\"Changed Type\" = Table.TransformColumnTypes(Source,{{\"Year\", Int64.Type}, {\"Energy\", type text}}),\n    Custom1 = #\"Changed Type\"[Energy],\n    Custom2 = Text.Combine(Custom1, \";\")\nin\n    Custom2\n<\/code><\/pre>\n\n<hr>\n\n<p>Once you have a function, you'll invoke it like <code>YourFunction(Custum2, 5)<\/code><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40108999",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1476896300823,
        "Question_original_content":"consum microsoft cluster api powerbi thank get basic subscrib cluster api servic cortana analyt sampl applic microsoft machin learn site http microsoftazuremachinelearn azurewebsit net clustermodel aspx argument pass input input row separ semi colon cluster number exampl consum api us powerbi edit queri data azur microsoft data marketplac list api subscrib refer link load function let call score got energi tabl load csv file want cluster energi consumpt cluster data layout year energi let got row data tri pass score function dont know convert tabl record pass argument record constant valu hope make sens help thank advanc",
        "Question_preprocessed_content":"consum microsoft cluster api powerbi thank get basic subscrib cluster api servic sampl applic microsoft machin learn site argument pass input input cluster number consum api us powerbi edit queri data azur microsoft data marketplac list api subscrib refer link load function let call score got energi tabl load csv file want cluster energi consumpt cluster data layout year energi let got row data tri pass score function dont know convert tabl record pass argument record constant valu hope make sens help thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges in consuming a Cluster API service using PowerBI Edit Query. They are struggling to convert their energy consumption data table into records and pass two argument records and a constant value of 5 as K to the API.",
        "Question_gpt_summary":"user face challeng consum cluster api servic powerbi edit queri struggl convert energi consumpt data tabl record pass argument record constant valu api",
        "Answer_original_content":"convert column number semicolon delimit text tabl convert energi column type text add energi tabl give list number us text combin turn list text valu seper mashup let sourc tabl fromrow json document binari decompress binari fromtext ncjbcqagdapaxfkwyqkrildfwfjnqbbckquevbnehllxzgniaod binaryencod base compress deflat let type text meta serial text true type tabl year energi chang type tabl transformcolumntyp sourc year int type energi type text custom chang type energi custom text combin custom custom function invok like yourfunct custum",
        "Answer_preprocessed_content":"convert column number semicolon delimit text tabl convert energi column type text add tabl give list number us turn list text valu seper mashup function invok like",
        "Answer_gpt_summary_original":"Solution:\n- Convert the Energy column to type text.\n- Add [Energy] after the name of the table to get a list of numbers.\n- Use Text.Combine to turn the list into a text value separated by \";\".\n- Invoke the function with the converted Energy column and a constant value of 5 as arguments.",
        "Answer_gpt_summary":"solut convert energi column type text add energi tabl list number us text combin turn list text valu separ invok function convert energi column constant valu argument"
    },
    {
        "Question_title":"Permission \"artifactregistry.repositories.downloadArtifacts\" denied on resource",
        "Question_body":"<p>While the artifact repository was successfully creating, running a docker push to push the image to the google artifact registry fails with a permissions error even after granting all artifact permissions to the accounting I am using on gcloud cli.<\/p>\n<p><strong>Command used to push image:<\/strong><\/p>\n<pre><code>docker push us-central1-docker.pkg.dev\/project-id\/repo-name:v2\n<\/code><\/pre>\n<p><strong>Error message:<\/strong><\/p>\n<pre><code>The push refers to repository [us-central1-docker.pkg.dev\/project-id\/repo-name]\n6f6f4a472f31: Preparing\nbc096d7549c4: Preparing\n5f70bf18a086: Preparing\n20bed28d4def: Preparing\n2a3255c6d9fb: Preparing\n3f5d38b4936d: Waiting\n7be8268e2fb0: Waiting\nb889a93a79dd: Waiting\n9d4550089a93: Waiting\na7934564e6b9: Waiting\n1b7cceb6a07c: Waiting\nb274e8788e0c: Waiting\n78658088978a: Waiting\ndenied: Permission &quot;artifactregistry.repositories.downloadArtifacts&quot; denied on resource &quot;projects\/project-id\/locations\/us-central1\/repositories\/repo-name&quot; (or it may not exist)\n\n\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1652644857243,
        "Question_favorite_count":null,
        "Question_last_edit_time":1652667678670,
        "Question_score":12.0,
        "Question_view_count":5722.0,
        "Answer_body":"<p>I was able to recreate your use case. This happens when you are trying to push an image on a <code>repository<\/code> in which its specific hostname (associated with it's repository location) is not yet  added to the credential helper configuration for authentication. You may refer to this <a href=\"https:\/\/cloud.google.com\/artifact-registry\/docs\/docker\/authentication\" rel=\"noreferrer\">Setting up authentication for Docker <\/a> as also provided by @DazWilkin in the comments for more details.<\/p>\n<p>In my example, I was trying to push an image on a repository that has a location of <code>us-east1<\/code> and got the same error since it is not yet added to the credential helper configuration.\n<a href=\"https:\/\/i.stack.imgur.com\/NQeIf.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NQeIf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>And after I ran the authentication using below command (specifically for us-east1 since it is the <code>location<\/code> of my repository), the image was successfully pushed:<\/p>\n<pre><code>gcloud auth configure-docker us-east1-docker.pkg.dev\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q2Q9x.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q2Q9x.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><em><strong>QUICK TIP<\/strong><\/em>: You may  get your authentication command specific for your repository when you open your desired repository in the <a href=\"https:\/\/console.cloud.google.com\/artifacts\" rel=\"noreferrer\">console<\/a>, and then click on the <code>SETUP INSTRUCTIONS<\/code>.\n<a href=\"https:\/\/i.stack.imgur.com\/KBjqa.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KBjqa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1654468583460,
        "Answer_score":23.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72251787",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1652683203067,
        "Question_original_content":"permiss artifactregistri repositori downloadartifact deni resourc artifact repositori successfulli creat run docker push push imag googl artifact registri fail permiss error grant artifact permiss account gcloud cli command push imag docker push central docker pkg dev project repo error messag push refer repositori central docker pkg dev project repo ffaf prepar bcdc prepar fbfa prepar bedddef prepar acdfb prepar fdbd wait beefb wait baadd wait wait aeb wait bccebac wait beec wait wait deni permiss artifactregistri repositori downloadartifact deni resourc project project locat central repositori repo exist",
        "Question_preprocessed_content":"permiss deni resourc artifact repositori successfulli creat run docker push push imag googl artifact registri fail permiss error grant artifact permiss account gcloud cli command push imag error messag",
        "Question_gpt_summary_original":"The user encountered a permissions error while trying to push an image to the Google Artifact Registry using the \"docker push\" command. The error message indicates that the user was denied permission to download artifacts on the resource \"projects\/project-id\/locations\/us-central1\/repositories\/repo-name\". Despite granting all artifact permissions to the accounting being used on gcloud cli, the error persisted.",
        "Question_gpt_summary":"user encount permiss error try push imag googl artifact registri docker push command error messag indic user deni permiss download artifact resourc project project locat central repositori repo despit grant artifact permiss account gcloud cli error persist",
        "Answer_original_content":"abl recreat us case happen try push imag repositori specif hostnam associ repositori locat ad credenti helper configur authent refer set authent docker provid dazwilkin comment detail exampl try push imag repositori locat east got error ad credenti helper configur ran authent command specif east locat repositori imag successfulli push gcloud auth configur docker east docker pkg dev quick tip authent command specif repositori open desir repositori consol click setup instruct",
        "Answer_preprocessed_content":"abl recreat us case happen try push imag specif hostnam ad credenti helper configur authent refer set authent docker provid comment detail exampl try push imag repositori locat got error ad credenti helper configur ran authent command imag successfulli push quick tip authent command specif repositori open desir repositori consol click",
        "Answer_gpt_summary_original":"Solution:\n- The user needs to add the specific hostname associated with the repository location to the credential helper configuration for authentication.\n- The user can refer to the Setting up authentication for Docker documentation for more details.\n- The user can run the authentication command specific to their repository location, which can be found in the SETUP INSTRUCTIONS section of the repository in the console.",
        "Answer_gpt_summary":"solut user need add specif hostnam associ repositori locat credenti helper configur authent user refer set authent docker document detail user run authent command specif repositori locat setup instruct section repositori consol"
    },
    {
        "Question_title":"R model deployment with custom Docker image: \"ModuleNotFoundError: No module named 'azureml.api'\"",
        "Question_body":"I am trying to deploy an R inference script to Azure ML Service Endpoint as an Azure Container Instance. I have made the following steps:\n\ncreated a custom Docker image from scratch and pushed it to the Azure Container Registry (associated with AML Workspace)\n\n\nregistered a custom environment in AML Workspace, based on the image in ACR\n\n\ndeployed R entry script (just a simple hello world script with init() and run() functions defined)\n\n\nthe inference configuration uses the custom AML environment\n\n\ndeployment is made with Azure ML R SDK\n\nThe container instance is created, but the endpoint startup runs into error. Here is the output from the container instance:\n\n 2020-10-16T12:56:21,639812796+00:00 - gunicorn\/run \n 2020-10-16T12:56:21,639290594+00:00 - iot-server\/run \n 2020-10-16T12:56:21,640405198+00:00 - rsyslog\/run \n 2020-10-16T12:56:21,735291424+00:00 - nginx\/run \n EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n 2020-10-16T12:56:23,736657191+00:00 - iot-server\/finish 1 0\n 2020-10-16T12:56:23,834747728+00:00 - Exit code 1 is normal. Not restarting iot-server.\n Starting gunicorn 20.0.4\n Listening at: http:\/\/127.0.0.1:31311 (11)\n Using worker: sync\n worker timeout is set to 300\n Booting worker with pid: 38\n \/bin\/bash: \/root\/miniconda3\/lib\/libtinfo.so.6: no version information available (required by \/bin\/bash)\n SPARK_HOME not set. Skipping PySpark Initialization.\n Exception in worker process\n Traceback (most recent call last):\n   File \"\/var\/azureml-server\/app.py\", line 43, in <module>\n     from azureml.api.exceptions.ClientSideException import ClientSideException\n ModuleNotFoundError: No module named 'azureml.api'\n    \n During handling of the above exception, another exception occurred:\n    \n Traceback (most recent call last):\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/arbiter.py\", line 583, in spawn_worker\n     worker.init_process()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/workers\/base.py\", line 119, in init_process\n     self.load_wsgi()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/workers\/base.py\", line 144, in load_wsgi\n     self.wsgi = self.app.wsgi()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/base.py\", line 67, in wsgi\n     self.callable = self.load()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/wsgiapp.py\", line 49, in load\n     return self.load_wsgiapp()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/wsgiapp.py\", line 39, in load_wsgiapp\n     return util.import_app(self.app_uri)\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/util.py\", line 383, in import_app\n     mod = importlib.import_module(module)\n   File \"\/usr\/lib\/python3.8\/importlib\/__init__.py\", line 127, in import_module\n     return _bootstrap._gcd_import(name[level:], package, level)\n   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\n   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n   File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n   File \"\/var\/azureml-server\/wsgi.py\", line 1, in <module>\n     import create_app\n   File \"\/var\/azureml-server\/create_app.py\", line 3, in <module>\n     from app import main\n   File \"\/var\/azureml-server\/app.py\", line 45, in <module>\n     from azure.ml.api.exceptions.ClientSideException import ClientSideException\n ModuleNotFoundError: No module named 'azure.ml'\n Worker exiting (pid: 38)\n Shutting down: Master\n Reason: Worker failed to boot.\n 2020-10-16T12:56:39,434787859+00:00 - gunicorn\/finish 3 0\n 2020-10-16T12:56:39,435715063+00:00 - Exit code 3 is not normal. Killing image.\n\n\n\nHow do I install the azureml.api dependency, which can not be found? It doesn't seem to be part of the Azure ML SDK. I have installed the following dependencies in my Dockerfile:\n\n RUN apt-get -y install python3-flask python3-rpy2 python3-azure python3-applicationinsights\n RUN pip install azureml-core\n\n\n\nI also have Miniconda installed. Pip refers to Miniconda's pip.\n\nOr, is this dependency available to install at all? Should I use some pre-defined AML environment as the base Docker image? (Note: I am currently using bare FROM: ubuntu). Suggestions how to find and use the base images are also welcome, since this is not documented very well.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1602855221470,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@LauriLehman-8626 Thanks for the question. Here is the document to Create Custom Docker Base Images for Azure Machine Learning Environments for R people.\nWe have used the AzureML RScriptStep pipeline feature which allows you to point to CRAN or Github or custom URLS, but this requires authoring the pipeline in python or YAML.. In R You can also these arguments in the Azuremlsdk R estimator function: https:\/\/azure.github.io\/azureml-sdk-for-r\/reference\/estimator.html\nAnother option that are not available through conda install as part of the R script with install.packages(\u201cpath\/*.tar.gz\u201d, repos=NULL))\n\nOne of the challenges is that the build at runtime can take a while to prepare the environment. R likes to compile packages on Linux environments and a large package could have lots of dependencies which would take a while. This is an R on Linux\/PaaS thing, rather than specific to AzureML\n\nTo make start up fast we created a custom docker image where you can tightly control the image ahead of runtime. If you want to go in this direction you can find an example Dockerfile to get you started here..\nhttps:\/\/github.com\/Azure\/azureml-sdk-for-r\/blob\/master\/.azure-pipelines\/docker\/Dockerfile",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/129038\/r-model-deployment-with-custom-docker-image-34modu.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-19T10:44:00.927Z",
                "Answer_score":1,
                "Answer_body":"@LauriLehman-8626 Thanks for the question. Here is the document to Create Custom Docker Base Images for Azure Machine Learning Environments for R people.\nWe have used the AzureML RScriptStep pipeline feature which allows you to point to CRAN or Github or custom URLS, but this requires authoring the pipeline in python or YAML.. In R You can also these arguments in the Azuremlsdk R estimator function: https:\/\/azure.github.io\/azureml-sdk-for-r\/reference\/estimator.html\nAnother option that are not available through conda install as part of the R script with install.packages(\u201cpath\/*.tar.gz\u201d, repos=NULL))\n\nOne of the challenges is that the build at runtime can take a while to prepare the environment. R likes to compile packages on Linux environments and a large package could have lots of dependencies which would take a while. This is an R on Linux\/PaaS thing, rather than specific to AzureML\n\nTo make start up fast we created a custom docker image where you can tightly control the image ahead of runtime. If you want to go in this direction you can find an example Dockerfile to get you started here..\nhttps:\/\/github.com\/Azure\/azureml-sdk-for-r\/blob\/master\/.azure-pipelines\/docker\/Dockerfile",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1603104240927,
        "Question_original_content":"model deploy custom docker imag modulenotfounderror modul name api try deploi infer script servic endpoint azur contain instanc follow step creat custom docker imag scratch push azur contain registri associ aml workspac regist custom environ aml workspac base imag acr deploi entri script simpl hello world script init run function defin infer configur us custom aml environ deploy sdk contain instanc creat endpoint startup run error output contain instanc gunicorn run iot server run rsyslog run nginx run edgehubconnectionstr iotedg iothubhostnam set exit iot server finish exit code normal restart iot server start gunicorn listen http worker sync worker timeout set boot worker pid bin bash root miniconda lib libtinfo version inform avail requir bin bash spark home set skip pyspark initi except worker process traceback recent file var server app line api except clientsideexcept import clientsideexcept modulenotfounderror modul name api handl except except occur traceback recent file usr lib python dist packag gunicorn arbit line spawn worker worker init process file usr lib python dist packag gunicorn worker base line init process self load wsgi file usr lib python dist packag gunicorn worker base line load wsgi self wsgi self app wsgi file usr lib python dist packag gunicorn app base line wsgi self callabl self load file usr lib python dist packag gunicorn app wsgiapp line load return self load wsgiapp file usr lib python dist packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file usr lib python dist packag gunicorn util line import app mod importlib import modul modul file usr lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file var server wsgi line import creat app file var server creat app line app import main file var server app line azur api except clientsideexcept import clientsideexcept modulenotfounderror modul name azur worker exit pid shut master reason worker fail boot gunicorn finish exit code normal kill imag instal api depend sdk instal follow depend dockerfil run apt instal python flask python rpy python azur python applicationinsight run pip instal core miniconda instal pip refer miniconda pip depend avail instal us pre defin aml environ base docker imag note current bare ubuntu suggest us base imag welcom document",
        "Question_preprocessed_content":"model deploy custom docker imag modulenotfounderror modul name try deploi infer script servic endpoint azur contain instanc follow step creat custom docker imag scratch push azur contain registri regist custom environ aml workspac base imag acr deploi entri script run function defin infer configur us custom aml environ deploy sdk contain instanc creat endpoint startup run error output contain instanc edgehubconnectionstr set exit code normal restart start gunicorn listen worker sync worker timeout set boot worker pid version inform avail set skip pyspark initi except worker process traceback file line import clientsideexcept modulenotfounderror modul name handl except except occur traceback file line file line file line file line wsgi file line load return file line return file line mod file line return packag level file line file line file line file line file line file line file line import file line app import main file line import clientsideexcept modulenotfounderror modul name worker exit shut master reason worker fail boot exit code normal kill imag instal api depend sdk instal follow depend dockerfil run instal run pip instal core miniconda instal pip refer miniconda pip depend avail instal us aml environ base docker imag suggest us base imag welcom document",
        "Question_gpt_summary_original":"The user is encountering an error while trying to deploy an R inference script to Azure ML Service Endpoint as an Azure Container Instance. The error message indicates that the azureml.api dependency cannot be found. The user has installed several dependencies in their Dockerfile and is using a custom Docker image, but is unsure how to install the missing dependency or if it is available to install at all. The user is also considering using a pre-defined AML environment as the base Docker image but is unsure how to find and use them.",
        "Question_gpt_summary":"user encount error try deploi infer script servic endpoint azur contain instanc error messag indic api depend user instal depend dockerfil custom docker imag unsur instal miss depend avail instal user consid pre defin aml environ base docker imag unsur us",
        "Answer_original_content":"laurilehman thank question document creat custom docker base imag environ peopl rscriptstep pipelin featur allow point cran github custom url requir author pipelin python yaml argument sdk estim function http azur github sdk refer estim html option avail conda instal script instal packag path tar repo null challeng build runtim prepar environ like compil packag linux environ larg packag lot depend linux paa thing specif start fast creat custom docker imag tightli control imag ahead runtim want direct exampl dockerfil start http github com azur sdk blob master azur pipelin docker dockerfil",
        "Answer_preprocessed_content":"thank question document creat custom docker base imag environ peopl rscriptstep pipelin featur allow point cran github custom url requir author pipelin python argument sdk estim function option avail conda instal script repo null challeng build runtim prepar environ like compil packag linux environ larg packag lot depend thing specif start fast creat custom docker imag tightli control imag ahead runtim want direct exampl dockerfil start",
        "Answer_gpt_summary_original":"Solutions provided in the discussion include using the AzureML RScriptStep pipeline feature, using the Azuremlsdk R estimator function, and installing packages using install.packages(). The user can also create a custom Docker image to control the environment ahead of runtime. The discussion also notes that the build at runtime can take a while due to R compiling packages on Linux environments with many dependencies. No personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"solut provid discuss includ rscriptstep pipelin featur sdk estim function instal packag instal packag user creat custom docker imag control environ ahead runtim discuss note build runtim compil packag linux environ depend person opinion bias includ summari"
    },
    {
        "Question_title":"How to add data from \"scratch\" filesystem directly to \"scratch\" cache dir",
        "Question_body":"<p>My scenario is that I\u2019m on a cluster with really limited home directory quota (40GB) but with TB of \u201cscratch\u201d space on a separate filesystem. I\u2019d like to keep my Git repo on the home filesystem (say at <code>~\/myrepo\/<\/code>), but the DVC cache on the scratch file system (say at <code>\/scratch\/dvc<\/code>).<\/p>\n<p>Its easy enough to configure the cache on the scratch filesystem with e.g. <code>dvc cache dir \/scratch\/dvc<\/code> and then use the <code>symlink<\/code> cache type so files just symlink to here.<\/p>\n<p>But now, suppose I\u2019ve generated a large file at <code>\/scratch\/newfile.dat<\/code>. How can I add it to the repo at a chosen path without it ever touching the home filesystem?<\/p>\n<p>I would have thought:<\/p>\n<pre><code class=\"lang-bash\">dvc add -o ~\/myrepo\/newfile.dat \/scratch\/newfile.dat\n<\/code><\/pre>\n<p>would do it but that still seems to go through the home filesystem.<\/p>\n<p>It seems if I add <code>--to-remote<\/code> it works as expected, but it <em>also<\/em> (obviously) pushes to remote, which I don\u2019t want.<\/p>\n<p>Thanks for any suggestions.<\/p>",
        "Question_answer_count":10,
        "Question_comment_count":0,
        "Question_creation_time":1615839224546,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":458.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/marius\">@marius<\/a>,<\/p>\n<aside class=\"quote no-group\" data-username=\"marius\" data-post=\"1\" data-topic=\"702\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/m\/ed655f\/40.png\" class=\"avatar\"> marius:<\/div>\n<blockquote>\n<p>Its easy enough to configure the cache on the scratch filesystem with e.g. <code>dvc cache dir \/scratch\/dvc<\/code> and then use the <code>symlink<\/code> cache type so files just symlink to here.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Glad you were able to find that info already <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/+1.png?v=9\" title=\":+1:\" class=\"emoji\" alt=\":+1:\"> That\u2019s the first step indeed.<\/p>\n<aside class=\"quote no-group quote-modified\" data-username=\"marius\" data-post=\"1\" data-topic=\"702\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/m\/ed655f\/40.png\" class=\"avatar\"> marius:<\/div>\n<blockquote>\n<p>How can I add it to the repo at a chosen path without it ever touching the home filesystem?<\/p>\n<pre><code class=\"lang-auto\">dvc add -o ~\/myrepo\/newfile.dat \/scratch\/newfile.dat\n<\/code><\/pre>\n<\/blockquote>\n<\/aside>\n<p>If you\u2019re using DVC 2.x, then that exact command is what you need. It should transfer the data to the cache without needing much space available in the local system (it uses file chunking on as an intermediate step).<\/p>\n<p>See also <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-the-cache\">https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-the-cache<\/a><\/p>\n<p>Thanks.<\/p>. <aside class=\"quote no-group\" data-username=\"marius\" data-post=\"1\" data-topic=\"702\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/m\/ed655f\/40.png\" class=\"avatar\"> marius:<\/div>\n<blockquote>\n<pre><code class=\"lang-auto\">dvc add -o ~\/myrepo\/newfile.dat \/scratch\/newfile.dat\n<\/code><\/pre>\n<\/blockquote>\n<\/aside>\n<p>Not sure about the absolute path you\u2019re providing to <code>-o<\/code> though, please use a relative path from your project directory if needed, i.e.<\/p>\n<pre><code class=\"lang-auto\">$ cd ~\/myrepo\n$ dvc add \/scratch\/newfile.dat -o newfile.dat\n<\/code><\/pre>. <aside class=\"quote no-group\" data-username=\"jorgeorpinel\" data-post=\"2\" data-topic=\"702\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/jorgeorpinel\/40\/46_2.png\" class=\"avatar\"> jorgeorpinel:<\/div>\n<blockquote>\n<p>If you\u2019re using DVC 2.x, then that exact command is what you need. It should transfer the data to the cache without needing much space available in the local system (it uses file chunking on as an intermediate step).<\/p>\n<\/blockquote>\n<\/aside>\n<p>Ahhh I see, that piece of info is key, thanks (and I am on 2.x). The reason I thought the home filesystem was being touched was because the write speed was kind of slow, indicative of having gone scratch \u2192 home \u2192 scratch, as compared to a straight scratch \u2192 scratch copy which on this system is much faster. So I suppose that\u2019s just because the data <em>does<\/em> pass through the home filesystem, but it does so in chunks so it shouldn\u2019t overwhelm my quota. I guess a question I have is why not a direct copy, but in any case this is very helpful and solves my major worry about the quota. Thanks!<\/p>. <aside class=\"quote no-group\" data-username=\"marius\" data-post=\"4\" data-topic=\"702\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/m\/ed655f\/40.png\" class=\"avatar\"> marius:<\/div>\n<blockquote>\n<p>as compared to a straight scratch \u2192 scratch copy which on this system is much faster<\/p>\n<\/blockquote>\n<\/aside>\n<p>Yes that would be ideal. I\u2019ll check with engineering on this\u2026 Thanks for the feedback!<\/p>. <p>Another quick clarification, BTW: the transfer doesn\u2019t actually use any local disk space at all, as the chunks are stored locally in memory only.<\/p>. <aside class=\"quote no-group\" data-username=\"jorgeorpinel\" data-post=\"5\" data-topic=\"702\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/jorgeorpinel\/40\/46_2.png\" class=\"avatar\"> jorgeorpinel:<\/div>\n<blockquote>\n<p>that would be ideal. I\u2019ll check with engineering<\/p>\n<\/blockquote>\n<\/aside>\n<p>Finally, I did check and for now DVC can\u2019t support direct transfers from external data locations to remote storage (without in-memory chunking locally) because we need to calculate the <code>md5<\/code> hash of the data (which can only be done locally) in order to <code>dvc add<\/code> it properly. Maybe for DVC 3.0 though, we\u2019ll see! And we still welcome any feature requests on our Github repo.<\/p>\n<p>Thanks again!<\/p>. <aside class=\"quote no-group\" data-username=\"jorgeorpinel\" data-post=\"6\" data-topic=\"702\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/jorgeorpinel\/40\/46_2.png\" class=\"avatar\"> jorgeorpinel:<\/div>\n<blockquote>\n<p>BTW: the transfer doesn\u2019t actually use any local disk space at all, as the chunks are stored locally in memory only.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Hmm I suppose then there\u2019s something I\u2019m not understanding about what was happening, will look more, but in any case the info here is very helpful, thanks!<\/p>. <p>Well, it still needs to download <strong>and upload<\/strong> each chunk (into memory) so the transfer still gets bottlenecked locally <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>. <p>To clarify, \u201cscratch\u201d and \u201chome\u201d filesytems I\u2019m referring to here are two different network filesystems both mounted on the same machine. As far as DVC sees, there are only file copies, no upload\/download. It happens that scratch &lt;-&gt; scratch is fast scratch &lt;-&gt; home is slow, the bottleneck I think being home disk and\/or network speed, so I don\u2019t think its an issue that data passes through memory (which in this configuration to me seems unavoidable to copy from one filesystem to another).<\/p>. <p>Ah yes good point. I guess still in this case the chunked transfer with Python-level i\/o implemented in DVC may still be significantly slower than a native file system call i.e. <code>cp<\/code>.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-add-data-from-scratch-filesystem-directly-to-scratch-cache-dir\/702",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-15T22:40:47.227Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/marius\">@marius<\/a>,<\/p>\n<aside class=\"quote no-group\" data-username=\"marius\" data-post=\"1\" data-topic=\"702\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/m\/ed655f\/40.png\" class=\"avatar\"> marius:<\/div>\n<blockquote>\n<p>Its easy enough to configure the cache on the scratch filesystem with e.g. <code>dvc cache dir \/scratch\/dvc<\/code> and then use the <code>symlink<\/code> cache type so files just symlink to here.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Glad you were able to find that info already <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/+1.png?v=9\" title=\":+1:\" class=\"emoji\" alt=\":+1:\"> That\u2019s the first step indeed.<\/p>\n<aside class=\"quote no-group quote-modified\" data-username=\"marius\" data-post=\"1\" data-topic=\"702\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/m\/ed655f\/40.png\" class=\"avatar\"> marius:<\/div>\n<blockquote>\n<p>How can I add it to the repo at a chosen path without it ever touching the home filesystem?<\/p>\n<pre><code class=\"lang-auto\">dvc add -o ~\/myrepo\/newfile.dat \/scratch\/newfile.dat\n<\/code><\/pre>\n<\/blockquote>\n<\/aside>\n<p>If you\u2019re using DVC 2.x, then that exact command is what you need. It should transfer the data to the cache without needing much space available in the local system (it uses file chunking on as an intermediate step).<\/p>\n<p>See also <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-the-cache\">https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-the-cache<\/a><\/p>\n<p>Thanks.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-15T22:41:59.499Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"marius\" data-post=\"1\" data-topic=\"702\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/m\/ed655f\/40.png\" class=\"avatar\"> marius:<\/div>\n<blockquote>\n<pre><code class=\"lang-auto\">dvc add -o ~\/myrepo\/newfile.dat \/scratch\/newfile.dat\n<\/code><\/pre>\n<\/blockquote>\n<\/aside>\n<p>Not sure about the absolute path you\u2019re providing to <code>-o<\/code> though, please use a relative path from your project directory if needed, i.e.<\/p>\n<pre><code class=\"lang-auto\">$ cd ~\/myrepo\n$ dvc add \/scratch\/newfile.dat -o newfile.dat\n<\/code><\/pre>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-15T23:48:04.081Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"jorgeorpinel\" data-post=\"2\" data-topic=\"702\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/jorgeorpinel\/40\/46_2.png\" class=\"avatar\"> jorgeorpinel:<\/div>\n<blockquote>\n<p>If you\u2019re using DVC 2.x, then that exact command is what you need. It should transfer the data to the cache without needing much space available in the local system (it uses file chunking on as an intermediate step).<\/p>\n<\/blockquote>\n<\/aside>\n<p>Ahhh I see, that piece of info is key, thanks (and I am on 2.x). The reason I thought the home filesystem was being touched was because the write speed was kind of slow, indicative of having gone scratch \u2192 home \u2192 scratch, as compared to a straight scratch \u2192 scratch copy which on this system is much faster. So I suppose that\u2019s just because the data <em>does<\/em> pass through the home filesystem, but it does so in chunks so it shouldn\u2019t overwhelm my quota. I guess a question I have is why not a direct copy, but in any case this is very helpful and solves my major worry about the quota. Thanks!<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-16T02:20:09.955Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"marius\" data-post=\"4\" data-topic=\"702\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/m\/ed655f\/40.png\" class=\"avatar\"> marius:<\/div>\n<blockquote>\n<p>as compared to a straight scratch \u2192 scratch copy which on this system is much faster<\/p>\n<\/blockquote>\n<\/aside>\n<p>Yes that would be ideal. I\u2019ll check with engineering on this\u2026 Thanks for the feedback!<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-16T06:20:14.662Z",
                "Answer_body":"<p>Another quick clarification, BTW: the transfer doesn\u2019t actually use any local disk space at all, as the chunks are stored locally in memory only.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-16T06:31:22.177Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"jorgeorpinel\" data-post=\"5\" data-topic=\"702\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/jorgeorpinel\/40\/46_2.png\" class=\"avatar\"> jorgeorpinel:<\/div>\n<blockquote>\n<p>that would be ideal. I\u2019ll check with engineering<\/p>\n<\/blockquote>\n<\/aside>\n<p>Finally, I did check and for now DVC can\u2019t support direct transfers from external data locations to remote storage (without in-memory chunking locally) because we need to calculate the <code>md5<\/code> hash of the data (which can only be done locally) in order to <code>dvc add<\/code> it properly. Maybe for DVC 3.0 though, we\u2019ll see! And we still welcome any feature requests on our Github repo.<\/p>\n<p>Thanks again!<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-17T01:07:39.292Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"jorgeorpinel\" data-post=\"6\" data-topic=\"702\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/jorgeorpinel\/40\/46_2.png\" class=\"avatar\"> jorgeorpinel:<\/div>\n<blockquote>\n<p>BTW: the transfer doesn\u2019t actually use any local disk space at all, as the chunks are stored locally in memory only.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Hmm I suppose then there\u2019s something I\u2019m not understanding about what was happening, will look more, but in any case the info here is very helpful, thanks!<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-17T01:56:06.266Z",
                "Answer_body":"<p>Well, it still needs to download <strong>and upload<\/strong> each chunk (into memory) so the transfer still gets bottlenecked locally <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-17T02:13:52.575Z",
                "Answer_body":"<p>To clarify, \u201cscratch\u201d and \u201chome\u201d filesytems I\u2019m referring to here are two different network filesystems both mounted on the same machine. As far as DVC sees, there are only file copies, no upload\/download. It happens that scratch &lt;-&gt; scratch is fast scratch &lt;-&gt; home is slow, the bottleneck I think being home disk and\/or network speed, so I don\u2019t think its an issue that data passes through memory (which in this configuration to me seems unavoidable to copy from one filesystem to another).<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-17T05:50:08.644Z",
                "Answer_body":"<p>Ah yes good point. I guess still in this case the chunked transfer with Python-level i\/o implemented in DVC may still be significantly slower than a native file system call i.e. <code>cp<\/code>.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"add data scratch filesystem directli scratch cach dir scenario cluster limit home directori quota scratch space separ filesystem like git repo home filesystem myrepo cach scratch file scratch easi configur cach scratch filesystem cach dir scratch us symlink cach type file symlink suppos iv gener larg file scratch newfil dat add repo chosen path touch home filesystem thought add myrepo newfil dat scratch newfil dat home filesystem add remot work expect obvious push remot dont want thank suggest",
        "Question_preprocessed_content":"add data scratch filesystem directli scratch cach dir scenario cluster limit home directori quota scratch space separ filesystem like git repo home filesystem cach scratch file easi configur cach scratch filesystem us cach type file symlink suppos iv gener larg file add repo chosen path touch home filesystem thought home filesystem add work expect push remot dont want thank suggest",
        "Question_gpt_summary_original":"The user is facing a challenge of limited home directory quota on a cluster but with TB of \"scratch\" space on a separate filesystem. They want to keep their Git repo on the home filesystem but the DVC cache on the scratch filesystem. The user wants to add a large file to the repo at a chosen path without it ever touching the home filesystem, but the command they tried still seems to go through the home filesystem. The user found that adding \"--to-remote\" works as expected, but it also pushes to remote, which they don't want.",
        "Question_gpt_summary":"user face challeng limit home directori quota cluster scratch space separ filesystem want git repo home filesystem cach scratch filesystem user want add larg file repo chosen path touch home filesystem command tri home filesystem user ad remot work expect push remot want",
        "Answer_original_content":"mariu mariu easi configur cach scratch filesystem cach dir scratch us symlink cach type file symlink glad abl info that step mariu add repo chosen path touch home filesystem add myrepo newfil dat scratch newfil dat your exact command need transfer data cach need space avail local us file chunk intermedi step http org doc command refer add exampl transfer cach thank mariu add myrepo newfil dat scratch newfil dat sure absolut path your provid us rel path project directori need myrepo add scratch newfil dat newfil dat jorgeorpinel your exact command need transfer data cach need space avail local us file chunk intermedi step piec info kei thank reason thought home filesystem touch write speed kind slow indic have gone scratch home scratch compar straight scratch scratch copi faster suppos that data pass home filesystem chunk shouldnt overwhelm quota guess question direct copi case help solv major worri quota thank mariu compar straight scratch scratch copi faster ye ideal ill check engin thank feedback quick clarif btw transfer doesnt actual us local disk space chunk store local memori jorgeorpinel ideal ill check engin final check support direct transfer extern data locat remot storag memori chunk local need calcul hash data local order add properli mayb welcom featur request github repo thank jorgeorpinel btw transfer doesnt actual us local disk space chunk store local memori hmm suppos there understand happen look case info help thank need download upload chunk memori transfer get bottleneck local clarifi scratch home filesytem refer differ network filesystem mount machin far see file copi upload download happen scratch scratch fast scratch home slow bottleneck think home disk network speed dont think issu data pass memori configur unavoid copi filesystem ye good point guess case chunk transfer python level implement significantli slower nativ file",
        "Answer_preprocessed_content":"mariu easi configur cach scratch filesystem us cach type file symlink glad abl info that step mariu add repo chosen path touch home filesystem your exact command need transfer data cach need space avail local thank mariu sure absolut path your provid us rel path project directori need jorgeorpinel your exact command need transfer data cach need space avail local piec info kei thank reason thought home filesystem touch write speed kind slow indic have gone scratch home scratch compar straight scratch scratch copi faster suppos that data pass home filesystem chunk shouldnt overwhelm quota guess question direct copi case help solv major worri quota thank mariu compar straight scratch scratch copi faster ye ideal ill check engin thank feedback quick clarif btw transfer doesnt actual us local disk space chunk store local memori jorgeorpinel ideal ill check engin final check support direct transfer extern data locat remot storag need calcul hash data order properli mayb welcom featur request github repo thank jorgeorpinel btw transfer doesnt actual us local disk space chunk store local memori hmm suppos there understand happen look case info help thank need download upload chunk transfer get bottleneck local clarifi scratch home filesytem refer differ network filesystem mount machin far see file copi happen scratch scratch fast scratch home slow bottleneck think home disk network speed dont think issu data pass memori ye good point guess case chunk transfer implement significantli slower nativ file",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Configure the cache on the scratch filesystem with `dvc cache dir \/scratch\/dvc` and use the `symlink` cache type so files just symlink to here.\n- Use the command `dvc add -o ~\/myrepo\/newfile.dat \/scratch\/newfile.dat` to add a large file to the repo at a chosen path without it ever touching the home filesystem. This command transfers the data to the cache without needing much space available in the local system (it uses file chunking on as an intermediate step).\n- Use a relative path from the project directory if needed while using the `dvc add` command.\n- DVC can't support direct transfers from external data locations to remote storage",
        "Answer_gpt_summary":"possibl solut mention discuss configur cach scratch filesystem cach dir scratch us symlink cach type file symlink us command add myrepo newfil dat scratch newfil dat add larg file repo chosen path touch home filesystem command transfer data cach need space avail local us file chunk intermedi step us rel path project directori need add command support direct transfer extern data locat remot storag"
    },
    {
        "Question_title":"What is the difference between SageMaker Pipelines and SageMaker Step Function SDK?",
        "Question_body":"What is the difference between SageMaker Pipelines and SageMaker Step Function SDK?\n\nThe official Pipelines notebook is basically only doing a workflow - pretty much a copy cat of what step functions has been doing for years. In the nice video from Julien Simon I see CICD capacities mentioned, where are those? any demos?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1606994100000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":673.0,
        "Answer_body":"Hey, that demo is missing the project part of Pipelines and therefore the SM provided project templates. Go to SM studio and on the Studio summary hit edit settings and then enable access and provisioning of Service Catalog Portfolio of products in SM Studio. Then check your service catalog portfolios. Haven't tried it out yet though.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2iheeTzhSTmWw4aqVEeqOQ\/what-is-the-difference-between-sage-maker-pipelines-and-sage-maker-step-function-sdk",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-03T17:04:25.000Z",
                "Answer_score":0,
                "Answer_body":"Hey, that demo is missing the project part of Pipelines and therefore the SM provided project templates. Go to SM studio and on the Studio summary hit edit settings and then enable access and provisioning of Service Catalog Portfolio of products in SM Studio. Then check your service catalog portfolios. Haven't tried it out yet though.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-08-29T14:23:35.610Z",
                "Answer_score":0,
                "Answer_body":"both StepFunctions and SageMaker pipeline can be used to build a pipeline for ML. In the end, we need a SageMaker DAG StepFunction can integrate with different services, and good when one already used it. SageMaker pipeline is natively integrated with SageMaker system.\n\nThis is my example using both to build a ML Pipeline GitHub",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1607015065000,
        "Question_original_content":"differ pipelin step function sdk differ pipelin step function sdk offici pipelin notebook basic workflow pretti copi cat step function year nice video julien simon cicd capac mention demo",
        "Question_preprocessed_content":"differ pipelin step function sdk differ pipelin step function sdk offici pipelin notebook basic workflow pretti copi cat step function year nice video julien simon cicd capac mention demo",
        "Question_gpt_summary_original":"The user is seeking clarification on the differences between SageMaker Pipelines and SageMaker Step Function SDK. They are also looking for information on the CICD capabilities mentioned in a video by Julien Simon and any available demos.",
        "Question_gpt_summary":"user seek clarif differ pipelin step function sdk look inform cicd capabl mention video julien simon avail demo",
        "Answer_original_content":"hei demo miss project pipelin provid project templat studio studio summari hit edit set enabl access provis servic catalog portfolio product studio check servic catalog portfolio haven tri",
        "Answer_preprocessed_content":"hei demo miss project pipelin provid project templat studio studio summari hit edit set enabl access provis servic catalog portfolio product studio check servic catalog portfolio haven tri",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"How to solve the error with deploying a model in aws sagemaker?",
        "Question_body":"<p>I have to deploy a custom keras model in AWS Sagemaker. I have a created a notebook instance and I have the following files:<\/p>\n\n<pre><code>AmazonSagemaker-Codeset16\n   -ann\n      -nginx.conf\n      -predictor.py\n      -serve\n      -train.py\n      -wsgi.py\n   -Dockerfile\n<\/code><\/pre>\n\n<p>I now open the AWS terminal and build the docker image and push the image in the ECR repository. Then I open a new jupyter python notebook and try to fit the model and deploy the same. The training is done correctly but while deploying I get the following error:<\/p>\n\n<blockquote>\n  <p>\"Error hosting endpoint sagemaker-example-2019-10-25-06-11-22-366: Failed. >Reason: The primary container for production variant AllTraffic did not pass >the ping health check. Please check CloudWatch logs for this endpoint...\"<\/p>\n<\/blockquote>\n\n<p>When I check the logs, I find the following:<\/p>\n\n<blockquote>\n  <p>2019\/11\/11 11:53:32 [crit] 19#19: *3 connect() to unix:\/tmp\/gunicorn.sock >failed (2: No such file or directory) while connecting to upstream, client: >10.32.0.4, server: , request: \"GET \/ping HTTP\/1.1\", upstream: >\"<a href=\"http:\/\/unix:\/tmp\/gunicorn.sock:\/ping\" rel=\"nofollow noreferrer\">http:\/\/unix:\/tmp\/gunicorn.sock:\/ping<\/a>\", host: \"model.aws.local:8080\"<\/p>\n<\/blockquote>\n\n<p>and <\/p>\n\n<blockquote>\n  <p>Traceback (most recent call last):\n   File \"\/usr\/local\/bin\/serve\", line 8, in \n     sys.exit(main())\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/cli\/serve.py\", line 19, in main\n     server.start(env.ServingEnv().framework_module)\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/_server.py\", line 107, in start\n     module_app,\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 711, in <strong>init<\/strong>\n     errread, errwrite)\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 1343, in _execute_child\n     raise child_exception<\/p>\n<\/blockquote>\n\n<p>I tried to deploy the same model in AWS Sagemaker with these files in my local computer and the model was deployed successfully but inside AWS, I am facing this problem.<\/p>\n\n<p>Here is my serve file code:<\/p>\n\n<pre><code>from __future__ import print_function\nimport multiprocessing\nimport os\nimport signal\nimport subprocess\nimport sys\n\ncpu_count = multiprocessing.cpu_count()\n\nmodel_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)\nmodel_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))\n\n\ndef sigterm_handler(nginx_pid, gunicorn_pid):\n    try:\n        os.kill(nginx_pid, signal.SIGQUIT)\n    except OSError:\n        pass\n    try:\n        os.kill(gunicorn_pid, signal.SIGTERM)\n    except OSError:\n        pass\n\n    sys.exit(0)\n\n\ndef start_server():\n    print('Starting the inference server with {} workers.'.format(model_server_workers))\n\n\n    # link the log streams to stdout\/err so they will be logged to the container logs\n    subprocess.check_call(['ln', '-sf', '\/dev\/stdout', '\/var\/log\/nginx\/access.log'])\n    subprocess.check_call(['ln', '-sf', '\/dev\/stderr', '\/var\/log\/nginx\/error.log'])\n\n    nginx = subprocess.Popen(['nginx', '-c', '\/opt\/ml\/code\/nginx.conf'])\n    gunicorn = subprocess.Popen(['gunicorn',\n                                 '--timeout', str(model_server_timeout),\n                                 '-b', 'unix:\/tmp\/gunicorn.sock',\n                                 '-w', str(model_server_workers),\n                                 'wsgi:app'])\n\n    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\n\n    # If either subprocess exits, so do we.\n    pids = set([nginx.pid, gunicorn.pid])\n    while True:\n        pid, _ = os.wait()\n        if pid in pids:\n            break\n\n    sigterm_handler(nginx.pid, gunicorn.pid)\n    print('Inference server exiting')\n\n\n# The main routine just invokes the start function.\nif __name__ == '__main__':\n    start_server()\n<\/code><\/pre>\n\n<p>I deploy the model using the following:<\/p>\n\n<blockquote>\n  <p>predictor = classifier.deploy(1, 'ml.t2.medium', serializer=csv_serializer)<\/p>\n<\/blockquote>\n\n<p>Kindly let me know the mistake I am doing while deploying.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1573549904240,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1573550204732,
        "Question_score":1.0,
        "Question_view_count":4605.0,
        "Answer_body":"<p>Using Sagemaker script mode can be much simpler than dealing with container and nginx low-level stuff like you're trying to do, have you considered that?<br>\nYou only need to provide the keras script:   <\/p>\n\n<blockquote>\n  <p>With Script Mode, you can use training scripts similar to those you would use outside SageMaker with SageMaker's prebuilt containers for various deep learning frameworks such TensorFlow, PyTorch, and Apache MXNet.<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58815367",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1573630807523,
        "Question_original_content":"solv error deploi model deploi custom kera model creat notebook instanc follow file amazon codeset ann nginx conf predictor serv train wsgi dockerfil open aw termin build docker imag push imag ecr repositori open new jupyt python notebook try fit model deploi train correctli deploi follow error error host endpoint exampl fail reason primari contain product variant alltraff pass ping health check check cloudwatch log endpoint check log follow crit connect unix tmp gunicorn sock fail file directori connect upstream client server request ping http upstream http unix tmp gunicorn sock ping host model aw local traceback recent file usr local bin serv line sy exit main file usr local lib python dist packag contain cli serv line main server start env servingenv framework modul file usr local lib python dist packag contain server line start modul app file usr lib python subprocess line init eead errwrit file usr lib python subprocess line execut child rais child except tri deploi model file local model deploi successfulli insid aw face problem serv file code futur import print function import multiprocess import import signal import subprocess import sy cpu count multiprocess cpu count model server timeout environ model server timeout model server worker int environ model server worker cpu count def sigterm handler nginx pid gunicorn pid try kill nginx pid signal sigquit oserror pass try kill gunicorn pid signal sigterm oserror pass sy exit def start server print start infer server worker format model server worker link log stream stdout err log contain log subprocess check dev stdout var log nginx access log subprocess check dev stderr var log nginx error log nginx subprocess popen nginx opt code nginx conf gunicorn subprocess popen gunicorn timeout str model server timeout unix tmp gunicorn sock str model server worker wsgi app signal signal signal sigterm lambda sigterm handler nginx pid gunicorn pid subprocess exit pid set nginx pid gunicorn pid true pid wait pid pid break sigterm handler nginx pid gunicorn pid print infer server exit main routin invok start function main start server deploi model follow predictor classifi deploi medium serial csv serial kindli let know mistak deploi",
        "Question_preprocessed_content":"solv error deploi model deploi custom kera model creat notebook instanc follow file open aw termin build docker imag push imag ecr repositori open new jupyt python notebook try fit model deploi train correctli deploi follow error error host endpoint fail reason primari contain product variant alltraff pass ping health check check cloudwatch log check log follow connect fail connect upstream client server request upstream host traceback file line file line main file line start file line init eead errwrit file line rais tri deploi model file local model deploi successfulli insid aw face problem serv file code deploi model follow predictor kindli let know mistak deploi",
        "Question_gpt_summary_original":"The user is facing an error while deploying a custom Keras model in AWS Sagemaker. The training is done correctly, but while deploying, the primary container for production variant AllTraffic did not pass the ping health check. The user checked the logs and found that the connection to unix:\/tmp\/gunicorn.sock failed, and the subprocess failed to execute child. The user tried to deploy the same model in AWS Sagemaker with the same files on their local computer, and the model was deployed successfully, but inside AWS, they are facing this problem. The user has provided the serve file code and the deployment code used.",
        "Question_gpt_summary":"user face error deploi custom kera model train correctli deploi primari contain product variant alltraff pass ping health check user check log connect unix tmp gunicorn sock fail subprocess fail execut child user tri deploi model file local model deploi successfulli insid aw face problem user provid serv file code deploy code",
        "Answer_original_content":"script mode simpler deal contain nginx low level stuff like try consid need provid kera script script mode us train script similar us outsid prebuilt contain deep learn framework tensorflow pytorch apach mxnet http github com aw sampl amazon script mode blob master sentiment script mode sentiment analysi ipynb",
        "Answer_preprocessed_content":"script mode simpler deal contain nginx stuff like try consid need provid kera script script mode us train script similar us outsid prebuilt contain deep learn framework tensorflow pytorch apach mxnet",
        "Answer_gpt_summary_original":"Solution: One possible solution suggested in the discussion is to use Sagemaker script mode instead of dealing with container and nginx low-level stuff. With script mode, the user can use training scripts similar to those used outside Sagemaker with Sagemaker's prebuilt containers for various deep learning frameworks such as TensorFlow, PyTorch, and Apache MXNet. The user can refer to the provided link for more information on how to use script mode.",
        "Answer_gpt_summary":"solut possibl solut suggest discuss us script mode instead deal contain nginx low level stuff script mode user us train script similar outsid prebuilt contain deep learn framework tensorflow pytorch apach mxnet user refer provid link inform us script mode"
    },
    {
        "Question_title":"Feedback Welcome: dropping Python 2 support in MLflow 1.8.0",
        "Question_body":"Hi all,\n\nThis github issue\u00a0contains a proposal for dropping Python 2 support in MLflow 1.8.0, with a target release date of end-of-March, 2020.\n\n\nThis move will help reduce MLflow's ongoing maintenance burden, and we believe it makes sense given that Python 2 is EOL & many other ML libraries (pandas, numpy, sklearn, tensorflow) have already dropped support.\n\n\nPlease review the issue & leave questions or comments - barring major concerns, we'll publish a formal announcement the week of March 8 - thanks!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1583092647000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":10.0,
        "Answer_body":"Just a final ping on this, thanks all for your feedback!\n\n\nHaven't heard any blocking issues\/commentary related to this - barring new input, will go ahead & publish the announcement next week.\n\nThank you!\nSid\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/fMRrlEdZHos",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-03-06T16:45:33",
                "Answer_body":"Just a final ping on this, thanks all for your feedback!\n\n\nHaven't heard any blocking issues\/commentary related to this - barring new input, will go ahead & publish the announcement next week.\n\nThank you!\nSid\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"feedback welcom drop python support github issuecontain propos drop python support target releas date end march help reduc ongo mainten burden believ make sens given python eol librari panda numpi sklearn tensorflow drop support review issu leav question comment bar major concern publish formal announc week march thank",
        "Question_preprocessed_content":"feedback welcom drop python support github issuecontain propos drop python support target releas date help reduc ongo mainten burden believ make sens given python eol librari drop support review issu leav question comment bar major concern publish formal announc week march thank",
        "Question_gpt_summary_original":"The user is facing the challenge of MLflow dropping support for Python 2 in their upcoming release, which is scheduled for the end of March 2020. This decision is based on the fact that Python 2 is EOL and other ML libraries have already dropped support, and it will help reduce MLflow's maintenance burden. The user is invited to review the proposal and leave any questions or comments.",
        "Question_gpt_summary":"user face challeng drop support python upcom releas schedul end march decis base fact python eol librari drop support help reduc mainten burden user invit review propos leav question comment",
        "Answer_original_content":"final ping thank feedback haven heard block issu commentari relat bar new input ahead publish announc week thank sid",
        "Answer_preprocessed_content":"final ping thank feedback haven heard block relat bar new input ahead publish announc week thank sid",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion. The user was invited to review the proposal and leave any questions or comments.",
        "Answer_gpt_summary":"solut mention discuss user invit review propos leav question comment"
    },
    {
        "Question_title":"AWS Sagemaker AttributeError: can't set attribute error",
        "Question_body":"<p>I am new to python programming. Following the AWS learning path:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult<\/a><\/p>\n<p>I am getting an error when excuting the following block (in conda_python3):<\/p>\n<pre><code>test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\nxgb_predictor.content_type = 'text\/csv' # set the data type for an inference\nxgb_predictor.serializer = csv_serializer # set the serializer type\npredictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\npredictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an \narray\nprint(predictions_array.shape)\n<\/code><\/pre>\n<blockquote>\n<p>AttributeError                            Traceback (most recent call last)\n in \n1 test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\n----&gt; 2 xgb_predictor.content_type = 'text\/csv' # set the data type for an inference\n3 xgb_predictor.serializer = csv_serializer # set the serializer type\n4 predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n5 predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array<\/p>\n<\/blockquote>\n<blockquote>\n<p>AttributeError: can't set attribute<\/p>\n<\/blockquote>\n<p>I have looked at several prior questions but couldn't find much information related to this error when it comes to creating data types.<\/p>\n<p>Thanks in advance for any help.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1609067497070,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":2479.0,
        "Answer_body":"<p>If you just remove it then the prediction will work. Therefore, recommend removing this code line.\nxgb_predictor.content_type = 'text\/csv'<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":8.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65465114",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1609399068572,
        "Question_original_content":"attributeerror set attribut error new python program follow aw learn path http aw amazon com get start hand build train deploi machin learn model trk apywilaa trkcampaign tutori channel campaign data scientist hand tutori outcom product market geo mult get error excut follow block conda python test data arrai test data drop ye axi valu load data arrai xgb predictor content type text csv set data type infer xgb predictor serial csv serial set serial type predict xgb predictor predict test data arrai decod utf predict predict arrai fromstr predict sep turn predict arrai print predict arrai shape attributeerror traceback recent test data arrai test data drop ye axi valu load data arrai xgb predictor content type text csv set data type infer xgb predictor serial csv serial set serial type predict xgb predictor predict test data arrai decod utf predict predict arrai fromstr predict sep turn predict arrai attributeerror set attribut look prior question couldn inform relat error come creat data type thank advanc help",
        "Question_preprocessed_content":"attributeerror set attribut error new python program follow aw learn path get error excut follow block attributeerror traceback load data arrai set data type infer set serial type predict predict sep turn predict arrai attributeerror set attribut look prior question couldn inform relat error come creat data type thank advanc help",
        "Question_gpt_summary_original":"The user encountered an AttributeError while executing a block of code in AWS Sagemaker. The error occurred when trying to set the data type for an inference, and the user has not been able to find much information related to this error when creating data types.",
        "Question_gpt_summary":"user encount attributeerror execut block code error occur try set data type infer user abl inform relat error creat data type",
        "Answer_original_content":"remov predict work recommend remov code line xgb predictor content type text csv",
        "Answer_preprocessed_content":"remov predict work recommend remov code line",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to remove the code line that sets the data type for the inference, as it is causing the AttributeError.",
        "Answer_gpt_summary":"solut solut suggest discuss remov code line set data type infer caus attributeerror"
    },
    {
        "Question_title":"Using DVC without Cache (file references only)",
        "Question_body":"<p>We have a HDFS data directory, which is only growing, so no files are deleted or edited. And we use these files for some machine learning stuff. Analysis A uses files for time period [1-a] and Analysis B uses files of time period [1-b] (see example below).<\/p>\n<p>Since our (HDFS) data directory is only increasing, we wonder if it is possible to use DVC for tracking a \u201cfile list\u201d or \u201cfile reference\u201d only, instead of copying the current directory content into some cache.<\/p>\n<p>For our case, something like a pointer would be enough and no cache directory for tracking deleted\/edited files is needed.<\/p>\n<p>For example:<\/p>\n<pre><code>Data File of Day 1\nData File of Day 2  ---&gt; Run Analysis A (DVC must hold a reference to files of day 1 to 2)\nData File of Day 3\nData File of Day 4\nData File of Day 5 ---&gt; Run Analysis B (DVC must hold a reference to files of day 1 to 5)\nData File of Day 6 ---&gt; Run Analysis C (DVC must hold a reference to files of day 1 to 6)\nData File of Day 7\n...\n\n=&gt; No cache needed, since data is only growing and not deleted or edited. \n=&gt; Every Analysis uses data from time period starting at day 1 to current day.\n<\/code><\/pre>\n<p>Is there a way to realize something with DVC? E.g. when I want to re-run analysis A, DVC knows which files to download from out HDFS data directory, without using a separate cache directory?<\/p>\n<p>Thank you for your help <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1614671673780,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":882.0,
        "Answer_body":"<p>This is not really something you can do with DVC. The way that DVC tracks files is by storing them in the DVC cache so that they can be addressed by their hashes.<\/p>\n<p>However, DVC only stores one copy of a file, so if your concern is that DVC would be storing multiple copies of everything in your directory (even though it has not changed across days\/versions), that is not an issue. Using your example, the DVC cache would only contain a single version of a file that exists across \u201cdays 1 through 7\u201d.<\/p>. <p>Thank you for the fast reply. And yes, one of our thoughts was, that we want to avoid storing the files multiple time. If this is not the case in the DVC cache, we\u2019ll have a try on that solution, thank you.<\/p>. <p><a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a> Do I need to track each file separately to avoid storing multiple file copies in cache? Or can I also use DVC for tracking a whole HDFS directory for only caching the new files from commit to commit?<\/p>. <p>You can track the whole directory, DVC will identify which files already exist. It also matches files according to the actual binary file content, so if you have two files with different paths\/names but identical content, DVC will only store one copy.<\/p>. <p>Hi, I\u2019m new to DVC and this is the only thread where I could find this chache topic. I would like to disable the use of the cache folder so that I don\u2019t have my tracked (large) files basically duplicated. I\u2019m currenty using DVC to track a 5Gb dataset folder, and this of course makes the cache folder equally large, and it may come a time where the dataset it\u2019s too big to allow having the same data duplicated as a cache.<\/p>. <p><a class=\"mention\" href=\"\/u\/theyisus96\">@TheYisus96<\/a><\/p>\n<p>You cannot disable the use of the cache directory, but what you can do is configure DVC to use the appropriate link types so that files are only stored in the cache directory, rather than keeping multiple copies of each file in both your workspace and the cache directory. This way your workspace only contains links (either symlink\/hardlink\/reflink) to the cache directory files.<\/p>\n<p>Please refer to <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a> for more information<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/using-dvc-without-cache-file-references-only\/694",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-02T08:04:40.942Z",
                "Answer_body":"<p>This is not really something you can do with DVC. The way that DVC tracks files is by storing them in the DVC cache so that they can be addressed by their hashes.<\/p>\n<p>However, DVC only stores one copy of a file, so if your concern is that DVC would be storing multiple copies of everything in your directory (even though it has not changed across days\/versions), that is not an issue. Using your example, the DVC cache would only contain a single version of a file that exists across \u201cdays 1 through 7\u201d.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-02T08:12:57.323Z",
                "Answer_body":"<p>Thank you for the fast reply. And yes, one of our thoughts was, that we want to avoid storing the files multiple time. If this is not the case in the DVC cache, we\u2019ll have a try on that solution, thank you.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-02T08:45:39.813Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a> Do I need to track each file separately to avoid storing multiple file copies in cache? Or can I also use DVC for tracking a whole HDFS directory for only caching the new files from commit to commit?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-02T08:48:25.852Z",
                "Answer_body":"<p>You can track the whole directory, DVC will identify which files already exist. It also matches files according to the actual binary file content, so if you have two files with different paths\/names but identical content, DVC will only store one copy.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-25T18:55:37.658Z",
                "Answer_body":"<p>Hi, I\u2019m new to DVC and this is the only thread where I could find this chache topic. I would like to disable the use of the cache folder so that I don\u2019t have my tracked (large) files basically duplicated. I\u2019m currenty using DVC to track a 5Gb dataset folder, and this of course makes the cache folder equally large, and it may come a time where the dataset it\u2019s too big to allow having the same data duplicated as a cache.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-26T04:35:31.834Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/theyisus96\">@TheYisus96<\/a><\/p>\n<p>You cannot disable the use of the cache directory, but what you can do is configure DVC to use the appropriate link types so that files are only stored in the cache directory, rather than keeping multiple copies of each file in both your workspace and the cache directory. This way your workspace only contains links (either symlink\/hardlink\/reflink) to the cache directory files.<\/p>\n<p>Please refer to <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a> for more information<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"cach file refer hdf data directori grow file delet edit us file machin learn stuff analysi us file time period analysi us file time period exampl hdf data directori increas wonder possibl us track file list file refer instead copi current directori content cach case like pointer cach directori track delet edit file need exampl data file dai data file dai run analysi hold refer file dai data file dai data file dai data file dai run analysi hold refer file dai data file dai run analysi hold refer file dai data file dai cach need data grow delet edit analysi us data time period start dai current dai wai realiz want run analysi know file download hdf data directori separ cach directori thank help",
        "Question_preprocessed_content":"cach hdf data directori grow file delet edit us file machin learn stuff analysi us file time period analysi us file time period data directori increas wonder possibl us track file list file refer instead copi current directori content cach case like pointer cach directori track file need exampl wai realiz want analysi know file download hdf data directori separ cach directori thank help",
        "Question_gpt_summary_original":"The user is facing a challenge of using DVC for tracking a \"file list\" or \"file reference\" only, instead of copying the current directory content into some cache. They have a HDFS data directory that is only growing, and they want to use DVC to hold a reference to files of a specific time period without using a separate cache directory. The user is looking for a way to re-run analysis A without using a separate cache directory.",
        "Question_gpt_summary":"user face challeng track file list file refer instead copi current directori content cach hdf data directori grow want us hold refer file specif time period separ cach directori user look wai run analysi separ cach directori",
        "Answer_original_content":"wai track file store cach address hash store copi file concern store multipl copi directori chang dai version issu exampl cach contain singl version file exist dai thank fast repli ye thought want avoid store file multipl time case cach try solut thank pmrowla need track file separ avoid store multipl file copi cach us track hdf directori cach new file commit commit track directori identifi file exist match file accord actual binari file content file differ path name ident content store copi new thread chach topic like disabl us cach folder dont track larg file basic duplic currenti track dataset folder cours make cach folder equal larg come time dataset big allow have data duplic cach theyisu disabl us cach directori configur us appropri link type file store cach directori keep multipl copi file workspac cach directori wai workspac contain link symlink hardlink reflink cach directori file refer http org doc user guid larg dataset optim inform",
        "Answer_preprocessed_content":"wai track file store cach address hash store copi file concern store multipl copi directori issu exampl cach contain singl version file exist dai thank fast repli ye thought want avoid store file multipl time case cach try solut thank need track file separ avoid store multipl file copi cach us track hdf directori cach new file commit commit track directori identifi file exist match file accord actual binari file content file differ ident content store copi new thread chach topic like disabl us cach folder dont track file basic duplic currenti track dataset folder cours make cach folder equal larg come time dataset big allow have data duplic cach disabl us cach directori configur us appropri link type file store cach directori keep multipl copi file workspac cach directori wai workspac contain link cach directori file refer inform",
        "Answer_gpt_summary_original":"Solutions provided:\n- DVC only stores one copy of a file, so there is no need to track each file separately to avoid storing multiple file copies in cache.\n- DVC can track a whole HDFS directory for only caching the new files from commit to commit.\n- Configure DVC to use the appropriate link types so that files are only stored in the cache directory, rather than keeping multiple copies of each file in both your workspace and the cache directory.\n\nNo solution provided:\n- None",
        "Answer_gpt_summary":"solut provid store copi file need track file separ avoid store multipl file copi cach track hdf directori cach new file commit commit configur us appropri link type file store cach directori keep multipl copi file workspac cach directori solut provid"
    },
    {
        "Question_title":"SageMaker Managed Spot Training with Object Detection algorithm",
        "Question_body":"<p>I'm trying to train an object detection model starting from an existing model using the new Managed Spot Training feature,  The paramters used when creating my Estimator are as follows:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>od_model = sagemaker.estimator.Estimator(get_image_uri(sagemaker.Session().boto_region_name, 'object-detection', repo_version=\"latest\"),\n                                         Config['role'],\n                                         train_instance_count = 1,\n                                         train_instance_type = 'ml.p3.16xlarge',\n                                         train_volume_size = 50,\n                                         train_max_run = (48 * 60 * 60),\n                                         train_use_spot_instances = True,\n                                         train_max_wait = (72 * 60 * 60),\n                                         input_mode = 'File',\n                                         checkpoint_s3_uri = Config['train_checkpoint_uri'],\n                                         output_path = Config['s3_output_location'],\n                                         sagemaker_session = sagemaker.Session()\n                                         )\n<\/code><\/pre>\n\n<p>(The references to <code>Config<\/code> in the above are a config data structure I'm using to extract\/centralise some parameters)<\/p>\n\n<p>When I run the above, I get the following exception:<\/p>\n\n<blockquote>\n  <p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: MaxWaitTimeInSeconds above 3600 is not supported for the given algorithm.<\/p>\n<\/blockquote>\n\n<p>If I change <code>train_max_wait<\/code> to 3600 I get this exception instead:<\/p>\n\n<blockquote>\n  <p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Invalid MaxWaitTimeInSeconds. It must be present and be greater than or equal to MaxRuntimeInSeconds<\/p>\n<\/blockquote>\n\n<p>However changing <code>max_run_time<\/code> to 3600 or less isn't going to work for me as I expect this model to take several days to train (large data set), in fact a single epoch takes more than an hour.<\/p>\n\n<p>The <a href=\"https:\/\/aws.amazon.com\/blogs\/aws\/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs\/\" rel=\"nofollow noreferrer\">AWS blog post on Managed Spot Training<\/a> say that <code>MaxWaitTimeInSeconds<\/code> is limited to an 60 minutes for:<\/p>\n\n<blockquote>\n  <p>For built-in algorithms and AWS Marketplace algorithms that don\u2019t use checkpointing, we\u2019re enforcing a maximum training time of 60 minutes (MaxWaitTimeInSeconds parameter).<\/p>\n<\/blockquote>\n\n<p>Earlier, the same blog post says:<\/p>\n\n<blockquote>\n  <p>Built-in algorithms: computer vision algorithms support checkpointing (Object Detection, Semantic Segmentation, and very soon Image Classification).<\/p>\n<\/blockquote>\n\n<p>So I don't think it's that my algorithm doesn't support Checkpointing.  In fact that blog post uses object detection and max run times of 48 hours.  So I don't think it's an algorithm limitation.<\/p>\n\n<p>As you can see above, I've set up a S3 URL for the checkpoints.  The S3 bucket does exist, and the training container has access to it (it's the same bucket that the training data and model outputs are placed, and I had no problems with access to those before turning on spot training.<\/p>\n\n<p>My boto and sagemaker libraries are current versions:<\/p>\n\n<pre><code>boto3 (1.9.239)\nbotocore (1.12.239)\nsagemaker (1.42.3)\n<\/code><\/pre>\n\n<p>As best I can tell from reading various docs, I've got everything set up correctly.  My use case is almost exactly what's described in the blog post linked above, but I'm using the SageMaker Python SDK instead of the console.<\/p>\n\n<p>I'd really like to try Managed Spot Training to save some money, as I have a very long training run coming up.  But limiting timeouts to an hour isn't going to work for my use case.  Any suggestions?<\/p>\n\n<p><strong>Update:<\/strong>  If I comment out the <code>train_use_spot_instances<\/code> and <code>train_max_wait<\/code> options to train on regular on-demand instances my training job is created successfully.  If I then try to use the console to clone the job and turn on Spot instances on the clone I get the same ValidationException.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1569898766770,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1569900383487,
        "Question_score":4.0,
        "Question_view_count":1234.0,
        "Answer_body":"<p>I ran my script again today and it worked fine, no <code>botocore.exceptions.ClientError<\/code> exceptions.  Given that this issue affected both the Python SDK for Sagemaker and the console, I suspect it might have been an issue with the backend API and not my client code.<\/p>\n<p>Either way, it's working now.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1646271695950,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58177548",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1570492274472,
        "Question_original_content":"manag spot train object detect algorithm try train object detect model start exist model new manag spot train featur paramt creat estim follow model estim estim imag uri session boto region object detect repo version latest config role train instanc count train instanc type xlarg train volum size train max run train us spot instanc true train max wait input mode file checkpoint uri config train checkpoint uri output path config output locat session session refer config config data structur extract centralis paramet run follow except botocor except clienterror error occur validationexcept call createtrainingjob oper maxwaittimeinsecond support given algorithm chang train max wait except instead botocor except clienterror error occur validationexcept call createtrainingjob oper invalid maxwaittimeinsecond present greater equal maxruntimeinsecond chang max run time isn go work expect model dai train larg data set fact singl epoch take hour aw blog post manag spot train maxwaittimeinsecond limit minut built algorithm aw marketplac algorithm dont us checkpoint enforc maximum train time minut maxwaittimeinsecond paramet earlier blog post sai built algorithm vision algorithm support checkpoint object detect semant segment soon imag classif think algorithm support checkpoint fact blog post us object detect max run time hour think algorithm limit set url checkpoint bucket exist train contain access bucket train data model output place problem access turn spot train boto librari current version boto botocor best tell read doc got set correctli us case exactli describ blog post link python sdk instead consol like try manag spot train save monei long train run come limit timeout hour isn go work us case suggest updat comment train us spot instanc train max wait option train regular demand instanc train job creat successfulli try us consol clone job turn spot instanc clone validationexcept",
        "Question_preprocessed_content":"manag spot train object detect algorithm try train object detect model start exist model new manag spot train featur paramt creat estim follow refer config data structur paramet run follow except error occur call createtrainingjob oper maxwaittimeinsecond support given algorithm chang except instead error occur call createtrainingjob oper invalid maxwaittimeinsecond present greater equal maxruntimeinsecond chang isn go work expect model dai train fact singl epoch take hour aw blog post manag spot train limit minut algorithm aw marketplac algorithm dont us checkpoint enforc maximum train time minut earlier blog post sai algorithm vision algorithm support checkpoint think algorithm support checkpoint fact blog post us object detect max run time hour think algorithm limit set url checkpoint bucket exist train contain access bucket train data model output place problem access turn spot train boto librari current version best tell read doc got set correctli us case exactli describ blog post link python sdk instead consol like try manag spot train save monei long train run come limit timeout hour isn go work us case suggest updat comment option train regular instanc train job creat successfulli try us consol clone job turn spot instanc clone validationexcept",
        "Question_gpt_summary_original":"The user is encountering challenges when trying to use the SageMaker Managed Spot Training feature with an object detection algorithm. They are receiving a ValidationException error when setting the train_max_wait parameter to a value greater than 3600, and when they set it to 3600 or less, they receive another ValidationException error. The user needs to train their model for several days, and the algorithm they are using supports checkpointing. The AWS blog post on Managed Spot Training states that the MaxWaitTimeInSeconds parameter is limited to 60 minutes for built-in algorithms and AWS Marketplace algorithms that do not use checkpointing, but the user's algorithm supports checkpointing. The user has checked that their S3 bucket exists and that the training container has access to it. When they comment out the train_use_spot_instances and train_max_wait options, they can create a training job successfully on regular on-demand instances. However, when they try to clone the job and turn on Spot instances on the clone, they receive the",
        "Question_gpt_summary":"user encount challeng try us manag spot train featur object detect algorithm receiv validationexcept error set train max wait paramet valu greater set receiv validationexcept error user need train model dai algorithm support checkpoint aw blog post manag spot train state maxwaittimeinsecond paramet limit minut built algorithm aw marketplac algorithm us checkpoint user algorithm support checkpoint user check bucket exist train contain access comment train us spot instanc train max wait option creat train job successfulli regular demand instanc try clone job turn spot instanc clone receiv",
        "Answer_original_content":"ran script todai work fine botocor except clienterror except given issu affect python sdk consol suspect issu backend api client code wai work",
        "Answer_preprocessed_content":"ran script todai work fine except given issu affect python sdk consol suspect issu backend api client code wai work",
        "Answer_gpt_summary_original":"There are no solutions provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Version control of the raw data with the colleagues simultaneously",
        "Question_body":"<p>Hello, I recently came across DVC while looking for a tool for data versioning and I\u2019ve found DVC very useful, so I\u2019m testing several features for data versioning with my colleagues.<\/p>\n<p>But we have difficulties with managing raw dataset when we deal with it simultaneously and want to ask you some help\/guide.<\/p>\n<p>First, we describe our system as follows:(Assume there are two colleagues - denoted as Col1, Col2, respectively)<\/p>\n<ul>\n<li>\n<p>Col1 &amp; Col2 have a workspace respectively (denoted as WS1 and WS2) and each workspace has \u2018mounted NAS\u2019 which stores image data and should be shared.<br>\nPath for WS1 : \/home\/work\/dvc_col1    %% Here Col1 execute git init, dvc init, etc.<br>\nPath for image data to share among colleagues in NAS: \/home\/work\/mnt\/storage\/img_data<\/p>\n<p>Path for WS2: \/home\/work\/dvc_col2     %% Here Col2 execute git init, dvc init, etc.<br>\nPath for image data to share among colleagues in NAS: \/home\/work\/mnt\/storage\/img_data<\/p>\n<\/li>\n<\/ul>\n<p>Col1 initially starts data versioning following DVC docs as follows:<br>\nSince the data to be versioned is located outside of the workspace, Col1 execute the followings in WS1:<\/p>\n<pre><code>dvc cache dir \/home\/work\/mnt\/save_cache\ndvc config cache.shared group\ngit add .dvc\ngit commit -m \"set cache dir\"\ndvc add \/home\/work\/mnt\/storage\/img_data --external\ngit add img_data.dvc\ngit commit -m \"1st version data, 300 images\"\ngit tag -a \"v1.0\" -m \"1st version\"\n<\/code><\/pre>\n<p>(\u2014&gt;&gt; This 1st version includes 300 images.)<\/p>\n<ul>\n<li>\n<p>And additional 200 images are added to \/home\/work\/mnt\/storage\/img_data. (so in total 500 images.)<br>\nTo mange the version of the dataset, Col1 execute the followings:<\/p>\n<p>dvc add \/home\/work\/mnt\/storage\/img_data --external<br>\ngit add img_data.dvc<br>\ngit commit -m \u201c2nd version data, 200 images added, in total 500 images\u201d<br>\ngit tag -a \u201cv2.0\u201d -m \u201c2nd version\u201d<\/p>\n<\/li>\n<li>\n<p>Then Col1 can version the image data with \u2018git checkout v1.0 or v2.0\u2019 and \u2018dvc checkout\u2019.<\/p>\n<\/li>\n<li>\n<p>So, Col1 uses \u2018git push\u2019 to their GitLab project and Col2 downloaded it using git pull.<\/p>\n<\/li>\n<li>\n<p>After this, Col2 can also version the same image data with \u2018git checkout\u2019 and \u2018dvc checkout\u2019 and it works fine.<\/p>\n<\/li>\n<\/ul>\n<p>But problem arises here:<\/p>\n<ul>\n<li>\n<p>When Col1 execute \u2018git checkout v2.0\u2019 &amp; \u2018dvc checkout\u2019 in WS1, the number of images in \u2018\/home\/work\/mnt\/storage\/img_data\u2019 becomes 500 and Col2 can also recognize it.<\/p>\n<\/li>\n<li>\n<p>Since Col2 wants to use the 1st version of dataset, Col2 execute \u2018git checkout v1.0\u2019 &amp; \u2018dvc checkout\u2019 in WS2, and the number of images in \u2018\/home\/work\/mnt\/storage\/img_data\u2019 becomes 300. This makes problem, because now Col1 can\u2019t access the added 200 images anymore. \u2018\/home\/work\/mnt\/storage\/img_data\u2019 shows only 300 images, so Col1 and Col2 can\u2019t access to the dataset each wants to use SIMULTANEOUSLY.<\/p>\n<\/li>\n<\/ul>\n<p>Are we using DVC wrong? We want to access each of the versioned dataset simultaneous with DVC. We\u2019re very appreciated with your comment and help. Thank you.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1646030816402,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":258.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/schakal\">@schakal<\/a> . You should avoid to use \u201cexternal\u201d and \u201cshared cache\u201d strategies combined.<\/p>\n<p>From the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data#managing-external-data\" rel=\"noopener nofollow ugc\">Managing External Data<\/a> guide:<\/p>\n<blockquote>\n<p><img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/warning.png?v=12\" title=\":warning:\" class=\"emoji\" alt=\":warning:\" loading=\"lazy\" width=\"20\" height=\"20\"> An external cache could be <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/share-a-dvc-cache\" rel=\"noopener nofollow ugc\">shared<\/a> among copies of a DVC project. Please <strong>do not<\/strong> use external outputs in that scenario, as <a href=\"https:\/\/dvc.org\/doc\/command-reference\/checkout\" rel=\"noopener nofollow ugc\"> <code>dvc checkout<\/code> <\/a> in any project would overwrite the working data for all projects.<\/p>\n<\/blockquote>\n<p>What\u2019s the use case behind wanting to use <code>external<\/code> and <code>shared cache<\/code> combined? Would some alternatives work for you?<\/p>\n<p>Highlighted from the above link:<\/p>\n<blockquote>\n<p>In most cases, alternatives like the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-an-external-cache\" rel=\"noopener nofollow ugc\">to-cache<\/a> or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage\" rel=\"noopener nofollow ugc\">to-remote<\/a> strategies of <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add\" rel=\"noopener nofollow ugc\"> <code>dvc add<\/code> <\/a> and <a href=\"https:\/\/dvc.org\/doc\/command-reference\/import-url\" rel=\"noopener nofollow ugc\"> <code>dvc import-url<\/code> <\/a> are more convenient.<\/p>\n<\/blockquote>. <p><a class=\"mention\" href=\"\/u\/daavoo\">@daavoo<\/a> Thank you for the answer.<\/p>\n<p>Could you share some example code that can be used for our case?<\/p>\n<p>I\u2019m not sure if any other method other than dvc checkout can version control the shared datasets.<\/p>. <p>I think that you should be able to use the same code you shared but just removing the <code>--external<\/code> option.<\/p>. <p><a class=\"mention\" href=\"\/u\/schakal\">@schakal<\/a> Hello, I\u2019m also struggling solve the same situation. Have you solved it?<\/p>. <p><a class=\"mention\" href=\"\/u\/schakal\">@schakal<\/a> <a class=\"mention\" href=\"\/u\/eririri\">@eririri<\/a><br>\nWhat <span class=\"mention\">@daavo<\/span> is pointing out here, is that in the case of external datasets, we do not recommend simultaneous work on them. What you could do, is to create a separate repository that would only take care of versioning the data (so called data registry: <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\">https:\/\/dvc.org\/doc\/use-cases\/data-registries<\/a>) And then leverage <code>dvc import<\/code> command to obtain the particular version of the dataset in your project, so that both colleagues can work on the proper dataset version.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/version-control-of-the-raw-data-with-the-colleagues-simultaneously\/1080",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-28T11:40:50.111Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/schakal\">@schakal<\/a> . You should avoid to use \u201cexternal\u201d and \u201cshared cache\u201d strategies combined.<\/p>\n<p>From the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data#managing-external-data\" rel=\"noopener nofollow ugc\">Managing External Data<\/a> guide:<\/p>\n<blockquote>\n<p><img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/warning.png?v=12\" title=\":warning:\" class=\"emoji\" alt=\":warning:\" loading=\"lazy\" width=\"20\" height=\"20\"> An external cache could be <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/share-a-dvc-cache\" rel=\"noopener nofollow ugc\">shared<\/a> among copies of a DVC project. Please <strong>do not<\/strong> use external outputs in that scenario, as <a href=\"https:\/\/dvc.org\/doc\/command-reference\/checkout\" rel=\"noopener nofollow ugc\"> <code>dvc checkout<\/code> <\/a> in any project would overwrite the working data for all projects.<\/p>\n<\/blockquote>\n<p>What\u2019s the use case behind wanting to use <code>external<\/code> and <code>shared cache<\/code> combined? Would some alternatives work for you?<\/p>\n<p>Highlighted from the above link:<\/p>\n<blockquote>\n<p>In most cases, alternatives like the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-an-external-cache\" rel=\"noopener nofollow ugc\">to-cache<\/a> or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage\" rel=\"noopener nofollow ugc\">to-remote<\/a> strategies of <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add\" rel=\"noopener nofollow ugc\"> <code>dvc add<\/code> <\/a> and <a href=\"https:\/\/dvc.org\/doc\/command-reference\/import-url\" rel=\"noopener nofollow ugc\"> <code>dvc import-url<\/code> <\/a> are more convenient.<\/p>\n<\/blockquote>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-02T04:47:43.980Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/daavoo\">@daavoo<\/a> Thank you for the answer.<\/p>\n<p>Could you share some example code that can be used for our case?<\/p>\n<p>I\u2019m not sure if any other method other than dvc checkout can version control the shared datasets.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-04T17:32:47.725Z",
                "Answer_body":"<p>I think that you should be able to use the same code you shared but just removing the <code>--external<\/code> option.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-14T07:51:32.993Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/schakal\">@schakal<\/a> Hello, I\u2019m also struggling solve the same situation. Have you solved it?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-14T09:33:34.731Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/schakal\">@schakal<\/a> <a class=\"mention\" href=\"\/u\/eririri\">@eririri<\/a><br>\nWhat <span class=\"mention\">@daavo<\/span> is pointing out here, is that in the case of external datasets, we do not recommend simultaneous work on them. What you could do, is to create a separate repository that would only take care of versioning the data (so called data registry: <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\">https:\/\/dvc.org\/doc\/use-cases\/data-registries<\/a>) And then leverage <code>dvc import<\/code> command to obtain the particular version of the dataset in your project, so that both colleagues can work on the proper dataset version.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"version control raw data colleagu simultan hello recent came look tool data version iv us test featur data version colleagu difficulti manag raw dataset deal simultan want ask help guid follow assum colleagu denot col col respect col col workspac respect denot workspac mount na store imag data share path home work col col execut git init init path imag data share colleagu na home work mnt storag img data path home work col col execut git init init path imag data share colleagu na home work mnt storag img data col initi start data version follow doc follow data version locat outsid workspac col execut follow cach dir home work mnt save cach config cach share group git add git commit set cach dir add home work mnt storag img data extern git add img data git commit version data imag git tag version version includ imag addit imag ad home work mnt storag img data total imag mang version dataset col execut follow add home work mnt storag img data extern git add img data git commit version data imag ad total imag git tag version col version imag data git checkout checkout col us git push gitlab project col download git pull col version imag data git checkout checkout work fine problem aris col execut git checkout checkout number imag home work mnt storag img data col recogn col want us version dataset col execut git checkout checkout number imag home work mnt storag img data make problem col access ad imag anymor home work mnt storag img data show imag col col access dataset want us simultan wrong want access version dataset simultan appreci comment help thank",
        "Question_preprocessed_content":"version control raw data colleagu simultan hello recent came look tool data version iv us test featur data version colleagu difficulti manag raw dataset deal simultan want ask follow assum colleagu denot col col respect col col workspac respect workspac mount na store imag data share path col execut git init init path imag data share colleagu na path col execut git init init path imag data share colleagu na col initi start data version follow doc follow data version locat outsid workspac col execut follow version includ addit imag ad mang version dataset col execut follow add git add git commit version data imag ad total imag git tag version col version imag data git checkout checkout col us git push gitlab project col download git pull col version imag data git checkout checkout work fine problem aris col execut git checkout checkout number imag col recogn col want us version dataset col execut git checkout checkout number imag make problem col access ad imag anymor show imag col col access dataset want us simultan wrong want access version dataset simultan appreci comment help thank",
        "Question_gpt_summary_original":"The user is facing challenges with managing raw datasets simultaneously with colleagues using DVC for data versioning. The issue arises when one colleague checks out an earlier version of the dataset, causing the number of images to differ from the current version, making it inaccessible to other colleagues. The user is seeking guidance on how to access each version of the dataset simultaneously with DVC.",
        "Question_gpt_summary":"user face challeng manag raw dataset simultan colleagu data version issu aris colleagu check earlier version dataset caus number imag differ current version make inaccess colleagu user seek guidanc access version dataset simultan",
        "Answer_original_content":"schakal avoid us extern share cach strategi combin manag extern data guid extern cach share copi project us extern output scenario checkout project overwrit work data project what us case want us extern share cach combin altern work highlight link case altern like cach remot strategi add import url conveni daavoo thank answer share exampl code case sure method checkout version control share dataset think abl us code share remov extern option schakal hello struggl solv situat solv schakal eririri daavo point case extern dataset recommend simultan work creat separ repositori care version data call data registri http org doc us case data registri leverag import command obtain particular version dataset project colleagu work proper dataset version",
        "Answer_preprocessed_content":"avoid us extern share cach strategi combin manag extern data guid extern cach share copi project us extern output scenario project overwrit work data project what us case want us combin altern work highlight link case altern like strategi conveni thank answer share exampl code case sure method checkout version control share dataset think abl us code share remov option hello struggl solv situat solv point case extern dataset recommend simultan work creat separ repositori care version data leverag command obtain particular version dataset project colleagu work proper dataset version",
        "Answer_gpt_summary_original":"Solution:\n- Avoid using \"external\" and \"shared cache\" strategies combined.\n- Use alternatives like \"to-cache\" or \"to-remote\" strategies of \"dvc add\" and \"dvc import-url\" instead.\n- Create a separate repository that would only take care of versioning the data (data registry) and then leverage \"dvc import\" command to obtain the particular version of the dataset in your project, so that both colleagues can work on the proper dataset version.",
        "Answer_gpt_summary":"solut avoid extern share cach strategi combin us altern like cach remot strategi add import url instead creat separ repositori care version data data registri leverag import command obtain particular version dataset project colleagu work proper dataset version"
    },
    {
        "Question_title":"Compliance - administering data that is under DVC control",
        "Question_body":"<p>Hi there<\/p>\n<p>We are currently looking to implement DVC on our stack.<\/p>\n<p>One thing that isn\u2019t clear from the available documentation, however, is how our data compliance team would go about administering and managing our data store using DVC. What kind of user access control is available to us? Or is this based on our existing Git\/Bitbucket setup?<\/p>\n<p>We\u2019ve been through the tutorials available to us and we understand how to add data to the repository, branch off, update etc, but what we\u2019re looking for clarity on is how we would centrally manage this for multiple users.<\/p>\n<p>Any advice or guidance would be appreciated.<\/p>\n<p>EDIT: To add clarity to the question, our requirement is for all of our incoming data sources to be handled by a specialist compliance team who will act as gatekeepers to the data and make it available to the data scientists within the firm as needed. The intention is for our DS team to pull data as is needed but not be able to update it without approval from the compliance team. Thanks.<\/p>",
        "Question_answer_count":9,
        "Question_comment_count":0,
        "Question_creation_time":1556203634652,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":8.0,
        "Question_view_count":1482.0,
        "Answer_body":"<p>Hi!<\/p>\n<p>Great question. DVC does not have any embedded features for access control yet but it relies on the access control models of your code repositories and storages.<\/p>\n<ul>\n<li>code: use Git\\Bitbucket access setting to gain access for projects meta information<\/li>\n<li>storage: use S3\\GCP\\SSH access setting per project (per S3 bucket which is DVC remote)<\/li>\n<\/ul>\n<p>If we assume that you use separate Git repos per project (not a mono repo) and a separate data remote per project (S3 bucket for example) then you can give access by managing these two services - the repository and the S3 bucket.<\/p>\n<p>Your compliance team can create a separate repository per data source (or set of the sources) with the right repository and data bucket setting and provide it to the right data scientists.<\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/michaelwdownie\">@michaelwdownie<\/a>!<\/p>\n<p>Would you mind to clarify a few things? Do all data scientists have more or less the same access rights to the data that was released by the compliance team?<\/p>\n<p>How do you enforce these policies now?<\/p>. <p>Yeah, the intention is for the data scientists to have the same level of access to read data, create branches and issue pull requests, which the compliance team would then be required to approve\/deny after a review.<\/p>\n<p>Currently we don\u2019t have a process like this yet. As it stands, the data scientists just use a shared folder on a server and there\u2019s no version control on the files they\u2019re accessing, so we have no idea if anyone has changed anything, nor any audit trail or metadata to say what is and isn\u2019t in there. Hence why we\u2019re looking at DVC<\/p>. <p>Thank you <a class=\"mention\" href=\"\/u\/dmitry\">@dmitry<\/a>, that\u2019s good to know.<\/p>\n<p>Also great to know we can integrate with our existing Bitbucket service. We won\u2019t be using GCP\/S3 for storage, we have a storage server so it\u2019ll be using SSH to connect to that through bitbucket.<\/p>. <p>Makes sense <a class=\"mention\" href=\"\/u\/michaelwdownie\">@michaelwdownie<\/a>. There are a few ways to organize this with DVC, but since you are looking for a central place where your data scientists and the compliance team collaborate, I would probably got with one git\/dvc repo with all data sets you want to audit and expose in it.<\/p>\n<p>Each dataset will be represented by the <code>.dvc<\/code> meta-file in this repository. Data scientists will be using these files an entry point to the dataset - to pull it to their project, get a link to the actual data, etc. They will be using DVC commands to make an update to this file which then will have to be committed via PR to the master branch of the central registry. Thus you will have access to the previous versions, you will know who changed what, and will have a way to block a change to the \u201cmaster\u201d version of the dataset.<\/p>\n<p>Btw, it does not mean that you need to create a separate Git repo for this. If you are using a mono-repo where you keep all your data science projects, you can place you datasets here as well.<\/p>\n<p>If it\u2019s all confusing, we can schedule a meeting and help to brainstorm the details of this workflow.<\/p>. <p><a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a> that would be great if you could. Could you PM me and we\u2019ll sort something out?<\/p>. <p><a class=\"mention\" href=\"\/u\/michaelwdownie\">@michaelwdownie<\/a>  We\u2019d be happy to chat! Please shoot me an email - dmitry@iterative.ai<\/p>\n<p>It would be helpful if you can provide your availability. We are based in San Francisco, CA (PST time zone).<\/p>. <p>Hey,  <a class=\"mention\" href=\"\/u\/michaelwdownie\">@michaelwdownie<\/a>! We are back from the conference and are ready to help you in case your team has any questions, issues. Have you tried to use DVC project as a registry yet?<\/p>. <p><a class=\"mention\" href=\"\/u\/michaelwdownie\">@michaelwdownie<\/a> if it\u2019s still relevant, please take a look at this <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\">article<\/a> and the new features we introduced to facilitate Git-like workflow for data. It should be easier to setup and use \u201cdata registries\u201d now. Let\u2019s me know what are your thoughts on this <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> .<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/compliance-administering-data-that-is-under-dvc-control\/174",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-04-26T00:24:15.986Z",
                "Answer_body":"<p>Hi!<\/p>\n<p>Great question. DVC does not have any embedded features for access control yet but it relies on the access control models of your code repositories and storages.<\/p>\n<ul>\n<li>code: use Git\\Bitbucket access setting to gain access for projects meta information<\/li>\n<li>storage: use S3\\GCP\\SSH access setting per project (per S3 bucket which is DVC remote)<\/li>\n<\/ul>\n<p>If we assume that you use separate Git repos per project (not a mono repo) and a separate data remote per project (S3 bucket for example) then you can give access by managing these two services - the repository and the S3 bucket.<\/p>\n<p>Your compliance team can create a separate repository per data source (or set of the sources) with the right repository and data bucket setting and provide it to the right data scientists.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-26T00:27:35.385Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/michaelwdownie\">@michaelwdownie<\/a>!<\/p>\n<p>Would you mind to clarify a few things? Do all data scientists have more or less the same access rights to the data that was released by the compliance team?<\/p>\n<p>How do you enforce these policies now?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-26T08:00:56.408Z",
                "Answer_body":"<p>Yeah, the intention is for the data scientists to have the same level of access to read data, create branches and issue pull requests, which the compliance team would then be required to approve\/deny after a review.<\/p>\n<p>Currently we don\u2019t have a process like this yet. As it stands, the data scientists just use a shared folder on a server and there\u2019s no version control on the files they\u2019re accessing, so we have no idea if anyone has changed anything, nor any audit trail or metadata to say what is and isn\u2019t in there. Hence why we\u2019re looking at DVC<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-26T08:09:01.341Z",
                "Answer_body":"<p>Thank you <a class=\"mention\" href=\"\/u\/dmitry\">@dmitry<\/a>, that\u2019s good to know.<\/p>\n<p>Also great to know we can integrate with our existing Bitbucket service. We won\u2019t be using GCP\/S3 for storage, we have a storage server so it\u2019ll be using SSH to connect to that through bitbucket.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-26T18:52:38.460Z",
                "Answer_body":"<p>Makes sense <a class=\"mention\" href=\"\/u\/michaelwdownie\">@michaelwdownie<\/a>. There are a few ways to organize this with DVC, but since you are looking for a central place where your data scientists and the compliance team collaborate, I would probably got with one git\/dvc repo with all data sets you want to audit and expose in it.<\/p>\n<p>Each dataset will be represented by the <code>.dvc<\/code> meta-file in this repository. Data scientists will be using these files an entry point to the dataset - to pull it to their project, get a link to the actual data, etc. They will be using DVC commands to make an update to this file which then will have to be committed via PR to the master branch of the central registry. Thus you will have access to the previous versions, you will know who changed what, and will have a way to block a change to the \u201cmaster\u201d version of the dataset.<\/p>\n<p>Btw, it does not mean that you need to create a separate Git repo for this. If you are using a mono-repo where you keep all your data science projects, you can place you datasets here as well.<\/p>\n<p>If it\u2019s all confusing, we can schedule a meeting and help to brainstorm the details of this workflow.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-29T12:29:57.524Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a> that would be great if you could. Could you PM me and we\u2019ll sort something out?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-29T21:30:18.573Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/michaelwdownie\">@michaelwdownie<\/a>  We\u2019d be happy to chat! Please shoot me an email - dmitry@iterative.ai<\/p>\n<p>It would be helpful if you can provide your availability. We are based in San Francisco, CA (PST time zone).<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-05-11T01:00:17.211Z",
                "Answer_body":"<p>Hey,  <a class=\"mention\" href=\"\/u\/michaelwdownie\">@michaelwdownie<\/a>! We are back from the conference and are ready to help you in case your team has any questions, issues. Have you tried to use DVC project as a registry yet?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-01-01T19:42:44.670Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/michaelwdownie\">@michaelwdownie<\/a> if it\u2019s still relevant, please take a look at this <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\">article<\/a> and the new features we introduced to facilitate Git-like workflow for data. It should be easier to setup and use \u201cdata registries\u201d now. Let\u2019s me know what are your thoughts on this <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> .<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"complianc administ data control current look implement stack thing isnt clear avail document data complianc team administ manag data store kind user access control avail base exist git bitbucket setup weve tutori avail understand add data repositori branch updat look clariti central manag multipl user advic guidanc appreci edit add clariti question requir incom data sourc handl specialist complianc team act gatekeep data avail data scientist firm need intent team pull data need abl updat approv complianc team thank",
        "Question_preprocessed_content":"complianc administ data control current look implement stack thing isnt clear avail document data complianc team administ manag data store kind user access control avail base exist setup weve tutori avail understand add data repositori branch updat look clariti central manag multipl user advic guidanc appreci edit add clariti question requir incom data sourc handl specialist complianc team act gatekeep data avail data scientist firm need intent team pull data need abl updat approv complianc team thank",
        "Question_gpt_summary_original":"The user is seeking guidance on how to administer and manage their data store using DVC while ensuring compliance with data regulations. They are specifically looking for information on user access control and central management for multiple users. The user's compliance team needs to act as gatekeepers to the data and approve any updates made by the data scientists.",
        "Question_gpt_summary":"user seek guidanc administ manag data store ensur complianc data regul specif look inform user access control central manag multipl user user complianc team need act gatekeep data approv updat data scientist",
        "Answer_original_content":"great question embed featur access control reli access control model code repositori storag code us git bitbucket access set gain access project meta inform storag us gcp ssh access set project bucket remot assum us separ git repo project mono repo separ data remot project bucket exampl access manag servic repositori bucket complianc team creat separ repositori data sourc set sourc right repositori data bucket set provid right data scientist michaelwdowni mind clarifi thing data scientist access right data releas complianc team enforc polici yeah intent data scientist level access read data creat branch issu pull request complianc team requir approv deni review current dont process like stand data scientist us share folder server there version control file theyr access idea chang audit trail metadata isnt look thank dmitri that good know great know integr exist bitbucket servic wont gcp storag storag server itll ssh connect bitbucket make sens michaelwdowni wai organ look central place data scientist complianc team collabor probabl got git repo data set want audit expos dataset repres meta file repositori data scientist file entri point dataset pull project link actual data command updat file commit master branch central registri access previou version know chang wai block chang master version dataset btw mean need creat separ git repo mono repo data scienc project place dataset confus schedul meet help brainstorm detail workflow shcheklein great sort michaelwdowni wed happi chat shoot email dmitri iter help provid avail base san francisco pst time zone hei michaelwdowni confer readi help case team question issu tri us project registri michaelwdowni relev look articl new featur introduc facilit git like workflow data easier setup us data registri let know thought",
        "Answer_preprocessed_content":"great question embed featur access control reli access control model code repositori storag code us access set gain access project meta inform storag us access set project assum us separ git repo project separ data remot project access manag servic repositori bucket complianc team creat separ repositori data sourc right repositori data bucket set provid right data scientist mind clarifi thing data scientist access right data releas complianc team enforc polici yeah intent data scientist level access read data creat branch issu pull request complianc team requir review current dont process like stand data scientist us share folder server there version control file theyr access idea chang audit trail metadata isnt look thank that good know great know integr exist bitbucket servic wont storag storag server itll ssh connect bitbucket make sens wai organ look central place data scientist complianc team collabor probabl got git repo data set want audit expos dataset repres repositori data scientist file entri point dataset pull project link actual data command updat file commit master branch central registri access previou version know chang wai block chang master version dataset btw mean need creat separ git repo data scienc project place dataset confus schedul meet help brainstorm detail workflow great sort wed happi chat shoot email help provid avail base san francisco hei confer readi help case team question issu tri us project registri relev look articl new featur introduc facilit workflow data easier setup us data registri let know thought",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Use Git\/Bitbucket access setting to gain access for projects meta information and S3\/GCP\/SSH access setting per project (per S3 bucket which is DVC remote) to manage access control for multiple users.\n- Create a separate repository per data source (or set of the sources) with the right repository and data bucket setting and provide it to the right data scientists.\n- Use one git\/DVC repo with all data sets to audit and expose in it. Each dataset will be represented by the .dvc meta-file in this repository. Data scientists will be using these files as an entry point to the dataset - to pull it to their project, get a link to the actual data, etc",
        "Answer_gpt_summary":"possibl solut mention discuss us git bitbucket access set gain access project meta inform gcp ssh access set project bucket remot manag access control multipl user creat separ repositori data sourc set sourc right repositori data bucket set provid right data scientist us git repo data set audit expos dataset repres meta file repositori data scientist file entri point dataset pull project link actual data"
    },
    {
        "Question_title":"azure cosmos db as a datastore in ml",
        "Question_body":"Hi, I'm wondering if I can register azure cosmos db as a datastore in azure machine learning?\nFrom your documentation, it seems not https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore%28class%29?view=azure-ml-py\n\nDo you have a plan to implement the feature in near future?\n\n\nAny recommended alternative solutions for now?\n\nThanks.",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1597331768270,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Hi, thanks for reaching out. Currently, Cosmos DB isn't a supported datasource when using Azure ML datastores. However, the product team are aware of this request and will provide updates accordingly. An alternative for now will be to use Azure ML Studio (Classic) which supports Cosmos DB as data source. You can also try a heuristic approach via Execute Python Script module in Designer to import data using python. Hope this helps.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/66297\/azure-cosmos-db-as-a-datastore-in-ml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-08-14T22:43:18.18Z",
                "Answer_score":0,
                "Answer_body":"Hi, thanks for reaching out. Currently, Cosmos DB isn't a supported datasource when using Azure ML datastores. However, the product team are aware of this request and will provide updates accordingly. An alternative for now will be to use Azure ML Studio (Classic) which supports Cosmos DB as data source. You can also try a heuristic approach via Execute Python Script module in Designer to import data using python. Hope this helps.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-10-14T06:27:11.747Z",
                "Answer_score":2,
                "Answer_body":"Hi, Is there an update on this as a year has passed? For us, Cosmos DB is an important datasource (with no workable workarounds ), and not being able to use it as a datastore in Azure ML is forcing us to make a choice to continue with Azure ML or not.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-20T13:32:02.98Z",
                "Answer_score":0,
                "Answer_body":"I'm also interested in an update. We're in the process of migrating to azure and have lots of metadata associated with our training set stored in json files. It would be very nice to store it as a Table under our storage account and connect to it from AzureML via a datastore.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1597444998180,
        "Question_original_content":"azur cosmo datastor wonder regist azur cosmo datastor document http doc microsoft com python api core core datastor class view azur plan implement featur near futur recommend altern solut thank",
        "Question_preprocessed_content":"azur cosmo datastor wonder regist azur cosmo datastor document plan implement featur near futur recommend altern solut thank",
        "Question_gpt_summary_original":"The user is facing a challenge of not being able to register Azure Cosmos DB as a datastore in Azure Machine Learning, as it is not supported according to the documentation. The user is inquiring about any plans to implement this feature in the future and is also seeking alternative solutions for the time being.",
        "Question_gpt_summary":"user face challeng abl regist azur cosmo datastor support accord document user inquir plan implement featur futur seek altern solut time",
        "Answer_original_content":"thank reach current cosmo isn support datasourc datastor product team awar request provid updat accordingli altern us studio classic support cosmo data sourc try heurist approach execut python script modul design import data python hope help",
        "Answer_preprocessed_content":"thank reach current cosmo isn support datasourc datastor product team awar request provid updat accordingli altern us studio support cosmo data sourc try heurist approach execut python script modul design import data python hope help",
        "Answer_gpt_summary_original":"Solution:\n- Currently, Azure Cosmos DB is not a supported datastore in Azure Machine Learning.\n- The product team is aware of this request and will provide updates accordingly.\n- An alternative solution for now is to use Azure ML Studio (Classic) which supports Cosmos DB as a data source.\n- Another option is to use the Execute Python Script module in Designer to import data using Python.",
        "Answer_gpt_summary":"solut current azur cosmo support datastor product team awar request provid updat accordingli altern solut us studio classic support cosmo data sourc option us execut python script modul design import data python"
    },
    {
        "Question_title":"Azure Machine Learning Prediction - Input and Outputs",
        "Question_body":"<p>I am attempting to follow this <a href=\"http:\/\/www.toptal.com\/machine-learning\/predicting-gas-prices-using-azure-machine-learning-studio\" rel=\"nofollow\">tutorial<\/a> however I was attempting to predict MPG for a set of cars rather than oil prices and have the following set up:<\/p>\n\n<ol>\n<li>MPG Sample dataset<\/li>\n<li>Remove missing values, project everything (weight, displacement, cylinders, etc) except model name<\/li>\n<li>Split 75 to train model, 25 to score model<\/li>\n<li>Train model on MPG column with neural network<\/li>\n<li>Score model which is fed by Train Model and Split<\/li>\n<li>Score model is fed to Evaluate model<\/li>\n<\/ol>\n\n<p>This all seems to run fine and without issue, so I create a scoring experiment and then publish it as a web service, however when I attempt to input values it is asking for an MPG input. My understanding is that this would be the predicted value, so it seems somewhat opposite to have to enter this as a value, or am I just understanding a basic tenet of machine learning? <\/p>\n\n<p>In short: Ideally I would like to be able to enter everything but the MPG and get a prediction on what the MPG is for a given set of value.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1432296670560,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1527.0,
        "Answer_body":"<p>You could also add project columns to exclude label as part of scoring experiment and connect web service output port to the output of project columns<\/p>",
        "Answer_comment_count":8.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/30396392",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1432778316968,
        "Question_original_content":"predict input output attempt follow tutori attempt predict mpg set car oil price follow set mpg sampl dataset remov miss valu project weight displac cylind model split train model score model train model mpg column neural network score model fed train model split score model fed evalu model run fine issu creat score experi publish web servic attempt input valu ask mpg input understand predict valu somewhat opposit enter valu understand basic tenet machin learn short ideal like abl enter mpg predict mpg given set valu",
        "Question_preprocessed_content":"predict input output attempt follow tutori attempt predict mpg set car oil price follow set mpg sampl dataset remov miss valu project model split train model score model train model mpg column neural network score model fed train model split score model fed evalu model run fine issu creat score experi publish web servic attempt input valu ask mpg input understand predict valu somewhat opposit enter valu understand basic tenet machin learn short ideal like abl enter mpg predict mpg given set valu",
        "Question_gpt_summary_original":"The user is encountering a challenge in Azure Machine Learning Prediction where they are attempting to predict MPG for a set of cars, but the system is asking for an MPG input. The user is unsure if they are misunderstanding a basic tenet of machine learning and would like to be able to enter everything but the MPG and get a prediction on what the MPG is for a given set of values.",
        "Question_gpt_summary":"user encount challeng predict attempt predict mpg set car ask mpg input user unsur misunderstand basic tenet machin learn like abl enter mpg predict mpg given set valu",
        "Answer_original_content":"add project column exclud label score experi connect web servic output port output project column",
        "Answer_preprocessed_content":"add project column exclud label score experi connect web servic output port output project column",
        "Answer_gpt_summary_original":"Solution: The discussion suggests adding project columns to exclude the label as part of the scoring experiment and connecting the web service output port to the output of project columns. This solution can help the user enter everything but the MPG and get a prediction on what the MPG is for a given set of values.",
        "Answer_gpt_summary":"solut discuss suggest ad project column exclud label score experi connect web servic output port output project column solut help user enter mpg predict mpg given set valu"
    },
    {
        "Question_title":"How to create a new VertexAI Workbench Managed Notebook using gcloud",
        "Question_body":"<p>There is a gcloud command to create a user-managed notebook instance.<\/p>\n<pre><code>gcloud notebooks instances create \n<\/code><\/pre>\n<p>Is is possible to create a managed notebook with gcloud?<\/p>\n<p>It looks to be possible in the <a href=\"https:\/\/stackoverflow.com\/questions\/70223161\/how-to-create-a-new-workbench-managed-notebook-using-rest-api\">API<\/a>. I can't find a gcloud reference.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1659584245230,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":107.0,
        "Answer_body":"<p>As mentioned by @gogasca, the gcloud SDK for creating managed notebook is currently under work. Meanwhile you can try client libraries and REST API to create the same.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73230096",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1660204307547,
        "Question_original_content":"creat new vertexai workbench manag notebook gcloud gcloud command creat user manag notebook instanc gcloud notebook instanc creat possibl creat manag notebook gcloud look possibl api gcloud refer",
        "Question_preprocessed_content":"creat new vertexai workbench manag notebook gcloud gcloud command creat notebook instanc possibl creat manag notebook gcloud look possibl api gcloud refer",
        "Question_gpt_summary_original":"The user is facing a challenge in creating a managed notebook instance using gcloud command as there is no reference available for it. They are exploring the possibility of creating it using the API.",
        "Question_gpt_summary":"user face challeng creat manag notebook instanc gcloud command refer avail explor possibl creat api",
        "Answer_original_content":"mention gogasca gcloud sdk creat manag notebook current work try client librari rest api creat",
        "Answer_preprocessed_content":"mention gcloud sdk creat manag notebook current work try client librari rest api creat",
        "Answer_gpt_summary_original":"Solution: The gcloud SDK for creating a managed notebook instance is currently under development. In the meantime, the user can try using client libraries or the REST API to create the instance.",
        "Answer_gpt_summary":"solut gcloud sdk creat manag notebook instanc current develop meantim user try client librari rest api creat instanc"
    },
    {
        "Question_title":"Unable to load Spark model",
        "Question_body":"Hi, I have saved my pyspark pipelines with mlflow. The pipelines includes several custom transformers.\u00a0\n\n\nWhen i tried to load the pipeline with\u00a0model=mlflow.spark.load_model(\"location of model on local\"), it is giving me the error as below:\n```AttributeError: module '__main__' has no attribute 'AgeCalc'``` and 'AgeCalc' is one of the name of my custom transformer.\u00a0\n\n\nAny idea how to fix this?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1574980477000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":12.0,
        "Answer_body":"error message is as below:\n2019\/11\/29 12:31:06 INFO mlflow.spark: File '\/mnt\/disk1\/project\/mlflow_preprocess1\/sparkml' not found on DFS. Will attempt to upload the file.\n2019\/11\/29 12:31:07 INFO mlflow.spark: Copied SparkML model to \/tmp\/mlflow\/73a2a40c-941f-47da-a8cf-b398b9c91586\nTraceback (most recent call last):\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\n\u00a0 File \"<stdin>\", line 1, in <module>\n\u00a0 File \"\/home\/davidooi\/.conda\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/spark.py\", line 417, in load_model\n\u00a0 \u00a0 return _load_model(model_uri=model_uri, dfs_tmpdir=dfs_tmpdir)\n\u00a0 File \"\/home\/davidooi\/.conda\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/spark.py\", line 378, in _load_model\n\u00a0 \u00a0 return PipelineModel.load(model_path)\n\u00a0 File \"\/usr\/lib\/spark-current\/python\/pyspark\/ml\/util.py\", line 362, in load\n\u00a0 \u00a0 return cls.read().load(path)\n\u00a0 File \"\/usr\/lib\/spark-current\/python\/pyspark\/ml\/pipeline.py\", line 244, in load\n\u00a0 \u00a0 uid, stages = PipelineSharedReadWrite.load(metadata, self.sc, path)\n\u00a0 File \"\/usr\/lib\/spark-current\/python\/pyspark\/ml\/pipeline.py\", line 378, in load\n\u00a0 \u00a0 stage = DefaultParamsReader.loadParamsInstance(stagePath, sc)\n\u00a0 File \"\/usr\/lib\/spark-current\/python\/pyspark\/ml\/util.py\", line 611, in loadParamsInstance\n\u00a0 \u00a0 py_type = DefaultParamsReader.__get_class(pythonClassName)\n\u00a0 File \"\/usr\/lib\/spark-current\/python\/pyspark\/ml\/util.py\", line 539, in __get_class\n\u00a0 \u00a0 m = getattr(m, comp)\n\ue5d3\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/IjZxr890xlY",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-11-28T23:32:51",
                "Answer_body":"error message is as below:\n2019\/11\/29 12:31:06 INFO mlflow.spark: File '\/mnt\/disk1\/project\/mlflow_preprocess1\/sparkml' not found on DFS. Will attempt to upload the file.\n2019\/11\/29 12:31:07 INFO mlflow.spark: Copied SparkML model to \/tmp\/mlflow\/73a2a40c-941f-47da-a8cf-b398b9c91586\nTraceback (most recent call last):\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\n\u00a0 File \"<stdin>\", line 1, in <module>\n\u00a0 File \"\/home\/davidooi\/.conda\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/spark.py\", line 417, in load_model\n\u00a0 \u00a0 return _load_model(model_uri=model_uri, dfs_tmpdir=dfs_tmpdir)\n\u00a0 File \"\/home\/davidooi\/.conda\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/spark.py\", line 378, in _load_model\n\u00a0 \u00a0 return PipelineModel.load(model_path)\n\u00a0 File \"\/usr\/lib\/spark-current\/python\/pyspark\/ml\/util.py\", line 362, in load\n\u00a0 \u00a0 return cls.read().load(path)\n\u00a0 File \"\/usr\/lib\/spark-current\/python\/pyspark\/ml\/pipeline.py\", line 244, in load\n\u00a0 \u00a0 uid, stages = PipelineSharedReadWrite.load(metadata, self.sc, path)\n\u00a0 File \"\/usr\/lib\/spark-current\/python\/pyspark\/ml\/pipeline.py\", line 378, in load\n\u00a0 \u00a0 stage = DefaultParamsReader.loadParamsInstance(stagePath, sc)\n\u00a0 File \"\/usr\/lib\/spark-current\/python\/pyspark\/ml\/util.py\", line 611, in loadParamsInstance\n\u00a0 \u00a0 py_type = DefaultParamsReader.__get_class(pythonClassName)\n\u00a0 File \"\/usr\/lib\/spark-current\/python\/pyspark\/ml\/util.py\", line 539, in __get_class\n\u00a0 \u00a0 m = getattr(m, comp)\n\ue5d3\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"unabl load spark model save pyspark pipelin pipelin includ custom transform tri load pipelin withmodel spark load model locat model local give error attributeerror modul main attribut agecalc agecalc custom transform idea fix",
        "Question_preprocessed_content":"unabl load spark model save pyspark pipelin pipelin includ custom transform tri load pipelin model local give error agecalc custom transform idea fix",
        "Question_gpt_summary_original":"The user is facing a challenge while loading a Spark model that includes custom transformers saved with mlflow. The error message \"AttributeError: module '__main__' has no attribute 'AgeCalc'\" is displayed, where 'AgeCalc' is the name of one of the custom transformers. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user face challeng load spark model includ custom transform save error messag attributeerror modul main attribut agecalc displai agecalc custom transform user seek help resolv issu",
        "Answer_original_content":"error messag info spark file mnt disk project preprocess sparkml df attempt upload file info spark copi sparkml model tmp aac acf bbc traceback recent file line file home davidooi conda env lib python site packag spark line load model return load model model uri model uri df tmpdir df tmpdir file home davidooi conda env lib python site packag spark line load model return pipelinemodel load model path file usr lib spark current python pyspark util line load return cl read load path file usr lib spark current python pyspark pipelin line load uid stage pipelinesharedreadwrit load metadata self path file usr lib spark current python pyspark pipelin line load stage defaultparamsread loadparamsinst stagepath file usr lib spark current python pyspark util line loadparamsinst type defaultparamsread class pythonclassnam file usr lib spark current python pyspark util line class getattr comp",
        "Answer_preprocessed_content":"error messag info spark file df attempt upload file info spark copi sparkml model traceback file line file line return file line return file line load return file line load uid stage path file line load stage file line loadparamsinst file line getattr",
        "Answer_gpt_summary_original":"No solutions are provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"How to move DVC files",
        "Question_body":"<p>Search for this topic, I found <a href=\"https:\/\/discuss.dvc.org\/t\/dvc-heartbeat-discord-gems\/159\/2\">this summary post<\/a> but I cannot open the original question (discord tells me it cannot find it) so I\u2019m creating a new topic. Apologies in advance.<\/p>\n<p>I tried following the instructions given i.e. I ran<\/p>\n<pre><code class=\"lang-auto\">mv my_file.dvc dvc_links\/my_file.dvc\n<\/code><\/pre>\n<p>and edited the working directory in the moved file.<br>\nHowever running <code>dvc status<\/code> gives<\/p>\n<pre><code class=\"lang-auto\">dvc_links\/my_file.dvc:\n        changed checksum\n<\/code><\/pre>\n<p>How do I get dvc to update the checksum in the relocated file? I even tried removing the md5: xx line at the top of the new file, calculating the md5 sum and reinserting it but this didn\u2019t work.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1566355790691,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":1238.0,
        "Answer_body":"<p>Try these steps:<\/p>\n<pre><code class=\"lang-bash\">rm my_file.dvc\ndvc add -f dvc_links\/my_file.dvc my_file\n<\/code><\/pre>\n<p>I think this is not documented and it is not obvious at all. I found it the hard way (by trial and error).<\/p>. <p>Thank you. This worked<\/p>. <p><a class=\"mention\" href=\"\/u\/dashohoxha\">@dashohoxha<\/a> Thank you for your answer! <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p><a class=\"mention\" href=\"\/u\/jcf76\">@jcf76<\/a> This is a bug in <code>dvc move<\/code> a patch for which was merged today <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/2421\" rel=\"nofollow noopener\">https:\/\/github.com\/iterative\/dvc\/pull\/2421<\/a> and will soon be released in a new dvc version. Thank you for the feedback! <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-move-dvc-files\/205",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-08-21T07:58:31.486Z",
                "Answer_body":"<p>Try these steps:<\/p>\n<pre><code class=\"lang-bash\">rm my_file.dvc\ndvc add -f dvc_links\/my_file.dvc my_file\n<\/code><\/pre>\n<p>I think this is not documented and it is not obvious at all. I found it the hard way (by trial and error).<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-21T08:09:10.699Z",
                "Answer_body":"<p>Thank you. This worked<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-21T18:06:26.870Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/dashohoxha\">@dashohoxha<\/a> Thank you for your answer! <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p><a class=\"mention\" href=\"\/u\/jcf76\">@jcf76<\/a> This is a bug in <code>dvc move<\/code> a patch for which was merged today <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/2421\" rel=\"nofollow noopener\">https:\/\/github.com\/iterative\/dvc\/pull\/2421<\/a> and will soon be released in a new dvc version. Thank you for the feedback! <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"file search topic summari post open origin question discord tell creat new topic apolog advanc tri follow instruct given ran file link file edit work directori move file run statu give link file chang checksum updat checksum reloc file tri remov line new file calcul sum reinsert didnt work",
        "Question_preprocessed_content":"file search topic summari post open origin question creat new topic apolog advanc tri follow instruct given ran edit work directori move file run give updat checksum reloc file tri remov line new file calcul sum reinsert didnt work",
        "Question_gpt_summary_original":"The user is facing challenges in moving DVC files and updating the checksum in the relocated file. The user followed the instructions to move the file and edited the working directory, but running \"dvc status\" shows a changed checksum. The user tried removing and recalculating the md5 sum but it did not work.",
        "Question_gpt_summary":"user face challeng move file updat checksum reloc file user follow instruct file edit work directori run statu show chang checksum user tri remov recalcul sum work",
        "Answer_original_content":"try step file add link file file think document obviou hard wai trial error thank work dashohoxha thank answer jcf bug patch merg todai http github com iter pull soon releas new version thank feedback",
        "Answer_preprocessed_content":"try step think document obviou hard wai thank work thank answer bug patch merg todai soon releas new version thank feedback",
        "Answer_gpt_summary_original":"Solution:\n- One solution suggested is to remove the DVC file and add it again using the command \"dvc add -f dvc_links\/my_file.dvc my_file\".\n- Another solution mentioned is that this issue is a bug in \"dvc move\" and a patch for it has been merged and will soon be released in a new version of DVC.",
        "Answer_gpt_summary":"solut solut suggest remov file add command add link file file solut mention issu bug patch merg soon releas new version"
    },
    {
        "Question_title":"Contextual version conflict error, Microsoft Azure Machine Learning Studio",
        "Question_body":"<p>I'm trying to run this 10 line .ipynb file from Google Colab in Microsoft Azure Machine Learning Studio<\/p>\n<p><a href=\"https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing\" rel=\"nofollow noreferrer\">https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing<\/a><\/p>\n<p>When I get to this step:<\/p>\n<pre><code>import gradio as gr\nimport tensorflow as tf\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n<\/code><\/pre>\n<p>I get this error:<\/p>\n<pre><code>ContextualVersionConflict: (Flask 1.0.3 (\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages), Requirement.parse('Flask&gt;=1.1.1'), {'gradio'})\n<\/code><\/pre>\n<p>I tried to install the Flask 1.1.1 version but I get more errors. Any idea what I should do to get past this step in Azure ML Studio?<\/p>\n<pre><code>!pip install \u2013force-reinstall Flask==1.1.1\n\/\/ More errors\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630103458310,
        "Question_favorite_count":null,
        "Question_last_edit_time":1630103807136,
        "Question_score":2.0,
        "Question_view_count":237.0,
        "Answer_body":"<p>The issue is because <code>gradio<\/code> package using existing Flask package (version 1.0.3). But as your application required Flask&gt;=1.1.1, therefore it is showing error. You need to uninstall the existing Flask package and then install the latest required version.<\/p>\n<p>To uninstall the existing package:\n<code>!pip uninstall Flask -y<\/code><\/p>\n<p>To install latest package:\n<code>!pip install Flask&gt;=1.1.1<\/code><\/p>\n<p><strong>Then, make sure to restart your runtime to pick up the new Flask using the Runtime -&gt; Restart runtime menu.<\/strong><\/p>\n<p>Finally, import gradio.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68959934",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1630303052768,
        "Question_original_content":"contextu version conflict error studio try run line ipynb file googl colab studio http colab research googl com drive qiryvphfnbnzgyemyerchhxsv usp share step import gradio import tensorflow transform import tfgptlmheadmodel gpttoken error contextualversionconflict flask anaconda env lib python site packag requir pars flask gradio tri instal flask version error idea past step studio pip instal forc reinstal flask error",
        "Question_preprocessed_content":"contextu version conflict error studio try run line ipynb file googl colab studio step error tri instal flask version error idea past step studio",
        "Question_gpt_summary_original":"The user is encountering a ContextualVersionConflict error while trying to run a 10 line .ipynb file from Google Colab in Microsoft Azure Machine Learning Studio. The error occurs when importing gradio, tensorflow, and transformers. The user attempted to install Flask 1.1.1 version but encountered more errors. The user is seeking advice on how to resolve the issue and proceed with the step in Azure ML Studio.",
        "Question_gpt_summary":"user encount contextualversionconflict error try run line ipynb file googl colab studio error occur import gradio tensorflow transform user attempt instal flask version encount error user seek advic resolv issu proce step studio",
        "Answer_original_content":"issu gradio packag exist flask packag version applic requir flask show error need uninstal exist flask packag instal latest requir version uninstal exist packag pip uninstal flask instal latest packag pip instal flask sure restart runtim pick new flask runtim restart runtim menu final import gradio",
        "Answer_preprocessed_content":"issu packag exist flask packag applic requir show error need uninstal exist flask packag instal latest requir version uninstal exist packag instal latest packag sure restart runtim pick new flask runtim restart runtim menu final import gradio",
        "Answer_gpt_summary_original":"Solution:\n- Uninstall the existing Flask package using \"!pip uninstall Flask -y\" command.\n- Install the latest required version of Flask using \"!pip install Flask>=1.1.1\" command.\n- Restart the runtime to pick up the new Flask using the Runtime -> Restart runtime menu.\n- Import gradio.",
        "Answer_gpt_summary":"solut uninstal exist flask packag pip uninstal flask command instal latest requir version flask pip instal flask command restart runtim pick new flask runtim restart runtim menu import gradio"
    },
    {
        "Question_title":"Lock error with parallelized dvc repro",
        "Question_body":"<p>I have a pipeline with many parallel stages. I use a combination of a (generated) <code>params.yaml<\/code> file and <code>foreach<\/code> approach. See below for simplified examples of <code>dvc.yaml<\/code>, <code>params.yaml<\/code>, and <code>dvc.lock<\/code>.<\/p>\n<p>I wrote a simple scheduler in Python to run the stages with <code>dvc repro<\/code> in parallel using multiprocessing. Essentially, it\u2019s running running <code>dvc repro setup@sim1<\/code>, <code>dvc repro setup@sim2<\/code>, etc. in parallel.<\/p>\n<p>From what I <a href=\"https:\/\/dvc.org\/doc\/command-reference\/repro#parallel-stage-execution\" rel=\"noopener nofollow ugc\">understand<\/a> this should work. However, I\u2019m getting lock errors: \u201cERROR: Unable to acquire lock. Most likely another DVC process is running or was terminated abruptly.\u201d<\/p>\n<p>dvc.yaml:<\/p>\n<pre><code class=\"lang-auto\">stages:\n  setup:\n    foreach: ${sims}\n    do:\n      cmd: python my_script.py --simlabel ${item.label}\n      deps:\n        - my_script.py\n      outs:\n        - ${item.work_dir}\/outputs.txt\n<\/code><\/pre>\n<p>params.yaml<\/p>\n<pre><code class=\"lang-auto\">sims:\n\tsim1:\n\t\tlabel: label1\n\t\tparameters:\n\t\t  param1: 1\n\t\t  param2: 2\n\t\twork_dir: label1_workdir\n\tsim2:\n\t\tlabel: label2\n\t\tparameters:\n\t\t  param1: 1\n\t\t  param2: 2\n\t\twork_dir: label2_workdir\n<\/code><\/pre>\n<p>dvc.lock:<\/p>\n<pre><code class=\"lang-auto\">schema: '2.0'\nstages:\n  setup@sim1\n    cmd: python python my_script.py --simlabel label1\n    deps:\n    - path: my_script.py\n      md5: 614d9f23b42a36de65981aa76026600e\n      size: 17401\n    outs:\n    - path: label1_workdir\/outputs.txt\n      md5: eaf7cdf7540bb96a6307394ad421ad7b\n      size: 4412\n  setup@sim2\n    cmd: python python my_script.py --simlabel label2\n    deps:\n    - path: my_script.py\n      md5: 614d9f23b42a36de65981aa76026600e\n      size: 17401\n    outs:\n    - path: label2_workdir\/outputs.txt\n      md5: eaf7cdf7540bb96a6307394ad421ad7b\n      size: 4412\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634713211772,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":255.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/maartenb\">@maartenb<\/a> sorry for late response,<br>\nSo the problem here is that both stages (while different due to parameters use) use the same dependency:<\/p>\n<ul>\n<li>my_script.py<\/li>\n<\/ul>\n<p>dvc is locking it and hence you get an error. So this is a little bit different use case than the one linked by you in your post, where the pipelines are separate completely.<br>\nI am afraid there is no straightforward workaround for that, besides creating several copies of the script and naming them differently, though that would probably not be very elegant.<\/p>\n<p>Having said so, you might be interested in <code>exp<\/code> command, which should allow you to achieve your goal:<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/run#exp-run\">\n  <header class=\"source\">\n      <img src=\"https:\/\/dvc.org\/favicon.ico\" class=\"site-icon\" width=\"64\" height=\"64\">\n\n      <a href=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/run#exp-run\" target=\"_blank\" rel=\"noopener\">dvc.org<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/dvc.org\/social-share.png\" class=\"thumbnail\" width=\"690\" height=\"362\"><\/div>\n\n<h3><a href=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/run#exp-run\" target=\"_blank\" rel=\"noopener\">exp run<\/a><\/h3>\n\n  <p>Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>You could get parallel execution with the following commands:<\/p>\n<p><code>dvc exp run setup@sim1 --queue<\/code><br>\n<code>dvc exp run setup@sim2 --queue<\/code><br>\n<code>dvc exp run -j {2}<\/code><\/p>\n<p>Please read on <code>exp<\/code> and share your thoughts on whether this is something you might find useful.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/lock-error-with-parallelized-dvc-repro\/929",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-26T18:57:42.709Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/maartenb\">@maartenb<\/a> sorry for late response,<br>\nSo the problem here is that both stages (while different due to parameters use) use the same dependency:<\/p>\n<ul>\n<li>my_script.py<\/li>\n<\/ul>\n<p>dvc is locking it and hence you get an error. So this is a little bit different use case than the one linked by you in your post, where the pipelines are separate completely.<br>\nI am afraid there is no straightforward workaround for that, besides creating several copies of the script and naming them differently, though that would probably not be very elegant.<\/p>\n<p>Having said so, you might be interested in <code>exp<\/code> command, which should allow you to achieve your goal:<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/run#exp-run\">\n  <header class=\"source\">\n      <img src=\"https:\/\/dvc.org\/favicon.ico\" class=\"site-icon\" width=\"64\" height=\"64\">\n\n      <a href=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/run#exp-run\" target=\"_blank\" rel=\"noopener\">dvc.org<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/dvc.org\/social-share.png\" class=\"thumbnail\" width=\"690\" height=\"362\"><\/div>\n\n<h3><a href=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/run#exp-run\" target=\"_blank\" rel=\"noopener\">exp run<\/a><\/h3>\n\n  <p>Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>You could get parallel execution with the following commands:<\/p>\n<p><code>dvc exp run setup@sim1 --queue<\/code><br>\n<code>dvc exp run setup@sim2 --queue<\/code><br>\n<code>dvc exp run -j {2}<\/code><\/p>\n<p>Please read on <code>exp<\/code> and share your thoughts on whether this is something you might find useful.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"lock error parallel repro pipelin parallel stage us combin gener param yaml file foreach approach simplifi exampl yaml param yaml lock wrote simpl schedul python run stage repro parallel multiprocess essenti run run repro setup sim repro setup sim parallel understand work get lock error error unabl acquir lock like process run termin abruptli yaml stage setup foreach sim cmd python script simlabel item label dep script out item work dir output txt param yaml sim sim label label paramet param param work dir label workdir sim label label paramet param param work dir label workdir lock schema stage setup sim cmd python python script simlabel label dep path script dfbadeaa size out path label workdir output txt eafcdfbbaadadb size setup sim cmd python python script simlabel label dep path script dfbadeaa size out path label workdir output txt eafcdfbbaadadb size",
        "Question_preprocessed_content":"lock error parallel repro pipelin parallel stage us combin file approach simplifi exampl wrote simpl schedul python run stage parallel multiprocess essenti run run parallel understand work get lock error error unabl acquir lock like process run termin abruptli yaml lock",
        "Question_gpt_summary_original":"The user is encountering lock errors while running parallel stages in their pipeline using a combination of a generated params.yaml file and foreach approach. They are using a Python scheduler to run the stages with dvc repro in parallel using multiprocessing, but are receiving the error message \"Unable to acquire lock. Most likely another DVC process is running or was terminated abruptly.\"",
        "Question_gpt_summary":"user encount lock error run parallel stage pipelin combin gener param yaml file foreach approach python schedul run stage repro parallel multiprocess receiv error messag unabl acquir lock like process run termin abruptli",
        "Answer_original_content":"maartenb sorri late respons problem stage differ paramet us us depend script lock error littl bit differ us case link post pipelin separ complet afraid straightforward workaround creat copi script name differ probabl eleg have said interest exp command allow achiev goal org exp run open sourc version control data scienc machin learn project git like experi organ data model experi parallel execut follow command exp run setup sim queue exp run setup sim queue exp run read exp share thought us",
        "Answer_preprocessed_content":"sorri late respons problem stage us depend lock error littl bit differ us case link post pipelin separ complet afraid straightforward workaround creat copi script name differ probabl eleg have said interest command allow achiev goal org exp run version control data scienc machin learn project experi organ data model experi parallel execut follow command read share thought us",
        "Answer_gpt_summary_original":"Solution:\n- There is no straightforward workaround for the lock error caused by using the same dependency in different stages.\n- One possible solution is to create several copies of the script and name them differently, but it may not be elegant.\n- Another solution is to use the `exp` command to achieve parallel execution with different parameters.",
        "Answer_gpt_summary":"solut straightforward workaround lock error caus depend differ stage possibl solut creat copi script differ eleg solut us exp command achiev parallel execut differ paramet"
    },
    {
        "Question_title":"Azure ML Tabular Dataset : missing 1 required positional argument: 'stream_column'",
        "Question_body":"<p>For the Python API for tabular dataset of AzureML (<code>azureml.data.TabularDataset<\/code>), there are two experimental methods which have been introduced:<\/p>\n<ol>\n<li><code>download(stream_column, target_path=None, overwrite=False, ignore_not_found=True)<\/code><\/li>\n<li><code>mount(stream_column, mount_point=None)<\/code><\/li>\n<\/ol>\n<p>Parameter <code>stream_column<\/code> has been defined as The stream column to mount or download.<\/p>\n<p>What is the actual meaning of <code>stream_column<\/code>? I don't see any example any where?<\/p>\n<p>Any pointer will be helpful.<\/p>\n<p>The stack trace:<\/p>\n<pre><code>Method download: This is an experimental method, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n\/tmp\/ipykernel_11561\/3904436543.py in &lt;module&gt;\n----&gt; 1 tab_dataset.download(target_path=&quot;..\/data\/tabular&quot;)\n\n\/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/_base_sdk_common\/_docstring_wrapper.py in wrapped(*args, **kwargs)\n     50     def wrapped(*args, **kwargs):\n     51         module_logger.warning(&quot;Method {0}: {1} {2}&quot;.format(func.__name__, _method_msg, _experimental_link_msg))\n---&gt; 52         return func(*args, **kwargs)\n     53     return wrapped\n     54 \n\n\/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/data\/_loggerfactory.py in wrapper(*args, **kwargs)\n    130             with _LoggerFactory.track_activity(logger, func.__name__, activity_type, custom_dimensions) as al:\n    131                 try:\n--&gt; 132                     return func(*args, **kwargs)\n    133                 except Exception as e:\n    134                     if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\nTypeError: download() missing 1 required positional argument: 'stream_column'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1644217302490,
        "Question_favorite_count":null,
        "Question_last_edit_time":1645197572643,
        "Question_score":0.0,
        "Question_view_count":356.0,
        "Answer_body":"<p><strong>Update on 5th March, 2022<\/strong><\/p>\n<p>I posted this as a support ticket with Azure. Following is the answer I have received:<\/p>\n<blockquote>\n<p>As you can see from our documentation of <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">TabularDataset Class<\/a>,\nthe \u201cstream_column\u201d parameter is required. So, that error is occurring\nbecause you are not passing any parameters when you are calling the\ndownload method.    The \u201cstream_column\u201d parameter should have the\nstream column to download\/mount. So, you need to pass the column name\nthat contains the paths from which the data will be streamed.<br \/>\nPlease find an example <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-labeled-dataset#explore-labeled-datasets-via-pandas-dataframe\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1648643627872,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71014584",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1646484376340,
        "Question_original_content":"tabular dataset miss requir posit argument stream column python api tabular dataset data tabulardataset experiment method introduc download stream column target path overwrit fals ignor true mount stream column mount point paramet stream column defin stream column mount download actual mean stream column exampl pointer help stack trace method download experiment method chang time http aka experiment inform typeerror traceback recent tmp ipykernel tab dataset download target path data tabular anaconda env lib python site packag base sdk common docstr wrapper wrap arg kwarg def wrap arg kwarg modul logger warn method format func method msg experiment link msg return func arg kwarg return wrap anaconda env lib python site packag data loggerfactori wrapper arg kwarg loggerfactori track activ logger func activ type custom dimens try return func arg kwarg except hasattr activ info hasattr error code typeerror download miss requir posit argument stream column",
        "Question_preprocessed_content":"tabular dataset miss requir posit argument python api tabular dataset experiment method introduc paramet defin stream column mount download actual mean exampl pointer help stack trace",
        "Question_gpt_summary_original":"The user is facing a challenge with the Python API for tabular dataset of AzureML, specifically with the experimental methods 'download' and 'mount'. The user is unsure about the meaning of the parameter 'stream_column' and is seeking clarification. The user has encountered an error message stating that the 'download' method is missing one required positional argument: 'stream_column'.",
        "Question_gpt_summary":"user face challeng python api tabular dataset specif experiment method download mount user unsur mean paramet stream column seek clarif user encount error messag state download method miss requir posit argument stream column",
        "Answer_original_content":"updat march post support ticket azur follow answer receiv document tabulardataset class stream column paramet requir error occur pass paramet call download method stream column paramet stream column download mount need pass column contain path data stream exampl",
        "Answer_preprocessed_content":"updat march post support ticket azur follow answer receiv document tabulardataset class paramet requir error occur pass paramet call download method paramet stream column need pass column contain path data stream exampl",
        "Answer_gpt_summary_original":"Solution: The user needs to pass the 'stream_column' parameter with the column name that contains the paths from which the data will be streamed while calling the 'download' method. The 'stream_column' parameter is required as per the documentation of the TabularDataset Class. An example is provided in the documentation for further clarification.",
        "Answer_gpt_summary":"solut user need pass stream column paramet column contain path data stream call download method stream column paramet requir document tabulardataset class exampl provid document clarif"
    },
    {
        "Question_title":"Vertex AI Endpoints scales to 0 before increasing number of replicas",
        "Question_body":"<p>I have an endpoint in <code>us-east<\/code> which serves a custom imported model (docker image).<\/p>\n<p>This endpoint uses <code>min replicas = 1<\/code> and <code>max replicas = 100<\/code>.<\/p>\n<p>Sometimes, Vertex AI will require the model to scale from 1 to 2.<\/p>\n<p>However, there seems to be an issue causing the number of replicas to go from <code>1 -&gt; 0 -&gt; 2<\/code> instead of <code>1 -&gt; 2<\/code>.<\/p>\n<p>This causes several 504 (Gateway Timeout) errors in my API and the way to solve that was setting <code>min replicas &gt; 1<\/code>, highly impacting the monthly cost of the application.<\/p>\n<p>Is this some known issue to Vertex AI\/GCP services, is there anyway to fix it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640176354140,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":77.0,
        "Answer_body":"<p>The intermittent <code>504<\/code> errors could be a result of an endpoint that is under-provisioned to handle the load. It can also happen if too many prediction requests are sent to the endpoint before it has a chance to scale up.<\/p>\n<p>Traffic splitting of the incoming prediction requests is done randomly. So, multiple requests may end up on the same model server at the same time. This can happen even if the overall Queries Per Second (QPS) is low, and especially when the QPS is spiky. This contributes to the requests being queued up if the model server isn't able to handle the load. This is what results in a 504 error.<\/p>\n<p><strong>Recommendations to mitigate the <code>504<\/code> errors are as follows:<\/strong><\/p>\n<ul>\n<li><p>Improve the container's ability to use all resources in the container. One thing to keep in mind about resource utilization is whether the model server is single-threaded or multi-threaded. The container may not be using up all the cores and\/or requests may be queuing up, hence are served only one-at-a-time.<\/p>\n<\/li>\n<li><p>Autoscaling is happening, it just might need to be tuned to the prediction workload and expectations. A lower utilization threshold would trigger autoscaling sooner.<\/p>\n<\/li>\n<li><p>Perform an exponential backoff while the deployment is scaling. This way, there is a retry mechanism to handle failed requests.<\/p>\n<\/li>\n<li><p>Provision a higher minimum replica count for the endpoint, which you have already implemented.<\/p>\n<\/li>\n<\/ul>\n<p>If the above recommendations do not solve the problem or in general require further investigation of these errors, please reach out to <a href=\"https:\/\/cloud.google.com\/support-hub#section-2\" rel=\"nofollow noreferrer\">GCP support<\/a> in case you have a <a href=\"https:\/\/cloud.google.com\/support\/\" rel=\"nofollow noreferrer\">support plan<\/a>. Otherwise, please open an issue in the <a href=\"https:\/\/issuetracker.google.com\/issues\/new?component=1134259&amp;template=1640573\" rel=\"nofollow noreferrer\">issue tracker<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1641204696607,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70449117",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1641201707430,
        "Question_original_content":"endpoint scale increas number replica endpoint east serv custom import model docker imag endpoint us min replica max replica requir model scale issu caus number replica instead caus gatewai timeout error api wai solv set min replica highli impact monthli cost applic known issu gcp servic fix",
        "Question_preprocessed_content":"endpoint scale increas number replica endpoint serv custom import model endpoint us requir model scale issu caus number replica instead caus error api wai solv set highli impact monthli cost applic known issu servic fix",
        "Question_gpt_summary_original":"The user is encountering challenges with Vertex AI Endpoints scaling from 1 to 2 replicas. Instead of going from 1 to 2, the number of replicas goes from 1 to 0 to 2, causing 504 errors in the API. The user had to set the minimum replicas to a value greater than 1 to solve the issue, which increased the monthly cost of the application. The user is seeking a solution to this issue and wondering if it is a known issue with Vertex AI\/GCP services.",
        "Question_gpt_summary":"user encount challeng endpoint scale replica instead go number replica goe caus error api user set minimum replica valu greater solv issu increas monthli cost applic user seek solut issu wonder known issu gcp servic",
        "Answer_original_content":"intermitt error result endpoint provis handl load happen predict request sent endpoint chanc scale traffic split incom predict request randomli multipl request end model server time happen overal queri second qp low especi qp spiki contribut request queu model server isn abl handl load result error recommend mitig error follow improv contain abil us resourc contain thing mind resourc util model server singl thread multi thread contain core request queu serv time autosc happen need tune predict workload expect lower util threshold trigger autosc sooner perform exponenti backoff deploy scale wai retri mechan handl fail request provis higher minimum replica count endpoint implement recommend solv problem gener requir investig error reach gcp support case support plan open issu issu tracker",
        "Answer_preprocessed_content":"intermitt error result endpoint handl load happen predict request sent endpoint chanc scale traffic split incom predict request randomli multipl request end model server time happen overal queri second low especi qp spiki contribut request queu model server isn abl handl load result error recommend mitig error follow improv contain abil us resourc contain thing mind resourc util model server contain core request queu serv autosc happen need tune predict workload expect lower util threshold trigger autosc sooner perform exponenti backoff deploy scale wai retri mechan handl fail request provis higher minimum replica count endpoint implement recommend solv problem gener requir investig error reach gcp support case support plan open issu issu tracker",
        "Answer_gpt_summary_original":"The discussion provides several recommendations to mitigate the 504 errors encountered when scaling Vertex AI Endpoints, including improving container resource utilization, tuning autoscaling, performing exponential backoff, and provisioning a higher minimum replica count. It is suggested to reach out to GCP support or open an issue in the issue tracker if these recommendations do not solve the problem.",
        "Answer_gpt_summary":"discuss provid recommend mitig error encount scale endpoint includ improv contain resourc util tune autosc perform exponenti backoff provis higher minimum replica count suggest reach gcp support open issu issu tracker recommend solv problem"
    },
    {
        "Question_title":"How cache is maintained for big data size locally",
        "Question_body":"<p>Hi,<br>\nI have a VM on which my local git repo exists. On top of that i have installed dvc on same machine. Now when i add data to dvc it will be in dvc cache and on git push, commit same data will go to git repo as well. Is my understanding correct? If yes then their will be two copies of data and size will keep increasing as data grows. I am not using any remote repo for data.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1568253347446,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":876.0,
        "Answer_body":"<p><a class=\"mention\" href=\"\/u\/writetoneeraj\">@writetoneeraj<\/a> Could you please have a look at this tutorial: <a href=\"https:\/\/katacoda.com\/dvc\" rel=\"nofollow noopener\">https:\/\/katacoda.com\/dvc<\/a> ? It explains how DVC manages the data. If you still have any questions please come back and ask.<\/p>. <p>Thanks <a class=\"mention\" href=\"\/u\/dashohoxha\">@dashohoxha<\/a> , that looks great!<\/p>\n<p><a class=\"mention\" href=\"\/u\/writetoneeraj\">@writetoneeraj<\/a> No, data won\u2019t go into your git repo, only tiny metafiles (DVC-files), so there won\u2019t be duplication happening. E.g. if you <code>dvc add data<\/code>, then <code>data.dvc<\/code>(tiny yaml metafile) will be stored by git, but the <code>data<\/code> itself will be stored by dvc, so on <code>git push<\/code> only that <code>data.dvc<\/code> will be uploaded, but the <code>data<\/code> itself will stay on your machine.<\/p>. <aside class=\"quote no-group\" data-post=\"2\" data-topic=\"217\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/dashohoxha\/40\/45_2.png\" class=\"avatar\"> dashohoxha:<\/div>\n<blockquote>\n<p>Could you please have a look at this tutorial: <a href=\"https:\/\/katacoda.com\/dvc\" rel=\"nofollow noopener\">https:\/\/katacoda.com\/dvc<\/a><\/p>\n<\/blockquote>\n<\/aside>\n<p>p.s. <a class=\"mention\" href=\"\/u\/writetoneeraj\">@writetoneeraj<\/a> the official get started guide is at <a href=\"https:\/\/dvc.org\/doc\/get-started\" rel=\"nofollow noopener\">https:\/\/dvc.org\/doc\/get-started<\/a>. Your question has to do with the <strong>Add Files<\/strong> chapter.<\/p>. <p><a class=\"mention\" href=\"\/u\/dashohoxha\">@dashohoxha<\/a> this tutorial is really good.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-cache-is-maintained-for-big-data-size-locally\/217",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-09-12T03:36:35.541Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/writetoneeraj\">@writetoneeraj<\/a> Could you please have a look at this tutorial: <a href=\"https:\/\/katacoda.com\/dvc\" rel=\"nofollow noopener\">https:\/\/katacoda.com\/dvc<\/a> ? It explains how DVC manages the data. If you still have any questions please come back and ask.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-09-13T02:44:01.763Z",
                "Answer_body":"<p>Thanks <a class=\"mention\" href=\"\/u\/dashohoxha\">@dashohoxha<\/a> , that looks great!<\/p>\n<p><a class=\"mention\" href=\"\/u\/writetoneeraj\">@writetoneeraj<\/a> No, data won\u2019t go into your git repo, only tiny metafiles (DVC-files), so there won\u2019t be duplication happening. E.g. if you <code>dvc add data<\/code>, then <code>data.dvc<\/code>(tiny yaml metafile) will be stored by git, but the <code>data<\/code> itself will be stored by dvc, so on <code>git push<\/code> only that <code>data.dvc<\/code> will be uploaded, but the <code>data<\/code> itself will stay on your machine.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-09-19T17:56:25.101Z",
                "Answer_body":"<aside class=\"quote no-group\" data-post=\"2\" data-topic=\"217\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/dashohoxha\/40\/45_2.png\" class=\"avatar\"> dashohoxha:<\/div>\n<blockquote>\n<p>Could you please have a look at this tutorial: <a href=\"https:\/\/katacoda.com\/dvc\" rel=\"nofollow noopener\">https:\/\/katacoda.com\/dvc<\/a><\/p>\n<\/blockquote>\n<\/aside>\n<p>p.s. <a class=\"mention\" href=\"\/u\/writetoneeraj\">@writetoneeraj<\/a> the official get started guide is at <a href=\"https:\/\/dvc.org\/doc\/get-started\" rel=\"nofollow noopener\">https:\/\/dvc.org\/doc\/get-started<\/a>. Your question has to do with the <strong>Add Files<\/strong> chapter.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-15T06:22:30.965Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/dashohoxha\">@dashohoxha<\/a> this tutorial is really good.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"cach maintain big data size local local git repo exist instal machin add data cach git push commit data git repo understand correct ye copi data size increas data grow remot repo data",
        "Question_preprocessed_content":"cach maintain big data size local local git repo exist instal machin add data cach git push commit data git repo understand correct ye copi data size increas data grow remot repo data",
        "Question_gpt_summary_original":"The user is facing a challenge in maintaining cache for big data size locally. They are concerned about the duplication of data and the increase in size as the data grows, as they are not using any remote repository for data.",
        "Question_gpt_summary":"user face challeng maintain cach big data size local concern duplic data increas size data grow remot repositori data",
        "Answer_original_content":"writetoneeraj look tutori http katacoda com explain manag data question come ask thank dashohoxha look great writetoneeraj data wont git repo tini metafil file wont duplic happen add data data tini yaml metafil store git data store git push data upload data stai machin dashohoxha look tutori http katacoda com writetoneeraj offici start guid http org doc start question add file chapter dashohoxha tutori good",
        "Answer_preprocessed_content":"look tutori explain manag data question come ask thank look great data wont git repo tini metafil wont duplic happen store git store upload stai machin dashohoxha look tutori offici start guid question add file chapter tutori good",
        "Answer_gpt_summary_original":"Solution:\n- DVC (Data Version Control) can be used to manage the data and avoid duplication. DVC stores the data locally and only stores tiny metafiles (DVC-files) in the git repository. This way, the data won't be duplicated and the size won't increase as the data grows.",
        "Answer_gpt_summary":"solut data version control manag data avoid duplic store data local store tini metafil file git repositori wai data won duplic size won increas data grow"
    },
    {
        "Question_title":"AML Studio: Register mutliple gateways on the same server",
        "Question_body":"<p>I am struggling to find a way to register multiple gateways. I have a local instance of my SQL server and have created a gateway to access to it from the AML Studio workspace. It works fine but now I would like to access to the same SQL server instance from another workspace. So the question is: how to register a new gateway without removing the previous one?\nI followed this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/use-data-from-an-on-premises-sql-server\" rel=\"nofollow noreferrer\">documentation<\/a>.\nDoes the following explanation mean that there is no way to do that?<\/p>\n\n<blockquote>\n  <p>You can create and set up multiple gateways in Studio for each workspace. For example, you may have a gateway that you want to connect to your test data sources during development, and a different gateway for your production data sources. Azure Machine Learning gives you the flexibility to set up multiple gateways depending upon your corporate environment. Currently you can\u2019t share a gateway between workspaces and only one gateway can be installed on a single computer.<\/p>\n<\/blockquote>\n\n<p>It is quite limiting as connecting to the same server from multiple workspaces may be sometimes crucial.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1538466090420,
        "Question_favorite_count":null,
        "Question_last_edit_time":1538485474347,
        "Question_score":0.0,
        "Question_view_count":40.0,
        "Answer_body":"<p>Well, finally I have found a way to bypass this limitation. From this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/use-data-from-an-on-premises-sql-server\" rel=\"nofollow noreferrer\">documentation<\/a> I have found that: <\/p>\n\n<blockquote>\n  <p>The IR does not need to be on the same machine as the data source. But staying closer to the data source reduces the time for the gateway to connect to the data source. We recommend that you install the IR on a machine that's different from the one that hosts the on-premises data source so that the gateway and data source don't compete for resources.<\/p>\n<\/blockquote>\n\n<p>So the  logic is pretty simple. You provide access to your local server to another machine on vpn and install your gateway there. Important: I have set up the firewall rules on the server before, to be able to establish the connection remotely.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52603929",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1538648158568,
        "Question_original_content":"aml studio regist mutlipl gatewai server struggl wai regist multipl gatewai local instanc sql server creat gatewai access aml studio workspac work fine like access sql server instanc workspac question regist new gatewai remov previou follow document follow explan mean wai creat set multipl gatewai studio workspac exampl gatewai want connect test data sourc develop differ gatewai product data sourc give flexibl set multipl gatewai depend corpor environ current share gatewai workspac gatewai instal singl limit connect server multipl workspac crucial",
        "Question_preprocessed_content":"aml studio regist mutlipl gatewai server struggl wai regist multipl gatewai local instanc sql server creat gatewai access aml studio workspac work fine like access sql server instanc workspac question regist new gatewai remov previou follow document follow explan mean wai creat set multipl gatewai studio workspac exampl gatewai want connect test data sourc develop differ gatewai product data sourc give flexibl set multipl gatewai depend corpor environ current share gatewai workspac gatewai instal singl limit connect server multipl workspac crucial",
        "Question_gpt_summary_original":"The user is facing a challenge in registering multiple gateways on the same server in AML Studio. They have successfully created a gateway to access their local SQL server instance from one workspace but are struggling to access it from another workspace without removing the previous gateway. The documentation suggests that only one gateway can be installed on a single computer and cannot be shared between workspaces, which limits the user's ability to connect to the same server from multiple workspaces.",
        "Question_gpt_summary":"user face challeng regist multipl gatewai server aml studio successfulli creat gatewai access local sql server instanc workspac struggl access workspac remov previou gatewai document suggest gatewai instal singl share workspac limit user abil connect server multipl workspac",
        "Answer_original_content":"final wai bypass limit document need machin data sourc stai closer data sourc reduc time gatewai connect data sourc recommend instal machin differ host premis data sourc gatewai data sourc compet resourc logic pretti simpl provid access local server machin vpn instal gatewai import set firewal rule server abl establish connect remot",
        "Answer_preprocessed_content":"final wai bypass limit document need machin data sourc stai closer data sourc reduc time gatewai connect data sourc recommend instal machin differ host data sourc gatewai data sourc compet resourc logic pretti simpl provid access local server machin vpn instal gatewai import set firewal rule server abl establish connect remot",
        "Answer_gpt_summary_original":"Solution: The user found a way to bypass the limitation of only being able to install one gateway on a single computer by installing the Integration Runtime (IR) on a different machine that has access to the local server through VPN. The documentation suggests that the IR does not need to be on the same machine as the data source, and it is recommended to install it on a different machine to avoid resource competition between the gateway and data source. The user also set up firewall rules on the server to establish the connection remotely.",
        "Answer_gpt_summary":"solut user wai bypass limit abl instal gatewai singl instal integr runtim differ machin access local server vpn document suggest need machin data sourc recommend instal differ machin avoid resourc competit gatewai data sourc user set firewal rule server establish connect remot"
    },
    {
        "Question_title":"How to log artifacts in wandb while using saimpletransformers?",
        "Question_body":"<p>I am creating a Question Answering model using <a href=\"https:\/\/simpletransformers.ai\/docs\/qa-specifics\/\" rel=\"nofollow noreferrer\">simpletransformers<\/a>. I would also like to use wandb to track model artifacts. As I understand from <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/other\/simpletransformers\" rel=\"nofollow noreferrer\">wandb docs<\/a>, there is an integration touchpoint for simpletransformers but there is no mention of logging artifacts.<\/p>\n<p>I would like to log artifacts generated at the train, validation, and test phase such as train.json, eval.json, test.json, output\/nbest_predictions_test.json and best performing model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634705743473,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":53.0,
        "Answer_body":"<p>Currently simpleTransformers doesn't support logging artifacts within the training\/testing scripts. But you can do it manually:<\/p>\n<pre><code>import os \n\nwith wandb.init(id=model.wandb_run_id, resume=&quot;allow&quot;, project=wandb_project) as training_run:\n    for dir in sorted(os.listdir(&quot;outputs&quot;)):\n        if &quot;checkpoint&quot; in dir:\n            artifact = wandb.Artifact(&quot;model-checkpoints&quot;, type=&quot;checkpoints&quot;)\n            artifact.add_dir(&quot;outputs&quot; + &quot;\/&quot; + dir)\n            training_run.log_artifact(artifact)\n<\/code><\/pre>\n<p>For more info, you can follow along with the W&amp;B notebook in the SimpleTransofrmer's README.md<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69640534",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1634729204572,
        "Question_original_content":"log artifact saimpletransform creat question answer model simpletransform like us track model artifact understand doc integr touchpoint simpletransform mention log artifact like log artifact gener train valid test phase train json eval json test json output nbest predict test json best perform model",
        "Question_preprocessed_content":"log artifact saimpletransform creat question answer model simpletransform like us track model artifact understand doc integr touchpoint simpletransform mention log artifact like log artifact gener train valid test phase best perform model",
        "Question_gpt_summary_original":"The user is facing a challenge in logging artifacts while using simpletransformers and wandb for a Question Answering model. Although there is an integration touchpoint for simpletransformers in wandb, there is no mention of logging artifacts. The user wants to log artifacts generated during the train, validation, and test phases, including train.json, eval.json, test.json, output\/nbest_predictions_test.json, and the best performing model.",
        "Question_gpt_summary":"user face challeng log artifact simpletransform question answer model integr touchpoint simpletransform mention log artifact user want log artifact gener train valid test phase includ train json eval json test json output nbest predict test json best perform model",
        "Answer_original_content":"current simpletransform support log artifact train test script manual import init model run resum allow project project train run dir sort listdir output checkpoint dir artifact artifact model checkpoint type checkpoint artifact add dir output dir train run log artifact artifact info follow notebook simpletransofrm readm",
        "Answer_preprocessed_content":"current simpletransform support log artifact script manual info follow notebook simpletransofrm",
        "Answer_gpt_summary_original":"Solution: Currently, simpleTransformers does not support logging artifacts within the training\/testing scripts. However, the user can manually log artifacts by importing os and using the wandb.Artifact() function to add the directories containing the artifacts. The user can follow the W&B notebook in the SimpleTransformers' README.md for more information.",
        "Answer_gpt_summary":"solut current simpletransform support log artifact train test script user manual log artifact import artifact function add directori contain artifact user follow notebook simpletransform readm inform"
    },
    {
        "Question_title":"AzureML: experiment working for a subset and not for the whole dataset",
        "Question_body":"<p>some times ago I had written a code in AzureML meeting \"out of memory\" issues. So I tried to split the code in three different codes and that partially worked. It remains a part that (I think) is affected by memory issues too.<\/p>\n\n<p>I have created an experiment that I have published in this <a href=\"http:\/\/gallery.cortanaintelligence.com\/Experiment\/TextMining-sample-NA-v1-1\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n\n<p>There is a module that considers only a sample of my dataset, and it does work. This means that the code is supposed to work correctly. If you remove the sampling code (the second module starting from the top) <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Cbzhj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Cbzhj.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>and you connect directly the original dataset you have the following situation<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/XOo8e.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XOo8e.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>producing the following error:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/mRSSQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mRSSQ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Does someone have some way to understand where Azure crashes?<\/p>\n\n<p>Thanks you,<\/p>\n\n<p>Andrea<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1472651915230,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":102.0,
        "Answer_body":"<p>Thanks so much for publishing the example -- this really helped to understand the issue. I suspect that you want to modify the <code>gsub()<\/code> calls in your script by adding the argument \"<code>fixed=TRUE<\/code>\" to each. (The documentation for this function is <a href=\"https:\/\/stat.ethz.ch\/R-manual\/R-devel\/library\/base\/html\/grep.html\" rel=\"nofollow\">here<\/a>.)<\/p>\n\n<p>What appears to have happened is that somewhere in your full dataset -- but not in the subsampled dataset -- there is some text that winds up being included in <code>df[i, \"names\"]<\/code> as \"<code>(art.<\/code>\".  Your script pads this into \"<code>\\\\b(art.\\\\b<\/code>\". The <code>gsub()<\/code> function tries to interpret this as a regular expression instead of a simple string, then throws an error because it is not a valid regular expression: it contains an opening parenthesis but no closing parenthesis. I believe that you actually did not want <code>gsub()<\/code> to interpret the input as a regular expression in the first place, and specifying <code>gsub(..., fixed=TRUE)<\/code> will correct that.<\/p>\n\n<p>I believe the reason why this error disappears when you add the sample\/partition module is because, by chance, the problematic input value was dropped on subsampling. I do not think it is an issue of available resources on Azure ML. (Caveat: I cannot confirm the fix works yet; I made the suggested update and started running the experiment, but it has not yet completed successfully.)<\/p>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/39251701",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1472676812496,
        "Question_original_content":"experi work subset dataset time ago written code meet memori issu tri split code differ code partial work remain think affect memori issu creat experi publish link modul consid sampl dataset work mean code suppos work correctli remov sampl code second modul start connect directli origin dataset follow situat produc follow error wai understand azur crash thank andrea",
        "Question_preprocessed_content":"experi work subset dataset time ago written code meet memori issu tri split code differ code partial work remain affect memori issu creat experi publish link modul consid sampl dataset work mean code suppos work correctli remov sampl code connect directli origin dataset follow situat produc follow error wai understand azur crash thank andrea",
        "Question_gpt_summary_original":"The user has encountered memory issues while working with AzureML. They tried to split the code into three different codes, which partially worked, but there is still a part that is affected by memory issues. The experiment created by the user works for a subset of the dataset, but not for the whole dataset, resulting in an error. The user is seeking help to understand where Azure crashes.",
        "Question_gpt_summary":"user encount memori issu work tri split code differ code partial work affect memori issu experi creat user work subset dataset dataset result error user seek help understand azur crash",
        "Answer_original_content":"thank publish exampl help understand issu suspect want modifi gsub call script ad argument fix true document function appear happen dataset subsampl dataset text wind includ name art script pad art gsub function tri interpret regular express instead simpl string throw error valid regular express contain open parenthesi close parenthesi believ actual want gsub interpret input regular express place specifi gsub fix true correct believ reason error disappear add sampl partit modul chanc problemat input valu drop subsampl think issu avail resourc caveat confirm fix work suggest updat start run experi complet successfulli",
        "Answer_preprocessed_content":"thank publish exampl help understand issu suspect want modifi call script ad argument appear happen dataset subsampl dataset text wind includ script pad function tri interpret regular express instead simpl string throw error valid regular express contain open parenthesi close parenthesi believ actual want interpret input regular express place specifi correct believ reason error disappear add modul chanc problemat input valu drop subsampl think issu avail resourc",
        "Answer_gpt_summary_original":"Solution:\n- Modify the `gsub()` calls in the script by adding the argument \"fixed=TRUE\" to each. This will prevent `gsub()` from interpreting the input as a regular expression and correct the error caused by an invalid regular expression.\n- The error disappears when adding the sample\/partition module because the problematic input value was dropped on subsampling, not because of available resources on Azure ML.",
        "Answer_gpt_summary":"solut modifi gsub call script ad argument fix true prevent gsub interpret input regular express correct error caus invalid regular express error disappear ad sampl partit modul problemat input valu drop subsampl avail resourc"
    },
    {
        "Question_title":"Configuring GPU in aws sagemaker with keras and tensorflow as backend",
        "Question_body":"<p>I am a newbie to aws sagemaker.\nI am trying to setup a model in aws sagemaker using keras with GPU support.\nThe docker base image used to infer the model is given below<\/p>\n\n<pre><code>FROM tensorflow\/tensorflow:1.10.0-gpu-py3\n\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends nginx curl\n...\n<\/code><\/pre>\n\n<p>This is the keras code I'm using to check if a GPU is identified by keras in flask.<\/p>\n\n<pre><code>import keras\n@app.route('\/ping', methods=['GET'])\ndef ping():\n\n    keras.backend.tensorflow_backend._get_available_gpus()\n\n    return flask.Response(response='\\n', status=200,mimetype='application\/json')\n<\/code><\/pre>\n\n<p>When I spin up a notebook instance in a sagemaker using the GPU the keras code shows available GPUs.\nSo, in order to access GPU in the inference phase(model) do I need to install any additional libraries in the docker file apart from the tensorflow GPU base image?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1544505483170,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":1958.0,
        "Answer_body":"<p>You shouldn't need to install anything else. Keras relies on TensorFlow for GPU detection and configuration.<\/p>\n\n<p>The only thing worth noting is how to use multiple GPUs during training. I'd recommend passing 'gpu_count' as an hyper parameter, and setting things up like so:<\/p>\n\n<pre><code>from keras.utils import multi_gpu_model\nmodel = Sequential()\nmodel.add(...)\n...\nif gpu_count &gt; 1:\n    model = multi_gpu_model(model, gpus=gpu_count)\nmodel.compile(...)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53717800",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1545210167392,
        "Question_original_content":"configur gpu kera tensorflow backend newbi try setup model kera gpu support docker base imag infer model given tensorflow tensorflow gpu run apt updat apt instal instal recommend nginx curl kera code check gpu identifi kera flask import kera app rout ping method def ping kera backend tensorflow backend avail gpu return flask respons respons statu mimetyp applic json spin notebook instanc gpu kera code show avail gpu order access gpu infer phase model need instal addit librari docker file apart tensorflow gpu base imag thank advanc",
        "Question_preprocessed_content":"configur gpu kera tensorflow backend newbi try setup model kera gpu support docker base imag infer model given kera code check gpu identifi kera flask spin notebook instanc gpu kera code show avail gpu order access gpu infer phase need instal addit librari docker file apart tensorflow gpu base imag thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges in configuring GPU support for a model in AWS Sagemaker using Keras. They are unsure if they need to install additional libraries in the Docker file to access the GPU during the inference phase. The user has confirmed that the GPU is identified by Keras when they spin up a notebook instance in Sagemaker.",
        "Question_gpt_summary":"user face challeng configur gpu support model kera unsur need instal addit librari docker file access gpu infer phase user confirm gpu identifi kera spin notebook instanc",
        "Answer_original_content":"shouldn need instal kera reli tensorflow gpu detect configur thing worth note us multipl gpu train recommend pass gpu count hyper paramet set thing like kera util import multi gpu model model sequenti model add gpu count model multi gpu model model gpu gpu count model compil",
        "Answer_preprocessed_content":"shouldn need instal kera reli tensorflow gpu detect configur thing worth note us multipl gpu train recommend pass hyper paramet set thing like",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that the user does not need to install any additional libraries in the Docker file to access the GPU during the inference phase. Keras relies on TensorFlow for GPU detection and configuration. However, if the user wants to use multiple GPUs during training, they can pass 'gpu_count' as a hyperparameter and set up the model using the 'multi_gpu_model' function from Keras.",
        "Answer_gpt_summary":"solut discuss suggest user need instal addit librari docker file access gpu infer phase kera reli tensorflow gpu detect configur user want us multipl gpu train pass gpu count hyperparamet set model multi gpu model function kera"
    },
    {
        "Question_title":"Can I make Neptune talk to git?",
        "Question_body":"<p>In <a href=\"https:\/\/neptune.ml\/\" rel=\"nofollow noreferrer\">Neptune<\/a> (this machine learning experiment tracker) is it possible to make it git-aware? I mean - using <code>.gitignore<\/code> for excluded files and saving commit hashes for each run?<\/p>\n\n<p>In particular, when I review an already finished job, can I go directly to GitHub commit?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1495124614783,
        "Question_favorite_count":null,
        "Question_last_edit_time":1503919276067,
        "Question_score":0.0,
        "Question_view_count":140.0,
        "Answer_body":"<p>Starting form version 2.0 Neptune provides integration with git, see: <a href=\"https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/\" rel=\"nofollow noreferrer\">https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44053141",
        "Tool":"Neptune",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1503919410423,
        "Question_original_content":"talk git machin learn experi tracker possibl git awar mean gitignor exclud file save commit hash run particular review finish job directli github commit",
        "Question_preprocessed_content":"talk git possibl mean exclud file save commit hash run particular review finish job directli github commit",
        "Question_gpt_summary_original":"The user is facing challenges in making Neptune, a machine learning experiment tracker, git-aware. They want to use .gitignore for excluded files and save commit hashes for each run. Additionally, they want to be able to go directly to GitHub commit when reviewing an already finished job.",
        "Question_gpt_summary":"user face challeng make machin learn experi tracker git awar want us gitignor exclud file save commit hash run addition want abl directli github commit review finish job",
        "Answer_original_content":"start form version provid integr git http doc advanc topic git integr",
        "Answer_preprocessed_content":"start form version provid integr git",
        "Answer_gpt_summary_original":"Solution: The user can use Neptune's integration with git starting from version 2.0. This integration allows the user to use .gitignore for excluded files and save commit hashes for each run. Additionally, the user can go directly to the GitHub commit when reviewing an already finished job. The documentation for this integration can be found at https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/.",
        "Answer_gpt_summary":"solut user us integr git start version integr allow user us gitignor exclud file save commit hash run addition user directli github commit review finish job document integr http doc advanc topic git integr"
    },
    {
        "Question_title":"MLFlow tracking ui not showing experiments on local machine (laptop)",
        "Question_body":"<p>I am a beginner in mlflow and was trying to set it up locally using Anaconda 3.\nI have created a new environment in anaconda and install mlflow and sklearn in it. Now I am using jupyter notebook to run my sample code for mlflow.<\/p>\n<p>'''<\/p>\n<pre><code>import os\nimport warnings\nimport sys\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nfrom urllib.parse import urlparse\nimport mlflow\nimport mlflow.sklearn\n\nimport logging\n\nlogging.basicConfig(level=logging.WARN)\nlogger = logging.getLogger(__name__)\n\nwarnings.filterwarnings(&quot;ignore&quot;)\nnp.random.seed(40)\n\n\nmlflow.set_tracking_uri(&quot;file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun&quot;)\n\nmlflow.get_tracking_uri()\n\nmlflow.get_experiment\n\n#experiment_id = mlflow.create_experiment(&quot;Mlflow_demo&quot;)\nexperiment_id = mlflow.create_experiment(&quot;Demo3&quot;)\nexperiment = mlflow.get_experiment(experiment_id)\nprint(&quot;Name: {}&quot;.format(experiment.name))\nprint(&quot;Experiment_id: {}&quot;.format(experiment.experiment_id))\nprint(&quot;Artifact Location: {}&quot;.format(experiment.artifact_location))\nprint(&quot;Tags: {}&quot;.format(experiment.tags))\nprint(&quot;Lifecycle_stage: {}&quot;.format(experiment.lifecycle_stage))\n\nmlflow.set_experiment(&quot;Demo3&quot;)\n\ndef eval_metrics(actual, pred):\n    rmse = np.sqrt(mean_squared_error(actual, pred))\n    mae = mean_absolute_error(actual, pred)\n    r2 = r2_score(actual, pred)\n    return rmse, mae, r2\n\n# Read the wine-quality csv file from the URL\ncsv_url =\\\n    'http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine-quality\/winequality-red.csv'\ntry:\n    data = pd.read_csv(csv_url, sep=';')\nexcept Exception as e:\n    logger.exception(\n        &quot;Unable to download training &amp; test CSV, check your internet connection. Error: %s&quot;, e)\n\ndata.head(2)\n\n\ndef train_model(data, alpha, l1_ratio):\n    \n    # Split the data into training and test sets. (0.75, 0.25) split.\n    train, test = train_test_split(data)\n\n    # The predicted column is &quot;quality&quot; which is a scalar from [3, 9]\n    train_x = train.drop([&quot;quality&quot;], axis=1)\n    test_x = test.drop([&quot;quality&quot;], axis=1)\n    train_y = train[[&quot;quality&quot;]]\n    test_y = test[[&quot;quality&quot;]]\n\n    # Set default values if no alpha is provided\n    alpha = alpha\n    l1_ratio = l1_ratio\n\n\n    # Execute ElasticNet\n    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    lr.fit(train_x, train_y)\n\n    # Evaluate Metrics\n    predicted_qualities = lr.predict(test_x)\n    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n    # Print out metrics\n    print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))\n    print(&quot;  RMSE: %s&quot; % rmse)\n    print(&quot;  MAE: %s&quot; % mae)\n    print(&quot;  R2: %s&quot; % r2)\n    \n    # Log parameter, metrics, and model to MLflow\n    with mlflow.start_run(experiment_id = experiment_id):\n        mlflow.log_param(&quot;alpha&quot;, alpha)\n        mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio)\n        mlflow.log_metric(&quot;rmse&quot;, rmse)\n        mlflow.log_metric(&quot;r2&quot;, r2)\n        mlflow.log_metric(&quot;mae&quot;, mae)\n        mlflow.sklearn.log_model(lr, &quot;model&quot;)\n        \n\ntrain_model(data, 0.5, 0.5)\n\ntrain_model(data, 0.5, 0.3)\n\ntrain_model(data, 0.4, 0.3)\n<\/code><\/pre>\n<p>'''<\/p>\n<p>using above code, I am successfully able to create 3 different experiment as I can see the folders created in my local directory as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jKqgX.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Now, I am trying to run the mlflow ui using the jupyter terminal in my chrome browser and I am able to open the mlflow ui but cannot see and experiments as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6KaQK.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Could you help me in finding where I am going wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1648821712310,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":936.0,
        "Answer_body":"<p>Where do you run <code>mlflow ui<\/code> command?<\/p>\n<p>I think if you pass tracking ui path in the arguments, it would work:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>mlflow ui --backend-store-uri file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1659992652860,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71708147",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1650761211590,
        "Question_original_content":"track show experi local machin laptop beginn try set local anaconda creat new environ anaconda instal sklearn jupyt notebook run sampl code import import warn import sy import panda import numpi sklearn metric import mean squar error mean absolut error score sklearn model select import train test split sklearn linear model import elasticnet urllib pars import urlpars import import sklearn import log log basicconfig level log warn logger log getlogg warn filterwarn ignor random seed set track uri file user swapnil document localpython demo mlrun track uri experi experi creat experi demo experi creat experi demo experi experi experi print format experi print experi format experi experi print artifact locat format experi artifact locat print tag format experi tag print lifecycl stage format experi lifecycl stage set experi demo def eval metric actual pred rmse sqrt mean squar error actual pred mae mean absolut error actual pred score actual pred return rmse mae read wine qualiti csv file url csv url http archiv ic uci edu machin learn databas wine qualiti winequ red csv try data read csv csv url sep except logger except unabl download train test csv check internet connect error data head def train model data alpha ratio split data train test set split train test train test split data predict column qualiti scalar train train drop qualiti axi test test drop qualiti axi train train qualiti test test qualiti set default valu alpha provid alpha alpha ratio ratio execut elasticnet elasticnet alpha alpha ratio ratio random state fit train train evalu metric predict qualiti predict test rmse mae eval metric test predict qualiti print metric print elasticnet model alpha ratio alpha ratio print rmse rmse print mae mae print log paramet metric model start run experi experi log param alpha alpha log param ratio ratio log metric rmse rmse log metric log metric mae mae sklearn log model model train model data train model data train model data code successfulli abl creat differ experi folder creat local directori shown enter imag descript try run jupyt termin chrome browser abl open experi shown enter imag descript help find go wrong",
        "Question_preprocessed_content":"track show experi local machin beginn try set local anaconda creat new environ anaconda instal sklearn jupyt notebook run sampl code code successfulli abl creat differ experi folder creat local directori shown enter imag descript try run jupyt termin chrome browser abl open experi shown enter imag descript help find go wrong",
        "Question_gpt_summary_original":"The user is a beginner in MLFlow and has set it up locally using Anaconda 3. They have created a new environment in Anaconda and installed MLFlow and Sklearn in it. The user has successfully created three different experiments using Jupyter Notebook, but when trying to run the MLFlow UI using the Jupyter terminal in their Chrome browser, they are unable to see any experiments. The user is seeking help in finding where they are going wrong.",
        "Question_gpt_summary":"user beginn set local anaconda creat new environ anaconda instal sklearn user successfulli creat differ experi jupyt notebook try run jupyt termin chrome browser unabl experi user seek help find go wrong",
        "Answer_original_content":"run command think pass track path argument work backend store uri file user swapnil document localpython demo mlrun",
        "Answer_preprocessed_content":"run command think pass track path argument work",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to pass the tracking ui path in the arguments when running the \"mlflow ui\" command. The suggested command is \"mlflow ui --backend-store-uri file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun\".",
        "Answer_gpt_summary":"solut possibl solut mention discuss pass track path argument run command suggest command backend store uri file user swapnil document localpython demo mlrun"
    },
    {
        "Question_title":"Kubeflow Pipeline Training Component Failing | Unknown return type: <class 'inspect._empty'>",
        "Question_body":"<p>I am running an ML pipeline and the training component\/step (see code below) continues to fail with the following error: &quot;RuntimeError: Unknown return type: &lt;class 'inspect._empty'&gt;. Must be one of <code>str<\/code>, <code>int<\/code>, <code>float<\/code>, a subclass of <code>Artifact<\/code>, or a NamedTuple collection of these types.&quot;<\/p>\n<p>Any ideas on what might be causing the issue\/error and how to resolve it?<\/p>\n<p>Thank you!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>\n@component(\n    # this component builds an xgboost classifier with xgboost\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;\n)\n\ndef build_xgb_xgboost(project: str, \n                            bq_dataset: str, \n                            test_view_name: str,\n                            bq_location: str,\n                            metrics: Output[Metrics],\n                            model: Output[Model]\n\n):\n    from google.cloud import bigquery\n    import xgboost as xgb\n    import pandas as pd\n    from xgboost import XGBRegressor\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.metrics import mean_squared_error as MSE\n    from sklearn.metrics import mean_absolute_error\n    import joblib\n    import pyarrow\n    import db_dtypes\n     \n\n    client = bigquery.Client(project=project) \n\n    view_uri = f&quot;{project}.{bq_dataset}.{test_view_name}&quot; #replace view_name with test_view_name\n    \n    build_df_for_xgboost = '''\n    SELECT * FROM `{view_uri}`\n    '''.format(view_uri = view_uri)\n\n    job_config = bigquery.QueryJobConfig()\n    df_1 = client.query(build_df_for_xgboost).to_dataframe()\n    \n    #client.query(build_df_for_xgboost, job_config=job_config).to_dataframe()  \n    \n    df = df_1.drop(['int64_field_0'], axis=1)\n    \n    def onehot_encode(df, column):\n        df = df.copy()\n        dummies = pd.get_dummies(df[column], prefix=column)\n        df = pd.concat([df, dummies], axis=1)\n        df = df.drop(column, axis=1)\n    return df\n    \n    # Binary encoding\n    df['preferred_foot'] = df['preferred_foot'].replace({'left': 0, 'right': 1})\n    \n    # One-hot encoding\n    for column in ['attacking_work_rate', 'defensive_work_rate']:\n        df = onehot_encode(df, column=column)\n    \n    # Split df into X and y\n    y = df['overall_rating']\n    X = df.drop('overall_rating', axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n\n    #specify parameters\n    \n    #define your model \n    bst = XGBRegressor(\n    objective='reg:linear',\n    learning_rate = '.1',\n    alpha = '0.001'\n    )\n    \n    #fit your model\n    bst.fit(X_train, y_train)\n    \n    # Predict the model \n    y_pred = bst.predict(X_test)\n    rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n    mae = mean_absolute_error(y_test, y_pred)\n    \n    metrics.log_metric(&quot;RMSE&quot;, rmse)\n    metrics.log_metric(&quot;framework&quot;, &quot;xgboost&quot;)\n    metrics.log_metric(&quot;dataset_size&quot;, len(df))\n    metrics.log_metric(&quot;MAE&quot;, mae)\n    \n    dump(bst, model.path + &quot;.joblib&quot;)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655350561587,
        "Question_favorite_count":null,
        "Question_last_edit_time":1659256989790,
        "Question_score":0.0,
        "Question_view_count":107.0,
        "Answer_body":"<p>I think this might just be a bug in the version of KFP v2 SDK code you're using.<\/p>\n<p>I mostly use the stable KFPv1 methods to avoid problems.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\n\n\ndef train_xgboost_model(\n    project: str, \n    bq_dataset: str, \n    test_view_name: str,\n    bq_location: str,\n    metrics_path: OutputPath(Metrics),\n    model_path: OutputPath(Model),\n):\n    import json\n    from pathlib import Path\n\n    metrics = {\n       ...\n    }\n    Path(metrics_path).write_text(json.dumps(metrics))\n\n    dump(bst, model_path)\n\ntrain_xgboost_model_op = create_component_from_func(\n    func=train_xgboost_model,\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;,\n)\n\n<\/code><\/pre>\n<p>You can also find many examples of real-world components in this repo: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components<\/a><\/p>\n<p>including an XGBoost trainer <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py<\/a><\/p>\n<p>and a full XGBoost pipeline: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72640182",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1659257407332,
        "Question_original_content":"kubeflow pipelin train compon fail unknown return type run pipelin train compon step code continu fail follow error runtimeerror unknown return type str int float subclass artifact namedtupl collect type idea caus issu error resolv thank compon compon build xgboost classifi xgboost packag instal googl cloud bigqueri xgboost panda sklearn joblib pyarrow dtype base imag python output compon file creat xgb model xgboost yaml def build xgb xgboost project str dataset str test view str locat str metric output metric model output model googl cloud import bigqueri import xgboost xgb import panda xgboost import xgbregressor sklearn model select import train test split sklearn preprocess import standardscal sklearn metric import mean squar error mse sklearn metric import mean absolut error import joblib import pyarrow import dtype client bigqueri client project project view uri project dataset test view replac view test view build xgboost select view uri format view uri view uri job config bigqueri queryjobconfig client queri build xgboost datafram client queri build xgboost job config job config datafram drop int field axi def onehot encod column copi dummi dummi column prefix column concat dummi axi drop column axi return binari encod prefer foot prefer foot replac left right hot encod column attack work rate defens work rate onehot encod column column split overal rate drop overal rate axi train test split train test train test train test split train size shuffl true random state scale scaler standardscal scaler fit train train datafram scaler transform train index train index column train column test datafram scaler transform test index test index column test column specifi paramet defin model bst xgbregressor object reg linear learn rate alpha fit model bst fit train train predict model pred bst predict test rmse sqrt mean test pred mae mean absolut error test pred metric log metric rmse rmse metric log metric framework xgboost metric log metric dataset size len metric log metric mae mae dump bst model path joblib",
        "Question_preprocessed_content":"kubeflow pipelin train compon fail unknown return type run pipelin train continu fail follow error runtimeerror unknown return type subclass namedtupl collect idea caus resolv thank",
        "Question_gpt_summary_original":"The user is encountering an error while running an ML pipeline's training component\/step. The error message states that the return type is unknown and must be one of the specified types. The user is seeking help to identify the cause of the issue and resolve it. The code for the training component is also provided.",
        "Question_gpt_summary":"user encount error run pipelin train compon step error messag state return type unknown specifi type user seek help identifi caus issu resolv code train compon provid",
        "Answer_original_content":"think bug version kfp sdk code us stabl kfpv method avoid problem kfp compon import inputpath outputpath creat compon func def train xgboost model project str dataset str test view str locat str metric path outputpath metric model path outputpath model import json pathlib import path metric path metric path write text json dump metric dump bst model path train xgboost model creat compon func func train xgboost model packag instal googl cloud bigqueri xgboost panda sklearn joblib pyarrow dtype base imag python output compon file creat xgb model xgboost yaml exampl real world compon repo http github com ark kun pipelin compon tree master compon includ xgboost trainer http github com ark kun pipelin compon blob dccf compon xgboost train compon xgboost pipelin http github com ark kun pipelin compon blob fbefeaafbaddbea sampl googl cloud vertex train tabular regress model xgboost import vertex pipelin",
        "Answer_preprocessed_content":"think bug version kfp sdk code us stabl kfpv method avoid problem exampl compon repo includ xgboost trainer xgboost pipelin",
        "Answer_gpt_summary_original":"No solutions are provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Accessing Google BigQuery from AWS SageMaker",
        "Question_body":"<p>When running locally, my Jupyter notebook is able to reference Google BigQuery like so:<\/p>\n\n<pre><code>%%bigquery some_bq_table\n\nSELECT *\nFROM\n  `some_bq_dataset.some_bq_table` \n<\/code><\/pre>\n\n<p>So that later in my notebook I can reference some_bq_table as a pandas dataframe, as exemplified here: <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/visualize-jupyter\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/bigquery\/docs\/visualize-jupyter<\/a><\/p>\n\n<p>I want to run my notebook on AWS SageMaker to test a few things. To authenticate with BigQuery it seems that the only two ways are using a service account on GCP (or locally) or pointing the the SDK to a credentials JSON using an env var (as explained here: <a href=\"https:\/\/cloud.google.com\/docs\/authentication\/getting-started\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/docs\/authentication\/getting-started<\/a>).<\/p>\n\n<p>For example<\/p>\n\n<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"\/home\/user\/Downloads\/[FILE_NAME].json\"\n<\/code><\/pre>\n\n<p>Is there an easy way to connect to bigquery from SageMaker? My best idea right now is to download the JSON from somewhere to the SageMaker instnace and then set the env var from the python code.<\/p>\n\n<p>For example, I would do this:<\/p>\n\n<pre><code>os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"\/home\/user\/Downloads\/[FILE_NAME].json\"\n<\/code><\/pre>\n\n<p>However, this isn't very secure - I don't like the idea of downloading my credentials JSON to a SageMaker instance (this means I would have to upload the credentials to some private s3 bucket and then store them on the SageMaker instance). Not the end of the world but I rather avoid this. <\/p>\n\n<p>Any ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1554454321543,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":1078.0,
        "Answer_body":"<p>As you mentioned GCP currently authenticates using service account, credentials JSON and API tokens. Instead of storing credentials in S3 bucket you can consider using AWS Secrets Manager or AWS Systems Manager Parameter Store to store the GCP credentials and then fetch them in Jupyter notebook. This way credentials can be secured and the credentials file will be created from Secrets Manager only when needed. <\/p>\n\n<p>This is sample code I used previously to connect to BigQuery from SageMaker instance.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nimport boto3\nfrom google.cloud.bigquery import magics\nfrom google.oauth2 import service_account\n\ndef get_gcp_credentials_from_ssm(param_name):\n    # read credentials from SSM parameter store\n    ssm = boto3.client('ssm')\n    # Get the requested parameter\n    response = ssm.get_parameters(Names=[param_name], WithDecryption=True)\n    # Store the credentials in a variable\n    gcp_credentials = response['Parameters'][0]['Value']\n    # save credentials temporarily to a file\n    credentials_file = '\/tmp\/.gcp\/service_credentials.json'\n    with open(credentials_file, 'w') as outfile:  \n        json.dump(json.loads(gcp_credentials), outfile)\n    # create google.auth.credentials.Credentials to use for queries \n    credentials = service_account.Credentials.from_service_account_file(credentials_file)\n    # remove temporary file\n    if os.path.exists(credentials_file):\n        os.remove(credentials_file)\n    return credentials\n\n# this will set the context credentials to use for queries performed in jupyter \n# using bigquery cell magic\nmagics.context.credentials = get_gcp_credentials_from_ssm('my_gcp_credentials')\n<\/code><\/pre>\n\n<p>Please note that SageMaker execution role should have access to SSM and of course other necessary route to connect to GCP. I am not sure if this is the best way though. Hope someone has better way.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55531608",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1554514774276,
        "Question_original_content":"access googl bigqueri run local jupyt notebook abl refer googl bigqueri like bigqueri tabl select dataset tabl later notebook refer tabl panda datafram exemplifi http cloud googl com bigqueri doc visual jupyt want run notebook test thing authent bigqueri wai servic account gcp local point sdk credenti json env var explain http cloud googl com doc authent get start exampl export googl applic credenti home user download file json easi wai connect bigqueri best idea right download json instnac set env var python code exampl environ googl applic credenti home user download file json isn secur like idea download credenti json instanc mean upload credenti privat bucket store instanc end world avoid idea",
        "Question_preprocessed_content":"access googl bigqueri run local jupyt notebook abl refer googl bigqueri like later notebook refer panda datafram exemplifi want run notebook test thing authent bigqueri wai servic account gcp point sdk credenti json env var exampl easi wai connect bigqueri best idea right download json instnac set env var python code exampl isn secur like idea download credenti json instanc end world avoid idea",
        "Question_gpt_summary_original":"The user is facing challenges in accessing Google BigQuery from AWS SageMaker. While the Jupyter notebook can reference BigQuery, the user needs to authenticate with BigQuery to run the notebook on AWS SageMaker. The only two ways to authenticate are using a service account on GCP or pointing the SDK to a credentials JSON using an env var. The user's best idea is to download the JSON to the SageMaker instance and set the env var from the python code, but this is not secure. The user is seeking ideas to connect to BigQuery from SageMaker without compromising security.",
        "Question_gpt_summary":"user face challeng access googl bigqueri jupyt notebook refer bigqueri user need authent bigqueri run notebook wai authent servic account gcp point sdk credenti json env var user best idea download json instanc set env var python code secur user seek idea connect bigqueri compromis secur",
        "Answer_original_content":"mention gcp current authent servic account credenti json api token instead store credenti bucket consid aw secret manag aw system manag paramet store store gcp credenti fetch jupyt notebook wai credenti secur credenti file creat secret manag need sampl code previous connect bigqueri instanc import import json import boto googl cloud bigqueri import magic googl oauth import servic account def gcp credenti ssm param read credenti ssm paramet store ssm boto client ssm request paramet respons ssm paramet name param withdecrypt true store credenti variabl gcp credenti respons paramet valu save credenti temporarili file credenti file tmp gcp servic credenti json open credenti file outfil json dump json load gcp credenti outfil creat googl auth credenti credenti us queri credenti servic account credenti servic account file credenti file remov temporari file path exist credenti file remov credenti file return credenti set context credenti us queri perform jupyt bigqueri cell magic magic context credenti gcp credenti ssm gcp credenti note execut role access ssm cours necessari rout connect gcp sure best wai hope better wai",
        "Answer_preprocessed_content":"mention gcp current authent servic account credenti json api token instead store credenti bucket consid aw secret manag aw system manag paramet store store gcp credenti fetch jupyt notebook wai credenti secur credenti file creat secret manag need sampl code previous connect bigqueri instanc note execut role access ssm cours necessari rout connect gcp sure best wai hope better wai",
        "Answer_gpt_summary_original":"Solution: One possible solution suggested in the discussion is to use AWS Secrets Manager or AWS Systems Manager Parameter Store to store the GCP credentials securely and fetch them in the Jupyter notebook. The user can use the sample code provided in the discussion to connect to BigQuery from SageMaker instance. The code reads the credentials from SSM parameter store, saves them temporarily to a file, creates google.auth.credentials.Credentials to use for queries, and removes the temporary file. The user should ensure that the SageMaker execution role has access to SSM and other necessary routes to connect to GCP.",
        "Answer_gpt_summary":"solut possibl solut suggest discuss us aw secret manag aw system manag paramet store store gcp credenti secur fetch jupyt notebook user us sampl code provid discuss connect bigqueri instanc code read credenti ssm paramet store save temporarili file creat googl auth credenti credenti us queri remov temporari file user ensur execut role access ssm necessari rout connect gcp"
    },
    {
        "Question_title":"Got error while pushing dvc file",
        "Question_body":"<p>I got an error when I triled to use <strong>dvc push<\/strong>. The error message is like that<\/p>\n<pre><code class=\"lang-auto\">ERROR: unexpected error - gdrive is supported, but requires 'dvc-gdrive' to be installed: No module named 'dvc_gdrive'\n<\/code><\/pre>\n<p>Version<\/p>\n<p>OS - Ubuntu 22.04<br>\nPython - 3.10.6<br>\ndvc - 2.38.1<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1671342258326,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":99.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mkmm19\">@mkmm19<\/a> ! As the error message indicates, you need an additional package for gdrive support. So, assuming you installed dvc with pip, you should run <code>pip install dvc[gdrive]<\/code>.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/got-error-while-pushing-dvc-file\/1420",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-12-19T14:13:43.336Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mkmm19\">@mkmm19<\/a> ! As the error message indicates, you need an additional package for gdrive support. So, assuming you installed dvc with pip, you should run <code>pip install dvc[gdrive]<\/code>.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"got error push file got error trile us push error messag like error unexpect error gdrive support requir gdrive instal modul name gdrive version ubuntu python",
        "Question_preprocessed_content":"got error push file got error trile us push error messag like version ubuntu python",
        "Question_gpt_summary_original":"The user encountered an error while trying to use \"dvc push\" and received an error message stating that \"dvc-gdrive\" needs to be installed. The user's OS is Ubuntu 22.04, Python version is 3.10.6, and dvc version is 2.38.1.",
        "Question_gpt_summary":"user encount error try us push receiv error messag state gdrive need instal user ubuntu python version version",
        "Answer_original_content":"mkmm error messag indic need addit packag gdrive support assum instal pip run pip instal gdrive",
        "Answer_preprocessed_content":"error messag indic need addit packag gdrive support assum instal pip run",
        "Answer_gpt_summary_original":"Solution: The solution mentioned in the discussion is to install the \"dvc-gdrive\" package by running the command \"pip install dvc[gdrive]\" assuming that dvc was installed with pip.",
        "Answer_gpt_summary":"solut solut mention discuss instal gdrive packag run command pip instal gdrive assum instal pip"
    },
    {
        "Question_title":"start, monitor and define script of SageMaker processing job from local machine",
        "Question_body":"<p>I am looking at <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">this<\/a>, which makes all sense. Let us focus on this bit of code:<\/p>\n<pre><code>from sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,\n    inputs=[\n        ProcessingInput(source=&quot;s3:\/\/your-bucket\/path\/to\/your\/data&quot;, destination=&quot;\/opt\/ml\/processing\/input&quot;),\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n\npreprocessing_job_description = sklearn_processor.jobs[-1].describe() \n<\/code><\/pre>\n<p>Here preprocessing.py has to be obviously in the cloud. I am curious, could one also put scripts into an S3 bucket and trigger the job remotely. I can easily to this with hyper parameter optimisation, which does not require dedicated scripts though as I use an OOTB training image.<\/p>\n<p>In this case I can fire off the job like so:<\/p>\n<pre><code>tuning_job_name = &quot;amazing-hpo-job-&quot; + strftime(&quot;%d-%H-%M-%S&quot;, gmtime())\n\nsmclient = boto3.Session().client(&quot;sagemaker&quot;)\nsmclient.create_hyper_parameter_tuning_job(\n    HyperParameterTuningJobName=tuning_job_name,\n    HyperParameterTuningJobConfig=tuning_job_config,\n    TrainingJobDefinition=training_job_definition\n)\n<\/code><\/pre>\n<p>and then monitor the job's progress:<\/p>\n<pre><code>smclient = boto3.Session().client(&quot;sagemaker&quot;)\n\ntuning_job_result = smclient.describe_hyper_parameter_tuning_job(\n    HyperParameterTuningJobName=tuning_job_name\n)\n\nstatus = tuning_job_result[&quot;HyperParameterTuningJobStatus&quot;]\nif status != &quot;Completed&quot;:\n    print(&quot;Reminder: the tuning job has not been completed.&quot;)\n\njob_count = tuning_job_result[&quot;TrainingJobStatusCounters&quot;][&quot;Completed&quot;]\nprint(&quot;%d training jobs have completed&quot; % job_count)\n\nobjective = tuning_job_result[&quot;HyperParameterTuningJobConfig&quot;][&quot;HyperParameterTuningJobObjective&quot;]\nis_minimize = objective[&quot;Type&quot;] != &quot;Maximize&quot;\nobjective_name = objective[&quot;MetricName&quot;]\n\nif tuning_job_result.get(&quot;BestTrainingJob&quot;, None):\n    print(&quot;Best model found so far:&quot;)\n    pprint(tuning_job_result[&quot;BestTrainingJob&quot;])\nelse:\n    print(&quot;No training jobs have reported results yet.&quot;) \n<\/code><\/pre>\n<p>I would think starting and monitoring a SageMaker processing job from a local machine should be possible as with an HPO job but what about the script(s)? Ideally I would like to develop and test them locally and the run remotely. Hope this makes sense?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662209372887,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":38.0,
        "Answer_body":"<p>Im not sure I understand the comparison to a Tuning Job.<\/p>\n<p>Based on what you have described, in this case the <code>preprocessing.py<\/code> is actually stored locally. The SageMaker SDK will upload it to S3 for the remote Processing Job to access it. I suggest launching the Job and then taking a look at the inputs in the SageMaker Console.<\/p>\n<p>If you wanted to test the Processing Job locally you can do so using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a>. This will basically imitate the Job locally which aids in debugging the script before kicking off a remote Processing Job. Kindly note docker is required to make use of Local Mode.<\/p>\n<p>Example code for local mode:<\/p>\n<pre><code>from sagemaker.local import LocalSession\nfrom sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n\nsagemaker_session = LocalSession()\nsagemaker_session.config = {'local': {'local_code': True}}\n\n# For local training a dummy role will be sufficient\nrole = 'arn:aws:iam::111111111111:role\/service-role\/AmazonSageMaker-ExecutionRole-20200101T000001'\n\nprocessor = ScriptProcessor(command=['python3'],\n                    image_uri='sagemaker-scikit-learn-processing-local',\n                    role=role,\n                    instance_count=1,\n                    instance_type='local')\n\nprocessor.run(code='processing_script.py',\n                    inputs=[ProcessingInput(\n                        source='.\/input_data\/',\n                        destination='\/opt\/ml\/processing\/input_data\/')],\n                    outputs=[ProcessingOutput(\n                        output_name='word_count_data',\n                        source='\/opt\/ml\/processing\/processed_data\/')],\n                    arguments=['job-type', 'word-count']\n                    )\n\npreprocessing_job_description = processor.jobs[-1].describe()\noutput_config = preprocessing_job_description['ProcessingOutputConfig']\n\nprint(output_config)\n\nfor output in output_config['Outputs']:\n    if output['OutputName'] == 'word_count_data':\n        word_count_data_file = output['S3Output']['S3Uri']\n\nprint('Output file is located on: {}'.format(word_count_data_file))\n\n\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73592371",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1662504468407,
        "Question_original_content":"start monitor defin script process job local machin look make sens let focu bit code process import processinginput processingoutput sklearn processor run code preprocess input processinginput sourc bucket path data destin opt process input output processingoutput output train data sourc opt process train processingoutput output test data sourc opt process test argument train test split ratio preprocess job descript sklearn processor job preprocess obvious cloud curiou script bucket trigger job remot easili hyper paramet optimis requir dedic script us ootb train imag case job like tune job amaz hpo job strftime gmtime smclient boto session client smclient creat hyper paramet tune job hyperparametertuningjobnam tune job hyperparametertuningjobconfig tune job config trainingjobdefinit train job definit monitor job progress smclient boto session client tune job result smclient hyper paramet tune job hyperparametertuningjobnam tune job statu tune job result hyperparametertuningjobstatu statu complet print remind tune job complet job count tune job result trainingjobstatuscount complet print train job complet job count object tune job result hyperparametertuningjobconfig hyperparametertuningjobobject minim object type maxim object object metricnam tune job result besttrainingjob print best model far pprint tune job result besttrainingjob print train job report result think start monitor process job local machin possibl hpo job script ideal like develop test local run remot hope make sens",
        "Question_preprocessed_content":"start monitor defin script process job local machin look make sens let focu bit code obvious cloud curiou script bucket trigger job remot easili hyper paramet optimis requir dedic script us ootb train imag case job like monitor job progress think start monitor process job local machin possibl hpo job script ideal like develop test local run remot hope make sens",
        "Question_gpt_summary_original":"The user is trying to start, monitor, and define a script for a SageMaker processing job from their local machine. They are able to trigger hyperparameter optimization remotely using an out-of-the-box training image, but they are unsure if they can put scripts into an S3 bucket and trigger the job remotely. The user would like to develop and test the scripts locally before running them remotely.",
        "Question_gpt_summary":"user try start monitor defin script process job local machin abl trigger hyperparamet optim remot box train imag unsur script bucket trigger job remot user like develop test script local run remot",
        "Answer_original_content":"sure understand comparison tune job base describ case preprocess actual store local sdk upload remot process job access suggest launch job take look input consol want test process job local local mode basic imit job local aid debug script kick remot process job kindli note docker requir us local mode exampl code local mode local import localsess process import scriptprocessor processinginput processingoutput session localsess session config local local code true local train dummi role suffici role arn aw iam role servic role amazon executionrol processor scriptprocessor command python imag uri scikit learn process local role role instanc count instanc type local processor run code process script input processinginput sourc input data destin opt process input data output processingoutput output word count data sourc opt process process data argument job type word count preprocess job descript processor job output config preprocess job descript processingoutputconfig print output config output output config output output outputnam word count data word count data file output soutput suri print output file locat format word count data file",
        "Answer_preprocessed_content":"sure understand comparison tune job base describ case actual store local sdk upload remot process job access suggest launch job take look input consol want test process job local local mode basic imit job local aid debug script kick remot process job kindli note docker requir us local mode exampl code local mode",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. The user can store the `preprocessing.py` script locally and the SageMaker SDK will upload it to S3 for the remote Processing Job to access it. They can launch the job and then check the inputs in the SageMaker Console.\n2. The user can test the Processing Job locally using Local Mode, which imitates the job locally and aids in debugging the script before kicking off a remote Processing Job. Docker is required to make use of Local Mode. \n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss user store preprocess script local sdk upload remot process job access launch job check input consol user test process job local local mode imit job local aid debug script kick remot process job docker requir us local mode person opinion bias includ summari"
    },
    {
        "Question_title":"How to cancel a running job from the UI?",
        "Question_body":"<p>Am I missing something but how can I cancel a run in my workspace from <a href=\"https:\/\/ms.portal.azure.com\/\" rel=\"nofollow noreferrer\">https:\/\/ms.portal.azure.com\/<\/a> ? The cancel button is always greyed out.<\/p>\n\n<p>I know I can use use the sdk to cancel a run using:<\/p>\n\n<pre><code>run = [ r for r in Experiment(ws, 'myExp').get_runs() if r.id == '899b8314-26b6-458f-9f5c-539ffbf01b91'].pop()\nrun.cancel()\n<\/code><\/pre>\n\n<p>But it would be more convenient to be able to do it from the UI<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1568993659347,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":204.0,
        "Answer_body":"<p>What kind of run is this? Canceling is not currently enabled for pipeline runs in the UI, but is supported for other run types.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1569508172863,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58031370",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1569333175230,
        "Question_original_content":"cancel run job miss cancel run workspac http portal azur com cancel button grei know us us sdk cancel run run experi myexp run ffbfb pop run cancel conveni abl",
        "Question_preprocessed_content":"cancel run job miss cancel run workspac cancel button grei know us us sdk cancel run conveni abl",
        "Question_gpt_summary_original":"The user is facing a challenge in cancelling a running job from the UI in their workspace on https:\/\/ms.portal.azure.com\/. The cancel button is always greyed out, and they have to use the SDK to cancel a run, which is inconvenient.",
        "Question_gpt_summary":"user face challeng cancel run job workspac http portal azur com cancel button grei us sdk cancel run inconveni",
        "Answer_original_content":"kind run cancel current enabl pipelin run support run type",
        "Answer_preprocessed_content":"kind run cancel current enabl pipelin run support run type",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that cancelling pipeline runs from the UI is not currently supported, but cancelling other run types is possible. No other solutions are mentioned.",
        "Answer_gpt_summary":"solut discuss suggest cancel pipelin run current support cancel run type possibl solut mention"
    },
    {
        "Question_title":"MLflow: INVALID_PARAMETER_VALUE: Unsupported URI '.\/mlruns' for model registry store",
        "Question_body":"<p>I got this error when I was trying to have a model registered in the model registry. Could someone help me?<\/p>\n<pre><code>RestException: INVALID_PARAMETER_VALUE: Unsupported URI '.\/mlruns' for model registry store. \nSupported schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. \nSee https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#storage for how to setup a compatible server.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596578095187,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1598815085667,
        "Question_score":10.0,
        "Question_view_count":12594.0,
        "Answer_body":"<p>Mlflow required DB as datastore for Model Registry\nSo you have to run tracking server with DB as backend-store and log model to this tracking server.\nThe easiest way to use DB is to use SQLite.<\/p>\n<pre><code>mlflow server \\\n    --backend-store-uri sqlite:\/\/\/mlflow.db \\\n    --default-artifact-root .\/artifacts \\\n    --host 0.0.0.0\n<\/code><\/pre>\n<p>And set MLFLOW_TRACKING_URI environment variable to <em>http:\/\/localhost:5000<\/em> or<\/p>\n<pre><code>mlflow.set_tracking_uri(&quot;http:\/\/localhost:5000&quot;)\n<\/code><\/pre>\n<p>After got to http:\/\/localhost:5000 and you can register a logged model from UI or from the code.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":null,
        "Answer_score":27.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63255631",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1596626369480,
        "Question_original_content":"invalid paramet valu unsupport uri mlrun model registri store got error try model regist model registri help restexcept invalid paramet valu unsupport uri mlrun model registri store support scheme postgresql mysql sqlite mssql http org doc latest track html storag setup compat server",
        "Question_preprocessed_content":"unsupport uri model registri store got error try model regist model registri help",
        "Question_gpt_summary_original":"The user encountered an error while trying to register a model in the model registry. The error message stated that the URI '.\/mlruns' is unsupported for the model registry store and provided a list of supported schemes. The user requested assistance in resolving the issue.",
        "Question_gpt_summary":"user encount error try regist model model registri error messag state uri mlrun unsupport model registri store provid list support scheme user request assist resolv issu",
        "Answer_original_content":"requir datastor model registri run track server backend store log model track server easiest wai us us sqlite server backend store uri sqlite default artifact root artifact host set track uri environ variabl http localhost set track uri http localhost got http localhost regist log model code",
        "Answer_preprocessed_content":"requir datastor model registri run track server log model track server easiest wai us us sqlite set environ variabl got regist log model code",
        "Answer_gpt_summary_original":"Solution: The user needs to run the tracking server with a database as the backend store and log the model to this tracking server. The easiest way to use a database is to use SQLite. The user can set the MLFLOW_TRACKING_URI environment variable to \"http:\/\/localhost:5000\" or use the \"mlflow.set_tracking_uri\" command. After that, the user can register a logged model from the UI or from the code by going to http:\/\/localhost:5000.",
        "Answer_gpt_summary":"solut user need run track server databas backend store log model track server easiest wai us databas us sqlite user set track uri environ variabl http localhost us set track uri command user regist log model code go http localhost"
    },
    {
        "Question_title":"Sagemaker API to list Hyperparameters",
        "Question_body":"<p>I'm currently trying to implement MLFlow Tracking into my training pipeline and would like to log the hyperparameters of my hyperparameter Tuning of each training job.<\/p>\n\n<p>Does anyone know, how to pull the list of hyperparameters that can be seen on the sagemaker training job interface (on the AWS console)? Is there any other smarter way to list how models perform in comparison in Sagemaker (and displayed)?<\/p>\n\n<p>I would assume there must be an easy and Pythonic way to do this (either boto3 or the sagemaker api) to get this data. I wasn't able to find it in Cloudwatch.<\/p>\n\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1591864702897,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":484.0,
        "Answer_body":"<p>there is indeed a rather pythonic way in the SageMaker python SDK:<\/p>\n\n<pre><code>tuner = sagemaker.tuner.HyperparameterTuner.attach('&lt; your tuning jobname&gt;')\n\nresults = tuner.analytics().dataframe()  # all your tuning metadata, in pandas!\n<\/code><\/pre>\n\n<p>See full example here <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62320331",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1592173650467,
        "Question_original_content":"api list hyperparamet current try implement track train pipelin like log hyperparamet hyperparamet tune train job know pull list hyperparamet seen train job interfac aw consol smarter wai list model perform comparison displai assum easi python wai boto api data wasn abl cloudwatch thank advanc",
        "Question_preprocessed_content":"api list hyperparamet current try implement track train pipelin like log hyperparamet hyperparamet tune train job know pull list hyperparamet seen train job interfac smarter wai list model perform comparison assum easi python wai data wasn abl cloudwatch thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges in implementing MLFlow Tracking into their training pipeline and is seeking help to log the hyperparameters of their hyperparameter tuning for each training job. They are looking for a way to pull the list of hyperparameters seen on the Sagemaker training job interface and display how models perform in comparison in Sagemaker. The user has tried to find this data in Cloudwatch but was unsuccessful.",
        "Question_gpt_summary":"user face challeng implement track train pipelin seek help log hyperparamet hyperparamet tune train job look wai pull list hyperparamet seen train job interfac displai model perform comparison user tri data cloudwatch unsuccess",
        "Answer_original_content":"python wai python sdk tuner tuner hyperparametertun attach result tuner analyt datafram tune metadata panda exampl http github com aw sampl amazon tuneranalyt sampl blob master tune job analyt ipynb",
        "Answer_preprocessed_content":"python wai python sdk exampl",
        "Answer_gpt_summary_original":"Solution: The user can use the SageMaker python SDK to pull the list of hyperparameters seen on the Sagemaker training job interface and display how models perform in comparison in Sagemaker. They can attach the hyperparameter tuner to their tuning job name and use the analytics() function to get all the tuning metadata in pandas. A full example is provided in the given link.",
        "Answer_gpt_summary":"solut user us python sdk pull list hyperparamet seen train job interfac displai model perform comparison attach hyperparamet tuner tune job us analyt function tune metadata panda exampl provid given link"
    },
    {
        "Question_title":"How to show all active runs without selecting the status manually",
        "Question_body":"From slack\n\nThere used to be a switch to show\/hide all active runs (multiple states at once) under the flags dropdown, I can't find it anymore. Was it removed from the UI?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649332873000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"It's directly under statuses, we consolidated some flags\/config options:",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1479",
        "Tool":"Polyaxon",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-07T12:01:37Z",
                "Answer_score":1,
                "Answer_body":"It's directly under statuses, we consolidated some flags\/config options:"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0,
        "Question_closed_time":null,
        "Question_original_content":"activ run select statu manual slack switch hide activ run multipl state flag dropdown anymor remov",
        "Question_preprocessed_content":"activ run select statu manual slack switch activ run flag dropdown anymor remov",
        "Question_gpt_summary_original":"The user is having difficulty finding a switch to show all active runs without manually selecting the status. They are unsure if the switch has been removed from the UI.",
        "Question_gpt_summary":"user have difficulti find switch activ run manual select statu unsur switch remov",
        "Answer_original_content":"directli status consolid flag config option",
        "Answer_preprocessed_content":"directli status consolid option",
        "Answer_gpt_summary_original":"Solution: The switch to show all active runs is still available in the UI and can be found directly under the statuses section. The flags\/config options have been consolidated, but the switch has not been removed.",
        "Answer_gpt_summary":"solut switch activ run avail directli status section flag config option consolid switch remov"
    },
    {
        "Question_title":"Add model description when registering model after hyperdrive successful run",
        "Question_body":"<p>I have successfully trained a model on Azure Machine Learning Service using Hyperdrive that has now yielded a hyperdrive run instance<\/p>\n\n<pre><code>hyperdrive_run = exp.submit(config=hypertune_config)\nhyperdrive_run\nbest_run = hyperdrive_run.get_best_run_by_primary_metric()\n<\/code><\/pre>\n\n<p>As a next step, I would like to register a model while adding a description to the model.:<\/p>\n\n<pre><code>pumps_rf = best_run.register_model(model_name='pumps_rf', model_path='outputs\/rf.pkl')\n<\/code><\/pre>\n\n<p>There is a <code>description<\/code> column in the Models section of my AML Workspace on Azure portal but the <code>register_model<\/code> method does not seem to have a <code>description<\/code> flag. So how do I go about adding a description to the model so I see it in Azure Portal?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1550539380057,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":453.0,
        "Answer_body":"<p>Good question :).<\/p>\n\n<p>Looking at the current version of the API, it doesn't look like you can add the description using <code>Run.register_model<\/code>, as confirmed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#register-model-model-name--model-path-none--tags-none--properties-none----kwargs-\" rel=\"nofollow noreferrer\">by the docs<\/a>. <\/p>\n\n<p>You can go around this however by registering the model using the <code>Model.register<\/code> method which, fortunately, includes an argument for <code>description<\/code> as detailed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none-\" rel=\"nofollow noreferrer\">here<\/a>. In your case, you also need to <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py#download-file-name--output-file-path-none-\" rel=\"nofollow noreferrer\">download the files<\/a> first.<\/p>\n\n<p>In short, use something like:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>best_run.download_file('outputs\/rf.pkl', output_file_path='.\/rf.pkl')\n\nModel.register(workspace=ws, model_path='.\/rf.pkl', model_name=\"pumps_rf\", description=\"There are many models like it, but this one is mine.\")\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1550686459150,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54757598",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1550668517447,
        "Question_original_content":"add model descript regist model hyperdr success run successfulli train model servic hyperdr yield hyperdr run instanc hyperdr run exp submit config hypertun config hyperdr run best run hyperdr run best run primari metric step like regist model ad descript model pump best run regist model model pump model path output pkl descript column model section aml workspac azur portal regist model method descript flag ad descript model azur portal",
        "Question_preprocessed_content":"add model descript regist model hyperdr success run successfulli train model servic hyperdr yield hyperdr run instanc step like regist model ad descript column model section aml workspac azur portal method flag ad descript model azur portal",
        "Question_gpt_summary_original":"The user has successfully trained a model on Azure Machine Learning Service using Hyperdrive, and has registered a model using the <code>register_model<\/code> method. However, the user is facing a challenge in adding a description to the model as the method does not have a <code>description<\/code> flag, and is unsure how to add a description to the model in the Azure Portal.",
        "Question_gpt_summary":"user successfulli train model servic hyperdr regist model regist model method user face challeng ad descript model method descript flag unsur add descript model azur portal",
        "Answer_original_content":"good question look current version api look like add descript run regist model confirm doc regist model model regist method fortun includ argument descript detail case need download file short us like best run download file output pkl output file path pkl model regist workspac model path pkl model pump descript model like",
        "Answer_preprocessed_content":"good question look current version api look like add descript confirm doc regist model method fortun includ argument detail case need download file short us like",
        "Answer_gpt_summary_original":"Solution: The user can register the model using the <code>Model.register<\/code> method which includes an argument for <code>description<\/code>. The user needs to download the files first and then use the <code>Model.register<\/code> method to register the model with a description.",
        "Answer_gpt_summary":"solut user regist model model regist method includ argument descript user need download file us model regist method regist model descript"
    },
    {
        "Question_title":"mlflow ui throws exception when using sqlite as backend_store_uri",
        "Question_body":"I try to start the mlflow ui service using SQLite as a backend store and cannot get rid of this exception:\n\n\nTypeError: Invalid argument(s) 'pool_pre_ping' sent to create_engine(), using configuration SQLiteDialect_pysqlite\/NullPool\/Engine.\u00a0 Please check that the keyword arguments are appropriate for this combination of components.\n\n\n\ncommand:\n(mlflow_demo) netanel@netpy:~\/git\/mlflow\/backend_store$ mlflow ui --backend-store-uri 'sqlite:\/\/\/home\/netanel\/git\/mlflow\/backend_store\/mlflow_tracker.db'\n\n\n\nI created the SQLite DB using the sqlite3 command line and initialize the schema using the latest_schema.sql script.\n\n\npython version:\u00a0Python 3.6.0 :: Anaconda 4.3.1 (64-bit)\nmlflow version: 1.5.0\n\n\nThanks.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1580038416000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":18.0,
        "Answer_body":"Someone can help me with that?\nThanks.\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/qeRbVnClqSI",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-01-28T00:53:17",
                "Answer_body":"Someone can help me with that?\nThanks.\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"throw except sqlite backend store uri try start servic sqlite backend store rid except typeerror invalid argument pool pre ping sent creat engin configur sqlitedialect pysqlit nullpool engin check keyword argument appropri combin compon command demo netanel netpi git backend store backend store uri sqlite home netanel git backend store tracker creat sqlite sqlite command line initi schema latest schema sql script python version python anaconda bit version thank",
        "Question_preprocessed_content":"throw except sqlite try start servic sqlite backend store rid except typeerror invalid argument sent configur check keyword argument appropri combin compon command creat sqlite sqlite command line initi schema script python version python anaconda version thank",
        "Question_gpt_summary_original":"The user is encountering a challenge when trying to start the mlflow ui service using SQLite as a backend store. They are receiving a TypeError with the message \"Invalid argument(s) 'pool_pre_ping' sent to create_engine(), using configuration SQLiteDialect_pysqlite\/NullPool\/Engine.\" The user has created the SQLite DB using the sqlite3 command line and initialized the schema using the latest_schema.sql script. The Python version used is Python 3.6.0 :: Anaconda 4.3.1 (64-bit) and the mlflow version is 1.5.0.",
        "Question_gpt_summary":"user encount challeng try start servic sqlite backend store receiv typeerror messag invalid argument pool pre ping sent creat engin configur sqlitedialect pysqlit nullpool engin user creat sqlite sqlite command line initi schema latest schema sql script python version python anaconda bit version",
        "Answer_original_content":"help thank",
        "Answer_preprocessed_content":"help thank",
        "Answer_gpt_summary_original":"Solution: No solution provided.",
        "Answer_gpt_summary":"solut solut provid"
    },
    {
        "Question_title":"Azure Machine Learning - batch execution partially working",
        "Question_body":"<p>I have been following this <a href=\"https:\/\/gallery.azureml.net\/Experiment\/370c80490e774a6cb26edba69c583c9b\" rel=\"nofollow\">gallery sample<\/a> but I just can't seem to get batch execution to return multiple scores in one job.<\/p>\n\n<p>Everything works fine i.e. can deploy the prediction web API and request a single scoring. But whenever I send a batch execution job (using the <a href=\"https:\/\/studio.azureml.net\/apihelp\/workspaces\/9dbdce0846e64a5f9c925116e0cb6388\/webservices\/7df2a06ad50348d78f0e1cb81f3742ab\/endpoints\/87bfd4ea0615412bac19c34422ced730\/jobs\" rel=\"nofollow\">sample C# codes<\/a>) with more than one request e.g.:<\/p>\n\n<pre><code>ID1,ID2\n1,2\n3,1\n5,1\n<\/code><\/pre>\n\n<p>Azure ML will only return the prediction scores for the first request <code>1,2<\/code> but not for the other rows.<\/p>\n\n<p>I'm not sure where I'm doing wrong but I should be expecting results for all three requests. Any help would be appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1437353901330,
        "Question_favorite_count":null,
        "Question_last_edit_time":1437413945292,
        "Question_score":1.0,
        "Question_view_count":269.0,
        "Answer_body":"<p>It looks like you've chosen an unfortunate example: the custom scripts in the Retail Forecasting web service explicitly drop all but the first ID pair. To see this, try loading the \"Retail Forecasting: Step 6A of 6\" experiment and check out the code in the \"Create a complete time series. Add future time stamps\" module. You will find the following:<\/p>\n\n<pre><code>all.time &lt;- data.frame(ID1 = data$ID1[1], ID2 = data$ID2[1], time = all.time)\ndata &lt;- join(all.time, data, by = c(\"ID1\", \"ID2\", \"time\"), type = \"left\")\nmaml.mapOutputPort(\"data\");\n<\/code><\/pre>\n\n<p>The left join statement will ignore any rows where data$ID1 != data$ID1[1]  and data$ID2 != data$ID2[1]. That is why you are losing everything but the first ID pair.<\/p>\n\n<p>It appears batch prediction for multiple ID pairs in a single job was not a use case that the custom script authors envisioned for their web service. If you are proficient in R and particularly interested in this use case, you could modify the scripts in this experiment to support processing multiple time series concurrently. Otherwise, you might want to simply try another example experiment.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/31507547",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1472479254436,
        "Question_original_content":"batch execut partial work follow galleri sampl batch execut return multipl score job work fine deploi predict web api request singl score send batch execut job sampl code request return predict score request row sure wrong expect result request help appreci",
        "Question_preprocessed_content":"batch execut partial work follow galleri sampl batch execut return multipl score job work fine deploi predict web api request singl score send batch execut job request return predict score request row sure wrong expect result request help appreci",
        "Question_gpt_summary_original":"The user is facing challenges with Azure Machine Learning batch execution as it is only returning prediction scores for the first request and not for the other rows. The user is seeking help to understand where they are going wrong and expects results for all three requests.",
        "Question_gpt_summary":"user face challeng batch execut return predict score request row user seek help understand go wrong expect result request",
        "Answer_original_content":"look like chosen unfortun exampl custom script retail forecast web servic explicitli drop pair try load retail forecast step experi check code creat complet time seri add futur time stamp modul follow time data frame data data time time data join time data time type left maml mapoutputport data left join statement ignor row data data data data lose pair appear batch predict multipl pair singl job us case custom script author envis web servic profici particularli interest us case modifi script experi support process multipl time seri concurr want simpli try exampl experi",
        "Answer_preprocessed_content":"look like chosen unfortun exampl custom script retail forecast web servic explicitli drop pair try load retail forecast step experi check code creat complet time seri add futur time stamp modul follow left join statement ignor row data data data data lose pair appear batch predict multipl pair singl job us case custom script author envis web servic profici particularli interest us case modifi script experi support process multipl time seri concurr want simpli try exampl experi",
        "Answer_gpt_summary_original":"Solution: The issue is caused by the custom scripts in the Retail Forecasting web service which only returns prediction scores for the first request and not for the other rows. The left join statement in the code ignores any rows where data$ID1 != data$ID1[1] and data$ID2 != data$ID2[1], causing the loss of everything but the first ID pair. To resolve this, the user can modify the scripts in the experiment to support processing multiple time series concurrently if they are proficient in R. Otherwise, they can try another example experiment.",
        "Answer_gpt_summary":"solut issu caus custom script retail forecast web servic return predict score request row left join statement code ignor row data data data data caus loss pair resolv user modifi script experi support process multipl time seri concurr profici try exampl experi"
    },
    {
        "Question_title":"mlflow scaling issue",
        "Question_body":"Hi all,\nwhen running below script, we notice a visible performance degradation as number of experiments increases.\nimport mlflow\n  \nfor i in range(5000):\n\n    mlflow.set_experiment('exp%d' % i)\n\n    for j in range(5):\n\n        with mlflow.start_run() as run:\n\n            mlflow.log_param('n', '%d-%d' % (i, j))\n            mlflow.log_metric('fscore', 0.9242)\n            mlflow.set_tag('some tag', '%d-%d' % (i, j))\n            with open('test.json', 'w') as f:\n                f.write('{ \"i\": %d, \"j\": %d }' % (i, j))\n            mlflow.log_artifact('test.json')\n            print(mlflow.active_run())\n\nabove script creates 5000 experiments, with 5 dummy runs in each experiment.  After about 18 hours, it has only created 2500 experiments.  At this point, it is taking about a minute to create a new experiment, and 5 seconds to create a run.\n\nwhat is the bottleneck. \nThanks in advance ..",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1549379818000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":32.0,
        "Answer_body":"Are you running this on a local machine? Note that each experiment under \"mlruns\" directory has an experiment_id as the name of the directory, and\neach run_id for that experiment gets a run_id as a directory. Furthermore, each artifact, metrics, and\u00a0params receive its own directory. So what you seeing is an explosion of file descriptors across the UNIX filesystem, which\nexplains its degradation over time.\n\n\nThis issue is being addressed by using a DB to track experiments and run_ids, mitigating (or eliminating) file descriptor degradation over volumes of experiments\nwith its respective runs at scale.\u00a0\n\n\n\n\n\n--\u00a0\n\n\nThe Best Ideas are Simple\n\nJules S. Damji\n\nApache Spark Developer & Community Advocate\n\nDatabricks, Inc.\n\nju...@databricks.com\n\n(510) 304-7686\n\ndatabricks.com\n\n\n\n\n\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/bbb34a21-66d7-474e-9c3e-20355eebec07%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/MfgmfilrJsI",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-02-05T16:14:40",
                "Answer_body":"Are you running this on a local machine? Note that each experiment under \"mlruns\" directory has an experiment_id as the name of the directory, and\neach run_id for that experiment gets a run_id as a directory. Furthermore, each artifact, metrics, and\u00a0params receive its own directory. So what you seeing is an explosion of file descriptors across the UNIX filesystem, which\nexplains its degradation over time.\n\n\nThis issue is being addressed by using a DB to track experiments and run_ids, mitigating (or eliminating) file descriptor degradation over volumes of experiments\nwith its respective runs at scale.\u00a0\n\n\n\n\n\n--\u00a0\n\n\nThe Best Ideas are Simple\n\nJules S. Damji\n\nApache Spark Developer & Community Advocate\n\nDatabricks, Inc.\n\nju...@databricks.com\n\n(510) 304-7686\n\ndatabricks.com\n\n\n\n\n\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/bbb34a21-66d7-474e-9c3e-20355eebec07%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"scale issu run script notic visibl perform degrad number experi increas import rang set experi exp rang start run run log param log metric fscore set tag tag open test json write log artifact test json print activ run script creat experi dummi run experi hour creat experi point take minut creat new experi second creat run bottleneck thank advanc",
        "Question_preprocessed_content":"scale issu run script notic visibl perform degrad number experi increas import rang rang run tag script creat experi dummi run experi hour creat experi point take minut creat new experi second creat run bottleneck thank advanc",
        "Question_gpt_summary_original":"The user is facing a performance degradation issue while running a script that creates 5000 experiments with 5 dummy runs in each experiment using mlflow. After 18 hours, only 2500 experiments have been created, and it takes about a minute to create a new experiment and 5 seconds to create a run. The user is seeking help to identify the bottleneck causing this issue.",
        "Question_gpt_summary":"user face perform degrad issu run script creat experi dummi run experi hour experi creat take minut creat new experi second creat run user seek help identifi bottleneck caus issu",
        "Answer_original_content":"run local machin note experi mlrun directori experi directori run experi get run directori furthermor artifact metric andparam receiv directori see explos file descriptor unix filesystem explain degrad time issu address track experi run id mitig elimin file descriptor degrad volum experi respect run scale best idea simpl jule damji apach spark develop commun advoc databrick databrick com databrick com receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user eebec googlegroup com option visit http group googl com optout",
        "Answer_preprocessed_content":"run local machin note experi mlrun directori directori experi get directori furthermor artifact metric andparam receiv directori see explos file descriptor unix filesystem explain degrad time issu address track experi mitig file descriptor degrad volum experi respect run scale best idea simpl jule damji apach spark develop commun advoc databrick receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit",
        "Answer_gpt_summary_original":"Solution: The issue of performance degradation while running a script that creates 5000 experiments with 5 dummy runs in each experiment using mlflow can be addressed by using a DB to track experiments and run_ids, mitigating (or eliminating) file descriptor degradation over volumes of experiments with its respective runs at scale.",
        "Answer_gpt_summary":"solut issu perform degrad run script creat experi dummi run experi address track experi run id mitig elimin file descriptor degrad volum experi respect run scale"
    },
    {
        "Question_title":"How do I use Service Principal authentication with an Azure Machine Learning Pipeline Endpoint in C#?",
        "Question_body":"<p>I'm trying to call an Azure Machine Learning Pipeline Endpoint I've set up using C# &amp; the Machine Learning REST api.<\/p>\n<p>I am certain that I have the Service Principal configured correctly, as I can successfully authenticate &amp; hit the endpoint using the <code>azureml-core<\/code> python sdk:<\/p>\n<pre><code>sp = ServicePrincipalAuthentication(\n    tenant_id=tenant_id,\n    service_principal_id=service_principal_id,\n    service_principal_password=service_principal_password)\nws =Workspace.get(\n    name=workspace_name, \n    resource_group=resource_group, \n    subscription_id=subscription_id, \n    auth=sp)\n\nendpoint = PipelineEndpoint.get(ws, name='MyEndpoint')\nendpoint.submit('Test_Experiment')\n<\/code><\/pre>\n<p>I'm using the following example in C# to attempt to run my endpoint: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-pipelines#run-a-published-pipeline-using-c\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-pipelines#run-a-published-pipeline-using-c<\/a><\/p>\n<p>I'm attempting to fill <code>auth_key<\/code> with the following code:<\/p>\n<pre><code>var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\nvar clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\nvar tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\nvar cred = new ClientSecretCredential(tenantId, clientId, clientSecret);\nvar auth_key = cred.GetToken(new Azure.Core.TokenRequestContext(new string[] {&quot;.default&quot; }));\n<\/code><\/pre>\n<p>I receive a 401 (unauthorized).<\/p>\n<p>What am I am doing wrong?<\/p>\n<ul>\n<li>UPDATE *<\/li>\n<\/ul>\n<p>I changed the 'scopes' param in the <code>TokenRequestContext<\/code> to look like:<\/p>\n<pre><code>var auth_key = cred.GetToken(new Azure.Core.TokenRequestContext(new string[] { &quot;http:\/\/DataTriggerApp\/.default&quot; }));\n<\/code><\/pre>\n<p><code>http:\/\/DataTriggerApp<\/code> is one of the <code>servicePrincipalNames<\/code> that shows up when i query my Service Principal from the azure CLI.<\/p>\n<p>Now, when I attempt to use the returned token to call the Machine Learning Pipeline Endpoint, I receive a 403 instead of a 401.  Maybe some progress?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634153827710,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1634156473112,
        "Question_score":1.0,
        "Question_view_count":752.0,
        "Answer_body":"<p>Ok, through a lot of trial-and-error I was able to come up with two ways of acquiring a token that allows me to hit my Azure Machine Learning Pipeline Endpoint through the REST api.  One uses Microsoft.Identity.Client &amp; one uses Azure.Identity.<\/p>\n<pre><code>using Microsoft.Identity.Client;\n\n...\n\npublic static async Task&lt;string&gt; GetAccessToken()\n{\n      var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\n      var clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\n      var tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\n   \n      var app = ConfidentialClientApplicationBuilder.Create(clientId)\n                                                .WithClientSecret(clientSecret)                                                \n                                                .WithAuthority(AzureCloudInstance.AzurePublic, tenantId)\n                                                .Build();\n      var result = await app.AcquireTokenForClient(new string[] { &quot;https:\/\/ml.azure.com\/.default&quot; }).ExecuteAsync();\n      return result.AccessToken;\n}\n<\/code><\/pre>\n<p>Or:<\/p>\n<pre><code>using Azure.Identity;\n...\n\npublic static async Task&lt;string&gt; GetAccessToken()\n{\n      var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\n      var clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\n      var tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\n\n      var cred = new ClientSecretCredential(tenantId, clientId, clientSecret);\n      var token =  await cred.GetTokenAsync(new Azure.Core.TokenRequestContext(new string[] { &quot;https:\/\/ml.azure.com\/.default&quot; }));\n      return token.Token;\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1634160459928,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69561386",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1634160031172,
        "Question_original_content":"us servic princip authent pipelin endpoint try pipelin endpoint set machin learn rest api certain servic princip configur correctli successfulli authent hit endpoint core python sdk serviceprincipalauthent tenant tenant servic princip servic princip servic princip password servic princip password workspac workspac resourc group resourc group subscript subscript auth endpoint pipelineendpoint myendpoint endpoint submit test experi follow exampl attempt run endpoint http doc microsoft com azur machin learn deploi pipelin run publish pipelin attempt auth kei follow code var clientid environ getenvironmentvari azur client var clientsecret environ getenvironmentvari azur client secret var tenantid environ getenvironmentvari azur tenant var cred new clientsecretcredenti tenantid clientid clientsecret var auth kei cred gettoken new azur core tokenrequestcontext new string default receiv unauthor wrong updat chang scope param tokenrequestcontext look like var auth kei cred gettoken new azur core tokenrequestcontext new string http datatriggerapp default http datatriggerapp serviceprincipalnam show queri servic princip azur cli attempt us return token machin learn pipelin endpoint receiv instead mayb progress",
        "Question_preprocessed_content":"us servic princip authent pipelin endpoint try pipelin endpoint set machin learn rest api certain servic princip configur correctli successfulli authent hit endpoint python sdk follow exampl attempt run endpoint attempt follow code receiv wrong updat chang scope param look like show queri servic princip azur cli attempt us return token machin learn pipelin endpoint receiv instead mayb progress",
        "Question_gpt_summary_original":"The user is facing challenges in using Service Principal authentication with an Azure Machine Learning Pipeline Endpoint in C#. They have configured the Service Principal correctly and can successfully authenticate and hit the endpoint using the azureml-core python sdk. However, when attempting to run the endpoint in C#, they receive a 401 (unauthorized) error. They have tried changing the 'scopes' param in the TokenRequestContext to include one of the servicePrincipalNames that shows up when they query their Service Principal from the azure CLI, but now receive a 403 error instead of a 401.",
        "Question_gpt_summary":"user face challeng servic princip authent pipelin endpoint configur servic princip correctli successfulli authent hit endpoint core python sdk attempt run endpoint receiv unauthor error tri chang scope param tokenrequestcontext includ serviceprincipalnam show queri servic princip azur cli receiv error instead",
        "Answer_original_content":"lot trial error abl come wai acquir token allow hit pipelin endpoint rest api us microsoft ident client us azur ident microsoft ident client public static async task getaccesstoken var clientid environ getenvironmentvari azur client var clientsecret environ getenvironmentvari azur client secret var tenantid environ getenvironmentvari azur tenant var app confidentialclientapplicationbuild creat clientid withclientsecret clientsecret withauthor azurecloudinst azurepubl tenantid build var result await app acquiretokenforcli new string http azur com default executeasync return result accesstoken azur ident public static async task getaccesstoken var clientid environ getenvironmentvari azur client var clientsecret environ getenvironmentvari azur client secret var tenantid environ getenvironmentvari azur tenant var cred new clientsecretcredenti tenantid clientid clientsecret var token await cred gettokenasync new azur core tokenrequestcontext new string http azur com default return token token",
        "Answer_preprocessed_content":"lot abl come wai acquir token allow hit pipelin endpoint rest api us us",
        "Answer_gpt_summary_original":"The discussion provides two possible solutions to the challenge of using Service Principal authentication with an Azure Machine Learning Pipeline Endpoint in C#. The first solution uses Microsoft.Identity.Client and the second solution uses Azure.Identity. Both solutions involve acquiring a token that allows the user to hit the Azure Machine Learning Pipeline Endpoint through the REST API.",
        "Answer_gpt_summary":"discuss provid possibl solut challeng servic princip authent pipelin endpoint solut us microsoft ident client second solut us azur ident solut involv acquir token allow user hit pipelin endpoint rest api"
    },
    {
        "Question_title":"how to train and deploy YOLOv5 on aws sagemaker",
        "Question_body":"<p>I want to train YOLOv5 on aws sagemaker also deploy the model on sagemaker itself,need to know about entrypoint python script as well. how can I build a pipeline for this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1632686467053,
        "Question_favorite_count":null,
        "Question_last_edit_time":1632754422356,
        "Question_score":0.0,
        "Question_view_count":2510.0,
        "Answer_body":"<p>This official AWS <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/speed-up-yolov4-inference-to-twice-as-fast-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">blog post<\/a> has information on how to deploy YOLOv4. I wonder if you can use it as a guide and change the model to v5.<\/p>\n<p>If not, there is a 3rd party implementation of YOLOv5 <a href=\"https:\/\/github.com\/HKT-SSA\/yolov5-on-sagemaker\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69338516",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1632853607047,
        "Question_original_content":"train deploi yolov want train yolov deploi model need know entrypoint python script build pipelin",
        "Question_preprocessed_content":"train deploi yolov want train yolov deploi model need know entrypoint python script build pipelin",
        "Question_gpt_summary_original":"The user is facing challenges in training and deploying YOLOv5 on AWS Sagemaker. They are specifically seeking information on the entrypoint python script and how to build a pipeline for this process.",
        "Question_gpt_summary":"user face challeng train deploi yolov specif seek inform entrypoint python script build pipelin process",
        "Answer_original_content":"offici aw blog post inform deploi yolov wonder us guid chang model parti implement yolov",
        "Answer_preprocessed_content":"offici aw blog post inform deploi yolov wonder us guid chang model parti implement yolov",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Using the official AWS blog post on deploying YOLOv4 as a guide and modifying it for YOLOv5.\n2. Utilizing a third-party implementation of YOLOv5 available on GitHub.",
        "Answer_gpt_summary":"possibl solut mention discuss offici aw blog post deploi yolov guid modifi yolov util parti implement yolov avail github"
    },
    {
        "Question_title":"How to export tresained models to ECR as container image",
        "Question_body":"I want to train and build the model in Sagemaker studio and then be able to export the model as a container image to ECR, so I can use the model in external platform by sharing the ECR image to another account where I Can create container with the image from ECR",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663258467464,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":29.0,
        "Answer_body":"The models you train in SageMaker are stored in S3 as .tar.gz files that you can use to deploy to an endpoint, or even test locally (extracting the model file from the tar file). If you are using a built-in algorithm, you can share the .tar.gz file to the second account and deploy the model in the second account, since built-in algorithm containers can be accessed from any AWS account.\n\nIf you are using a custom training image (docs here), you can push this image to ECR and allow a second account to pull the image and then use the image with the model that you have trained. However, note that Studio at this time does not support building Docker images out of the box. You can use SageMaker Notebook Instances instead.\n\nI would recommend keeping the model (.tar.gz) and the image (Docker) separate, since you can easily retrain and deploy the newer versions of models without updating the image every single time.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZHWz5-hpSc-80dEIkuxwQw\/how-to-export-tresained-models-to-ecr-as-container-image",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-16T23:05:33.114Z",
                "Answer_score":0,
                "Answer_body":"The models you train in SageMaker are stored in S3 as .tar.gz files that you can use to deploy to an endpoint, or even test locally (extracting the model file from the tar file). If you are using a built-in algorithm, you can share the .tar.gz file to the second account and deploy the model in the second account, since built-in algorithm containers can be accessed from any AWS account.\n\nIf you are using a custom training image (docs here), you can push this image to ECR and allow a second account to pull the image and then use the image with the model that you have trained. However, note that Studio at this time does not support building Docker images out of the box. You can use SageMaker Notebook Instances instead.\n\nI would recommend keeping the model (.tar.gz) and the image (Docker) separate, since you can easily retrain and deploy the newer versions of models without updating the image every single time.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1663369533112,
        "Question_original_content":"export tresain model ecr contain imag want train build model studio abl export model contain imag ecr us model extern platform share ecr imag account creat contain imag ecr",
        "Question_preprocessed_content":"export tresain model ecr contain imag want train build model studio abl export model contain imag ecr us model extern platform share ecr imag account creat contain imag ecr",
        "Question_gpt_summary_original":"The user wants to export a trained model from Sagemaker studio as a container image to ECR, so that it can be used in an external platform by sharing the ECR image to another account where a container can be created with the image from ECR.",
        "Question_gpt_summary":"user want export train model studio contain imag ecr extern platform share ecr imag account contain creat imag ecr",
        "Answer_original_content":"model train store tar file us deploi endpoint test local extract model file tar file built algorithm share tar file second account deploi model second account built algorithm contain access aw account custom train imag doc push imag ecr allow second account pull imag us imag model train note studio time support build docker imag box us notebook instanc instead recommend keep model tar imag docker separ easili retrain deploi newer version model updat imag singl time",
        "Answer_preprocessed_content":"model train store file us deploi endpoint test local algorithm share file second account deploi model second account algorithm contain access aw account custom train imag push imag ecr allow second account pull imag us imag model train note studio time support build docker imag box us notebook instanc instead recommend keep model imag separ easili retrain deploi newer version model updat imag singl time",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Share the .tar.gz file of the trained model to the second account and deploy the model in the second account if using a built-in algorithm.\n2. Push the custom training image to ECR and allow a second account to pull the image and then use the image with the trained model if using a custom training image.\n3. Use SageMaker Notebook Instances instead of Studio to build Docker images.\n4. Keep the model (.tar.gz) and the image (Docker) separate to easily retrain and deploy newer versions of models without updating the image every time.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss share tar file train model second account deploi model second account built algorithm push custom train imag ecr allow second account pull imag us imag train model custom train imag us notebook instanc instead studio build docker imag model tar imag docker separ easili retrain deploi newer version model updat imag time person opinion bias includ summari"
    },
    {
        "Question_title":"How to open a model tarfile stored in S3 bucket in sagemaker notebook?",
        "Question_body":"<p>I know that loading a .csv file into sagemaker notebook from S3 bucket is pretty straightforward but I want to load a model.tar.gz file stored in S3 bucket. I tried to do the following<\/p>\n\n<pre><code>import botocore \nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.predictor import csv_serializer\nimport boto3\n\nsm_client = boto3.client(service_name='sagemaker')\nruntime_sm_client = boto3.client(service_name='sagemaker-runtime')\n\ns3 = boto3.resource('s3')\ns3_client = boto3.client('s3')\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\nACCOUNT_ID  = boto3.client('sts').get_caller_identity()['Account']\nREGION      = boto3.Session().region_name\nBUCKET      = 'sagemaker.prismade.net'\ndata_key    = 'DEMO_MME_ANN\/multi_model_artifacts\/axel.tar.gz'\nloc = 's3:\/\/{}\/{}'.format(BUCKET, data_key)\nprint(loc)\nwith tarfile.open(loc) as tar:\n    tar.extractall(path='.')\n<\/code><\/pre>\n\n<p>I get the following error:<\/p>\n\n<pre><code>--------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-215-bfdddac71b95&gt; in &lt;module&gt;()\n     20 loc = 's3:\/\/{}\/{}'.format(BUCKET, data_key)\n     21 print(loc)\n---&gt; 22 with tarfile.open(loc) as tar:\n     23     tar.extractall(path='.')\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/tarfile.py in open(cls, name, mode, fileobj, bufsize, **kwargs)\n   1567                     saved_pos = fileobj.tell()\n   1568                 try:\n-&gt; 1569                     return func(name, \"r\", fileobj, **kwargs)\n   1570                 except (ReadError, CompressionError):\n   1571                     if fileobj is not None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/tarfile.py in gzopen(cls, name, mode, fileobj, compresslevel, **kwargs)\n   1632 \n   1633         try:\n-&gt; 1634             fileobj = gzip.GzipFile(name, mode + \"b\", compresslevel, fileobj)\n   1635         except OSError:\n   1636             if fileobj is not None and mode == 'r':\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/gzip.py in __init__(self, filename, mode, compresslevel, fileobj, mtime)\n    161             mode += 'b'\n    162         if fileobj is None:\n--&gt; 163             fileobj = self.myfileobj = builtins.open(filename, mode or 'rb')\n    164         if filename is None:\n    165             filename = getattr(fileobj, 'name', '')\n\nFileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/sagemaker.prismade.net\/DEMO_MME_ANN\/multi_model_artifacts\/axel.tar.gz'\n<\/code><\/pre>\n\n<p>What is the mistake here and how can I accomplish this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1580895955387,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":2984.0,
        "Answer_body":"<p>Not every python library that is designed to work with a file system (tarfile.open, in this example) knows how to read an object from S3 as a file. <\/p>\n\n<p>The simple way to solve it is to first copy the object into the local file system as a file.<\/p>\n\n<pre><code>import boto3\n\ns3 = boto3.client('s3')\ns3.download_file('BUCKET_NAME', 'OBJECT_NAME', 'FILE_NAME')\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60072981",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1581004771980,
        "Question_original_content":"open model tarfil store bucket notebook know load csv file notebook bucket pretti straightforward want load model tar file store bucket tri follow import botocor import import execut role predictor import csv serial import boto client boto client servic runtim client boto client servic runtim boto resourc client boto client session session role execut role account boto client st caller ident account region boto session region bucket prismad net data kei demo mme ann multi model artifact axel tar loc format bucket data kei print loc tarfil open loc tar tar extractal path follow error filenotfounderror traceback recent loc format bucket data kei print loc tarfil open loc tar tar extractal path anaconda env python lib python tarfil open cl mode fileobj bufsiz kwarg save po fileobj tell try return func fileobj kwarg readerror compressionerror fileobj anaconda env python lib python tarfil gzopen cl mode fileobj compresslevel kwarg try fileobj gzip gzipfil mode compresslevel fileobj oserror fileobj mode anaconda env python lib python gzip init self filenam mode compresslevel fileobj mtime mode fileobj fileobj self myfileobj builtin open filenam mode filenam filenam getattr fileobj filenotfounderror errno file directori prismad net demo mme ann multi model artifact axel tar mistak accomplish",
        "Question_preprocessed_content":"open model tarfil store bucket notebook know load csv file notebook bucket pretti straightforward want load file store bucket tri follow follow error mistak accomplish",
        "Question_gpt_summary_original":"The user is trying to load a model.tar.gz file stored in an S3 bucket into a SageMaker notebook. They have attempted to use the tarfile module to extract the file but encountered a FileNotFoundError. The user is seeking assistance in identifying the mistake and finding a solution to load the model file.",
        "Question_gpt_summary":"user try load model tar file store bucket notebook attempt us tarfil modul extract file encount filenotfounderror user seek assist identifi mistak find solut load model file",
        "Answer_original_content":"python librari design work file tarfil open exampl know read object file simpl wai solv copi object local file file import boto boto client download file bucket object file",
        "Answer_preprocessed_content":"python librari design work file know read object file simpl wai solv copi object local file file",
        "Answer_gpt_summary_original":"Solution: The user can solve the issue by copying the object from the S3 bucket into the local file system as a file using the boto3 library. The following code can be used to download the file:\n\n```\nimport boto3\n\ns3 = boto3.client('s3')\ns3.download_file('BUCKET_NAME', 'OBJECT_NAME', 'FILE_NAME')\n```",
        "Answer_gpt_summary":"solut user solv issu copi object bucket local file file boto librari follow code download file import boto boto client download file bucket object file"
    },
    {
        "Question_title":"Getting Out of Memory error when using Image Classification in Sage Maker",
        "Question_body":"<p>When using a p2.xlarge or p3.2xlarge with up to 1TB of memory trying to use the predefined SageMaker Image Classification algorithm in a training job I\u2019m getting the following error:<\/p>\n\n<p><code>ClientError: Out of Memory. Please use a larger instance and\/or reduce the values of other parameters (e.g. batch size, number of layers etc.) if applicable<\/code><\/p>\n\n<p>I\u2019m using 450+ images, I\u2019ve tried resizing them from their original 2000x3000px size to a 244x244px size down to a 24x24px size and keep getting the same error.<\/p>\n\n<p>I\u2019ve tried adjusting my hyper parameters: num_classes, num_layers, num_training_samples, optimizer, image_shape, checkpoint frequency, batch_size and epochs. Also tried using pretrained model. But the same error keeps occurring.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548952048437,
        "Question_favorite_count":null,
        "Question_last_edit_time":1549513217232,
        "Question_score":1.0,
        "Question_view_count":2360.0,
        "Answer_body":"<p>Would've added this as a comment but I don't have enough rep yet.<\/p>\n\n<p>A few clarifying questions so that I can have some more context:<\/p>\n\n<p><em>How exactly are you achieving 1TB of RAM?<\/em><\/p>\n\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p2\/\" rel=\"nofollow noreferrer\"><code>p2.xlarge<\/code><\/a> servers have 61GB of RAM, and <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p3\/\" rel=\"nofollow noreferrer\"><code>p3.2xlarge<\/code><\/a> servers have 61GB memory + 16GB onboard the Tesla V100 GPU. <\/li>\n<\/ol>\n\n<p><em>How are you storing, resizing, and ingesting the images into the SageMaker algorithm?<\/em><\/p>\n\n<ol start=\"2\">\n<li>The memory error seems suspect considering it still occurs when downsizing images to 24x24. If you are resizing your original images (450 images at 2000x3000 resolution) as in-memory objects and aren't performing the transformations in-place (ie: not creating new images), you may have a substantial bit of memory pre-allocated, causing the SageMaker training algorithm to throw an OOM error.<\/li>\n<\/ol>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":1548962267732,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54465049",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1548960285576,
        "Question_original_content":"get memori error imag classif sage maker xlarg xlarg memori try us predefin imag classif algorithm train job get follow error clienterror memori us larger instanc reduc valu paramet batch size number layer applic imag iv tri resiz origin xpx size xpx size xpx size get error iv tri adjust hyper paramet num class num layer num train sampl optim imag shape checkpoint frequenc batch size epoch tri pretrain model error keep occur",
        "Question_preprocessed_content":"get memori error imag classif sage maker memori try us predefin imag classif algorithm train job get follow error imag iv tri resiz origin size size size get error iv tri adjust hyper paramet optim checkpoint frequenc epoch tri pretrain model error keep occur",
        "Question_gpt_summary_original":"The user is encountering an \"Out of Memory\" error when using the predefined SageMaker Image Classification algorithm in a training job, even when using a p2.xlarge or p3.2xlarge instance with up to 1TB of memory. The user has tried resizing images and adjusting hyperparameters, but the error persists.",
        "Question_gpt_summary":"user encount memori error predefin imag classif algorithm train job xlarg xlarg instanc memori user tri resiz imag adjust hyperparamet error persist",
        "Answer_original_content":"ad comment rep clarifi question context exactli achiev ram xlarg server ram xlarg server memori onboard tesla gpu store resiz ingest imag algorithm memori error suspect consid occur downsiz imag resiz origin imag imag resolut memori object aren perform transform place creat new imag substanti bit memori pre alloc caus train algorithm throw oom error",
        "Answer_preprocessed_content":"ad comment rep clarifi question context exactli achiev ram server ram server memori onboard tesla gpu store resiz ingest imag algorithm memori error suspect consid occur downsiz imag resiz origin imag object aren perform transform substanti bit memori caus train algorithm throw oom error",
        "Answer_gpt_summary_original":"Solutions provided: \n- No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid solut provid discuss"
    },
    {
        "Question_title":"Using DVC outside git",
        "Question_body":"<p>Hello here,<br>\nI am grateful on what dvc offers so far in data management. I am working on a labeling tool for image segmentation with the image labels stored in the cloud storage after generation, my question is, is it possible to manage this data set with dvc considering  the data folder is not a git\/dvc working directory?,  such that you can <code>dvc pull<\/code> from any project I wish to work on. The segmentation is added with every new labeling.<\/p>",
        "Question_answer_count":10,
        "Question_comment_count":0,
        "Question_creation_time":1640005495644,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":417.0,
        "Answer_body":"<aside class=\"quote no-group\" data-username=\"kongkip\" data-post=\"1\" data-topic=\"1014\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/k\/b5e925\/40.png\" class=\"avatar\"> kongkip:<\/div>\n<blockquote>\n<p>I am grateful on what dvc offers so far in data management. I am working on a labeling tool for image segmentation with the image labels stored in the cloud storage after generation, my question is, is it possible to manage this data set with dvc considering the data folder is not a git\/dvc working directory?, such that you can <code>dvc pull<\/code> from any project I wish to work on. The segmentation is added with every new labeling.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Hi, are you looking for a data registry in which you can <code>dvc import<\/code> the data to any projects you want?<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries#data-registries\">\n  <header class=\"source\">\n      <img src=\"https:\/\/dvc.org\/favicon-32x32.png?v=dfbc4a93a926127fc4495e9d640409f8\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries#data-registries\" target=\"_blank\" rel=\"noopener nofollow ugc\">Data Version Control \u00b7 DVC<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/dvc.org\/social-share.png\" class=\"thumbnail\" width=\"690\" height=\"362\"><\/div>\n\n<h3><a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries#data-registries\" target=\"_blank\" rel=\"noopener nofollow ugc\">Data Registries<\/a><\/h3>\n\n  <p>Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n. <p>Checked this out but I don\u2019t think it will solve my issue.  To provide clarity the raw images are originally stored in AWS S3, then the labeling tool accesses the  image URLs  which are then embedded as images on the UI where one labels and generates the segmentations. For this case this is not a DVC repo yet, so how can the data  be managed with DVC since there are no commands run  locally such as <code>dvc add<\/code> or <code>dvc push<\/code>.<\/p>. <p>To understand, data registry will work only if I start locally  creating a git repo on the data and pushing  it to the cloud with <code>dvc push<\/code>.  The way dvc also stores the data on the cloud storage  will make it hard for the labeling tool which is a JS based app.<\/p>. <aside class=\"quote no-group\" data-username=\"kongkip\" data-post=\"3\" data-topic=\"1014\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/k\/b5e925\/40.png\" class=\"avatar\"> kongkip:<\/div>\n<blockquote>\n<p>Checked this out but I don\u2019t think it will solve my issue. To provide clarity the raw images are originally stored in AWS S3, then the labeling tool accesses the image URLs which are then embedded as images on the UI where one labels and generates the segmentations. For this case this is not a DVC repo yet, so how can the data be managed with DVC since there are no commands run locally such as <code>dvc add<\/code> or <code>dvc push<\/code> .<\/p>\n<\/blockquote>\n<\/aside>\n<p>OK, thanks for your clarification. In my opinion, if you want to use DVC to track the data version. You might still need a dvc+git repo to store your DVC version files. Even if you are using some advanced features like <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data#setting-up-an-external-cache\" rel=\"noopener nofollow ugc\">eternal data<\/a> or some hacking method like using DVC\u2019s Repo API (<code>Repo.add<\/code>, <code>Repo.push<\/code>, <code>Repo.pull<\/code>). They all need a local repo to operate.<\/p>. <p>We are working on a new tool in DVC ecosystem that sounds like a better solution.<\/p>\n<p>Do you mind describing your workflow in a greater detail?<\/p>. <p>ok sure, let me drop a diagram here<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/b56a697206e5c68a87e3ca08bec449bf44085260.png\" data-download-href=\"\/uploads\/short-url\/pSSpkBNUetLrn5DnmyIJm641KWA.png?dl=1\" title=\"Data Pipeline Flow Chart.drawio (1)\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/b56a697206e5c68a87e3ca08bec449bf44085260.png\" alt=\"Data Pipeline Flow Chart.drawio (1)\" data-base62-sha1=\"pSSpkBNUetLrn5DnmyIJm641KWA\" width=\"597\" height=\"500\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/b56a697206e5c68a87e3ca08bec449bf44085260_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Data Pipeline Flow Chart.drawio (1)<\/span><span class=\"informations\">608\u00d7509 17.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>The final storage is the  dvc remote storage which can be exported to any project<\/p>. <p>what do you use csv for?<\/p>. <p>it contains points for the labels e.g, ellipse, circles and polygons which are used to generate image segmenations.<\/p>. <p>Just to clarify - your images and labels live in s3 and you consider this location to be immutable (in a sense that you can guarantee they will not be accidentally deleted or moved as you keep adding new labels).<\/p>\n<p>If this is the case, we can provide tracking of labels versions and datasets (defined as collections of pointers) with our new tool.<\/p>. <p>This sounds nice, update me once you do have it ready or in beta.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/using-dvc-outside-git\/1014",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-20T14:35:22.915Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"kongkip\" data-post=\"1\" data-topic=\"1014\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/k\/b5e925\/40.png\" class=\"avatar\"> kongkip:<\/div>\n<blockquote>\n<p>I am grateful on what dvc offers so far in data management. I am working on a labeling tool for image segmentation with the image labels stored in the cloud storage after generation, my question is, is it possible to manage this data set with dvc considering the data folder is not a git\/dvc working directory?, such that you can <code>dvc pull<\/code> from any project I wish to work on. The segmentation is added with every new labeling.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Hi, are you looking for a data registry in which you can <code>dvc import<\/code> the data to any projects you want?<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries#data-registries\">\n  <header class=\"source\">\n      <img src=\"https:\/\/dvc.org\/favicon-32x32.png?v=dfbc4a93a926127fc4495e9d640409f8\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries#data-registries\" target=\"_blank\" rel=\"noopener nofollow ugc\">Data Version Control \u00b7 DVC<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/dvc.org\/social-share.png\" class=\"thumbnail\" width=\"690\" height=\"362\"><\/div>\n\n<h3><a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries#data-registries\" target=\"_blank\" rel=\"noopener nofollow ugc\">Data Registries<\/a><\/h3>\n\n  <p>Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-20T19:42:15.684Z",
                "Answer_body":"<p>Checked this out but I don\u2019t think it will solve my issue.  To provide clarity the raw images are originally stored in AWS S3, then the labeling tool accesses the  image URLs  which are then embedded as images on the UI where one labels and generates the segmentations. For this case this is not a DVC repo yet, so how can the data  be managed with DVC since there are no commands run  locally such as <code>dvc add<\/code> or <code>dvc push<\/code>.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-20T19:47:21.592Z",
                "Answer_body":"<p>To understand, data registry will work only if I start locally  creating a git repo on the data and pushing  it to the cloud with <code>dvc push<\/code>.  The way dvc also stores the data on the cloud storage  will make it hard for the labeling tool which is a JS based app.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-21T02:31:38.393Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"kongkip\" data-post=\"3\" data-topic=\"1014\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/k\/b5e925\/40.png\" class=\"avatar\"> kongkip:<\/div>\n<blockquote>\n<p>Checked this out but I don\u2019t think it will solve my issue. To provide clarity the raw images are originally stored in AWS S3, then the labeling tool accesses the image URLs which are then embedded as images on the UI where one labels and generates the segmentations. For this case this is not a DVC repo yet, so how can the data be managed with DVC since there are no commands run locally such as <code>dvc add<\/code> or <code>dvc push<\/code> .<\/p>\n<\/blockquote>\n<\/aside>\n<p>OK, thanks for your clarification. In my opinion, if you want to use DVC to track the data version. You might still need a dvc+git repo to store your DVC version files. Even if you are using some advanced features like <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data#setting-up-an-external-cache\" rel=\"noopener nofollow ugc\">eternal data<\/a> or some hacking method like using DVC\u2019s Repo API (<code>Repo.add<\/code>, <code>Repo.push<\/code>, <code>Repo.pull<\/code>). They all need a local repo to operate.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-21T03:45:33.230Z",
                "Answer_body":"<p>We are working on a new tool in DVC ecosystem that sounds like a better solution.<\/p>\n<p>Do you mind describing your workflow in a greater detail?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-21T05:59:37.410Z",
                "Answer_body":"<p>ok sure, let me drop a diagram here<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/b56a697206e5c68a87e3ca08bec449bf44085260.png\" data-download-href=\"\/uploads\/short-url\/pSSpkBNUetLrn5DnmyIJm641KWA.png?dl=1\" title=\"Data Pipeline Flow Chart.drawio (1)\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/b56a697206e5c68a87e3ca08bec449bf44085260.png\" alt=\"Data Pipeline Flow Chart.drawio (1)\" data-base62-sha1=\"pSSpkBNUetLrn5DnmyIJm641KWA\" width=\"597\" height=\"500\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/b56a697206e5c68a87e3ca08bec449bf44085260_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Data Pipeline Flow Chart.drawio (1)<\/span><span class=\"informations\">608\u00d7509 17.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>The final storage is the  dvc remote storage which can be exported to any project<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-21T07:30:25.073Z",
                "Answer_body":"<p>what do you use csv for?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-21T08:59:08.134Z",
                "Answer_body":"<p>it contains points for the labels e.g, ellipse, circles and polygons which are used to generate image segmenations.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-21T16:14:37.498Z",
                "Answer_body":"<p>Just to clarify - your images and labels live in s3 and you consider this location to be immutable (in a sense that you can guarantee they will not be accidentally deleted or moved as you keep adding new labels).<\/p>\n<p>If this is the case, we can provide tracking of labels versions and datasets (defined as collections of pointers) with our new tool.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-11T10:09:32.930Z",
                "Answer_body":"<p>This sounds nice, update me once you do have it ready or in beta.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"outsid git hello grate offer far data manag work label tool imag segment imag label store cloud storag gener question possibl manag data set consid data folder git work directori pull project wish work segment ad new label",
        "Question_preprocessed_content":"outsid git hello grate offer far data manag work label tool imag segment imag label store cloud storag gener question possibl manag data set consid data folder git work directori project wish work segment ad new label",
        "Question_gpt_summary_original":"The user is facing a challenge of managing a data set for a labeling tool for image segmentation with the image labels stored in cloud storage. They are unsure if it is possible to manage this data set with DVC as the data folder is not a git\/dvc working directory, and they want to be able to use \"dvc pull\" from any project they wish to work on.",
        "Question_gpt_summary":"user face challeng manag data set label tool imag segment imag label store cloud storag unsur possibl manag data set data folder git work directori want abl us pull project wish work",
        "Answer_original_content":"kongkip grate offer far data manag work label tool imag segment imag label store cloud storag gener question possibl manag data set consid data folder git work directori pull project wish work segment ad new label look data registri import data project want data version control data registri open sourc version control data scienc machin learn project git like experi organ data model experi check dont think solv issu provid clariti raw imag origin store aw label tool access imag url embed imag label gener segment case repo data manag command run local add push understand data registri work start local creat git repo data push cloud push wai store data cloud storag hard label tool base app kongkip check dont think solv issu provid clariti raw imag origin store aw label tool access imag url embed imag label gener segment case repo data manag command run local add push thank clarif opinion want us track data version need git repo store version file advanc featur like etern data hack method like repo api repo add repo push repo pull need local repo oper work new tool ecosystem sound like better solut mind describ workflow greater sure let drop diagram data pipelin flow chart drawio final storag remot storag export project us csv contain point label ellips circl polygon gener imag segmen clarifi imag label live consid locat immut sens guarante accident delet move ad new label case provid track label version dataset defin collect pointer new tool sound nice updat readi beta",
        "Answer_preprocessed_content":"kongkip grate offer far data manag work label tool imag segment imag label store cloud storag gener question possibl manag data set consid data folder git work directori project wish work segment ad new label look data registri data project want data version control data registri version control data scienc machin learn project experi organ data model experi check dont think solv issu provid clariti raw imag origin store aw label tool access imag url embed imag label gener segment case repo data manag command run local understand data registri work start local creat git repo data push cloud wai store data cloud storag hard label tool base app kongkip check dont think solv issu provid clariti raw imag origin store aw label tool access imag url embed imag label gener segment case repo data manag command run local thank clarif opinion want us track data version need git repo store version file advanc featur like etern data hack method like repo api need local repo oper work new tool ecosystem sound like better solut mind describ workflow greater sure let drop diagram data pipelin flow final storag remot storag export project us csv contain point label ellips circl polygon gener imag segmen clarifi imag label live consid locat immut case provid track label version dataset new tool sound nice updat readi beta",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"How to refactor a directory of dvc managed files?",
        "Question_body":"<p>I have a case where I created a directory of files that are individually managed by DVC (so there are a number of files, each with a corresponding .dvc file).  I\u2019d like to refactor this so that I just tell DVC to managed the entire directory and all of its contents to get rid of all those .dvc files cluttering things up.<\/p>\n<p>I also have this situation in the recursive case (I\u2019d like to tell DVC to just manage a directory parent and all ancestor contents).<\/p>\n<p>What is the correct way to do this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1630713202727,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":187.0,
        "Answer_body":"<p><a class=\"mention\" href=\"\/u\/dvc.grasshopper\">@dvc.grasshopper<\/a> I think you can get rid of all the <code>.dvc<\/code> files and do <code>dvc add directory<\/code> instead. That should be it.<\/p>. <p>Ok, thanks.  I will try that!<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-refactor-a-directory-of-dvc-managed-files\/875",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-04T00:02:31.615Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/dvc.grasshopper\">@dvc.grasshopper<\/a> I think you can get rid of all the <code>.dvc<\/code> files and do <code>dvc add directory<\/code> instead. That should be it.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-04T00:06:53.158Z",
                "Answer_body":"<p>Ok, thanks.  I will try that!<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"refactor directori manag file case creat directori file individu manag number file correspond file like refactor tell manag entir directori content rid file clutter thing situat recurs case like tell manag directori parent ancestor content correct wai",
        "Question_preprocessed_content":"refactor directori manag file case creat directori file individu manag like refactor tell manag entir directori content rid file clutter thing situat recurs case correct wai",
        "Question_gpt_summary_original":"The user is facing challenges in refactoring a directory of files that are individually managed by DVC, and wants to manage the entire directory and its contents without cluttering it up with .dvc files. The user also wants to manage a directory parent and all ancestor contents in the recursive case. The user is seeking guidance on the correct way to do this.",
        "Question_gpt_summary":"user face challeng refactor directori file individu manag want manag entir directori content clutter file user want manag directori parent ancestor content recurs case user seek guidanc correct wai",
        "Answer_original_content":"grasshopp think rid file add directori instead thank try",
        "Answer_preprocessed_content":"think rid file instead thank try",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to use the command \"dvc add directory\" to add the entire directory and its contents without cluttering it up with .dvc files.",
        "Answer_gpt_summary":"solut solut suggest discuss us command add directori add entir directori content clutter file"
    },
    {
        "Question_title":"Large dataset, dvc pull\/add\/push jobs options",
        "Question_body":"<p>Hello , DVC users<\/p>\n<p>I am a newbie who wants to apply dvc to large dataset management.<br>\nTo understand dvc, I did the following experiment.<\/p>\n<pre><code class=\"lang-auto\">## normal transfer case \n\n# In DVC_Main directory \ngit init\ndvc init\ndvc remote add -d &lt;storage_name&gt;  &lt;local_data_storage_url&gt;\n\ndvc add mesh_dataset   # mesh_dataset (40GB)\ndvc push \n\n# In DVC_Sub directory \ngit init\ndvc init\n\n#copy &amp; pasted files from ( dvc_main directory)  -&gt; .dvc\/config , .mesh_dataset.dvc files \ndvc pull \n\n<\/code><\/pre>\n<p>The above process is a code that tests whether the data worked in the main directory can be loaded from the sub directory.<br>\nBut it took too long (about 80 minutes)<\/p>\n<p>So I looked for options and there was a -j option for dvc add\/pull\/push.<\/p>\n<pre><code class=\"lang-auto\">## parallel transfer case \n\n# In DVC_Main directory \ngit init\ndvc init\ndvc remote add -d &lt;storage_name&gt;  &lt;local_data_storage_url&gt;\ndvc remote modify jobs 64\n\ndvc add -j 64 --to-remote mesh_dataset   # mesh_dataset (40GB)\n\n# In DVC_Sub directory \ngit init\ndvc init\n\n#copy &amp; pasted files from ( dvc_main directory)  -&gt; .dvc\/config , .mesh_dataset.dvc files \ndvc pull -j 64\n\n<\/code><\/pre>\n<p>The parallel transmission method should be faster than the normal transmission method, but I don\u2019t understand why there is no speed difference.<\/p>\n<p>In fact, the parallel transmission method is slightly faster, but it was a difference that appeared because the \u2018dvc push\u2019 process was omitted in the dvc_main directory.<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/29914ea01d5fc60bbbfb026c5970afc795dc9522.png\" alt=\"image\" data-base62-sha1=\"5VITvQ8yavHQ6oc7bTYs1pe4u3g\" width=\"662\" height=\"114\"><\/p>\n<p>According to the explanation above, I have 1 cpu (8 cores, 14 logical cores), so the default value = 34.<\/p>\n<p>Can you explain why there is no difference in transfer speed between jobs option 32 and 64?<\/p>\n<p>Or does dvc not support parallel transmission of large data?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1675391803569,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":28.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dsa934\">@dsa934<\/a> !<br>\n<code>dvc<\/code> does push and pull data in parallel, whether you use the <code>-j<\/code> option or not. However, parallelisation has diminishing returns due to resource constraints (network bandwidth, unparallelisable per-job overhead, \u2026) From your testing, I guess that 32 is already past the threshold where adding more jobs doesn\u2019t bring any speed-up.<\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/ronan\">@ronan<\/a>  !<\/p>\n<p>According to you basically 32 means threshold (because 1 cpu has 8 cores, formula : 4 * cpu_count() ), does that mean we need to increase the number of actual cpus to improve speed?<\/p>\n<p>However, when I experimented with smaller numbers, such as 12 or 16 instead of 64, there was no change in speed.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/large-dataset-dvc-pull-add-push-jobs-options\/1496",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-07T12:50:47.590Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dsa934\">@dsa934<\/a> !<br>\n<code>dvc<\/code> does push and pull data in parallel, whether you use the <code>-j<\/code> option or not. However, parallelisation has diminishing returns due to resource constraints (network bandwidth, unparallelisable per-job overhead, \u2026) From your testing, I guess that 32 is already past the threshold where adding more jobs doesn\u2019t bring any speed-up.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-07T14:57:47.642Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ronan\">@ronan<\/a>  !<\/p>\n<p>According to you basically 32 means threshold (because 1 cpu has 8 cores, formula : 4 * cpu_count() ), does that mean we need to increase the number of actual cpus to improve speed?<\/p>\n<p>However, when I experimented with smaller numbers, such as 12 or 16 instead of 64, there was no change in speed.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"larg dataset pull add push job option hello user newbi want appli larg dataset manag understand follow experi normal transfer case main directori git init init remot add add mesh dataset mesh dataset push sub directori git init init copi past file main directori config mesh dataset file pull process code test data work main directori load sub directori took long minut look option option add pull push parallel transfer case main directori git init init remot add remot modifi job add remot mesh dataset mesh dataset sub directori git init init copi past file main directori config mesh dataset file pull parallel transmiss method faster normal transmiss method dont understand speed differ fact parallel transmiss method slightli faster differ appear push process omit main directori accord explan cpu core logic core default valu explain differ transfer speed job option support parallel transmiss larg data",
        "Question_preprocessed_content":"larg dataset job option hello user newbi want appli larg dataset manag understand follow experi process code test data work main directori load sub directori took long look option option parallel transmiss method faster normal transmiss method dont understand speed differ fact parallel transmiss method slightli faster differ appear push process omit directori accord explan cpu default valu explain differ transfer speed job option support parallel transmiss larg data",
        "Question_gpt_summary_original":"The user encountered challenges in managing a large dataset using DVC. They experimented with normal and parallel transfer cases using dvc add\/pull\/push jobs options but found that the parallel transmission method did not result in a significant speed difference. The user is seeking an explanation for why there is no difference in transfer speed between jobs option 32 and 64 and whether DVC supports parallel transmission of large data.",
        "Question_gpt_summary":"user encount challeng manag larg dataset experi normal parallel transfer case add pull push job option parallel transmiss method result signific speed differ user seek explan differ transfer speed job option support parallel transmiss larg data",
        "Answer_original_content":"dsa push pull data parallel us option parallelis diminish return resourc constraint network bandwidth unparallelis job overhead test guess past threshold ad job doesnt bring speed ronan accord basic mean threshold cpu core formula cpu count mean need increas number actual cpu improv speed experi smaller number instead chang speed",
        "Answer_preprocessed_content":"push pull data parallel us option parallelis diminish return resourc constraint test guess past threshold ad job doesnt bring accord basic mean threshold mean need increas number actual cpu improv speed experi smaller number instead chang speed",
        "Answer_gpt_summary_original":"Solution:\n- The discussion suggests that DVC supports parallel transmission of large data, whether the user uses the -j option or not.\n- However, parallelization has diminishing returns due to resource constraints such as network bandwidth and unparallelizable per-job overhead.\n- The user's testing indicates that 32 is already past the threshold where adding more jobs doesn't bring any speed-up.\n- Increasing the number of actual CPUs may improve speed, but it is not a guaranteed solution.\n- The user experimented with smaller numbers such as 12 or 16 instead of 64, but there was no change in speed. No solution was provided for this issue.",
        "Answer_gpt_summary":"solut discuss suggest support parallel transmiss larg data user us option parallel diminish return resourc constraint network bandwidth unparalleliz job overhead user test indic past threshold ad job bring speed increas number actual cpu improv speed guarante solut user experi smaller number instead chang speed solut provid issu"
    },
    {
        "Question_title":"Sagemaker KMeans Built-In - List of files csv as input",
        "Question_body":"<p>I Want to use <strong>Sagemaker KMeans BuilIn Algorithm<\/strong> in one of my applications. I have a large CSV file in S3 (raw data) that I split into several parts to be easy to clean. Before I had cleaned, I tried to use it as the input of Kmeans to perform the training job but It doesn't work.<\/p>\n\n<p>My manifest file:<\/p>\n\n<pre><code>[\n    {\"prefix\": \"s3:\/\/&lt;BUCKET_NAME&gt;\/kmeans_data\/KMeans-2019-28-07-13-40-00-001\/\"}, \n    \"file1.csv\", \n    \"file2.csv\"\n]\n<\/code><\/pre>\n\n<p>The error I've got:<\/p>\n\n<pre><code>Failure reason: ClientError: Unable to read data channel 'train'. Requested content-type is 'application\/x-recordio-protobuf'. Please verify the data matches the requested content-type. (caused by MXNetError) Caused by: [16:47:31] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsCppLibs\/AIAlgorithmsCppLibs-2.0.1620.0\/AL2012\/generic-flavor\/src\/src\/aialgs\/io\/iterator_base.cpp:100: (Input Error) The header of the MXNet RecordIO record at position 0 in the dataset does not start with a valid magic number. Stack trace returned 10 entries: [bt] (0) \/opt\/amazon\/lib\/libaialgs.so(+0xb1f0) [0x7fb5674c31f0] [bt] (1) \/opt\/amazon\/lib\/libaialgs.so(+0xb54a) [0x7fb5674c354a] [bt] (2) \/opt\/amazon\/lib\/libaialgs.so(aialgs::iterator_base::Next()+0x4a6) [0x7fb5674cc436] [bt] (3) \/opt\/amazon\/lib\/libmxnet.so(MXDataIterNext+0x21) [0x7fb54ecbcdb1] [bt] (4) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call_unix64+0x4c) [0x7fb567a1e858] [bt] (5) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call+0x15f) [0x7fb567a1d95f\n<\/code><\/pre>\n\n<p>My question is: It's possible to use multiple CSV files as input in Sagemaker KMeans BuilIn Algorithm only in GUI? If it's possible, How should I format my manifest?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1564336514917,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1564342604716,
        "Question_score":3.0,
        "Question_view_count":522.0,
        "Answer_body":"<p>the manifest looks fine, but based on the error message, it looks like you haven't set the right data format for you S3 data. It's expecting protobuf, which is the default format :)<\/p>\n\n<p>You have to set the CSV data format explicitly. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.s3_input\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.s3_input<\/a>. <\/p>\n\n<p>It should look something like this:<\/p>\n\n<pre><code>s3_input_train = sagemaker.s3_input(\n  s3_data='s3:\/\/{}\/{}\/train\/manifest_file'.format(bucket, prefix),    \n  s3_data_type='ManifestFile',\n  content_type='csv')\n\n...\n\nkmeans_estimator = sagemaker.estimator.Estimator(kmeans_image, ...)\nkmeans_estimator.set_hyperparameters(...)\n\ns3_data = {'train': s3_input_train}\nkmeans_estimator.fit(s3_data)\n<\/code><\/pre>\n\n<p>Please note the KMeans estimator in the SDK only supports protobuf, see <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/kmeans.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/kmeans.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57243583",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1564340123623,
        "Question_original_content":"kmean built list file csv input want us kmean builin algorithm applic larg csv file raw data split part easi clean clean tri us input kmean perform train job work manifest file prefix kmean data kmean file csv file csv error got failur reason clienterror unabl read data channel train request content type applic recordio protobuf verifi data match request content type caus mxneterror caus opt brazil pkg cach packag aialgorithmscpplib aialgorithmscpplib gener flavor src src aialg iter base cpp input error header mxnet recordio record posit dataset start valid magic number stack trace return entri opt amazon lib libaialg xbf xfbcf opt amazon lib libaialg xba xfbca opt amazon lib libaialg aialg iter base xfbcc opt amazon lib libmxnet mxdataiternext xfbecbcdb opt amazon python lib python lib dynload ctype ffi unix xfbae opt amazon python lib python lib dynload ctype ffi xfbadf question possibl us multipl csv file input kmean builin algorithm gui possibl format manifest",
        "Question_preprocessed_content":"kmean list file csv input want us kmean builin algorithm applic larg csv file split part easi clean clean tri us input kmean perform train job work manifest file error got question possibl us multipl csv file input kmean builin algorithm gui possibl format manifest",
        "Question_gpt_summary_original":"The user is facing challenges while using the Sagemaker KMeans Built-In Algorithm in their application. They have a large CSV file in S3 that they split into several parts to make it easier to clean. However, when they tried to use it as input for KMeans, they encountered an error related to the content-type. The user is asking if it is possible to use multiple CSV files as input in Sagemaker KMeans Built-In Algorithm and how to format the manifest file.",
        "Question_gpt_summary":"user face challeng kmean built algorithm applic larg csv file split part easier clean tri us input kmean encount error relat content type user ask possibl us multipl csv file input kmean built algorithm format manifest file",
        "Answer_original_content":"manifest look fine base error messag look like haven set right data format data expect protobuf default format set csv data format explicitli http readthedoc stabl session html session input look like input train input data train manifest file format bucket prefix data type manifestfil content type csv kmean estim estim estim kmean imag kmean estim set hyperparamet data train input train kmean estim fit data note kmean estim sdk support protobuf http readthedoc stabl kmean html",
        "Answer_preprocessed_content":"manifest look fine base error messag look like haven set right data format data expect protobuf default format set csv data format explicitli look like note kmean estim sdk support protobuf",
        "Answer_gpt_summary_original":"Solution: The user needs to set the CSV data format explicitly in the S3 input for the Sagemaker KMeans Built-In Algorithm. The KMeans estimator in the SDK only supports protobuf, so the user needs to set the content-type to 'csv' explicitly. The S3 input should be defined using the sagemaker.s3_input() function and the s3_data_type parameter should be set to 'ManifestFile'. The manifest file should list all the CSV files that the user wants to use as input.",
        "Answer_gpt_summary":"solut user need set csv data format explicitli input kmean built algorithm kmean estim sdk support protobuf user need set content type csv explicitli input defin input function data type paramet set manifestfil manifest file list csv file user want us input"
    },
    {
        "Question_title":"How to assign two or more time series identifier columns in Vertex AI Tabular Forecasting",
        "Question_body":"<p>I was wondering if it is possible to have more than one time series identifier column in the model? Let's assume I'd like to create a forecast at a product and store level (which the documentation suggests should be possible).<\/p>\n<p>If I select product as the series identifier, the only options I have left for store is either a covariate or an attribute and neither is applicable in this scenario.<\/p>\n<p>Would concatenating product and store and using the individual product and store code values for that concatenated ID as attributes be a solution? It doesn't feel right, but I can't see any other option - am I missing something?<\/p>\n<p>Note: I understand that this feature of Vertex AI is currently in preview and that because of that the options may be limited.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1632228068913,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":269.0,
        "Answer_body":"<p>There isn't an alternate way to assign 2 or more <strong>Time Series Identifiers<\/strong> in the <strong>Forecasting Model<\/strong> on <strong>Vertex AI<\/strong>. The &quot;<strong>Forecasting model<\/strong>&quot; is in the &quot;<strong>Preview<\/strong>&quot; <a href=\"https:\/\/cloud.google.com\/products#product-launch-stages\" rel=\"nofollow noreferrer\">Product launch stage<\/a>, as you are aware, with all consequences of that fact the options are limited. Please refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/bp-tabular#data_preparation_best_practices\" rel=\"nofollow noreferrer\">doc<\/a> for more information about the best practices for data preparation to train the forecasting model.<\/p>\n<p>As a workaround, the two columns can be concatenated and assigned a Time Series Identifier on that concatenated column, as you have mentioned in the question. This way, the concatenated column carries more contextual information into the training of the model.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1632482492492,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69269073",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1632376096768,
        "Question_original_content":"assign time seri identifi column tabular forecast wonder possibl time seri identifi column model let assum like creat forecast product store level document suggest possibl select product seri identifi option left store covari attribut applic scenario concaten product store individu product store code valu concaten attribut solut feel right option miss note understand featur current preview option limit",
        "Question_preprocessed_content":"assign time seri identifi column tabular forecast wonder possibl time seri identifi column model let assum like creat forecast product store level select product seri identifi option left store covari attribut applic scenario concaten product store individu product store code valu concaten attribut solut feel right option miss note understand featur current preview option limit",
        "Question_gpt_summary_original":"The user is facing a challenge in assigning two or more time series identifier columns in Vertex AI Tabular Forecasting. They want to create a forecast at a product and store level, but the only options available for store are either a covariate or an attribute, which are not applicable in this scenario. The user is considering concatenating product and store and using the individual product and store code values for that concatenated ID as attributes, but they are unsure if this is the right solution. The user acknowledges that this feature is currently in preview and the options may be limited.",
        "Question_gpt_summary":"user face challeng assign time seri identifi column tabular forecast want creat forecast product store level option avail store covari attribut applic scenario user consid concaten product store individu product store code valu concaten attribut unsur right solut user acknowledg featur current preview option limit",
        "Answer_original_content":"isn altern wai assign time seri identifi forecast model forecast model preview product launch stage awar consequ fact option limit refer doc inform best practic data prepar train forecast model workaround column concaten assign time seri identifi concaten column mention question wai concaten column carri contextu inform train model",
        "Answer_preprocessed_content":"isn altern wai assign time seri identifi forecast model forecast model preview product launch stage awar consequ fact option limit refer doc inform best practic data prepar train forecast model workaround column concaten assign time seri identifi concaten column mention question wai concaten column carri contextu inform train model",
        "Answer_gpt_summary_original":"Solution: The only solution mentioned in the discussion is to concatenate the two columns and assign a Time Series Identifier on that concatenated column. This workaround can carry more contextual information into the training of the model. However, it is acknowledged that this feature is currently in preview and the options may be limited.",
        "Answer_gpt_summary":"solut solut mention discuss concaten column assign time seri identifi concaten column workaround carri contextu inform train model acknowledg featur current preview option limit"
    },
    {
        "Question_title":"DVC and version control systems other than Git",
        "Question_body":"<p>Hello,<\/p>\n<p>I am considering DVC as a tool that could help my team better organize our work. The problem is that we use Mercurial for versioning our code. I have read that DVC somehow depends on Git, but I\u2019m not sure in what way. Is Git the only versionioning software that DVC can work with? The only part that I\u2019ve spotted in the docs that really relies on Git is the <code>dvc init<\/code> command that creates <code>.dvc\/.gitignore<\/code>. Would adding contents of this file to hgignore (equivalent of gitignore in Mercurial) solve the problem? Are there other caveats that would prevent me from using Mercurial with DVC?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1536316201269,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":733.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/krysiul\">@Krysiul<\/a> !<\/p>\n<p>Thank you for considering dvc! I hope you don\u2019t mind me citing a question that has been asked before:<\/p>\n<aside class=\"quote\" data-post=\"1\" data-topic=\"55\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"><\/div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/5e9695\/40.png\" class=\"avatar\">\n    <a href=\"https:\/\/discuss.dvc.org\/t\/does-dvc-actually-require-git-or-would-mercurial-work-just-as-well\/55\">Does DVC actually require Git, or would Mercurial work just as well?<\/a> <a class=\"badge-wrapper  bar\" href=\"\/c\/questions\/9\"><span class=\"badge-category-bg\" style=\"background-color: #9EB83B;\"><\/span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"Need help? Have a question about how to use DVC? Ask it here!\">Questions<\/span><\/a>\n  <\/div>\n  <blockquote>\n    My question is as the title states really, does DVC actually require Git, or would Mercurial work just as well? Does DVC actually call Git to do anything, or does it just store information in normal files which the user commits.\n  <\/blockquote>\n<\/aside>\n\n<blockquote>\n<p>DVC uses Git for things such as:<\/p>\n<ol>\n<li>adding files tracked by dvc to .gitignore for user comfort;<\/li>\n<li>once calling  <code>git add<\/code>  after  <code>dvc init<\/code>  for user comfort;<\/li>\n<li>iterating git branches to retrieve information about used dvc files and metrics(e.g.  <code>dvc push --all-&gt;branches<\/code>  and  <code>dvc metrics show --all-branches<\/code> );<\/li>\n<\/ol>\n<p>That is pretty much it. User does all the other  <code>git add<\/code> -ing and  <code>git commit<\/code> -ing. Dvc stores information about your data in plain yaml files that are committed by user to git.<\/p>\n<p>DVC doesn\u2019t generally require Git, it can even work without any SCM system(use  <code>dvc init --no-scm<\/code>  for that) if you don\u2019t need those there features listed above. We have thought about supporting other SCMs and thus left a possibility to easily extend the list of supported SCMs by simply adding a proper driver to <a href=\"https:\/\/github.com\/iterative\/dvc\/blob\/master\/dvc\/scm.py\">https:\/\/github.com\/iterative\/dvc\/blob\/master\/dvc\/scm.py <\/a>(see Git class as an example). So it should be pretty easy to support Hg. I\u2019ve added <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/945\">https:\/\/github.com\/iterative\/dvc\/issues\/945 <\/a> to track the progress on it, we will be sure to take a look at it in the nearest future. That being said, if you wish to contribute a patch, please feel free to do so, we will be happy to merge it! <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=10\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>\n<\/blockquote>\n<p>So yes, you could indeed just add corresponding entries to hgignore yourself after <code>dvc init --no-scm<\/code> and then be adding every data file cached by dvc to appropriate hgignrore, but it would be a much better and easier solution to simply contribute a simple patch for dvc to support hg.<\/p>\n<p>Thanks,<br>\nRuslan<\/p>. <p>Hey, thanks for the qucik reply Ruslan. Shame on me I haven\u2019t found this question myself.<\/p>. <p>No worries <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=6\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-version-control-systems-other-than-git\/90",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-09-07T10:50:15.219Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/krysiul\">@Krysiul<\/a> !<\/p>\n<p>Thank you for considering dvc! I hope you don\u2019t mind me citing a question that has been asked before:<\/p>\n<aside class=\"quote\" data-post=\"1\" data-topic=\"55\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"><\/div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/5e9695\/40.png\" class=\"avatar\">\n    <a href=\"https:\/\/discuss.dvc.org\/t\/does-dvc-actually-require-git-or-would-mercurial-work-just-as-well\/55\">Does DVC actually require Git, or would Mercurial work just as well?<\/a> <a class=\"badge-wrapper  bar\" href=\"\/c\/questions\/9\"><span class=\"badge-category-bg\" style=\"background-color: #9EB83B;\"><\/span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"Need help? Have a question about how to use DVC? Ask it here!\">Questions<\/span><\/a>\n  <\/div>\n  <blockquote>\n    My question is as the title states really, does DVC actually require Git, or would Mercurial work just as well? Does DVC actually call Git to do anything, or does it just store information in normal files which the user commits.\n  <\/blockquote>\n<\/aside>\n\n<blockquote>\n<p>DVC uses Git for things such as:<\/p>\n<ol>\n<li>adding files tracked by dvc to .gitignore for user comfort;<\/li>\n<li>once calling  <code>git add<\/code>  after  <code>dvc init<\/code>  for user comfort;<\/li>\n<li>iterating git branches to retrieve information about used dvc files and metrics(e.g.  <code>dvc push --all-&gt;branches<\/code>  and  <code>dvc metrics show --all-branches<\/code> );<\/li>\n<\/ol>\n<p>That is pretty much it. User does all the other  <code>git add<\/code> -ing and  <code>git commit<\/code> -ing. Dvc stores information about your data in plain yaml files that are committed by user to git.<\/p>\n<p>DVC doesn\u2019t generally require Git, it can even work without any SCM system(use  <code>dvc init --no-scm<\/code>  for that) if you don\u2019t need those there features listed above. We have thought about supporting other SCMs and thus left a possibility to easily extend the list of supported SCMs by simply adding a proper driver to <a href=\"https:\/\/github.com\/iterative\/dvc\/blob\/master\/dvc\/scm.py\">https:\/\/github.com\/iterative\/dvc\/blob\/master\/dvc\/scm.py <\/a>(see Git class as an example). So it should be pretty easy to support Hg. I\u2019ve added <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/945\">https:\/\/github.com\/iterative\/dvc\/issues\/945 <\/a> to track the progress on it, we will be sure to take a look at it in the nearest future. That being said, if you wish to contribute a patch, please feel free to do so, we will be happy to merge it! <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=10\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>\n<\/blockquote>\n<p>So yes, you could indeed just add corresponding entries to hgignore yourself after <code>dvc init --no-scm<\/code> and then be adding every data file cached by dvc to appropriate hgignrore, but it would be a much better and easier solution to simply contribute a simple patch for dvc to support hg.<\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-09-10T13:29:59.156Z",
                "Answer_body":"<p>Hey, thanks for the qucik reply Ruslan. Shame on me I haven\u2019t found this question myself.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-09-10T13:44:08.704Z",
                "Answer_body":"<p>No worries <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=6\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"version control system git hello consid tool help team better organ work problem us mercuri version code read depend git sure wai git versionion softwar work iv spot doc reli git init command creat gitignor ad content file hgignor equival gitignor mercuri solv problem caveat prevent mercuri",
        "Question_preprocessed_content":"version control system git hello consid tool help team better organ work problem us mercuri version code read depend git sure wai git versionion softwar work iv spot doc reli git command creat ad content file hgignor solv problem caveat prevent mercuri",
        "Question_gpt_summary_original":"The user is considering using DVC to better organize their team's work, but they currently use Mercurial for versioning their code. They are unsure if DVC can work with Mercurial or if it only works with Git. They have identified that the <code>dvc init<\/code> command relies on Git to create <code>.dvc\/.gitignore<\/code>, but they are wondering if adding the contents of this file to hgignore (equivalent of gitignore in Mercurial) would solve the problem. The user is seeking clarification on whether there are any other caveats that would prevent them from using Mercurial with DVC.",
        "Question_gpt_summary":"user consid better organ team work current us mercuri version code unsur work mercuri work git identifi init command reli git creat gitignor wonder ad content file hgignor equival gitignor mercuri solv problem user seek clarif caveat prevent mercuri",
        "Answer_original_content":"krysiul thank consid hope dont mind cite question ask actual requir git mercuri work question question titl state actual requir git mercuri work actual git store inform normal file user commit us git thing ad file track gitignor user comfort call git add init user comfort iter git branch retriev inform file metric push branch metric branch pretti user git add ing git commit ing store inform data plain yaml file commit user git doesnt gener requir git work scm us init scm dont need featur list thought support scm left possibl easili extend list support scm simpli ad proper driver http github com iter blob master scm git class exampl pretti easi support iv ad http github com iter issu track progress sure look nearest futur said wish contribut patch feel free happi merg ye add correspond entri hgignor init scm ad data file cach appropri hgignror better easier solut simpli contribut simpl patch support thank ruslan hei thank qucik repli ruslan shame havent question worri",
        "Answer_preprocessed_content":"thank consid hope dont mind cite question ask actual requir git mercuri work question question titl state actual requir git mercuri work actual git store inform normal file user commit us git thing ad file track gitignor user comfort call user comfort iter git branch retriev inform file pretti user ing ing store inform data plain yaml file commit user git doesnt gener requir git work scm dont need featur list thought support scm left possibl easili extend list support scm simpli ad proper driver pretti easi support iv ad track progress sure look nearest futur said wish contribut patch feel free happi merg ye add correspond entri hgignor ad data file cach appropri hgignror better easier solut simpli contribut simpl patch support thank ruslan hei thank qucik repli ruslan shame havent question worri",
        "Answer_gpt_summary_original":"Solution:\n- DVC uses Git for adding files tracked by DVC to .gitignore for user comfort, calling git add after dvc init for user comfort, and iterating Git branches to retrieve information about used DVC files and metrics.\n- DVC doesn't generally require Git and can work without any SCM system if you don't need the features listed above.\n- DVC can support other SCMs by adding a proper driver to dvc\/scm.py, so it should be pretty easy to support Mercurial.\n- The user could add corresponding entries to hgignore themselves after dvc init --no-scm and then be adding every data file cached by DVC to appropriate hgignore, but it would be a much better and easier solution",
        "Answer_gpt_summary":"solut us git ad file track gitignor user comfort call git add init user comfort iter git branch retriev inform file metric gener requir git work scm need featur list support scm ad proper driver scm pretti easi support mercuri user add correspond entri hgignor init scm ad data file cach appropri hgignor better easier solut"
    },
    {
        "Question_title":"How to overcome TrainingException when training a large model with Azure Machine Learning service?",
        "Question_body":"<p>I'm training a large-ish model, trying to use for the purpose <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#create-an-estimator\" rel=\"nofollow noreferrer\">Azure Machine Learning service<\/a> in Azure notebooks.<\/p>\n\n<p>I thus create an <code>Estimator<\/code> to train locally:<\/p>\n\n<pre><code>from azureml.train.estimator import Estimator\n\nestimator = Estimator(source_directory='.\/source_dir',\n                      compute_target='local',\n                      entry_script='train.py')\n<\/code><\/pre>\n\n<p>(my <code>train.py<\/code> should load and train starting from a large word vector file).<\/p>\n\n<p>When running with <\/p>\n\n<pre><code>run = experiment.submit(config=estimator)\n<\/code><\/pre>\n\n<p>I get <\/p>\n\n<blockquote>\n  <p>TrainingException: <\/p>\n  \n  <p>====================================================================<\/p>\n  \n  <p>While attempting to take snapshot of\n  \/data\/home\/username\/notebooks\/source_dir Your total\n  snapshot size exceeds the limit of 300.0 MB. Please see\n  <a href=\"http:\/\/aka.ms\/aml-largefiles\" rel=\"nofollow noreferrer\">http:\/\/aka.ms\/aml-largefiles<\/a> on how to work with large files.<\/p>\n  \n  <p>====================================================================<\/p>\n<\/blockquote>\n\n<p>The link provided in the error is likely <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/26076\" rel=\"nofollow noreferrer\">broken<\/a>. \nContents in my <code>.\/source_dir<\/code> indeed exceed 300 MB.<br>\nHow can I solve this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1554414642303,
        "Question_favorite_count":null,
        "Question_last_edit_time":1554642637940,
        "Question_score":1.0,
        "Question_view_count":1127.0,
        "Answer_body":"<p>You can place the training files outside <code>source_dir<\/code> so that they don't get uploaded as part of submitting the experiment, and then upload them separately to the data store (which is basically using the Azure storage associated with your workspace). All you need to do then is reference the training files from <code>train.py<\/code>. <\/p>\n\n<p>See the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml\" rel=\"nofollow noreferrer\">Train model tutorial<\/a> for an example of how to upload data to the data store and then access it from the training file.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1554449103928,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55525445",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1554445793380,
        "Question_original_content":"overcom trainingexcept train larg model servic train larg ish model try us purpos servic azur notebook creat estim train local train estim import estim estim estim sourc directori sourc dir comput target local entri script train train load train start larg word vector file run run experi submit config estim trainingexcept attempt snapshot data home usernam notebook sourc dir total snapshot size exce limit http aka aml largefil work larg file link provid error like broken content sourc dir exce solv",
        "Question_preprocessed_content":"overcom trainingexcept train larg model servic train model try us purpos servic azur notebook creat train local load train start larg word vector file run trainingexcept attempt snapshot total snapshot size exce limit work larg file link provid error like broken content exce solv",
        "Question_gpt_summary_original":"The user is encountering a TrainingException while trying to train a large model using Azure Machine Learning service in Azure notebooks. The error message indicates that the total snapshot size exceeds the limit of 300 MB, and the link provided in the error is broken. The user is seeking a solution to overcome this challenge.",
        "Question_gpt_summary":"user encount trainingexcept try train larg model servic azur notebook error messag indic total snapshot size exce limit link provid error broken user seek solut overcom challeng",
        "Answer_original_content":"place train file outsid sourc dir upload submit experi upload separ data store basic azur storag associ workspac need refer train file train train model tutori exampl upload data data store access train file",
        "Answer_preprocessed_content":"place train file outsid upload submit experi upload separ data store need refer train file train model tutori exampl upload data data store access train file",
        "Answer_gpt_summary_original":"Solution: The user can place the training files outside the source directory and upload them separately to the data store. Then, the user can reference the training files from train.py. The Train model tutorial provides an example of how to upload data to the data store and access it from the training file.",
        "Answer_gpt_summary":"solut user place train file outsid sourc directori upload separ data store user refer train file train train model tutori provid exampl upload data data store access train file"
    },
    {
        "Question_title":"ClientError: object_detection_augmented_manifest_training template",
        "Question_body":"Hello,\n\nMy aim is to create a model for garden birds. I have 293 photos of birds that I have put through 2 custom labelling jobs in ground truth for training and validation. The issue I encountered was being able to have multiple labels on the bounding box which I managed to do via creating a custom labelling job with the following labels:\n\n<crowd-bounding-box\r\n    name=\"annotatedResult\"\r\n    labels=\"['Blackbird', 'Blue tit', 'Coal tit', 'Dunnock', 'Great tit', 'Long-tailed tit', 'Nuthatch', 'Pigeon', 'Robin']\" .....\n\n\nI now have 2 output manifest files with many lines of this:\n\n{\"source-ref\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\",\"BirdLabel\":{\"workerId\":\"privateXXXXX\",\"imageSource\":{\"s3Uri\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\"},\"boxesInfo\":{\"annotatedResult\":{\"boundingBoxes\":[{\"width\":1619,\"top\":840,\"label\":\"Blackbird\",\"left\":1287,\"height\":753}],\"inputImageProperties\":{\"width\":3872,\"height\":2592}}}},\"BirdLabel-metadata\":{\"type\":\"groundtruth\/custom\",\"job-name\":\"birdlabel\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-01-10T15:41:52+0000\"}}\n\n\nAfter this job was successful, I made an ml.p3.2xlarge instance, using the object_detection_augmented_manifest_training template.\n\nI have filled in the necessary sections, I then run it and received this error when I have the Content Type to 'application\/x-image' with Record wrapper type:RecordIO : 'ClientError: train channel is not specified.'\n\nI then changed the channel to train_annotation instead of train and I receive this error message: \"ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)\\n\\nCaused by: u'train' is a required property\n\nAdditional information can be provided if neccessary.\nAny help would be much apreciated! Thank you.\n\nEdited by: LuciA on Jan 16, 2019 1:12 PM\n\nEdited by: LuciA on Jan 16, 2019 1:18 PM\n\nEdited by: LuciA on Jan 16, 2019 1:19 PM",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1547563664000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":52.0,
        "Answer_body":"Hi LuciA - I'm an engineer at AWS. Thanks for continuing to try the service in the face of some difficulties. Can you please cross-reference your augmented manifest against the samples shown in https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-ground-truth-build-highly-accurate-datasets-and-reduce-labeling-costs-by-up-to-70\/, https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/object_detection_augmented_manifest_training\/object_detection_augmented_manifest_training.ipynb, and https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb?\n\nIt looks like your format is a little different, e.g., the algorithm expects to see keys called \"annotations\" and \"image_size\". Can you please check the syntax and let us know if your results change?",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjk2-AZ6VQl68Zdm1Owq-_A\/client-error-object-detection-augmented-manifest-training-template",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-02-07T18:20:27.000Z",
                "Answer_score":0,
                "Answer_body":"Hi LuciA - I'm an engineer at AWS. Thanks for continuing to try the service in the face of some difficulties. Can you please cross-reference your augmented manifest against the samples shown in https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-ground-truth-build-highly-accurate-datasets-and-reduce-labeling-costs-by-up-to-70\/, https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/object_detection_augmented_manifest_training\/object_detection_augmented_manifest_training.ipynb, and https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb?\n\nIt looks like your format is a little different, e.g., the algorithm expects to see keys called \"annotations\" and \"image_size\". Can you please check the syntax and let us know if your results change?",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2019-01-29T22:57:02.000Z",
                "Answer_score":0,
                "Answer_body":"Hi LuciA,\n\nThanks for trying out the SageMaker object detection algorithm. When using the Pipe mode with an AugmentedManifestFile, you need to specify the RecordWrapperType as RecordIO and ContentType as application\/x-recordio. Can you please retry with these suggestions and revert back if you continue to see issues.\n\nThanks!\nRegards,\nAmazon SageMaker Team",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-02-10T12:15:37.000Z",
                "Answer_score":0,
                "Answer_body":"Hi vrkhareataws\n\nThank you for getting back to me. I have tried the changes suggested and now recieve a new error message, 'InternalServerError: We encountered an internal error. Please try again.' I have added a snippet of code below for more insight:\n\ntraining_params = \\\r\n{\r\n    \"AlgorithmSpecification\": {\r\n        \"TrainingImage\": training_image, \r\n        \"TrainingInputMode\": \"Pipe\"\r\n    },\r\n    \"RoleArn\": role,\r\n    \"OutputDataConfig\": {\r\n        \"S3OutputPath\": s3_output_path\r\n    },\r\n    \"ResourceConfig\": {\r\n        \"InstanceCount\": 1,   \r\n        \"InstanceType\": \"ml.p3.2xlarge\",\r\n        \"VolumeSizeInGB\": 50\r\n    },\r\n    \"TrainingJobName\": job_name,\r\n    \"HyperParameters\": { \r\n         \"base_network\": \"resnet-50\",\r\n         \"use_pretrained_model\": \"1\",\r\n         \"num_classes\": \"1\",\r\n         \"mini_batch_size\": \"1\",\r\n         \"epochs\": \"5\",\r\n         \"learning_rate\": \"0.001\",\r\n         \"lr_scheduler_step\": \"3,6\",\r\n         \"lr_scheduler_factor\": \"0.1\",\r\n         \"optimizer\": \"rmsprop\",\r\n         \"momentum\": \"0.9\",\r\n         \"weight_decay\": \"0.0005\",\r\n         \"overlap_threshold\": \"0.5\",\r\n         \"nms_threshold\": \"0.45\",\r\n         \"image_shape\": \"300\",\r\n         \"label_width\": \"350\",\r\n         \"num_training_samples\": str(num_training_samples)\r\n    },\r\n    \"StoppingCondition\": {\r\n        \"MaxRuntimeInSeconds\": 86400\r\n    },\r\n    \"InputDataConfig\": [\r\n        {\r\n            \"ChannelName\": \"train\",\r\n            \"DataSource\": {\r\n                \"S3DataSource\": {\r\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\r\n                    \"S3Uri\": s3_train_data_path,\r\n                    \"S3DataDistributionType\": \"FullyReplicated\",\r\n                    \"AttributeNames\": [\"source-ref\",\"Bird-Label-Train\"] \r\n                }\r\n            },\r\n            \"ContentType\": \"application\/x-recordio\",\r\n            \"RecordWrapperType\": \"RecordIO\",\r\n            \"CompressionType\": \"None\"\r\n        },\r\n        {\r\n            \"ChannelName\": \"validation\",\r\n            \"DataSource\": {\r\n                \"S3DataSource\": {\r\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\r\n                    \"S3Uri\": s3_validation_data_path,\r\n                    \"S3DataDistributionType\": \"FullyReplicated\",\r\n                    \"AttributeNames\": [\"source-ref\",\"Bird-Label\"] \r\n                }\r\n            },\r\n            \"ContentType\": \"application\/x-recordio\",\r\n            \"RecordWrapperType\": \"RecordIO\",\r\n            \"CompressionType\": \"None\"\r\n        }\r\n    ]\r\n}\n\n\nI had run 2 seperate labelling jobs to get my training and validation set which have different names. \"Bird-Label\" for my validation and \"Bird-Label-Train\" for training. As you can see in the \"AttributeNames\" fields.\n\nI would also like to point out that my decision for using the image content type was due to this sentence, \"However you can also train in pipe mode using the image files (image\/png, image\/jpeg, and application\/x-image), without creating RecordIO files, by using the augmented manifest format.\" from this guide (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html).\n\nPlease let me know if you understand what is causing this error. Thank You!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-01-30T10:05:35.000Z",
                "Answer_score":0,
                "Answer_body":"Hi JonathanB-AWS\n\nThank you for sending me really useful links.\n\nHaving sorted through the information, I understood that my JSON was slightly different. Having gone back to basics, I created another labelling job, but used the 'Bouding Box' type as opposed to the 'Custom - Bounding box template'. My output matched what was expected. This ran with no errors!\n\nAs my purpose was to have multiple labels, I was able to edit the files and mapping of my output manifests, which also worked!\n\ni.e.\n\n{\"source-ref\":\"s3:\/\/xxxxx\/Blackbird_15.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":0,\"width\":2023,\"top\":665,\"height\":1421,\"left\":1312}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.174131\",\"type\":\"groundtruth\/object-detection\"}}\r\n{\"source-ref\":\"s3:\/\/xxxx\/Pigeon_19.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":2,\"width\":784,\"top\":634,\"height\":1657,\"left\":1306}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"2\":\"Pigeon\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.074809\",\"type\":\"groundtruth\/object-detection\"}}\n\n\nThe original mapping was 0:'Bird' for all images through the labelling job.\n\nThank you for your help!",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1549563627000,
        "Question_original_content":"clienterror object detect augment manifest train templat hello aim creat model garden bird photo bird custom label job ground truth train valid issu encount abl multipl label bound box manag creat custom label job follow label crowd bound box annotatedresult label blackbird blue tit coal tit dunnock great tit long tail tit nuthatch pigeon robin output manifest file line sourc ref blackbird jpg birdlabel workerid privatexx imagesourc suri blackbird jpg boxesinfo annotatedresult boundingbox width label blackbird left height inputimageproperti width height birdlabel metadata type groundtruth custom job birdlabel human annot ye creation date job success xlarg instanc object detect augment manifest train templat fill necessari section run receiv error content type applic imag record wrapper type recordio clienterror train channel specifi chang channel train annot instead train receiv error messag clienterror unabl initi algorithm fail valid input data configur caus validationerror ncaus train requir properti addit inform provid neccessari help apreci thank edit lucia jan edit lucia jan edit lucia jan",
        "Question_preprocessed_content":"clienterror templat hello aim creat model garden bird photo bird custom label job ground truth train valid issu encount abl multipl label bound box manag creat custom label job follow label annotatedresult label blackbird blue tit coal tit dunnock great tit tit nuthatch pigeon robin output manifest file line job success instanc templat fill necessari section run receiv error content type record wrapper type recordio clienterror train channel chang channel instead train receiv error messag clienterror unabl initi algorithm fail valid input data configur train requir properti addit inform provid neccessari help apreci thank edit lucia jan edit lucia jan edit lucia jan",
        "Question_gpt_summary_original":"The user encountered challenges while creating a model for garden birds using 293 photos that were put through 2 custom labelling jobs in ground truth for training and validation. The user was able to create a custom labelling job with multiple labels on the bounding box. However, when the user tried to run the ml.p3.2xlarge instance using the object_detection_augmented_manifest_training template, they received an error message stating that the train channel was not specified. When the user changed the channel to train_annotation, they received another error message stating that 'train' is a required property.",
        "Question_gpt_summary":"user encount challeng creat model garden bird photo custom label job ground truth train valid user abl creat custom label job multipl label bound box user tri run xlarg instanc object detect augment manifest train templat receiv error messag state train channel specifi user chang channel train annot receiv error messag state train requir properti",
        "Answer_original_content":"lucia engin aw thank continu try servic face difficulti cross refer augment manifest sampl shown http aw amazon com blog aw amazon ground truth build highli accur dataset reduc label cost http github com awslab amazon exampl blob master ground truth label job object detect augment manifest train object detect augment manifest train ipynb http github com awslab amazon exampl blob master ground truth label job ground truth object detect tutori object detect tutori ipynb look like format littl differ algorithm expect kei call annot imag size check syntax let know result chang",
        "Answer_preprocessed_content":"lucia engin aw thank continu try servic face difficulti augment manifest sampl shown look like format littl differ algorithm expect kei call annot check syntax let know result chang",
        "Answer_gpt_summary_original":"Solution: The engineer suggested cross-referencing the augmented manifest against the samples provided in AWS blogs and GitHub repositories. The engineer also pointed out that the format of the user's augmented manifest was different from what the algorithm expected, and suggested checking the syntax and ensuring that the keys \"annotations\" and \"image_size\" were present. No further solutions were provided.",
        "Answer_gpt_summary":"solut engin suggest cross referenc augment manifest sampl provid aw blog github repositori engin point format user augment manifest differ algorithm expect suggest check syntax ensur kei annot imag size present solut provid"
    },
    {
        "Question_title":"xgboost on Sagemaker notebook import fails",
        "Question_body":"<p>I am trying to use XGBoost on Sagemaker notebook.<\/p>\n\n<p>I am using <code>conda_python3<\/code> kernel, and the following packages are installed:<\/p>\n\n<ul>\n<li>py-xgboost-mutex<\/li>\n<li>libxgboost<\/li>\n<li>py-xgboost<\/li>\n<li>py-xgboost-gpu<\/li>\n<\/ul>\n\n<p>But once I am trying to import xgboost it fails on import:<\/p>\n\n<pre><code>ModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-5-5943d1bfe3f1&gt; in &lt;module&gt;()\n----&gt; 1 import xgboost as xgb\n\nModuleNotFoundError: No module named 'xgboost'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1591825043587,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1532.0,
        "Answer_body":"<p>In Sagemaker notebooks  use the below steps <\/p>\n\n<h3>a) If in Notebook<\/h3>\n\n<p>i)  <code>!type python3<\/code><\/p>\n\n<p>ii) Say the above is \/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python3 for you <\/p>\n\n<p>iii) <code>!\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python3 -m pip install  xgboost<\/code><\/p>\n\n<p>iv)  <code>import xgboost<\/code><\/p>\n\n<hr>\n\n<h3>b) If using Terminal<\/h3>\n\n<p>i) <code>conda activate conda_python3<\/code><br>\nii) <code>pip install xgboost<\/code><\/p>\n\n<p>Disclaimer :  sometimes the installation would fail with gcc version ,in that case  update pip version before running install<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1592573769990,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62313532",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1592571589163,
        "Question_original_content":"xgboost notebook import fail try us xgboost notebook conda python kernel follow packag instal xgboost mutex libxgboost xgboost xgboost gpu try import xgboost fail import modulenotfounderror traceback recent import xgboost xgb modulenotfounderror modul name xgboost",
        "Question_preprocessed_content":"xgboost notebook import fail try us xgboost notebook kernel follow packag instal libxgboost try import xgboost fail import",
        "Question_gpt_summary_original":"The user is facing a challenge while trying to use XGBoost on Sagemaker notebook. Despite having installed the required packages, the import of xgboost fails with a ModuleNotFoundError.",
        "Question_gpt_summary":"user face challeng try us xgboost notebook despit have instal requir packag import xgboost fail modulenotfounderror",
        "Answer_original_content":"notebook us step notebook type python home user anaconda env python bin python home user anaconda env python bin python pip instal xgboost import xgboost termin conda activ conda python pip instal xgboost disclaim instal fail gcc version case updat pip version run instal",
        "Answer_preprocessed_content":"notebook us step notebook termin disclaim instal fail gcc version case updat pip version run instal",
        "Answer_gpt_summary_original":"The discussion provides two possible solutions for the challenge of failing to import xgboost on Sagemaker notebook. The first solution involves using the command \"!\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python3 -m pip install xgboost\" after checking the python version using \"!type python3\". The second solution involves activating the conda environment using \"conda activate conda_python3\" and then installing xgboost using \"pip install xgboost\" in the terminal. The discussion also includes a disclaimer that the installation may fail due to the gcc version, in which case the user should update the pip version before running the install.",
        "Answer_gpt_summary":"discuss provid possibl solut challeng fail import xgboost notebook solut involv command home user anaconda env python bin python pip instal xgboost check python version type python second solut involv activ conda environ conda activ conda python instal xgboost pip instal xgboost termin discuss includ disclaim instal fail gcc version case user updat pip version run instal"
    },
    {
        "Question_title":"Show only columns with different values in experiments table",
        "Question_body":"<p>In the table view of a project, is it possible to show only the columns that have different values among runs? This would be very useful to quickly explore how changing parameters affect the model.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1661183885637,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":69.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/enajx\">@enajx<\/a> , would the run comparer table work for your use case, see <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/run-comparer\">here<\/a>.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/show-only-columns-with-different-values-in-experiments-table\/2972",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-24T23:22:17.881Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/enajx\">@enajx<\/a> , would the run comparer table work for your use case, see <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/run-comparer\">here<\/a>.<\/p>",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-10-23T23:22:53.787Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1661383337880,
        "Question_original_content":"column differ valu experi tabl tabl view project possibl column differ valu run us quickli explor chang paramet affect model",
        "Question_preprocessed_content":"column differ valu experi tabl tabl view project possibl column differ valu run us quickli explor chang paramet affect model",
        "Question_gpt_summary_original":"The user is facing a challenge in the table view of a project where they want to show only the columns that have different values among runs. This would help them explore how changing parameters affect the model.",
        "Question_gpt_summary":"user face challeng tabl view project want column differ valu run help explor chang paramet affect model",
        "Answer_original_content":"enajx run compar tabl work us case",
        "Answer_preprocessed_content":"run compar tabl work us case",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use the \"run comparer table\" feature provided by the project, which can help the user compare different runs and show only the columns that have different values among them.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us run compar tabl featur provid project help user compar differ run column differ valu"
    },
    {
        "Question_title":"Domino API - Dataset snapshot details",
        "Question_body":"Hello Everyone,\n\nI would like if there is a Domino API to query dataset snapshot size. If not what are the other ways we could get this information?\u00a0\n\nThank you",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1626330700000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi Tasneem,\u00a0\n\nIt looks like our support team addressed your question in a ticket, but in case anyone else has a similar question:\u00a0\n\nThere is an undocumented API that may work for your use case:\u00a0\n\nhttps:\/\/<your_domino_base_url>\/v4\/datasetUi\/<your_snapshot_id>\n\nWhich you can CURL as:\u00a0\n\ncurl -X GET \"https:\/\/<your_domino_base_url>\/v4\/datasetUi\/<your_snapshot_id>\" -H  \"accept: application\/json\" -H  \"X-Domino-Api-Key: <your_API_key>\" \n\nBecause this is an undocumented API for retrieving this information, it is subject to change.\u00a0 If this is something that is needed and being frequently used, please let our support team know so that we can share that information with our product and engineering team.\u00a0\u00a0\n\nIf you have any further questions or suggestions, please feel free to post them here or send them to our support team (support@dominodatalab.com) for more urgent matters.\u00a0 Thank you!. Thanks Katie. I asked my question in the community first but as I didnt not get reply I asked in the support area.\u00a0\n\nI have another question. How do i check the size of the project ?",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/tickets.dominodatalab.com\/hc\/en-us\/community\/posts\/4403644052244-Domino-API-Dataset-snapshot-details",
        "Tool":"Domino",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-03T01:22:51Z",
                "Answer_score":0,
                "Answer_body":"Hi Tasneem,\u00a0\n\nIt looks like our support team addressed your question in a ticket, but in case anyone else has a similar question:\u00a0\n\nThere is an undocumented API that may work for your use case:\u00a0\n\nhttps:\/\/<your_domino_base_url>\/v4\/datasetUi\/<your_snapshot_id>\n\nWhich you can CURL as:\u00a0\n\ncurl -X GET \"https:\/\/<your_domino_base_url>\/v4\/datasetUi\/<your_snapshot_id>\" -H  \"accept: application\/json\" -H  \"X-Domino-Api-Key: <your_API_key>\" \n\nBecause this is an undocumented API for retrieving this information, it is subject to change.\u00a0 If this is something that is needed and being frequently used, please let our support team know so that we can share that information with our product and engineering team.\u00a0\u00a0\n\nIf you have any further questions or suggestions, please feel free to post them here or send them to our support team (support@dominodatalab.com) for more urgent matters.\u00a0 Thank you!"
            },
            {
                "Answer_creation_time":"2021-08-03T13:43:19Z",
                "Answer_score":0,
                "Answer_body":"Thanks Katie. I asked my question in the community first but as I didnt not get reply I asked in the support area.\u00a0\n\nI have another question. How do i check the size of the project ?"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"api dataset snapshot detail hello like api queri dataset snapshot size wai inform thank",
        "Question_preprocessed_content":"api dataset snapshot detail hello like api queri dataset snapshot size wai inform thank",
        "Question_gpt_summary_original":"The user is seeking information about whether there is a Domino API available to query dataset snapshot size and if not, what other methods can be used to obtain this information.",
        "Question_gpt_summary":"user seek inform api avail queri dataset snapshot size method obtain inform",
        "Answer_original_content":"tasneem look like support team address question ticket case similar question undocu api work us case http datasetui curl curl http datasetui accept applic json api kei undocu api retriev inform subject chang need frequent let support team know share inform product engin team question suggest feel free post send support team support datalab com urgent matter thank thank kati ask question commun didnt repli ask support area question check size project",
        "Answer_preprocessed_content":"tasneem look like support team address question ticket case similar question undocu api work us case curl curl accept undocu api retriev inform subject chang need frequent let support team know share inform product engin team question suggest feel free post send support team urgent matter thank thank kati ask question commun didnt repli ask support area question check size project",
        "Answer_gpt_summary_original":"Solution: An undocumented API is available to query dataset snapshot size. The API can be accessed using a CURL command. However, since this is an undocumented API, it is subject to change. If this is something that is needed and being frequently used, it is recommended to inform the support team so that they can share that information with the product and engineering team. No solution is provided for checking the size of the project.",
        "Answer_gpt_summary":"solut undocu api avail queri dataset snapshot size api access curl command undocu api subject chang need frequent recommend inform support team share inform product engin team solut provid check size project"
    },
    {
        "Question_title":"Can my workspace be an s3 location?",
        "Question_body":"<p>I would like to run <code>dvc pull<\/code> directly into an s3 bucket. Is that possible?<\/p>\n<p>Long story short: I would like end users to version control their csv files using dvc. I then would like to have a recurring process that will do a dvc pull, of the main branch, directly into an s3 bucket. I understand that dvc can use s3 for storage, but I would like to re-create the data in s3, in an identical structure as the dvc git repository.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1646192567568,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":198.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/zzztimbo\">@zzztimbo<\/a>!<br>\nAFAIK s3 is not a machine where you could run a terminal, and use <code>dvc pull<\/code> inside. It seems to me that what you want to do is to checkout the main branch of your repo and use <code>aws s3 cp {your_local_csv_path} s3:\/\/{path_on_s3}<\/code> in the recurring process that you are referring to.<\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/zzztimbo\">@zzztimbo<\/a> ! You might want to take a look at <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\">https:\/\/dvc.org\/doc\/user-guide\/managing-external-data<\/a> and <a href=\"https:\/\/dvc.org\/doc\/user-guide\/external-dependencies\">https:\/\/dvc.org\/doc\/user-guide\/external-dependencies<\/a> . Those features are advanced but should give you exactly what you are looking for I think.<\/p>\n<p>Keep in mind that it works fine if you don\u2019t share the project and multiple users don\u2019t work simultaneously. Otherwise you would need to put some extra measures to make sure that everyone is using unique location bucket in S3 when they do <code>dvc pull<\/code>, etc.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/can-my-workspace-be-an-s3-location\/1083",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-02T10:11:47.669Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/zzztimbo\">@zzztimbo<\/a>!<br>\nAFAIK s3 is not a machine where you could run a terminal, and use <code>dvc pull<\/code> inside. It seems to me that what you want to do is to checkout the main branch of your repo and use <code>aws s3 cp {your_local_csv_path} s3:\/\/{path_on_s3}<\/code> in the recurring process that you are referring to.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-02T15:47:44.825Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/zzztimbo\">@zzztimbo<\/a> ! You might want to take a look at <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\">https:\/\/dvc.org\/doc\/user-guide\/managing-external-data<\/a> and <a href=\"https:\/\/dvc.org\/doc\/user-guide\/external-dependencies\">https:\/\/dvc.org\/doc\/user-guide\/external-dependencies<\/a> . Those features are advanced but should give you exactly what you are looking for I think.<\/p>\n<p>Keep in mind that it works fine if you don\u2019t share the project and multiple users don\u2019t work simultaneously. Otherwise you would need to put some extra measures to make sure that everyone is using unique location bucket in S3 when they do <code>dvc pull<\/code>, etc.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"workspac locat like run pull directli bucket possibl long stori short like end user version control csv file like recur process pull main branch directli bucket understand us storag like creat data ident structur git repositori",
        "Question_preprocessed_content":"workspac locat like run directli bucket possibl long stori short like end user version control csv file like recur process pull main branch directli bucket understand us storag like data ident structur git repositori",
        "Question_gpt_summary_original":"The user is facing a challenge of running \"dvc pull\" directly into an s3 bucket. They want to version control csv files using dvc and have a recurring process that will do a dvc pull of the main branch directly into an s3 bucket. The user wants to recreate the data in s3 in an identical structure as the dvc git repository.",
        "Question_gpt_summary":"user face challeng run pull directli bucket want version control csv file recur process pull main branch directli bucket user want recreat data ident structur git repositori",
        "Answer_original_content":"timbo afaik machin run termin us pull insid want checkout main branch repo us aw local csv path path recur process refer timbo want look http org doc user guid manag extern data http org doc user guid extern depend featur advanc exactli look think mind work fine dont share project multipl user dont work simultan need extra measur sure uniqu locat bucket pull",
        "Answer_preprocessed_content":"afaik machin run termin us insid want checkout main branch repo us recur process refer want look featur advanc exactli look think mind work fine dont share project multipl user dont work simultan need extra measur sure uniqu locat bucket",
        "Answer_gpt_summary_original":"Solutions provided:\n- Use \"aws s3 cp {your_local_csv_path} s3:\/\/{path_on_s3}\" instead of \"dvc pull\" in the recurring process.\n- Check out the external data management and external dependencies features of dvc, which should give the user what they are looking for.\n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"solut provid us aw local csv path path instead pull recur process check extern data manag extern depend featur user look person opinion bias includ respons"
    },
    {
        "Question_title":"How to display artifacts in AWS S3?",
        "Question_body":"Hi there,\n\n\nI'm using Mlflow 1.27.0 tracking server with docker, log db (MySQL), Artifacts storage (AWS S3). I'm trying to make model with log_params, metrics, artifacts and checked the functions worked correctly. Also, I checked my artifacts storage(AWS S3) whether artifacts are stored or not and there was no problem.\n\n\nBut, if i access tracking server and select specific log in experiment, there is no display about artifacts with tracking server's error.\n\u00a0\nSo, I want to know display artifacts in aws s3 is impossible or just environment error.\n\n\nHere are links refer to solve this problem.\n\n\nhttps:\/\/docs.databricks.com\/applications\/mlflow\/tracking.html\n\nhttps:\/\/stackoverflow.com\/questions\/72280328\/mlflow-artifacts-on-s3-but-not-in-ui\n\n\n\nBest regards,\nKwon",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659662931000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":15.0,
        "Answer_body":"I'm sorry. I solved...\n\n\nThe reason was region name in .aws\/credentials\n\n\n\n\n\n2022\ub144 8\uc6d4 5\uc77c \uae08\uc694\uc77c \uc624\ud6c4 2\uc2dc 28\ubd84 51\ucd08 UTC+9\uc5d0 \uad8c\uc724\uc7ac\ub2d8\uc774 \uc791\uc131:\n\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/D3K4imRGS20",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-05T02:28:28",
                "Answer_body":"I'm sorry. I solved...\n\n\nThe reason was region name in .aws\/credentials\n\n\n\n\n\n2022\ub144 8\uc6d4 5\uc77c \uae08\uc694\uc77c \uc624\ud6c4 2\uc2dc 28\ubd84 51\ucd08 UTC+9\uc5d0 \uad8c\uc724\uc7ac\ub2d8\uc774 \uc791\uc131:\n\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"displai artifact aw track server docker log mysql artifact storag aw try model log param metric artifact check function work correctli check artifact storag aw artifact store problem access track server select specif log experi displai artifact track server error want know displai artifact aw imposs environ error link refer solv problem http doc databrick com applic track html http stackoverflow com question artifact best regard kwon",
        "Question_preprocessed_content":"displai artifact aw track server docker log artifact storag try model metric artifact check function work correctli check artifact storag artifact store problem access track server select specif log experi displai artifact track server error want know displai artifact aw imposs environ error link refer solv problem best regard kwon",
        "Question_gpt_summary_original":"The user is facing a challenge with displaying artifacts in AWS S3 while using Mlflow 1.27.0 tracking server with docker, log db (MySQL), and Artifacts storage (AWS S3). The user has checked that the artifacts are stored correctly in AWS S3, but there is no display about artifacts with tracking server's error when accessing a specific log in the experiment. The user is seeking to know whether displaying artifacts in AWS S3 is impossible or just an environment error. The user has provided links to solve this problem.",
        "Question_gpt_summary":"user face challeng displai artifact aw track server docker log mysql artifact storag aw user check artifact store correctli aw displai artifact track server error access specif log experi user seek know displai artifact aw imposs environ error user provid link solv problem",
        "Answer_original_content":"sorri solv reason region aw credenti utc",
        "Answer_preprocessed_content":"sorri reason region utc",
        "Answer_gpt_summary_original":"Solution(s) provided: There is only one solution mentioned in the discussion, which is that the issue was caused by an incorrect region name in the user's .aws\/credentials file. No other solutions or suggestions were provided.",
        "Answer_gpt_summary":"solut provid solut mention discuss issu caus incorrect region user aw credenti file solut suggest provid"
    },
    {
        "Question_title":"How do handle overlapping segmentation masks?",
        "Question_body":"<p>In maskrcnn I\u2019m pretty sure it\u2019s valid to output overlapping segmentation masks, but I\u2019m not sure how to handle this in wandb Masks and I haven\u2019t been able to find an example. The documentation says each \u201cMask\u201d is a 2D numpy array filled with class ids, which implies one class per pixel, right?<\/p>\n<p>Is there a way to visualize the masks if they\u2019re overlapping?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1675707126695,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":33.0,
        "Answer_body":"<p>I was told by wandb support this was not possible, posting here for transparency<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-do-handle-overlapping-segmentation-masks\/3836",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-06T18:56:27.252Z",
                "Answer_body":"<p>I was told by wandb support this was not possible, posting here for transparency<\/p>",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1675709787252,
        "Question_original_content":"handl overlap segment mask maskrcnn pretti sure valid output overlap segment mask sure handl mask havent abl exampl document sai mask numpi arrai fill class id impli class pixel right wai visual mask theyr overlap",
        "Question_preprocessed_content":"handl overlap segment mask maskrcnn pretti sure valid output overlap segment mask sure handl mask havent abl exampl document sai mask numpi arrai fill class id impli class pixel right wai visual mask theyr overlap",
        "Question_gpt_summary_original":"The user is facing challenges in handling overlapping segmentation masks in maskrcnn and is unsure how to handle this in wandb Masks. The user is also seeking guidance on whether it is possible to visualize the masks if they are overlapping.",
        "Question_gpt_summary":"user face challeng handl overlap segment mask maskrcnn unsur handl mask user seek guidanc possibl visual mask overlap",
        "Answer_original_content":"told support possibl post transpar",
        "Answer_preprocessed_content":"told support possibl post transpar",
        "Answer_gpt_summary_original":"Solution: No solution was mentioned in the discussion.",
        "Answer_gpt_summary":"solut solut mention discuss"
    },
    {
        "Question_title":"How can I transfer a csv file on an Azure Machine Learning compute instance directory back to the Datastore?",
        "Question_body":"I posted a similar question last week and didn't get a response to that yet so I'm posting another one now.\n\nThe code below is what I use to pull data into the compute instance from the Datastore. I transfer data from a Datastore to the compute instance and then save the data to my directory as a csv. The data originates from a SCOPE script and is transferred from Cosmos to the Datastore via Azure Data Factory.\n\nOnce the data is in the directory as a csv, I then utilize R to pull in the data into an RStudio session and then I run various tasks that create new data sets. I also save these new data sets to the compute instance directory as csv's. These new data sets are the ones I'd like to push back to the Datastore so they can be transferred elsewhere via Azure Data Factory and later consumed by a PowerBI app we're looking to create.\n\nI tried using Designer and it ran for 4 days without completing before I cancelled the job and started looking for an alternative route. I don't know if it would have completed or if it ran into memory issues and simply didn't fail. When I pull data into the compute instance from the datastore it takes less than a few minutes to complete so I'm not sure why it would take Designer multiple days to attempt to do the reverse operation.\n\nI've looked through a bunch of documentation and I am not able to find anything that tells us how we can transfer data from the compute instance back to the Datastore aside from Designer which is too slow or unable to handle.\n\nThis task seems like one that should be obvious for use and a major selling point of Azure Machine Learning so I'm a bit dumbfounded to see that this is a challenge figuring out how to do and that the documentation doesn't clearly show users how to achieve this task, assuming it's even possible. If it's not possible then I need to figure out a whole new system to use to get my work done. If it's not possible, the Azure Machine Learning team should enable this functionality as soon as possible.\n\n# Azure management\nfrom azureml.core import Workspace, Dataset\n# MetaData\nsubscription_id = '09b5fdb3-165d-4e2b-8ca0-34f998d176d5'\nresource_group = 'xCloudData'\nworkspace_name = 'xCloudML'\n# Create workspace \nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n# 1. Retention_Engagement_CombinedData\ndataset = Dataset.get_by_name(workspace, name='retention-engagement-combineddata')\n# Save data to file\ndf = dataset.to_pandas_dataframe()\ndf.to_csv('\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/v-aantico1\/code\/RetentionEngagement_CombinedData.csv')\n# 2. TitleNameJoin\ndataset = Dataset.get_by_name(workspace, name='TitleForJoiningInR')\n# Save data to file\ndf = dataset.to_pandas_dataframe()\ndf.to_csv('\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/v-aantico1\/code\/TitleNameJoin.csv')",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1632158641093,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@AdrianAnticoTEKsystemsInc-1526 Have you tried the following to upload data to your datastore?\n\n from azureml.core import Workspace\n ws = Workspace.from_config()\n datastore = ws.get_default_datastore()\n    \n datastore.upload(src_dir='.\/data',\n                  target_path='datasets\/',\n                  overwrite=True)\n\n\n\nI think datastore.upload() should work for you to upload the required datafiles from your compute instance to datastore.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/559227\/how-can-i-transfer-a-csv-file-on-an-azure-machine.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-21T08:00:14.827Z",
                "Answer_score":1,
                "Answer_body":"@AdrianAnticoTEKsystemsInc-1526 Have you tried the following to upload data to your datastore?\n\n from azureml.core import Workspace\n ws = Workspace.from_config()\n datastore = ws.get_default_datastore()\n    \n datastore.upload(src_dir='.\/data',\n                  target_path='datasets\/',\n                  overwrite=True)\n\n\n\nI think datastore.upload() should work for you to upload the required datafiles from your compute instance to datastore.",
                "Answer_comment_count":4,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1632211214827,
        "Question_original_content":"transfer csv file comput instanc directori datastor post similar question week respons post code us pull data comput instanc datastor transfer data datastor comput instanc save data directori csv data origin scope script transfer cosmo datastor azur data factori data directori csv util pull data rstudio session run task creat new data set save new data set comput instanc directori csv new data set on like push datastor transfer azur data factori later consum powerbi app look creat tri design ran dai complet cancel job start look altern rout know complet ran memori issu simpli fail pull data comput instanc datastor take minut complet sure design multipl dai attempt revers oper look bunch document abl tell transfer data comput instanc datastor asid design slow unabl handl task like obviou us major sell point bit dumbfound challeng figur document clearli user achiev task assum possibl possibl need figur new us work possibl team enabl function soon possibl azur manag core import workspac dataset metadata subscript bfdb fdd resourc group xclouddata workspac xcloudml creat workspac workspac workspac subscript resourc group workspac retent engag combineddata dataset dataset workspac retent engag combineddata save data file dataset panda datafram csv mnt batch task share root mount cluster aantico code retentionengag combineddata csv titlenamejoin dataset dataset workspac titleforjoininginr save data file dataset panda datafram csv mnt batch task share root mount cluster aantico code titlenamejoin csv",
        "Question_preprocessed_content":"transfer csv file comput instanc directori datastor post similar question week respons post code us pull data comput instanc datastor transfer data datastor comput instanc save data directori csv data origin scope script transfer cosmo datastor azur data factori data directori csv util pull data rstudio session run task creat new data set save new data set comput instanc directori csv new data set on like push datastor transfer azur data factori later consum powerbi app look creat tri design ran dai complet cancel job start look altern rout know complet ran memori issu simpli fail pull data comput instanc datastor take minut complet sure design multipl dai attempt revers oper look bunch document abl tell transfer data comput instanc datastor asid design slow unabl handl task like obviou us major sell point bit dumbfound challeng figur document clearli user achiev task assum possibl possibl need figur new us work possibl team enabl function soon possibl azur manag core import workspac dataset metadata xclouddata xcloudml creat workspac workspac dataset save data file titlenamejoin dataset titleforjoininginr save data file",
        "Question_gpt_summary_original":"The user is facing challenges in transferring a CSV file from an Azure Machine Learning compute instance directory back to the Datastore. They have tried using Designer, but it took too long to complete. The user is unable to find any documentation that explains how to transfer data from the compute instance back to the Datastore, and they are considering using a different system if this is not possible.",
        "Question_gpt_summary":"user face challeng transfer csv file comput instanc directori datastor tri design took long complet user unabl document explain transfer data comput instanc datastor consid differ possibl",
        "Answer_original_content":"adriananticoteksystemsinc tri follow upload data datastor core import workspac workspac config datastor default datastor datastor upload src dir data target path dataset overwrit true think datastor upload work upload requir datafil comput instanc datastor",
        "Answer_preprocessed_content":"tri follow upload data datastor core import workspac datastor overwrit true think work upload requir datafil comput instanc datastor",
        "Answer_gpt_summary_original":"Solution: The user can use the `datastore.upload()` method to transfer the CSV file from the Azure Machine Learning compute instance directory back to the Datastore. The code snippet provided in the discussion can be used to upload the required data files from the compute instance to the Datastore.",
        "Answer_gpt_summary":"solut user us datastor upload method transfer csv file comput instanc directori datastor code snippet provid discuss upload requir data file comput instanc datastor"
    },
    {
        "Question_title":"What would stop credentials from validation on a ClearML server?",
        "Question_body":"<p>I've set up a ClearML server in GCP using the sub-domain approach. I can access all three domains (<code>https:\/\/app.clearml.mydomain.com<\/code>, <code>https:\/\/api.clearml.mydomain.com<\/code> and <code>https:\/\/files.clearml.mydomain.com<\/code>) in a browser and see what I think is the correct response, but when connecting with the python SDK via <code>clearml-init<\/code> I get the following error:<\/p>\n<pre><code>clearml.backend_api.session.session.LoginError: Failed getting token (error 400 from https:\/\/api.clearml.mydomain.com): Bad Request\n<\/code><\/pre>\n<p>Are there any likely causes of this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639763350487,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":194.0,
        "Answer_body":"<p>Following the discussion <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/517\" rel=\"nofollow noreferrer\">here<\/a>, it seemed that the load balancer being used was blocking <code>GET<\/code> requests with a payload which are used by ClearML. A <a href=\"https:\/\/github.com\/allegroai\/clearml\/pull\/521\" rel=\"nofollow noreferrer\">fix<\/a> is being worked on to allow the method to be changed to a <code>POST<\/code> request via an environment variable.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70397010",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1640162037740,
        "Question_original_content":"stop credenti valid server set server gcp sub domain approach access domain http app mydomain com http api mydomain com http file mydomain com browser think correct respons connect python sdk init follow error backend api session session loginerror fail get token error http api mydomain com bad request like caus error",
        "Question_preprocessed_content":"stop credenti valid server set server gcp approach access domain browser think correct respons connect python sdk follow error like caus error",
        "Question_gpt_summary_original":"The user is encountering an error while trying to connect to a ClearML server using the python SDK via clearml-init. The error message indicates that the credentials are not being validated due to a Bad Request error. The user is seeking assistance in identifying the likely causes of this error.",
        "Question_gpt_summary":"user encount error try connect server python sdk init error messag indic credenti valid bad request error user seek assist identifi like caus error",
        "Answer_original_content":"follow discuss load balanc block request payload fix work allow method chang post request environ variabl",
        "Answer_preprocessed_content":"follow discuss load balanc block request payload fix work allow method chang request environ variabl",
        "Answer_gpt_summary_original":"Solution: A fix is being worked on to allow the method to be changed to a POST request via an environment variable.",
        "Answer_gpt_summary":"solut fix work allow method chang post request environ variabl"
    },
    {
        "Question_title":"Run.get_context() gives the same run id",
        "Question_body":"<p>I am submitting the training through a script file. Following is the content of the <code>train.py<\/code> script. Azure ML is treating all these as one run (instead of run per alpha value as coded below) as <code>Run.get_context()<\/code> is returning the same Run id.<\/p>\n<p><strong>train.py<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.opendatasets import Diabetes\nfrom azureml.core import Run\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.externals import joblib\n\nimport math\nimport os\nimport logging\n\n# Load dataset\ndataset = Diabetes.get_tabular_dataset()\nprint(dataset.take(1))\n\ndf = dataset.to_pandas_dataframe()\ndf.describe()\n\n# Split X (independent variables) &amp; Y (target variable)\nx_df = df.dropna()      # Remove rows that have missing values\ny_df = x_df.pop(&quot;Y&quot;)    # Y is the label\/target variable\n\nx_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=66)\nprint('Original dataset size:', df.size)\nprint(&quot;Size after dropping 'na':&quot;, x_df.size)\nprint(&quot;Training split size: &quot;, x_train.size)\nprint(&quot;Test split size: &quot;, x_test.size)\n\n# Training\nalphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Define hyperparameters\n\n# Create and log interactive runs\n\noutput_dir = os.path.join(os.getcwd(), 'outputs')\n\nfor hyperparam_alpha in alphas:\n    # Get the experiment run context\n    run = Run.get_context()\n    print(&quot;Started run: &quot;, run.id)\n    run.log(&quot;train_split_size&quot;, x_train.size)\n    run.log(&quot;test_split_size&quot;, x_train.size)\n    run.log(&quot;alpha_value&quot;, hyperparam_alpha)\n\n    # Train\n    print(&quot;Train ...&quot;)\n    model = Ridge(hyperparam_alpha)\n    model.fit(X = x_train, y = y_train)\n    \n    # Predict\n    print(&quot;Predict ...&quot;)\n    y_pred = model.predict(X = x_test)\n\n    # Calculate &amp; log error\n    rmse = math.sqrt(mean_squared_error(y_true = y_test, y_pred = y_pred))\n    run.log(&quot;rmse&quot;, rmse)\n    print(&quot;rmse&quot;, rmse)\n\n    # Serialize the model to local directory\n    if not os.path.isdir(output_dir):\n        os.makedirs(output_dir, exist_ok=True) \n\n    print(&quot;Save model ...&quot;)\n    model_name = &quot;model_alpha_&quot; + str(hyperparam_alpha) + &quot;.pkl&quot; # Pickle file\n    file_path = os.path.join(output_dir, model_name)\n    joblib.dump(value = model, filename = file_path)\n\n    # Upload the model\n    run.upload_file(name = model_name, path_or_stream = file_path)\n\n    # Complete the run\n    run.complete()\n<\/code><\/pre>\n<p><strong>Experiments view<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/E6xAG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E6xAG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Authoring code (i.e. control plane)<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom azureml.core import Workspace, Experiment, RunConfiguration, ScriptRunConfig, VERSION, Run\n\nws = Workspace.from_config()\nexp = Experiment(workspace = ws, name = &quot;diabetes-local-script-file&quot;)\n\n# Create new run config obj\nrun_local_config = RunConfiguration()\n\n# This means that when we run locally, all dependencies are already provided.\nrun_local_config.environment.python.user_managed_dependencies = True\n\n# Create new script config\nscript_run_cfg = ScriptRunConfig(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    script = 'train.py',\n    run_config = run_local_config) \n\nrun = exp.submit(script_run_cfg)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1599411710610,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":1599438192360,
        "Question_score":3.0,
        "Question_view_count":2523.0,
        "Answer_body":"<h2>Short Answer<\/h2>\n<h3>Option 1: create child runs within run<\/h3>\n<p><code>run = Run.get_context()<\/code> assigns the run object of the run that you're currently in to <code>run<\/code>. So in every iteration of the hyperparameter search, you're logging to the same run. To solve this, you need to create child (or sub-) runs for each hyperparameter value. You can do this with <code>run.child_run()<\/code>. Below is the template for making this happen.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>run = Run.get_context()\n\nfor hyperparam_alpha in alphas:\n    # Get the experiment run context\n    run_child = run.child_run()\n    print(&quot;Started run: &quot;, run_child.id)\n    run_child.log(&quot;train_split_size&quot;, x_train.size)\n<\/code><\/pre>\n<p>On the <code>diabetes-local-script-file<\/code> Experiment page, you can see that Run <code>9<\/code> was the parent run and Runs <code>10-19<\/code> were the child runs if you click &quot;Include child runs&quot; page. There is also a &quot;Child runs&quot; tab on Run 9 details page.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5IMcz.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5IMcz.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Long answer<\/h2>\n<p>I highly recommend abstracting the hyperparameter search away from the data plane (i.e. <code>train.py<\/code>) and into the control plane (i.e. &quot;authoring code&quot;). This becomes especially valuable as training time increases and you can arbitrarily parallelize and also choose Hyperparameters more intelligently by using Azure ML's <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters\" rel=\"noreferrer\"><code>Hyperdrive<\/code><\/a>.<\/p>\n<h3>Option 2 Create runs from control plane<\/h3>\n<p>Remove the loop from your code, add the code like below (<a href=\"https:\/\/gist.github.com\/swanderz\/f5c0dc1aefc796d37f6bc3600f2ae3cd\" rel=\"noreferrer\">full data and control here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import argparse\nfrom pprint import pprint\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--alpha', type=float, default=0.5)\nargs = parser.parse_args()\nprint(&quot;all args:&quot;)\npprint(vars(args))\n\n# use the variable like this\nmodel = Ridge(args.alpha)\n<\/code><\/pre>\n<p>below is how to submit a single run using a script argument. To submit multiple runs, just use a loop in the control plane.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Define hyperparameters\n\nlist_rcs = [ScriptRunConfig(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    script = 'train.py',\n    arguments=['--alpha',a],\n    run_config = run_local_config) for a in alphas]\n\nlist_runs = [exp.submit(rc) for rc in list_rcs]\n\n<\/code><\/pre>\n<h3>Option 3 Hyperdrive (IMHO the recommended approach)<\/h3>\n<p>In this way you outsource the hyperparameter source to <code>Hyperdrive<\/code>. The UI will also report results exactly how you want them, and via the API you can easily download the best model.  Note you can't use this locally anymore and must use <code>AMLCompute<\/code>, but to me it is a worthwhile trade-off.<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters#configure-experiment\" rel=\"noreferrer\">This is a great overview<\/a>. Excerpt below (<a href=\"https:\/\/gist.github.com\/swanderz\/f5c0dc1aefc796d37f6bc3600f2ae3cd#file-hyperdrive-ipynb\" rel=\"noreferrer\">full code here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>param_sampling = GridParameterSampling( {\n        &quot;alpha&quot;: choice(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)\n    }\n)\n\nestimator = Estimator(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    entry_script = 'train.py',\n    compute_target=cpu_cluster,\n    environment_definition=Environment.get(workspace=ws, name=&quot;AzureML-Tutorial&quot;)\n)\n\nhyperdrive_run_config = HyperDriveConfig(estimator=estimator,\n                          hyperparameter_sampling=param_sampling, \n                          policy=None,\n                          primary_metric_name=&quot;rmse&quot;, \n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                          max_total_runs=10,\n                          max_concurrent_runs=4)\n\nrun = exp.submit(hyperdrive_run_config)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q1AhJ.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q1AhJ.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63766714",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1599436122230,
        "Question_original_content":"run context give run submit train script file follow content train script treat run instead run alpha valu code run context return run train opendataset import diabet core import run sklearn model select import train test split sklearn linear model import ridg sklearn metric import mean squar error sklearn extern import joblib import math import import log load dataset dataset diabet tabular dataset print dataset dataset panda datafram split independ variabl target variabl dropna remov row miss valu pop label target variabl train test train test train test split test size random state print origin dataset size size print size drop size print train split size train size print test split size test size train alpha defin hyperparamet creat log interact run output dir path join getcwd output hyperparam alpha alpha experi run context run run context print start run run run log train split size train size run log test split size train size run log alpha valu hyperparam alpha train print train model ridg hyperparam alpha model fit train train predict print predict pred model predict test calcul log error rmse math sqrt mean squar error true test pred pred run log rmse rmse print rmse rmse serial model local directori path isdir output dir makedir output dir exist true print save model model model alpha str hyperparam alpha pkl pickl file file path path join output dir model joblib dump valu model filenam file path upload model run upload file model path stream file path complet run run complet experi view author code control plane import core import workspac experi runconfigur scriptrunconfig version run workspac config exp experi workspac diabet local script file creat new run config obj run local config runconfigur mean run local depend provid run local config environ python user manag depend true creat new script config script run cfg scriptrunconfig sourc directori path join getcwd code script train run config run local config run exp submit script run cfg run wait complet output true",
        "Question_preprocessed_content":"give run submit train script file follow content script treat run return run experi view author code",
        "Question_gpt_summary_original":"The user is encountering a challenge where Azure ML is treating all runs as one run instead of one run per alpha value as coded, as Run.get_context() is returning the same Run id.",
        "Question_gpt_summary":"user encount challeng treat run run instead run alpha valu code run context return run",
        "Answer_original_content":"short answer option creat child run run run run context assign run object run current run iter hyperparamet search log run solv need creat child sub run hyperparamet valu run child run templat make happen run run context hyperparam alpha alpha experi run context run child run child run print start run run child run child log train split size train size diabet local script file experi page run parent run run child run click includ child run page child run tab run detail page long answer highli recommend abstract hyperparamet search awai data plane train control plane author code especi valuabl train time increas arbitrarili parallel choos hyperparamet intellig hyperdr option creat run control plane remov loop code add code like data control import argpars pprint import pprint parser argpars argumentpars parser add argument alpha type float default arg parser pars arg print arg pprint var arg us variabl like model ridg arg alpha submit singl run script argument submit multipl run us loop control plane alpha defin hyperparamet list rc scriptrunconfig sourc directori path join getcwd code script train argument alpha run config run local config alpha list run exp submit list rc option hyperdr imho recommend approach wai outsourc hyperparamet sourc hyperdr report result exactli want api easili download best model note us local anymor us amlcomput worthwhil trade great overview excerpt code param sampl gridparametersampl alpha choic estim estim sourc directori path join getcwd code entri script train comput target cpu cluster environ definit environ workspac tutori hyperdr run config hyperdriveconfig estim estim hyperparamet sampl param sampl polici primari metric rmse primari metric goal primarymetricgo maxim max total run max concurr run run exp submit hyperdr run config run wait complet output true",
        "Answer_preprocessed_content":"short answer option creat child run run assign run object run current iter hyperparamet search log run solv need creat child run hyperparamet valu templat make happen experi page run parent run run child run click includ child run page child run tab run detail page long answer highli recommend abstract hyperparamet search awai data plane control plane especi valuabl train time increas arbitrarili parallel choos hyperparamet intellig option creat run control plane remov loop code add code like submit singl run script argument submit multipl run us loop control plane option hyperdr wai outsourc hyperparamet sourc report result exactli want api easili download best model note us local anymor us worthwhil great overview excerpt",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Create child runs within run: To solve the issue, the user needs to create child runs for each hyperparameter value using `run.child_run()`.\n2. Create runs from the control plane: The user can remove the loop from the code and add the code to submit a single run using a script argument. To submit multiple runs, the user can use a loop in the control plane.\n3. Use Hyperdrive: The user can outsource the hyperparameter source to Hyperdrive. The UI will report results exactly how the user wants them, and via the API, the user can easily download the best model. The user can use GridParameterSampling to define the hyperparameters and submit the HyperDrive",
        "Answer_gpt_summary":"possibl solut mention discuss creat child run run solv issu user need creat child run hyperparamet valu run child run creat run control plane user remov loop code add code submit singl run script argument submit multipl run user us loop control plane us hyperdr user outsourc hyperparamet sourc hyperdr report result exactli user want api user easili download best model user us gridparametersampl defin hyperparamet submit hyperdr"
    },
    {
        "Question_title":"Invoking SageMaker Endpoint for PyTorch Model",
        "Question_body":"<p>I'm trying to call my SageMaker model endpoint both from Postman and the AWS CLI. The endpoint's status is &quot;in service&quot; but whenever I try to call it it gives me an error. When I try to use the predict function in the SageMaker notebook and provide it a numpy array (ex. <code>np.array([1,2,3,4])<\/code>), it successfully gives me an output. I'm unsure what I'm doing wrong.<\/p>\n<pre><code>$ aws2 sagemaker-runtime invoke-endpoint \\\n$ --endpoint-name=pytorch-model \\\n$ --body=1,2 \\\n$ --content-type=text\/csv \\\n$ --cli-binary-format=raw-in-base64-out \\\n$ output.json\n\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message &quot;tensors used as indices must be long, byte or bool tensors\nTraceback (most recent call last):\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py&quot;, line 125, in transform\n    result = self._transform_fn(self._model, input_data, content_type, accept)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py&quot;, line 215, in _default_transform_fn\n    prediction = self._predict_fn(data, model)\n  File &quot;\/opt\/ml\/model\/code\/pytorch-model-reco.py&quot;, line 268, in predict_fn\n    return torch.argsort(- final_matrix[input_data, :], dim = 1)\nIndexError: tensors used as indices must be long, byte or bool tensors\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596530497410,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":394.0,
        "Answer_body":"<p>The clue is in the final few lines of your stacktrace:<\/p>\n<pre><code>  File &quot;\/opt\/ml\/model\/code\/pytorch-model-reco.py&quot;, line 268, in predict_fn\n    return torch.argsort(- final_matrix[input_data, :], dim = 1)\nIndexError: tensors used as indices must be long, byte or bool tensors\n<\/code><\/pre>\n<p>In your <code>predict_fn<\/code> in <code>pytorch-model-reco.py<\/code> on line 268, you're trying to use <code>input_data<\/code> as indices for <code>final_matrix<\/code>, but <code>input_data<\/code> is the wrong type.<\/p>\n<p>I would guess there is some type casting that your <code>predict_fn<\/code> should be doing when the input type is <code>text\/csv<\/code>. This type casting is happening outside of the <code>predict_fn<\/code> when your input type is numpy data. Taking a look at the <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\/tree\/master\/src\/sagemaker_inference\" rel=\"nofollow noreferrer\"><code>sagemaker_inference<\/code><\/a> source code might reveal more.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63243154",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1596555829307,
        "Question_original_content":"invok endpoint pytorch model try model endpoint postman aw cli endpoint statu servic try give error try us predict function notebook provid numpi arrai arrai successfulli give output unsur wrong aw runtim invok endpoint endpoint pytorch model bodi content type text csv cli binari format raw base output json error occur modelerror call invokeendpoint oper receiv server error model messag tensor indic long byte bool tensor traceback recent file opt conda lib python site packag infer transform line transform result self transform self model input data content type accept file opt conda lib python site packag infer transform line default transform predict self predict data model file opt model code pytorch model reco line predict return torch argsort final matrix input data dim indexerror tensor indic long byte bool tensor",
        "Question_preprocessed_content":"invok endpoint pytorch model try model endpoint postman aw cli endpoint statu servic try give error try us predict function notebook provid numpi arrai successfulli give output unsur wrong",
        "Question_gpt_summary_original":"The user is encountering challenges while trying to call their SageMaker model endpoint from Postman and AWS CLI. Although the endpoint's status is \"in service,\" it gives an error whenever the user tries to call it. However, when the user uses the predict function in the SageMaker notebook and provides it with a numpy array, it successfully gives an output. The error message received while invoking the endpoint indicates that tensors used as indices must be long, byte, or bool tensors.",
        "Question_gpt_summary":"user encount challeng try model endpoint postman aw cli endpoint statu servic give error user tri user us predict function notebook provid numpi arrai successfulli give output error messag receiv invok endpoint indic tensor indic long byte bool tensor",
        "Answer_original_content":"clue final line stacktrac file opt model code pytorch model reco line predict return torch argsort final matrix input data dim indexerror tensor indic long byte bool tensor predict pytorch model reco line try us input data indic final matrix input data wrong type guess type cast predict input type text csv type cast happen outsid predict input type numpi data take look infer sourc code reveal",
        "Answer_preprocessed_content":"clue final line stacktrac line try us indic wrong type guess type cast input type type cast happen outsid input type numpi data take look sourc code reveal",
        "Answer_gpt_summary_original":"Solution: The error message received while invoking the endpoint indicates that tensors used as indices must be long, byte, or bool tensors. The solution is to do some type casting that the predict_fn should be doing when the input type is text\/csv. This type casting is happening outside of the predict_fn when the input type is numpy data. Taking a look at the sagemaker_inference source code might reveal more.",
        "Answer_gpt_summary":"solut error messag receiv invok endpoint indic tensor indic long byte bool tensor solut type cast predict input type text csv type cast happen outsid predict input type numpi data take look infer sourc code reveal"
    },
    {
        "Question_title":"After selecting the \"Edit column\" button of the \"Select Columns in Dataset\" module in Designer, it will be stuck in the \"loading\" state.",
        "Question_body":"Hello everyone!\n\nI am taking the Create a Regression Model with Azure Machine Learning designer course in Microsoft Learn. When I perform the steps in the Explore Data section, after selecting the \"Edit column\" button of the \"Select Columns in Dataset\" module in Designer, it will be stuck in the \"loading\" state. Therefore, I cannot proceed to the next step.\n\n\n\n\n\nThank you very much!\n\nBest regards,\nLing",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1617523186537,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@KaiXiuGao This issue is now fixed in all regions and it does not require an additional parameter to be added to the URL. Please try and let us know if it works fine.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/343427\/after-selecting-the-34edit-column34-button-of-the.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-06T12:12:52.38Z",
                "Answer_score":1,
                "Answer_body":"@KaiXiuGao This issue is now fixed in all regions and it does not require an additional parameter to be added to the URL. Please try and let us know if it works fine.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1617711172380,
        "Question_original_content":"select edit column button select column dataset modul design stuck load state hello take creat regress model design cours microsoft learn perform step explor data section select edit column button select column dataset modul design stuck load state proce step thank best regard ling",
        "Question_preprocessed_content":"select edit column button select column dataset modul design stuck load state hello take creat regress model design cours microsoft learn perform step explor data section select edit column button select column dataset modul design stuck load state proce step thank best regard ling",
        "Question_gpt_summary_original":"The user is encountering a challenge where the \"Edit column\" button of the \"Select Columns in Dataset\" module in Designer is stuck in the \"loading\" state, preventing them from proceeding to the next step in the Create a Regression Model with Azure Machine Learning designer course in Microsoft Learn.",
        "Question_gpt_summary":"user encount challeng edit column button select column dataset modul design stuck load state prevent proceed step creat regress model design cours microsoft learn",
        "Answer_original_content":"kaixiugao issu fix region requir addit paramet ad url try let know work fine",
        "Answer_preprocessed_content":"issu fix region requir addit paramet ad url try let know work fine",
        "Answer_gpt_summary_original":"Solution: The issue with the \"Edit column\" button being stuck in the \"loading\" state has been fixed in all regions and does not require any additional parameter to be added to the URL. The user is advised to try again and see if the issue has been resolved.",
        "Answer_gpt_summary":"solut issu edit column button stuck load state fix region requir addit paramet ad url user advis try issu resolv"
    },
    {
        "Question_title":"Render hyperlink in pandas df in jupyterlab",
        "Question_body":"<p>I am trying to render a url inside a pandas dataframe output.  I followed along with some of the other examples out there, here is my implementation:<\/p>\n\n<pre><code>def create_url(product_name):\n    search = 'http:\/\/www.example.com\/search'\n    url = 'http:\/\/www.example.com\/search\/'+product_name\n    return url\n\ndef make_clickable(url):\n    return '&lt;a target=\"_blank\" href=\"{}\"&gt;{}&lt;\/a&gt;'.format(url, url)\n\n...\n\ndf['url'] = df['product_name'].apply(format_url)\ndf.style.format({'url': make_clickable})\n<\/code><\/pre>\n\n<p>This produces a correctly formatted raw text hyperlink, however its not clickable within the output.<\/p>\n\n<p>I should add that I'm doing this in an AWS sagemaker jupyterlab notebook which potentially disables hyperlinking in the output.  Not sure how I would check that though.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1557848494950,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":241.0,
        "Answer_body":"<p>If this doesn't work, I'm guessing it's an AWS thing<\/p>\n\n<ol>\n<li><code>IPython.display.HTML<\/code><\/li>\n<li><code>pandas.DataFrame.to_html<\/code> with <code>escape=False<\/code><\/li>\n<li><code>pandas.set_option('display.max_colwidth', 2000)<\/code> Must be a large number to accommodate length of link tag.  I'll say that I think this is broken.  It shouldn't be necessary to set <code>'display.max_colwidth'<\/code> in order to make sure <code>to_html<\/code> outputs properly.  But it is :-\/<\/li>\n<\/ol>\n\n<hr>\n\n<pre><code>from IPython import display\n\npd.set_option('display.max_colwidth', 2000)\n\ndisplay.HTML(df.assign(url=[*map(make_clickable, df.url)]).to_html(escape=False))\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YcVj6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YcVj6.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1557931098116,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56134165",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1557849422180,
        "Question_original_content":"render hyperlink panda jupyterlab try render url insid panda datafram output follow exampl implement def creat url product search http exampl com search url http exampl com search product return url def clickabl url return format url url url product appli format url style format url clickabl produc correctli format raw text hyperlink clickabl output add jupyterlab notebook potenti disabl hyperlink output sure check",
        "Question_preprocessed_content":"render hyperlink panda jupyterlab try render url insid panda datafram output follow exampl implement produc correctli format raw text hyperlink clickabl output add jupyterlab notebook potenti disabl hyperlink output sure check",
        "Question_gpt_summary_original":"The user is facing a challenge in rendering a clickable hyperlink in a pandas dataframe output in JupyterLab. The user has implemented a function to create a URL and another function to make the URL clickable, but the hyperlink is not clickable in the output. The user suspects that the issue may be due to AWS Sagemaker JupyterLab notebook potentially disabling hyperlinking in the output.",
        "Question_gpt_summary":"user face challeng render clickabl hyperlink panda datafram output jupyterlab user implement function creat url function url clickabl hyperlink clickabl output user suspect issu jupyterlab notebook potenti disabl hyperlink output",
        "Answer_original_content":"work guess aw thing ipython displai html panda datafram html escap fals panda set option displai max colwidth larg number accommod length link tag think broken shouldn necessari set displai max colwidth order sure html output properli ipython import displai set option displai max colwidth displai html assign url map clickabl url html escap fals",
        "Answer_preprocessed_content":"work guess aw thing larg number accommod length link tag think broken shouldn necessari set order sure output properli",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Using `IPython.display.HTML` to render the dataframe output as HTML and make the hyperlink clickable.\n2. Using `pandas.DataFrame.to_html` with `escape=False` to render the dataframe output as HTML and make the hyperlink clickable.\n3. Setting `pandas.set_option('display.max_colwidth', 2000)` to a large number to accommodate the length of the link tag.\n\nAll of these solutions involve rendering the dataframe output as HTML and making the hyperlink clickable. The user suspects that the issue may be due to AWS Sagemaker JupyterLab notebook potentially disabling hyperlinking in the output.",
        "Answer_gpt_summary":"possibl solut mention discuss ipython displai html render datafram output html hyperlink clickabl panda datafram html escap fals render datafram output html hyperlink clickabl set panda set option displai max colwidth larg number accommod length link tag solut involv render datafram output html make hyperlink clickabl user suspect issu jupyterlab notebook potenti disabl hyperlink output"
    },
    {
        "Question_title":"Rerun a deleted run in wandb sweep",
        "Question_body":"<p>Hi,<\/p>\n<p>Assuming I have a sweep of runs. For some reason, I wanna rerun a few of the runs. So I go ahead and delete those runs in the dashboard. But then even if I rerun the command (<code>wandb agent ...<\/code>), wandb is not able to rerun those runs. It will show all runs have been completed. Could wandb add the feature to rerun the runs that are not in the dashboards (for example, those that are deleted)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1676042396664,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":29.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/taochen\">@taochen<\/a>, rerunning deleted runs of a sweep is supported for grid search only. Please see the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/faq#can-i-rerun-a-grid-search\">following guide<\/a> on the steps to take to execute correctly. If you find that does not work for you, provide a link to your workspace and we\u2019ll take a closer look.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/rerun-a-deleted-run-in-wandb-sweep\/3860",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-10T23:12:55.536Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/taochen\">@taochen<\/a>, rerunning deleted runs of a sweep is supported for grid search only. Please see the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/faq#can-i-rerun-a-grid-search\">following guide<\/a> on the steps to take to execute correctly. If you find that does not work for you, provide a link to your workspace and we\u2019ll take a closer look.<\/p>",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1676070775536,
        "Question_original_content":"rerun delet run sweep assum sweep run reason wanna rerun run ahead delet run dashboard rerun command agent abl rerun run run complet add featur rerun run dashboard exampl delet",
        "Question_preprocessed_content":"rerun delet run sweep assum sweep run reason wanna rerun run ahead delet run dashboard rerun command abl rerun run run complet add featur rerun run dashboard",
        "Question_gpt_summary_original":"The user encountered a challenge with rerunning deleted runs in a wandb sweep. After deleting some runs in the dashboard, the user was unable to rerun those runs using the wandb agent command as wandb showed all runs as completed. The user suggests adding a feature to rerun deleted runs.",
        "Question_gpt_summary":"user encount challeng rerun delet run sweep delet run dashboard user unabl rerun run agent command show run complet user suggest ad featur rerun delet run",
        "Answer_original_content":"taochen rerun delet run sweep support grid search follow guid step execut correctli work provid link workspac closer look",
        "Answer_preprocessed_content":"rerun delet run sweep support grid search follow guid step execut correctli work provid link workspac closer look",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that rerunning deleted runs of a sweep is supported for grid search only. The user is directed to follow the steps provided in the guide on how to execute it correctly. If the suggested solution does not work, the user is advised to provide a link to their workspace for further assistance.",
        "Answer_gpt_summary":"solut discuss suggest rerun delet run sweep support grid search user direct follow step provid guid execut correctli suggest solut work user advis provid link workspac assist"
    },
    {
        "Question_title":"How to manage when remote is being updated",
        "Question_body":"<p>Hello community,<br>\nI am new to data version control. All the examples I see talk about how to update the remote if you update the dataset in the data folder in your local working environment. My question is the following:<\/p>\n<p>suppose I have a remote with 100 images and I pull. That command will copy those files to local, how do I correctly version those 100 images that I have in a remote folder?<br>\nThen, let\u2019s suppose someone adds 100 more images to the remote, getting 200. How do I indicate again that this is the second version of the dataset?<\/p>\n<p>If the data were in local, it would be more or less clear. However, I\u2019m not sure how to do it when the cycle starts on the remote.<\/p>\n<p>Best regards and thank you very much!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1621929845821,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":225.0,
        "Answer_body":"<p>DVC is heavily integrated with version control systems, so when you add some data and then push it to the remote you also need to commit the changes on the tracked DVC file. When the other person adds new data, they will also do the same and update the DVC file. This is the base versioning system, so when you checkout to the previous revision and pull you\u2019d see only the files from that revision.<\/p>. <p>I don\u2019t think I have explained my question properly.<br>\nThe question is if when I start the project there are already 100 images copied to the remote (without any previous DVC process), does it make sense to connect to a remote that already has data?<br>\nOn the other hand, the addition of new data, is if 100 more images are copied (without DVC push) to the remote. How to manage those 100 untracked files.<\/p>\n<p>Thanks for the answer<\/p>. <aside class=\"quote no-group\" data-username=\"jnaranjo\" data-post=\"3\" data-topic=\"760\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/j\/c67d28\/40.png\" class=\"avatar\"> jnaranjo:<\/div>\n<blockquote>\n<p>On the other hand, the addition of new data, is if 100 more images are copied (without DVC push) to the remote. How to manage those 100 untracked files.<\/p>\n<\/blockquote>\n<\/aside>\n<p>I see. DVC doesn\u2019t just copy the files as is and manage them in that way, but rather use it\u2019s own structure (which is an implementation detail). So it would be better if you could designate a different place as the remote storage unit, and then let the DVC add those files using <code>to-remote<\/code> functionality. This way, DVC will transfer the data from the actual storage where it is kept unversioned into the remote storage that it manages and create a DVC file for further tracking. See: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage\">https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage<\/a><\/p>. <p>Thank you very much for your reply.<br>\nYes, that\u2019s what I was thinking. Have the files uploaded to a different site and then transfer them with push to a remote versioned one.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-manage-when-remote-is-being-updated\/760",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-25T08:33:42.484Z",
                "Answer_body":"<p>DVC is heavily integrated with version control systems, so when you add some data and then push it to the remote you also need to commit the changes on the tracked DVC file. When the other person adds new data, they will also do the same and update the DVC file. This is the base versioning system, so when you checkout to the previous revision and pull you\u2019d see only the files from that revision.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-25T08:45:22.733Z",
                "Answer_body":"<p>I don\u2019t think I have explained my question properly.<br>\nThe question is if when I start the project there are already 100 images copied to the remote (without any previous DVC process), does it make sense to connect to a remote that already has data?<br>\nOn the other hand, the addition of new data, is if 100 more images are copied (without DVC push) to the remote. How to manage those 100 untracked files.<\/p>\n<p>Thanks for the answer<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-25T08:49:13.229Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"jnaranjo\" data-post=\"3\" data-topic=\"760\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/j\/c67d28\/40.png\" class=\"avatar\"> jnaranjo:<\/div>\n<blockquote>\n<p>On the other hand, the addition of new data, is if 100 more images are copied (without DVC push) to the remote. How to manage those 100 untracked files.<\/p>\n<\/blockquote>\n<\/aside>\n<p>I see. DVC doesn\u2019t just copy the files as is and manage them in that way, but rather use it\u2019s own structure (which is an implementation detail). So it would be better if you could designate a different place as the remote storage unit, and then let the DVC add those files using <code>to-remote<\/code> functionality. This way, DVC will transfer the data from the actual storage where it is kept unversioned into the remote storage that it manages and create a DVC file for further tracking. See: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage\">https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage<\/a><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-25T09:28:45.336Z",
                "Answer_body":"<p>Thank you very much for your reply.<br>\nYes, that\u2019s what I was thinking. Have the files uploaded to a different site and then transfer them with push to a remote versioned one.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"manag remot updat hello commun new data version control exampl talk updat remot updat dataset data folder local work environ question follow suppos remot imag pull command copi file local correctli version imag remot folder let suppos add imag remot get indic second version dataset data local clear sure cycl start remot best regard thank",
        "Question_preprocessed_content":"manag remot updat hello commun new data version control exampl talk updat remot updat dataset data folder local work environ question follow suppos remot imag pull command copi file local correctli version imag remot folder let suppos add imag remot get indic second version dataset data local clear sure cycl start remot best regard thank",
        "Question_gpt_summary_original":"The user is new to data version control and is facing challenges in managing updates to a remote dataset. They are unsure how to version the 100 images they have in a remote folder and how to indicate the second version of the dataset when someone adds 100 more images to the remote. The user is finding it difficult to manage updates when the cycle starts on the remote.",
        "Question_gpt_summary":"user new data version control face challeng manag updat remot dataset unsur version imag remot folder indic second version dataset add imag remot user find difficult manag updat cycl start remot",
        "Answer_original_content":"heavili integr version control system add data push remot need commit chang track file person add new data updat file base version checkout previou revis pull youd file revis dont think explain question properli question start project imag copi remot previou process sens connect remot data hand addit new data imag copi push remot manag untrack file thank answer jnaranjo hand addit new data imag copi push remot manag untrack file doesnt copi file manag wai us structur implement better design differ place remot storag unit let add file remot function wai transfer data actual storag kept unvers remot storag manag creat file track http org doc command refer add exampl transfer remot storag thank repli ye that think file upload differ site transfer push remot version",
        "Answer_preprocessed_content":"heavili integr version control system add data push remot need commit chang track file person add new data updat file base version checkout previou revis pull youd file revis dont think explain question properli question start project imag copi remot sens connect remot data hand addit new data imag copi remot manag untrack file thank answer jnaranjo hand addit new data imag copi remot manag untrack file doesnt copi file manag wai us structur better design differ place remot storag unit let add file function wai transfer data actual storag kept unvers remot storag manag creat file track thank repli ye that think file upload differ site transfer push remot version",
        "Answer_gpt_summary_original":"Solution:\n- DVC is heavily integrated with version control systems, so when you add some data and then push it to the remote you also need to commit the changes on the tracked DVC file.\n- Designate a different place as the remote storage unit, and then let the DVC add those files using `to-remote` functionality. This way, DVC will transfer the data from the actual storage where it is kept unversioned into the remote storage that it manages and create a DVC file for further tracking.",
        "Answer_gpt_summary":"solut heavili integr version control system add data push remot need commit chang track file design differ place remot storag unit let add file remot function wai transfer data actual storag kept unvers remot storag manag creat file track"
    },
    {
        "Question_title":"Sagemaker: DeepAR Hyperparameter Tuning Error",
        "Question_body":"<p>Running into a new issue with tuning DeepAR on SageMaker when trying to initialize a hyperparameter tuning job - this error also occurs when calling the test:mean_wQuantileLoss. I've upgraded the sagemaker package, restarted my instance, restarted the kernel (using a juptyer notebook), and yet the problem persists. <\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the \nCreateHyperParameterTuningJob operation: The objective metric type, [Maximize], that you specified for objective metric, [test:RMSE], isn\u2019t valid for the [156387875391.dkr.ecr.us-west-2.amazonaws.com\/forecasting-deepar:1] algorithm. Choose a valid objective metric type.\n<\/code><\/pre>\n\n<p>Code:<\/p>\n\n<pre><code>my_tuner = HyperparameterTuner(estimator=estimator,\n                               objective_metric_name=\"test:RMSE\",\n                               hyperparameter_ranges=hyperparams,\n                               max_jobs=20,\n                               max_parallel_jobs=2)\n\n# Start hyperparameter tuning job\nmy_tuner.fit(inputs=data_channels)\n\nStack Trace:\nClientError                               Traceback (most recent call last)\n&lt;ipython-input-66-9d6d8de89536&gt; in &lt;module&gt;()\n      7 \n      8 # Start hyperparameter tuning job\n----&gt; 9 my_tuner.fit(inputs=data_channels)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in fit(self, inputs, job_name, include_cls_metadata, **kwargs)\n    255 \n    256         self._prepare_for_training(job_name=job_name, include_cls_metadata=include_cls_metadata)\n--&gt; 257         self.latest_tuning_job = _TuningJob.start_new(self, inputs)\n    258 \n    259     @classmethod\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in start_new(cls, tuner, inputs)\n    525                                                output_config=(config['output_config']),\n    526                                                resource_config=(config['resource_config']),\n--&gt; 527                                                stop_condition=(config['stop_condition']), tags=tuner.tags)\n    528 \n    529         return cls(tuner.sagemaker_session, tuner._current_job_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in tune(self, job_name, strategy, objective_type, objective_metric_name, max_jobs, max_parallel_jobs, parameter_ranges, static_hyperparameters, image, input_mode, metric_definitions, role, input_config, output_config, resource_config, stop_condition, tags)\n    348         LOGGER.info('Creating hyperparameter tuning job with name: {}'.format(job_name))\n    349         LOGGER.debug('tune request: {}'.format(json.dumps(tune_request, indent=4)))\n--&gt; 350         self.sagemaker_client.create_hyper_parameter_tuning_job(**tune_request)\n    351 \n    352     def stop_tuning_job(self, name):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    312                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    313             # The \"self\" in this scope is referring to the BaseClient.\n--&gt; 314             return self._make_api_call(operation_name, kwargs)\n    315 \n    316         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    610             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n    611             error_class = self.exceptions.from_code(error_code)\n--&gt; 612             raise error_class(parsed_response, operation_name)\n    613         else:\n    614             return parsed_response\n\nClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: \nThe objective metric type, [Maximize], that you specified for objective metric, [test:RMSE], isn\u2019t valid for the [156387875391.dkr.ecr.us-west-2.amazonaws.com\/forecasting-deepar:1] algorithm. \nChoose a valid objective metric type.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1533924861517,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":1355.0,
        "Answer_body":"<p>It looks like you are trying to maximize this metric, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar-tuning.html\" rel=\"nofollow noreferrer\">test:RMSE can only be minimized<\/a> by SageMaker HyperParameter Tuning. <\/p>\n\n<p>To achieve this in the SageMaker Python SDK, create your HyperparameterTuner with objective_type='Minimize'. You can see the signature of the init method <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tuner.py#L158\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Here is the change you should make to your call to HyperparameterTuner:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>my_tuner = HyperparameterTuner(estimator=estimator,\n                               objective_metric_name=\"test:RMSE\",\n                               objective_type='Minimize',\n                               hyperparameter_ranges=hyperparams,\n                               max_jobs=20,\n                               max_parallel_jobs=2)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51792005",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1533929944952,
        "Question_original_content":"deepar hyperparamet tune error run new issu tune deepar try initi hyperparamet tune job error occur call test mean wquantileloss upgrad packag restart instanc restart kernel juptyer notebook problem persist clienterror error occur validationexcept call createhyperparametertuningjob oper object metric type maxim specifi object metric test rmse isnt valid dkr ecr west amazonaw com forecast deepar algorithm choos valid object metric type code tuner hyperparametertun estim estim object metric test rmse hyperparamet rang hyperparam max job max parallel job start hyperparamet tune job tuner fit input data channel stack trace clienterror traceback recent start hyperparamet tune job tuner fit input data channel anaconda env python lib python site packag tuner fit self input job includ cl metadata kwarg self prepar train job job includ cl metadata includ cl metadata self latest tune job tuningjob start new self input classmethod anaconda env python lib python site packag tuner start new cl tuner input output config config output config resourc config config resourc config stop condit config stop condit tag tuner tag return cl tuner session tuner current job anaconda env python lib python site packag session tune self job strategi object type object metric max job max parallel job paramet rang static hyperparamet imag input mode metric definit role input config output config resourc config stop condit tag logger info creat hyperparamet tune job format job logger debug tune request format json dump tune request indent self client creat hyper paramet tune job tune request def stop tune job self anaconda env python lib python site packag botocor client api self arg kwarg accept keyword argument oper self scope refer basecli return self api oper kwarg api str oper anaconda env python lib python site packag botocor client api self oper api param error code pars respons error code error class self except code error code rais error class pars respons oper return pars respons clienterror error occur validationexcept call createhyperparametertuningjob oper object metric type maxim specifi object metric test rmse isnt valid dkr ecr west amazonaw com forecast deepar algorithm choos valid object metric type",
        "Question_preprocessed_content":"deepar hyperparamet tune error run new issu tune deepar try initi hyperparamet tune job error occur call upgrad packag restart instanc restart kernel problem persist code",
        "Question_gpt_summary_original":"The user is encountering an error when trying to initialize a hyperparameter tuning job on SageMaker for DeepAR. The error occurs when calling the test:mean_wQuantileLoss and the user has tried upgrading the sagemaker package, restarting the instance and kernel, but the problem persists. The error message indicates that the objective metric type specified for objective metric, [test:RMSE], isn't valid for the [156387875391.dkr.ecr.us-west-2.amazonaws.com\/forecasting-deepar:1] algorithm.",
        "Question_gpt_summary":"user encount error try initi hyperparamet tune job deepar error occur call test mean wquantileloss user tri upgrad packag restart instanc kernel problem persist error messag indic object metric type specifi object metric test rmse isn valid dkr ecr west amazonaw com forecast deepar algorithm",
        "Answer_original_content":"look like try maxim metric test rmse minim hyperparamet tune achiev python sdk creat hyperparametertun object type minim signatur init method chang hyperparametertun tuner hyperparametertun estim estim object metric test rmse object type minim hyperparamet rang hyperparam max job max parallel job",
        "Answer_preprocessed_content":"look like try maxim metric test rmse minim hyperparamet tune achiev python sdk creat hyperparametertun signatur init method chang hyperparametertun",
        "Answer_gpt_summary_original":"Solution: The error message indicates that the objective metric type specified for objective metric, [test:RMSE], isn't valid for the [156387875391.dkr.ecr.us-west-2.amazonaws.com\/forecasting-deepar:1] algorithm. To resolve this issue, the user should create a HyperparameterTuner with objective_type='Minimize' in the SageMaker Python SDK. The user can make the change to the call to HyperparameterTuner by setting objective_type='Minimize' in the init method.",
        "Answer_gpt_summary":"solut error messag indic object metric type specifi object metric test rmse isn valid dkr ecr west amazonaw com forecast deepar algorithm resolv issu user creat hyperparametertun object type minim python sdk user chang hyperparametertun set object type minim init method"
    },
    {
        "Question_title":"'no SavedModel bundles found!' on tensorflow_hub model deployment to AWS SageMaker",
        "Question_body":"<p>I attempting to deploy the universal-sentence-encoder model to a aws Sagemaker endpoint and am getting the error <code>raise ValueError('no SavedModel bundles found!')<\/code><\/p>\n\n<p>I have shown my code below, I have a feeling that one of my paths is incorrect<\/p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nfrom sagemaker import get_execution_role\nfrom sagemaker.tensorflow.serving import Model\n\ndef tfhub_to_savedmodel(model_name,uri):\n    tfhub_uri = uri\n    model_path = 'encoder_model\/' + model_name\n\n    with tf.Session(graph=tf.Graph()) as sess:\n        module = hub.Module(tfhub_uri) \n        input_params = module.get_input_info_dict()\n        dtype = input_params['text'].dtype\n        shape = input_params['text'].get_shape()\n\n        # define the model inputs\n        inputs = {'text': tf.placeholder(dtype, shape, 'text')}\n\n        # define the model outputs\n        # we want the class ids and probabilities for the top 3 classes\n        logits = module(inputs['text'])\n        outputs = {\n            'vector': logits,\n        }\n\n        # export the model\n        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        tf.saved_model.simple_save(\n            sess,\n            model_path,\n            inputs=inputs,\n            outputs=outputs)  \n\n    return model_path\n\n\nsagemaker_role = get_execution_role()\n\n!tar -C \"$PWD\" -czf encoder.tar.gz encoder_model\/\nmodel_data = Session().upload_data(path='encoder.tar.gz',key_prefix='model')\n\nenv = {'SAGEMAKER_TFS_DEFAULT_MODEL_NAME': 'universal-sentence-encoder-large'}\n\nmodel = Model(model_data=model_data, role=sagemaker_role, framework_version=1.12, env=env)\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1563915680943,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":2621.0,
        "Answer_body":"<p>I suppose you started from this example? <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_serving_container\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_serving_container<\/a><\/p>\n\n<p>It looks like you're not saving the TF Serving bundle properly: the model version number is missing, because of this line:<\/p>\n\n<pre><code>model_path = 'encoder_model\/' + model_name\n<\/code><\/pre>\n\n<p>Replacing it with this should fix your problem:<\/p>\n\n<pre><code>model_path = '{}\/{}\/00000001'.format('encoder_model\/', model_name)\n<\/code><\/pre>\n\n<p>Your model artefact should look like this (I used the model in the notebook above):<\/p>\n\n<pre><code>mobilenet\/\nmobilenet\/mobilenet_v2_140_224\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/saved_model.pb\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.data-00000-of-00001\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.index\n<\/code><\/pre>\n\n<p>Then, upload to S3 and deploy.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57172147",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1563978858283,
        "Question_original_content":"savedmodel bundl tensorflow hub model deploy attempt deploi univers sentenc encod model endpoint get error rais valueerror savedmodel bundl shown code feel path incorrect import tensorflow import tensorflow hub hub import numpi import execut role tensorflow serv import model def tfhub savedmodel model uri tfhub uri uri model path encod model model session graph graph sess modul hub modul tfhub uri input param modul input info dict dtype input param text dtype shape input param text shape defin model input input text placehold dtype shape text defin model output want class id probabl class logit modul input text output vector logit export model sess run global variabl initi tabl initi save model simpl save sess model path input input output output return model path role execut role tar pwd czf encod tar encod model model data session upload data path encod tar kei prefix model env tf default model univers sentenc encod larg model model model data model data role role framework version env env predictor model deploi initi instanc count instanc type medium",
        "Question_preprocessed_content":"savedmodel bundl model deploy attempt deploi model endpoint get error shown code feel path incorrect",
        "Question_gpt_summary_original":"The user is encountering an error while attempting to deploy the universal-sentence-encoder model to an AWS SageMaker endpoint. The error message indicates that no SavedModel bundles were found. The user suspects that one of their paths may be incorrect. The user has provided their code and is using TensorFlow and TensorFlow Hub to convert the model to a SavedModel format.",
        "Question_gpt_summary":"user encount error attempt deploi univers sentenc encod model endpoint error messag indic savedmodel bundl user suspect path incorrect user provid code tensorflow tensorflow hub convert model savedmodel format",
        "Answer_original_content":"suppos start exampl http github com awslab amazon exampl tree master python sdk tensorflow serv contain look like save serv bundl properli model version number miss line model path encod model model replac fix problem model path format encod model model model artefact look like model notebook mobilenet mobilenet mobilenet mobilenet mobilenet mobilenet mobilenet save model mobilenet mobilenet variabl mobilenet mobilenet variabl variabl data mobilenet mobilenet variabl variabl index upload deploi",
        "Answer_preprocessed_content":"suppos start exampl look like save serv bundl properli model version number miss line replac fix problem model artefact look like upload deploi",
        "Answer_gpt_summary_original":"Solution: The error message indicates that no SavedModel bundles were found. One possible solution is to modify the code to include the model version number in the model path. The correct model artefact should have the following structure: \n\n```\nmodel_name\/\nmodel_name\/version_number\/\nmodel_name\/version_number\/saved_model.pb\nmodel_name\/version_number\/variables\/\nmodel_name\/version_number\/variables\/variables.data-00000-of-00001\nmodel_name\/version_number\/variables\/variables.index\n```\n\nAfter modifying the code, the user should upload the model to S3 and deploy it.",
        "Answer_gpt_summary":"solut error messag indic savedmodel bundl possibl solut modifi code includ model version number model path correct model artefact follow structur model model version number model version number save model model version number variabl model version number variabl variabl data model version number variabl variabl index modifi code user upload model deploi"
    },
    {
        "Question_title":"Sagemaker Notebook Instance Type Recommendation",
        "Question_body":"<p>I will be running ml models on a pretty large dataset. It is about 15 gb, with 200 columns and 4.3 million rows. I'm wondering what the best Notebook instance type is for this kind of dataset in AWS Sagemaker.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573155038577,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":2326.0,
        "Answer_body":"<p><strong>For choosing a SageMaker hosted notebook type:<\/strong><\/p>\n\n<p>Do you plan to do all of your preprocessing of your data in-memory on the notebook, or do you plan to orchestrate ETL with external services? <\/p>\n\n<p>If you're planning to load the dataset into memory on the notebook instance for exploration\/preprocessing, the primary bottleneck here would be ensuring the instance has enough memory for your dataset. This would require at least the 16gb types (<em>.xlarge<\/em>) (full list of ML instance types <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"noreferrer\">available here<\/a>). Further, depending on how compute intensive your pre-processing is, and your desired pre-processing completion time, you can opt for a compute optimized instance (<em>c4, c5<\/em>) to speed this up.<\/p>\n\n<hr>\n\n<p><strong>For the training job, specifically:<\/strong><\/p>\n\n<p>Using the Amazon SageMaker SDK, your training data will be loaded and distributed to the training cluster, allowing your training job to be completely separate from the instance your hosted notebook is running on.<\/p>\n\n<p>Figuring out the ideal instance type for training will depend on whether your algorithm of choice\/training job is memory, CPU, or IO bound. Since your dataset will likely be loaded onto your training cluster from S3, the instance you choose for your hosted notebook will have no bearing on the speed of your training job.<\/p>\n\n<hr>\n\n<p><strong>Broadly:<\/strong>\nWhen it comes to SageMaker notebooks, the best practice is to use your notebook as a \"puppeteer\" or orchestrator, that calls out to external services (AWS Glue or Amazon EMR for preprocessing, SageMaker for training, S3 for storage, etc). It is best to treat them as ephemeral forms of compute\/storage for building and kicking off your experiment pipeline.<\/p>\n\n<p>This will allow you to more closely pair compute, storage, and hosting resources\/services with the demands for your workload, ultimately resulting in the best bang for your buck by not having you pay for latent or unused resources.<\/p>\n\n<hr>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58755708",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1573157762368,
        "Question_original_content":"notebook instanc type recommend run model pretti larg dataset column million row wonder best notebook instanc type kind dataset",
        "Question_preprocessed_content":"notebook instanc type recommend run model pretti larg dataset column million row wonder best notebook instanc type kind dataset",
        "Question_gpt_summary_original":"The user is seeking a recommendation for the best AWS Sagemaker Notebook instance type to use for running machine learning models on a large dataset of 15 GB, 200 columns, and 4.3 million rows.",
        "Question_gpt_summary":"user seek recommend best notebook instanc type us run machin learn model larg dataset column million row",
        "Answer_original_content":"choos host notebook type plan preprocess data memori notebook plan orchestr etl extern servic plan load dataset memori notebook instanc explor preprocess primari bottleneck ensur instanc memori dataset requir type xlarg list instanc type avail depend comput intens pre process desir pre process complet time opt comput optim instanc speed train job specif sdk train data load distribut train cluster allow train job complet separ instanc host notebook run figur ideal instanc type train depend algorithm choic train job memori cpu bound dataset like load train cluster instanc choos host notebook bear speed train job broadli come notebook best practic us notebook puppet orchestr call extern servic aw glue amazon emr preprocess train storag best treat ephemer form comput storag build kick experi pipelin allow close pair comput storag host resourc servic demand workload ultim result best bang buck have pai latent unus resourc",
        "Answer_preprocessed_content":"choos host notebook type plan preprocess data notebook plan orchestr etl extern servic plan load dataset memori notebook instanc primari bottleneck ensur instanc memori dataset requir type depend comput intens desir complet time opt comput optim instanc speed train job specif sdk train data load distribut train cluster allow train job complet separ instanc host notebook run figur ideal instanc type train depend algorithm job memori cpu bound dataset like load train cluster instanc choos host notebook bear speed train job broadli come notebook best practic us notebook puppet orchestr call extern servic best treat ephemer form build kick experi pipelin allow close pair comput storag host demand workload ultim result best bang buck have pai latent unus resourc",
        "Answer_gpt_summary_original":"Solution:\n- For preprocessing of data in-memory on the notebook, the instance should have at least 16gb memory, and a compute optimized instance can be used to speed up the process.\n- For the training job, the instance type will depend on whether the algorithm is memory, CPU, or IO bound, and the instance used for the hosted notebook will have no bearing on the speed of the training job.\n- It is best to use the notebook as an orchestrator that calls out to external services for preprocessing, training, storage, etc.",
        "Answer_gpt_summary":"solut preprocess data memori notebook instanc memori comput optim instanc speed process train job instanc type depend algorithm memori cpu bound instanc host notebook bear speed train job best us notebook orchestr call extern servic preprocess train storag"
    },
    {
        "Question_title":"How to combine pipeline and hyperparameter in AzureML SDK in the training step",
        "Question_body":"<p>Short form:\nI am trying to figure out how can I run the hyperparam within a <strong>training step<\/strong> (i.e. train_step = PythonScriptStep(...)) in the pipeline, I am not sure where shall I put the &quot;config=hyperdrive&quot;<\/p>\n<p>Long form:<\/p>\n<p>General:<\/p>\n<pre><code># Register the environment \ndiabetes_env.register(workspace=ws)\nregistered_env = Environment.get(ws, 'diabetes-pipeline-env')\n\n# Create a new runconfig object for the pipeline\nrun_config = RunConfiguration()\n\n# Use the compute you created above. \nrun_config.target = ComputerTarget_Crea\n\n# Assign the environment to the run configuration\nrun_config.environment = registered_env\n<\/code><\/pre>\n<p>Hyperparam:<\/p>\n<pre><code>script_config = ScriptRunConfig(source_directory=experiment_folder,\n                                script='diabetes_training.py',\n                                # Add non-hyperparameter arguments -in this case, the training dataset\n                                arguments = ['--input-data', diabetes_ds.as_named_input('training_data')],\n                                environment=sklearn_env,\n                                compute_target = training_cluster)\n\n# Sample a range of parameter values\nparams = GridParameterSampling(\n    {\n        # Hyperdrive will try 6 combinations, adding these as script arguments\n        '--learning_rate': choice(0.01, 0.1, 1.0),\n        '--n_estimators' : choice(10, 100)\n    }\n)\n\n# Configure hyperdrive settings\nhyperdrive = HyperDriveConfig(run_config=script_config, \n                          hyperparameter_sampling=params, \n                          policy=None, # No early stopping policy\n                          primary_metric_name='AUC', # Find the highest AUC metric\n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                          max_total_runs=6, # Restict the experiment to 6 iterations\n                          max_concurrent_runs=2) # Run up to 2 iterations in parallel\n\n# Run the experiment if I only want to run hyperparam alone without the pipeline\n#experiment = Experiment(workspace=ws, name='mslearn-diabetes-hyperdrive')\n#run = experiment.submit(**config=hyperdrive**)\n<\/code><\/pre>\n<p>PipeLine:<\/p>\n<pre><code>prep_step = PythonScriptStep(name = &quot;Prepare Data&quot;,\n                                source_directory = experiment_folder,\n                                script_name = &quot;prep_diabetes.py&quot;,\n                                arguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n                                             '--prepped-data', prepped_data_folder],\n                                outputs=[prepped_data_folder],\n                                compute_target = ComputerTarget_Crea,\n                                runconfig = run_config,\n                                allow_reuse = True)\n\n# Step 2, run the training script\ntrain_step = PythonScriptStep(name = &quot;Train and Register Model&quot;,\n                                source_directory = experiment_folder,\n                                script_name = &quot;train_diabetes.py&quot;,\n                                arguments = ['--training-folder', prepped_data_folder],\n                                inputs=[prepped_data_folder],\n                                compute_target = ComputerTarget_Crea,\n                                runconfig = run_config,\n                                allow_reuse = True)\n# Construct the pipeline\npipeline_steps = [prep_step, train_step]\npipeline = Pipeline(workspace=ws, steps=pipeline_steps)\nprint(&quot;Pipeline is built.&quot;)\n\n# Create an experiment and run the pipeline\n**#How do I need to change these below lines to use hyperdrive????**\nexperiment = Experiment(workspace=ws, name = 'mslearn-diabetes-pipeline')\npipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n<\/code><\/pre>\n<p>Not sure where I need to put <strong>config=hyperdrive<\/strong> in the Pipeline section?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619929277080,
        "Question_favorite_count":null,
        "Question_last_edit_time":1619929940227,
        "Question_score":1.0,
        "Question_view_count":276.0,
        "Answer_body":"<p>here's how to combine hyperparameters with an AML pipeline: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.hyperdrivestep?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.hyperdrivestep?view=azure-ml-py<\/a><\/p>\n<p>Alternatively, here's a sample notebook: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67352949",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1633922581063,
        "Question_original_content":"combin pipelin hyperparamet sdk train step short form try figur run hyperparam train step train step pythonscriptstep pipelin sure shall config hyperdr long form gener regist environ diabet env regist workspac regist env environ diabet pipelin env creat new runconfig object pipelin run config runconfigur us comput creat run config target computertarget crea assign environ run configur run config environ regist env hyperparam script config scriptrunconfig sourc directori experi folder script diabet train add non hyperparamet argument case train dataset argument input data diabet name input train data environ sklearn env comput target train cluster sampl rang paramet valu param gridparametersampl hyperdr try combin ad script argument learn rate choic estim choic configur hyperdr set hyperdr hyperdriveconfig run config script config hyperparamet sampl param polici earli stop polici primari metric auc highest auc metric primari metric goal primarymetricgo maxim max total run restict experi iter max concurr run run iter parallel run experi want run hyperparam pipelin experi experi workspac mslearn diabet hyperdr run experi submit config hyperdr pipelin prep step pythonscriptstep prepar data sourc directori experi folder script prep diabet argument input data diabet name input raw data prep data prep data folder output prep data folder comput target computertarget crea runconfig run config allow reus true step run train script train step pythonscriptstep train regist model sourc directori experi folder script train diabet argument train folder prep data folder input prep data folder comput target computertarget crea runconfig run config allow reus true construct pipelin pipelin step prep step train step pipelin pipelin workspac step pipelin step print pipelin built creat experi run pipelin need chang line us hyperdr experi experi workspac mslearn diabet pipelin pipelin run experi submit pipelin regener output true sure need config hyperdr pipelin section",
        "Question_preprocessed_content":"combin pipelin hyperparamet sdk train step short form try figur run hyperparam train step pipelin sure shall config hyperdr long form gener hyperparam pipelin sure need config hyperdr pipelin section",
        "Question_gpt_summary_original":"The user is trying to figure out how to run hyperparameters within a training step in a pipeline using AzureML SDK. They have successfully created a hyperparameter configuration and a pipeline with two steps, but they are unsure where to include the hyperparameter configuration in the pipeline.",
        "Question_gpt_summary":"user try figur run hyperparamet train step pipelin sdk successfulli creat hyperparamet configur pipelin step unsur includ hyperparamet configur pipelin",
        "Answer_original_content":"combin hyperparamet aml pipelin http doc microsoft com python api pipelin step pipelin step hyperdrivestep view azur altern sampl notebook http github com azur machinelearningnotebook blob master us machin learn pipelin intro pipelin aml pipelin paramet tune hyperdr ipynb",
        "Answer_preprocessed_content":"combin hyperparamet aml pipelin altern sampl notebook",
        "Answer_gpt_summary_original":"Solution: The discussion provides two possible solutions for combining hyperparameters with an AML pipeline. The first solution is to use the AzureML SDK's HyperDriveStep class, which is documented in the provided link. The second solution is to refer to a sample notebook that demonstrates how to use hyperparameters with an AML pipeline.",
        "Answer_gpt_summary":"solut discuss provid possibl solut combin hyperparamet aml pipelin solut us sdk hyperdrivestep class document provid link second solut refer sampl notebook demonstr us hyperparamet aml pipelin"
    },
    {
        "Question_title":"DVC imports authentication to blob storage",
        "Question_body":"<p>I'm using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> to track and version data that is stored locally on the file system and in Azure Blob storage.<\/p>\n<p>My setup is as follows:<\/p>\n<ul>\n<li><p><code>DataProject1<\/code>, it uses a local file location as a remote therefore it does not require any authentication.<\/p>\n<\/li>\n<li><p><code>DataProject2<\/code>, it uses Azure Blob Storage as a remote, it is using sas_token for authentication, I can push pull data to\/from the remote when I'm within this project.<\/p>\n<\/li>\n<li><p><code>MLProject<\/code>, it uses dvc import to import data from <code>DataProjec1<\/code> and <code>DataProject2<\/code>.<\/p>\n<\/li>\n<\/ul>\n<p>When I run the import with the command against <code>DataProject1<\/code> everything works fine:<\/p>\n<p><code>dvc import -o 'data\/project1' 'https:\/\/company.visualstudio.com\/DefaultCollection\/proj\/_git\/DataProject1' 'data\/project1'<\/code> - Successful<\/p>\n<p>Howevever when I run a similar command against <code>DataProject2<\/code> the command fails:<\/p>\n<p><code>dvc import -o 'data\/project2' 'https:\/\/company.visualstudio.com\/DefaultCollection\/proj\/_git\/DataProject2' 'data\/project2'<\/code> - it fails with:<\/p>\n<blockquote>\n<p>ERROR: unexpected error - Operation returned an invalid status 'This\nrequest is not authorized to perform this operation using this\npermission.'  ErrorCode:AuthorizationPermissionMismatch.<\/p>\n<\/blockquote>\n<p>I would like to configure the <code>dvc import<\/code> so that I can set the required <code>sas_token<\/code> but I cannot find a way to do that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1662648529560,
        "Question_favorite_count":null,
        "Question_last_edit_time":1662650337132,
        "Question_score":1.0,
        "Question_view_count":34.0,
        "Answer_body":"<p>This happens since DVC is not using <code>MLProject<\/code>'s config when it clones and does <code>dvc fetch<\/code> in the <code>DataProject2<\/code> during the <code>import<\/code>. And it doesn't know where it can find the token (clearly, it's not in the Git repo, right?).<\/p>\n<p>There are a few ways to specify it: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#--system\" rel=\"nofollow noreferrer\"><code>global\/system<\/code> configs<\/a> and\/or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#authenticate-with-environment-variables\" rel=\"nofollow noreferrer\">environment variables<\/a>.<\/p>\n<p>To implement the first option:<\/p>\n<p>On a machine where you do <code>dvc import<\/code>, you could create a remote in the <code>--global<\/code>, or <code>--system<\/code> configs with the same name and specify the token there. Global config fields will be merged with the config in the <code>DataProject2<\/code> repo when DVC is pulling data to import.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>dvc remote add --global &lt;DataProject2-remote-name&gt; azure:\/\/DataProject2\/storage\ndvc remote modify --global &lt;DataProject2-remote-name&gt; account_name &lt;name&gt;\ndvc remote modify --global &lt;DataProject2-remote-name&gt; sas_token &lt;token&gt;\n<\/code><\/pre>\n<p>The second option:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>export AZURE_STORAGE_SAS_TOKEN='mysecret'\nexport AZURE_STORAGE_ACCOUNT='myaccount'\n<\/code><\/pre>\n<p>Please give it a try, let me know if that works or not.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73651050",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1662656090387,
        "Question_original_content":"import authent blob storag track version data store local file azur blob storag setup follow dataproject us local file locat remot requir authent dataproject us azur blob storag remot sa token authent push pull data remot project mlproject us import import data dataprojec dataproject run import command dataproject work fine import data project http compani visualstudio com defaultcollect proj git dataproject data project success howevev run similar command dataproject command fail import data project http compani visualstudio com defaultcollect proj git dataproject data project fail error unexpect error oper return invalid statu request author perform oper permiss errorcod authorizationpermissionmismatch like configur import set requir sa token wai",
        "Question_preprocessed_content":"import authent blob storag track version data store local file azur blob storag setup follow us local file locat remot requir authent us azur blob storag remot authent push pull data remot project us import import data run import command work fine success howevev run similar command command fail fail error unexpect error oper return invalid statu request author perform oper errorcod authorizationpermissionmismatch like configur set requir wai",
        "Question_gpt_summary_original":"The user is facing challenges while using DVC to track and version data stored locally and in Azure Blob storage. While importing data from DataProject2 using dvc import, the user is unable to set the required sas_token for authentication, resulting in an error message indicating that the request is not authorized to perform the operation using the permission.",
        "Question_gpt_summary":"user face challeng track version data store local azur blob storag import data dataproject import user unabl set requir sa token authent result error messag indic request author perform oper permiss",
        "Answer_original_content":"happen mlproject config clone fetch dataproject import know token clearli git repo right wai specifi global config environ variabl implement option machin import creat remot global config specifi token global config field merg config dataproject repo pull data import remot add global azur dataproject storag remot modifi global account remot modifi global sa token second option export azur storag sa token mysecret export azur storag account myaccount try let know work",
        "Answer_preprocessed_content":"happen config clone know token wai specifi config environ variabl implement option machin creat remot config specifi token global config field merg config repo pull data import second option try let know work",
        "Answer_gpt_summary_original":"Solutions provided in the discussion are:\n\n1. Create a remote in the global or system configs with the same name as the remote in DataProject2 and specify the token there. Global config fields will be merged with the config in the DataProject2 repo when DVC is pulling data to import.\n\n2. Use environment variables to specify the token and account name.\n\nIt is suggested to try these solutions and see if they work.",
        "Answer_gpt_summary":"solut provid discuss creat remot global config remot dataproject specifi token global config field merg config dataproject repo pull data import us environ variabl specifi token account suggest try solut work"
    },
    {
        "Question_title":"Version control of azure machine learning workspace notebooks",
        "Question_body":"<p>I'm trying to work with the capacities of the new Azure ML Workspace and I can't find any option to track my notebooks on git. <\/p>\n\n<p>It's this possible as well as you can do with Azure notebooks? If not is possible... how it's suposed to work with this notebooks? Only inside this workspace? <\/p>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1582811011920,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":344.0,
        "Answer_body":"<p>AFAIK, Git isn't currently supported by Azure Machine Learning Notebooks. If you're looking for a more fully-featured development environment, I suggest setting one up locally. There's more work up front, but it will give you the ability to version control. Check out this development environment set-up guide. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment<\/a><\/p>\n\n<pre><code>| Environment                                                   | Pros                                                                                                                                                                                                                                    | Cons                                                                                                                                                                                 |\n|---------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Cloud-based Azure Machine Learning compute instance (preview) | Easiest way to get started. The entire SDK is already installed in your workspace VM, and notebook tutorials are pre-cloned and ready to run.                                                                                           | Lack of control over your development environment and dependencies. Additional cost incurred for Linux VM (VM can be stopped when not in use to avoid charges). See pricing details. |\n| Local environment                                             | Full control of your development environment and dependencies. Run with any build tool, environment, or IDE of your choice.                                                                                                             | Takes longer to get started. Necessary SDK packages must be installed, and an environment must also be installed if you don't already have one.                                      |\n| Azure Databricks                                              | Ideal for running large-scale intensive machine learning workflows on the scalable Apache Spark platform.                                                                                                                               | Overkill for experimental machine learning, or smaller-scale experiments and workflows. Additional cost incurred for Azure Databricks. See pricing details.                          |\n| The Data Science Virtual Machine (DSVM)                       | Similar to the cloud-based compute instance (Python and the SDK are pre-installed), but with additional popular data science and machine learning tools pre-installed. Easy to scale and combine with other custom tools and workflows. | A slower getting started experience compared to the cloud-based compute instance.                                                                                                    |\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60434642",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1582827641316,
        "Question_original_content":"version control workspac notebook try work capac new workspac option track notebook git possibl azur notebook possibl supos work notebook insid workspac thank",
        "Question_preprocessed_content":"version control workspac notebook try work capac new workspac option track notebook git possibl azur notebook supos work notebook insid workspac thank",
        "Question_gpt_summary_original":"The user is facing challenges in tracking their Azure ML Workspace notebooks on git and is unsure if it is possible to do so. They are seeking clarification on whether the notebooks can only be used within the workspace or if there is another way to work with them.",
        "Question_gpt_summary":"user face challeng track workspac notebook git unsur possibl seek clarif notebook workspac wai work",
        "Answer_original_content":"afaik git isn current support notebook look fulli featur develop environ suggest set local work abil version control check develop environ set guid http doc microsoft com azur machin learn configur environ environ pro con cloud base comput instanc preview easiest wai start entir sdk instal workspac notebook tutori pre clone readi run lack control develop environ depend addit cost incur linux stop us avoid charg price detail local environ control develop environ depend run build tool environ id choic take longer start necessari sdk packag instal environ instal azur databrick ideal run larg scale intens machin learn workflow scalabl apach spark platform overkil experiment machin learn smaller scale experi workflow addit cost incur azur databrick price detail data scienc virtual machin dsvm similar cloud base comput instanc python sdk pre instal addit popular data scienc machin learn tool pre instal easi scale combin custom tool workflow slower get start experi compar cloud base comput instanc",
        "Answer_preprocessed_content":"afaik git isn current support notebook look develop environ suggest set local work abil version control check develop environ guid",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that Git is not currently supported by Azure Machine Learning Notebooks. However, the user can set up a local development environment to version control their notebooks. The discussion also provides a link to a development environment set-up guide.",
        "Answer_gpt_summary":"solut discuss suggest git current support notebook user set local develop environ version control notebook discuss provid link develop environ set guid"
    },
    {
        "Question_title":"Unable to use GPU to train a NN model in azure machine learning service using P100-NC6s-V2 compute. Fails wth CUDA error",
        "Question_body":"<p>I\u2019ve recently started working with azure for ML and am trying to use machine learning service workspace.\nI\u2019ve set up a workspace with the compute set to NC6s-V2 machines since I need train a NN using images on GPU. <\/p>\n\n<p>The issue is that the training still happens on the CPU \u2013 the logs say it\u2019s not able to find CUDA. Here\u2019s the warning log when running my script.\nAny clues how to solve this issue?<\/p>\n\n<p>I\u2019ve also mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. <\/p>\n\n<p>Here's my code for the estimator,<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>script_params = {\n         '--input_data_folder': ds.path('dataset').as_mount(),\n         '--zip_file_name': 'train.zip',\n         '--run_mode': 'train'\n    }\n\n\nest = Estimator(source_directory='.\/scripts',\n                     script_params=script_params,\n                     compute_target=compute_target,\n                     entry_script='main.py',\n                     conda_packages=['scikit-image', 'keras', 'tqdm', 'pillow', 'matplotlib', 'scipy', 'tensorflow-gpu']\n                     )\n\nrun = exp.submit(config=est)\n\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n\n<p>The compute target was made as per the sample code on github:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>compute_name = \"P100-NC6s-V2\"\ncompute_min_nodes = 0\ncompute_max_nodes = 4\n\nvm_size = \"STANDARD_NC6S_V2\"\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n                                                                min_nodes=compute_min_nodes,\n                                                                max_nodes=compute_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(\n        ws, compute_name, provisioning_config)\n\n    # can poll for a minimum number of nodes and for a specific timeout.\n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(\n        show_output=True, min_node_count=None, timeout_in_minutes=20)\n\n    # For a more detailed view of current AmlCompute status, use get_status()\n    print(compute_target.get_status().serialize())\n\n<\/code><\/pre>\n\n<p>This is the warning with which it fails to use the GPU:<\/p>\n\n<pre><code>2019-08-12 14:50:16.961247: I tensorflow\/compiler\/xla\/service\/service.cc:168] XLA service 0x55a7ce570830 executing computations on platform Host. Devices:\n2019-08-12 14:50:16.961278: I tensorflow\/compiler\/xla\/service\/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n2019-08-12 14:50:16.971025: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/azureml-envs\/azureml_5fdf05c5671519f307e0f43128b8610e\/lib:\n2019-08-12 14:50:16.971054: E tensorflow\/stream_executor\/cuda\/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n2019-08-12 14:50:16.971081: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971089: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:176] hostname: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971164: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\n2019-08-12 14:50:16.971202: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:204] kernel reported version is: 418.40.4\nDevice mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n2019-08-12 14:50:16.973301: I tensorflow\/core\/common_runtime\/direct_session.cc:296] Device mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n\n<\/code><\/pre>\n\n<p>It's currently using the CPU as per the logs. Any clues how to resolve the issue here?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1565670597500,
        "Question_favorite_count":null,
        "Question_last_edit_time":1565670701316,
        "Question_score":5.0,
        "Question_view_count":1402.0,
        "Answer_body":"<p>Instead of base Estimator, you can use the Tensorflow Estimator with Keras and other libraries layered on top. That way you don't have to worry about setting up and configuring the GPU libraries, as the Tensorflow Estimator uses a Docker image with GPU libraries pre-configured. <\/p>\n\n<p>See here for documentation:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.dnn.tensorflow?view=azure-ml-py\" rel=\"nofollow noreferrer\">API Reference<\/a> You can use <code>conda_packages<\/code> argument to specify additional libraries. Also set argument <code>use_gpu = True<\/code>.<\/p>\n\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training-with-deep-learning\/train-hyperparameter-tune-deploy-with-keras\/train-hyperparameter-tune-deploy-with-keras.ipynb\" rel=\"nofollow noreferrer\">Example Notebook<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57471129",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1565705412680,
        "Question_original_content":"unabl us gpu train model servic nc comput fail wth cuda error iv recent start work azur try us machin learn servic workspac iv set workspac comput set nc machin need train imag gpu issu train happen cpu log abl cuda here warn log run script clue solv issu iv mention explicitli tensorflow gpu packag conda packag option estim code estim script param input data folder path dataset mount zip file train zip run mode train est estim sourc directori script script param script param comput target comput target entri script main conda packag scikit imag kera tqdm pillow matplotlib scipi tensorflow gpu run exp submit config est run wait complet output true comput target sampl code github comput nc comput min node comput max node size standard nc comput comput target comput target comput target comput comput target type comput target amlcomput print comput target us comput print creat new comput target provis config amlcomput provis configur size size min node comput min node max node comput max node creat cluster comput target computetarget creat comput provis config poll minimum number node specif timeout min node count provid us scale set cluster comput target wait complet output true min node count timeout minut detail view current amlcomput statu us statu print comput target statu serial warn fail us gpu tensorflow compil xla servic servic xla servic xace execut comput platform host devic tensorflow compil xla servic servic streamexecutor devic tensorflow stream executor platform default dso loader dlopen librari libcuda dlerror libcuda open share object file file directori librari path opt intel compil librari linux mpi intel lib opt intel compil librari linux mpi mic lib opt intel compil librari linux mpi intel lib opt intel compil librari linux mpi mic lib env fdfcfefb lib tensorflow stream executor cuda cuda driver fail cuinit unknown error tensorflow stream executor cuda cuda diagnost retriev cuda diagnost inform host bddfbeedaaf tensorflow stream executor cuda cuda diagnost hostnam bddfbeedaaf tensorflow stream executor cuda cuda diagnost libcuda report version unabl libcuda dso load program tensorflow stream executor cuda cuda diagnost kernel report version devic map job localhost replica task devic xla cpu devic xla cpu devic tensorflow core common runtim direct session devic map job localhost replica task devic xla cpu devic xla cpu devic current cpu log clue resolv issu",
        "Question_preprocessed_content":"unabl us gpu train model servic comput fail wth cuda error iv recent start work azur try us machin learn servic workspac iv set workspac comput set machin need train imag gpu issu train happen cpu log abl cuda here warn log run script clue solv issu iv mention explicitli packag conda packag option estim code estim comput target sampl code github warn fail us gpu current cpu log clue resolv issu",
        "Question_gpt_summary_original":"The user is facing challenges in using GPU to train a neural network model in Azure Machine Learning Service using P100-NC6s-V2 compute. The training is happening on the CPU and the logs show that it's not able to find CUDA. The user has mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. The warning log shows that it fails to use the GPU and is currently using the CPU.",
        "Question_gpt_summary":"user face challeng gpu train neural network model servic nc comput train happen cpu log abl cuda user mention explicitli tensorflow gpu packag conda packag option estim warn log show fail us gpu current cpu",
        "Answer_original_content":"instead base estim us tensorflow estim kera librari layer wai worri set configur gpu librari tensorflow estim us docker imag gpu librari pre configur document api refer us conda packag argument specifi addit librari set argument us gpu true exampl notebook",
        "Answer_preprocessed_content":"instead base estim us tensorflow estim kera librari layer wai worri set configur gpu librari tensorflow estim us docker imag gpu librari document api refer us argument specifi addit librari set argument exampl notebook",
        "Answer_gpt_summary_original":"Solution: The user can use the Tensorflow Estimator with pre-configured GPU libraries instead of the base Estimator. They can specify additional libraries using the `conda_packages` argument and set `use_gpu=True`. Documentation and an example notebook are provided for reference.",
        "Answer_gpt_summary":"solut user us tensorflow estim pre configur gpu librari instead base estim specifi addit librari conda packag argument set us gpu true document exampl notebook provid refer"
    },
    {
        "Question_title":"Google Colab freezes my browser and pc when trying to reconnect to a notebook",
        "Question_body":"<p>I am training a Machine learning model in google colab, to be more specific I am training a GAN with PyTorch-lightning. The problem occurs is when I get disconnected from my current runtime due to inactivity. When I try to reconnect my Browser(tried on firefox and chrome) becomes first laggy and than freezes, my pc starts to lag so that I am not able to close my browser and it doesn't go away. I am forced to press the power button of my PC in order to restart the PC.\nI have no clue why this happens.\nI tried various batch sizes(also the size 1) but it still happens. It can't be that my dataset is too big either(since i tried it on a dataset with 10images for testing puposes).\nI hope someone can help me.<\/p>\n\n<p>Here is my code (For using the code you will need comet.nl and enter the comet.ml api key):<\/p>\n\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision  \nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\n\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport pytorch_lightning as pl\nfrom pytorch_lightning import loggers\n\nimport numpy as np\nfrom numpy.random import choice\n\nfrom PIL import Image\n\nimport os\nfrom pathlib import Path\nimport shutil\n\nfrom collections import OrderedDict\n\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n# randomly flip some labels\ndef noisy_labels(y, p_flip=0.05):  # # flip labels with 5% probability\n    # determine the number of labels to flip\n    n_select = int(p_flip * y.shape[0])\n    # choose labels to flip\n    flip_ix = choice([i for i in range(y.shape[0])], size=n_select)\n    # invert the labels in place\n    y[flip_ix] = 1 - y[flip_ix]\n    return y\n\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0.0, std=0.1):\n        self.std = std\n        self.mean = mean\n\n    def __call__(self, tensor):\n        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\ndef get_valid_labels(img):\n  return (0.8 - 1.1) * torch.rand(img.shape[0], 1, 1, 1) + 1.1  # soft labels\n\ndef get_unvalid_labels(img):\n  return noisy_labels((0.0 - 0.3) * torch.rand(img.shape[0], 1, 1, 1) + 0.3)  # soft labels\n\nclass Generator(nn.Module):\n    def __init__(self, ngf, nc, latent_dim):\n        super(Generator, self).__init__()\n        self.ngf = ngf\n        self.latent_dim = latent_dim\n        self.nc = nc\n\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d(latent_dim, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n             nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass Discriminator(nn.Module):\n    def __init__(self, ndf, nc):\n        super(Discriminator, self).__init__()\n        self.nc = nc\n        self.ndf = ndf\n\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass DCGAN(pl.LightningModule):\n\n    def __init__(self, hparams, logger, checkpoint_folder, experiment_name):\n        super().__init__()\n        self.hparams = hparams\n        self.logger = logger  # only compatible with comet_logger at the moment\n        self.checkpoint_folder = checkpoint_folder\n        self.experiment_name = experiment_name\n\n        # networks\n        self.generator = Generator(ngf=hparams.ngf, nc=hparams.nc, latent_dim=hparams.latent_dim)\n        self.discriminator = Discriminator(ndf=hparams.ndf, nc=hparams.nc)\n        self.generator.apply(weights_init)\n        self.discriminator.apply(weights_init)\n\n        # cache for generated images\n        self.generated_imgs = None\n        self.last_imgs = None\n\n        # For experience replay\n        self.exp_replay_dis = torch.tensor([])\n\n        # creating checkpoint folder\n        dirpath = Path(self.checkpoint_folder)\n        if not dirpath.exists():\n          os.makedirs(dirpath, 0o755)\n\n    def forward(self, z):\n        return self.generator(z)\n\n    def adversarial_loss(self, y_hat, y):\n        return F.binary_cross_entropy(y_hat, y)\n\n    def training_step(self, batch, batch_nb, optimizer_idx):\n        # For adding Instance noise for more visit: https:\/\/www.inference.vc\/instance-noise-a-trick-for-stabilising-gan-training\/\n        std_gaussian = max(0, self.hparams.level_of_noise - ((self.hparams.level_of_noise * 1.5) * (self.current_epoch \/ self.hparams.epochs)))\n        AddGaussianNoiseInst = AddGaussianNoise(std=std_gaussian) # the noise decays over time\n\n        imgs, _ = batch\n        imgs = AddGaussianNoiseInst(imgs) # Adding instance noise to real images\n        self.last_imgs = imgs\n\n        # train generator\n        if optimizer_idx == 0:\n            # sample noise\n            z = torch.randn(imgs.shape[0], self.hparams.latent_dim, 1, 1)\n\n            # generate images\n            self.generated_imgs = self(z)\n            self.generated_imgs = AddGaussianNoiseInst(self.generated_imgs) # Adding instance noise to fake images\n\n            # Experience replay\n            # for discriminator\n            perm = torch.randperm(self.generated_imgs.size(0))  # Shuffeling\n            r_idx = perm[:max(1, self.hparams.experience_save_per_batch)]  # Getting the index\n            self.exp_replay_dis = torch.cat((self.exp_replay_dis, self.generated_imgs[r_idx]), 0).detach()  # Add our new example to the replay buffer\n\n            # ground truth result (ie: all fake)\n            g_loss = self.adversarial_loss(self.discriminator(self.generated_imgs), get_valid_labels(self.generated_imgs)) # adversarial loss is binary cross-entropy\n\n            tqdm_dict = {'g_loss': g_loss}\n            log = {'g_loss': g_loss, \"std_gaussian\": std_gaussian}\n            output = OrderedDict({\n                'loss': g_loss,\n                'progress_bar': tqdm_dict,\n                'log': log\n            })\n            return output\n\n        # train discriminator\n        if optimizer_idx == 1:\n            # Measure discriminator's ability to classify real from generated samples\n            # how well can it label as real?\n            real_loss = self.adversarial_loss(self.discriminator(imgs), get_valid_labels(imgs))\n\n            # Experience replay\n            if self.exp_replay_dis.size(0) &gt;= self.hparams.experience_batch_size:\n              fake_loss = self.adversarial_loss(self.discriminator(self.exp_replay_dis.detach()), get_unvalid_labels(self.exp_replay_dis))  # train on already seen images\n\n              self.exp_replay_dis = torch.tensor([]) # Reset experience replay\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"d_exp_loss\": fake_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n            else:\n              fake_loss = self.adversarial_loss(self.discriminator(self.generated_imgs.detach()), get_unvalid_labels(self.generated_imgs))  # how well can it label as fake?\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n    def configure_optimizers(self):\n        lr = self.hparams.lr\n        b1 = self.hparams.b1\n        b2 = self.hparams.b2\n\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n        return [opt_g, opt_d], []\n\n    def train_dataloader(self):\n        transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize([0.5], [0.5])])\n        dataset = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n        return DataLoader(dataset, batch_size=self.hparams.batch_size)\n        # transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n        #                                 transforms.ToTensor(),\n        #                                 transforms.Normalize([0.5], [0.5])\n        #                                 ])\n\n        # train_dataset = torchvision.datasets.ImageFolder(\n        #     root=\".\/drive\/My Drive\/datasets\/ghibli_dataset_small_overfit\/\",\n        #     transform=transform\n        # )\n        # return DataLoader(train_dataset, num_workers=self.hparams.num_workers, shuffle=True, batch_size=self.hparams.batch_size)\n\n    def on_epoch_end(self):\n        z = torch.randn(4, self.hparams.latent_dim, 1, 1)\n        # match gpu device (or keep as cpu)\n        if self.on_gpu:\n            z = z.cuda(self.last_imgs.device.index)\n\n        # log sampled images\n        sample_imgs = self.generator(z)\n        sample_imgs = sample_imgs.view(-1, self.hparams.nc, self.hparams.image_size, self.hparams.image_size)\n        grid = torchvision.utils.make_grid(sample_imgs, nrow=2)\n        self.logger.experiment.log_image(grid.permute(1, 2, 0), f'generated_images_epoch{self.current_epoch}', step=self.current_epoch)\n\n        # save model\n        if self.current_epoch % self.hparams.save_model_every_epoch == 0:\n          trainer.save_checkpoint(self.checkpoint_folder + \"\/\" + self.experiment_name + \"_epoch_\" + str(self.current_epoch) + \".ckpt\")\n          comet_logger.experiment.log_asset_folder(self.checkpoint_folder, step=self.current_epoch)\n\n          # Deleting the folder where we saved the model so that we dont upload a thing twice\n          dirpath = Path(self.checkpoint_folder)\n          if dirpath.exists() and dirpath.is_dir():\n                shutil.rmtree(dirpath)\n\n          # creating checkpoint folder\n          access_rights = 0o755\n          os.makedirs(dirpath, access_rights)\n\nfrom argparse import Namespace\n\nargs = {\n    'batch_size': 48,\n    'lr': 0.0002,\n    'b1': 0.5,\n    'b2': 0.999,\n    'latent_dim': 128, # tested value which worked(in V4_1): 100\n    'nc': 1,\n    'ndf': 32,\n    'ngf': 32,\n    'epochs': 10,\n    'save_model_every_epoch': 5,\n    'image_size': 64,\n    'num_workers': 2,\n    'level_of_noise': 0.15,\n    'experience_save_per_batch': 1, # this value should be very low; tested value which works: 1\n    'experience_batch_size': 50 # this value shouldnt be too high; tested value which works: 50\n}\nhparams = Namespace(**args)\n\n# Parameters\nexperiment_name = \"DCGAN_V4_2_MNIST\"\ndataset_name = \"MNIST\"\ncheckpoint_folder = \"DCGAN\/\"\ntags = [\"DCGAN\", \"MNIST\", \"OVERFIT\", \"64x64\"]\ndirpath = Path(checkpoint_folder)\n\n# init logger\ncomet_logger = loggers.CometLogger(\n    api_key=\"\",\n    rest_api_key=\"\",\n    project_name=\"gan\",\n    experiment_name=experiment_name,\n    #experiment_key=\"f23d00c0fe3448ee884bfbe3fc3923fd\"  # used for resuming trained id can be found in comet.ml\n)\n\n#defining net\nnet = DCGAN(hparams, comet_logger, checkpoint_folder, experiment_name)\n\n#logging\ncomet_logger.experiment.set_model_graph(str(net))\ncomet_logger.experiment.add_tags(tags=tags)\ncomet_logger.experiment.log_dataset_info(dataset_name)\n\ntrainer = pl.Trainer(#resume_from_checkpoint=\"GHIBLI_DCGAN_OVERFIT_64px_epoch_6000.ckpt\",\n                     logger=comet_logger,\n                     max_epochs=args[\"epochs\"]\n                     )\ntrainer.fit(net)\ncomet_logger.experiment.end()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586987562523,
        "Question_favorite_count":null,
        "Question_last_edit_time":1587130797008,
        "Question_score":0.0,
        "Question_view_count":1013.0,
        "Answer_body":"<p>I fixed it with importing this:<\/p>\n\n<pre><code>from IPython.display import clear_output \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61239274",
        "Tool":"Comet",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1587034953183,
        "Question_original_content":"googl colab freez browser try reconnect notebook train machin learn model googl colab specif train gan pytorch lightn problem occur disconnect current runtim inact try reconnect browser tri firefox chrome laggi freez start lag abl close browser awai forc press power button order restart clue happen tri batch size size happen dataset big tri dataset imag test pupos hope help code code need enter api kei import torch import torch import torch function import torchvis import torchvis transform transform torch util data import dataload torchvis dataset import mnist pytorch lightn callback import modelcheckpoint import pytorch lightn pytorch lightn import logger import numpi numpi random import choic pil import imag import pathlib import path import shutil collect import ordereddict custom weight initi call netg netd def weight init classnam class classnam conv init normal weight data elif classnam batchnorm init normal weight data init constant bia data randomli flip label def noisi label flip flip label probabl determin number label flip select int flip shape choos label flip flip choic rang shape size select invert label place flip flip return class addgaussiannois object def init self mean std self std std self mean mean def self tensor return tensor torch randn tensor size self std self mean def repr self return self class mean std format self mean self std def valid label img return torch rand img shape soft label def unvalid label img return noisi label torch rand img shape soft label class gener modul def init self ngf latent dim super gener self init self ngf ngf self latent dim latent dim self self main sequenti input go convolut convtranspos latent dim ngf bia fals batchnormd ngf leakyrelu inplac true state size ngf convtranspos ngf ngf bia fals batchnormd ngf leakyrelu inplac true state size ngf convtranspos ngf ngf bia fals batchnormd ngf leakyrelu inplac true state size ngf convtranspos ngf ngf bia fals batchnormd ngf leakyrelu inplac true state size ngf convtranspos ngf bia fals tanh state size def forward self input return self main input class discrimin modul def init self ndf super discrimin self init self self ndf ndf self main sequenti input convd ndf bia fals leakyrelu inplac true state size ndf convd ndf ndf bia fals batchnormd ndf leakyrelu inplac true state size ndf convd ndf ndf bia fals batchnormd ndf leakyrelu inplac true state size ndf convd ndf ndf bia fals batchnormd ndf leakyrelu inplac true state size ndf convd ndf bia fals sigmoid def forward self input return self main input class dcgan lightningmodul def init self hparam logger checkpoint folder experi super init self hparam hparam self logger logger compat logger moment self checkpoint folder checkpoint folder self experi experi network self gener gener ngf hparam ngf hparam latent dim hparam latent dim self discrimin discrimin ndf hparam ndf hparam self gener appli weight init self discrimin appli weight init cach gener imag self gener img self img experi replai self exp replai di torch tensor creat checkpoint folder dirpath path self checkpoint folder dirpath exist makedir dirpath def forward self return self gener def adversari loss self hat return binari cross entropi hat def train step self batch batch optim idx ad instanc nois visit http infer instanc nois trick stabilis gan train std gaussian max self hparam level nois self hparam level nois self current epoch self hparam epoch addgaussiannoiseinst addgaussiannois std std gaussian nois decai time img batch img addgaussiannoiseinst img ad instanc nois real imag self img img train gener optim idx sampl nois torch randn img shape self hparam latent dim gener imag self gener img self self gener img addgaussiannoiseinst self gener img ad instanc nois fake imag experi replai discrimin perm torch randperm self gener img size shuffel idx perm max self hparam experi save batch get index self exp replai di torch cat self exp replai di self gener img idx detach add new exampl replai buffer ground truth result fake loss self adversari loss self discrimin self gener img valid label self gener img adversari loss binari cross entropi tqdm dict loss loss log loss loss std gaussian std gaussian output ordereddict loss loss progress bar tqdm dict log log return output train discrimin optim idx measur discrimin abil classifi real gener sampl label real real loss self adversari loss self discrimin img valid label img experi replai self exp replai di size self hparam experi batch size fake loss self adversari loss self discrimin self exp replai di detach unvalid label self exp replai di train seen imag self exp replai di torch tensor reset experi replai discrimin loss averag loss real loss fake loss tqdm dict loss loss log loss loss exp loss fake loss std gaussian std gaussian output ordereddict loss loss progress bar tqdm dict log log return output fake loss self adversari loss self discrimin self gener img detach unvalid label self gener img label fake discrimin loss averag loss real loss fake loss tqdm dict loss loss log loss loss std gaussian std gaussian output ordereddict loss loss progress bar tqdm dict log log return output def configur optim self self hparam self hparam self hparam opt torch optim adam self gener paramet beta opt torch optim adam self discrimin paramet beta return opt opt def train dataload self transform transform compos transform resiz self hparam imag size self hparam imag size transform totensor transform normal dataset mnist getcwd train true download true transform transform return dataload dataset batch size self hparam batch size transform transform compos transform resiz self hparam imag size self hparam imag size transform totensor transform normal train dataset torchvis dataset imagefold root drive drive dataset ghibli dataset small overfit transform transform return dataload train dataset num worker self hparam num worker shuffl true batch size self hparam batch size def epoch end self torch randn self hparam latent dim match gpu devic cpu self gpu cuda self img devic index log sampl imag sampl img self gener sampl img sampl img view self hparam self hparam imag size self hparam imag size grid torchvis util grid sampl img nrow self logger experi log imag grid permut gener imag epoch self current epoch step self current epoch save model self current epoch self hparam save model epoch trainer save checkpoint self checkpoint folder self experi epoch str self current epoch ckpt logger experi log asset folder self checkpoint folder step self current epoch delet folder save model dont upload thing twice dirpath path self checkpoint folder dirpath exist dirpath dir shutil rmtree dirpath creat checkpoint folder access right makedir dirpath access right argpars import namespac arg batch size latent dim test valu work ndf ngf epoch save model epoch imag size num worker level nois experi save batch valu low test valu work experi batch size valu shouldnt high test valu work hparam namespac arg paramet experi dcgan mnist dataset mnist checkpoint folder dcgan tag dcgan mnist overfit dirpath path checkpoint folder init logger logger logger logger api kei rest api kei project gan experi experi experi kei fdcfeeebfbefcfd resum train defin net net dcgan hparam logger checkpoint folder experi log logger experi set model graph str net logger experi add tag tag tag logger experi log dataset info dataset trainer trainer resum checkpoint ghibli dcgan overfit epoch ckpt logger logger max epoch arg epoch trainer fit net logger experi end",
        "Question_preprocessed_content":"googl colab freez browser try reconnect notebook train machin learn model googl colab specif train gan problem occur disconnect current runtim inact try reconnect browser laggi freez start lag abl close browser awai forc press power button order restart clue happen tri batch size happen dataset big hope help code",
        "Question_gpt_summary_original":"The user is encountering issues with Google Colab freezing their browser and PC when trying to reconnect to a notebook while training a machine learning model, specifically a GAN with PyTorch-lightning. The issue occurs when the user gets disconnected from their current runtime due to inactivity. They have tried various batch sizes and datasets, but the problem persists. The user has provided their code for reference.",
        "Question_gpt_summary":"user encount issu googl colab freez browser try reconnect notebook train machin learn model specif gan pytorch lightn issu occur user get disconnect current runtim inact tri batch size dataset problem persist user provid code refer",
        "Answer_original_content":"fix import ipython displai import clear output",
        "Answer_preprocessed_content":"fix import",
        "Answer_gpt_summary_original":"Solution: The user found a solution to their problem by importing \"clear_output\" from IPython.display.",
        "Answer_gpt_summary":"solut user solut problem import clear output ipython displai"
    },
    {
        "Question_title":"Probelm when running dvc exp run",
        "Question_body":"<p>Hello, upon running a dvc exp run i get the following error message, after all the stages are finished:<br>\nERROR: configuration error - config file error: expected \u2018url\u2019 for dictionary value \u2026<br>\nIn addition to that a very old version of the project is being restored eventhough commits to both git and dvc have been made. What could be the reason for that?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1662547455901,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":95.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/nenkoff\">@Nenkoff<\/a>. Can you please share the verbose output? Please add <code>-v<\/code> to the command that you were using. Thanks.<\/p>. <p>Hi, currently the command is working, even though no changes were made on the code. Cloning the git branch and using dvc destroy plus the same procedure used for the initializing dvc on the first branch yielded the expected normal behavior of \u201cdvc exp run\u201d. I was thinking if this could be a problem with the VPN or the remote storage access? Also is \u201cdvc destroy\u201d removing the cache and all .dvc files from all the branches of a git repo or just the one we are currently on?<\/p>. <p>The VPN is unlikely to have affected <code>dvc exp run<\/code>, please let us know if you encounter the issue again, also providing full logs (<code>dvc exp run -v<\/code>)  as well as version (<code>dvc doctor<\/code>): this will help us help you <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> .<\/p>\n<p>About <code>dvc destroy<\/code>: it will only remove <code>&lt;name&gt;.dvc<\/code> files from the current branch, but will also remove all cache files, config files stored in the <code>.dvc<\/code>, which are not tracked by git. Please refer to the docs for <a href=\"https:\/\/dvc.org\/doc\/command-reference\/destroy\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">destroy<\/a>.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/probelm-when-running-dvc-exp-run\/1329",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-13T07:42:36.971Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/nenkoff\">@Nenkoff<\/a>. Can you please share the verbose output? Please add <code>-v<\/code> to the command that you were using. Thanks.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-13T12:03:28.257Z",
                "Answer_body":"<p>Hi, currently the command is working, even though no changes were made on the code. Cloning the git branch and using dvc destroy plus the same procedure used for the initializing dvc on the first branch yielded the expected normal behavior of \u201cdvc exp run\u201d. I was thinking if this could be a problem with the VPN or the remote storage access? Also is \u201cdvc destroy\u201d removing the cache and all .dvc files from all the branches of a git repo or just the one we are currently on?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-13T15:29:42.694Z",
                "Answer_body":"<p>The VPN is unlikely to have affected <code>dvc exp run<\/code>, please let us know if you encounter the issue again, also providing full logs (<code>dvc exp run -v<\/code>)  as well as version (<code>dvc doctor<\/code>): this will help us help you <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> .<\/p>\n<p>About <code>dvc destroy<\/code>: it will only remove <code>&lt;name&gt;.dvc<\/code> files from the current branch, but will also remove all cache files, config files stored in the <code>.dvc<\/code>, which are not tracked by git. Please refer to the docs for <a href=\"https:\/\/dvc.org\/doc\/command-reference\/destroy\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">destroy<\/a>.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"probelm run exp run hello run exp run follow error messag stage finish error configur error config file error expect url dictionari valu addit old version project restor eventhough commit git reason",
        "Question_preprocessed_content":"probelm run exp run hello run exp run follow error messag stage finish error configur error config file error expect url dictionari valu addit old version project restor eventhough commit git reason",
        "Question_gpt_summary_original":"The user encountered an error message while running a dvc exp run, specifically an \"ERROR: configuration error - config file error: expected \u2018url\u2019 for dictionary value\". Additionally, an old version of the project is being restored despite recent commits to both git and dvc. The user is seeking assistance in identifying the cause of these issues.",
        "Question_gpt_summary":"user encount error messag run exp run specif error configur error config file error expect url dictionari valu addition old version project restor despit recent commit git user seek assist identifi caus issu",
        "Answer_original_content":"nenkoff share verbos output add command thank current command work chang code clone git branch destroi plu procedur initi branch yield expect normal behavior exp run think problem vpn remot storag access destroi remov cach file branch git repo current vpn unlik affect exp run let know encount issu provid log exp run version doctor help help destroi remov file current branch remov cach file config file store track git refer doc destroi",
        "Answer_preprocessed_content":"share verbos output add command thank current command work chang code clone git branch destroi plu procedur initi branch yield expect normal behavior exp run think problem vpn remot storag access destroi remov cach file branch git repo current vpn unlik affect let know encount issu provid log version help help remov file current branch remov cach file config file store track git refer doc destroi",
        "Answer_gpt_summary_original":"Solutions provided:\n- The user was advised to provide full logs and version information to help identify the issue.\n- The user was informed that \"dvc destroy\" removes all cache files and config files stored in the .dvc, which are not tracked by git, but only removes <name>.dvc files from the current branch.\n\nNo solution was provided for the error message encountered while running \"dvc exp run\" or the issue of an old version of the project being restored despite recent commits to both git and dvc.",
        "Answer_gpt_summary":"solut provid user advis provid log version inform help identifi issu user inform destroi remov cach file config file store track git remov file current branch solut provid error messag encount run exp run issu old version project restor despit recent commit git"
    },
    {
        "Question_title":"How to early stop bad runs in sweeps to save time",
        "Question_body":"<p>Hello,<br>\nthat\u2019s my first topic in the community, so I hope I am posting that in the correct category <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>I started exploring sweeps last week for a university project, and it is incredible! As we also got a new PyTorch version with support for the new apple silicon, I wanted to try that on my M1 Pro. As this is not as powerful as, for example, using GoogleColab for a fraction of the time, I wanted to ask if it is somehow possible to stop bad runs after a few epochs.<\/p>\n<p>As you can see in the report linked below, the run hopeful-sweep-2 does not look promising. It would be nice to cancel that run and start a new one instead.<\/p>\n<p>Thanks,<br>\nMarkus<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_500x500.jpeg\" class=\"thumbnail onebox-avatar\" width=\"500\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_500x500.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_750x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_10x10.png\">\n\n<h3><a href=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\" target=\"_blank\" rel=\"noopener\">Weights &amp; Biases<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1654584250764,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":270.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a> ,<\/p>\n<p>Thank you for writing in with your question. We do support early termination of sweeps, this reference <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#early_terminate\">doc<\/a> covers this. When the early stopping is triggered, the agent stops the current run and gets the next set of hyperparameters to try. Here is a <a href=\"https:\/\/github.com\/wandb\/examples\/blob\/master\/examples\/keras\/keras-cnn-fashion\/sweep-bayes-hyperband.yaml\" rel=\"noopener nofollow ugc\">link<\/a> to an example sweep configuration for reference. If after setting up your configuration and your require review \/ feedback. Please do write back in this thread and we can review your work more closely.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-early-stop-bad-runs-in-sweeps-to-save-time\/2563",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-07T18:41:53.182Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a> ,<\/p>\n<p>Thank you for writing in with your question. We do support early termination of sweeps, this reference <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#early_terminate\">doc<\/a> covers this. When the early stopping is triggered, the agent stops the current run and gets the next set of hyperparameters to try. Here is a <a href=\"https:\/\/github.com\/wandb\/examples\/blob\/master\/examples\/keras\/keras-cnn-fashion\/sweep-bayes-hyperband.yaml\" rel=\"noopener nofollow ugc\">link<\/a> to an example sweep configuration for reference. If after setting up your configuration and your require review \/ feedback. Please do write back in this thread and we can review your work more closely.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-06-08T05:11:46.045Z",
                "Answer_body":"<p>Thanks a lot <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/smiley.png?v=12\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\nThat was exactly what I was looking for, just searched for early stopping and not terminating and somehow didn\u2019t find it <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>Is there also a way to change the configuration of a running sweep? Or stop it and continue it with a new configuration?<\/p>\n<p>Thanks &amp; Regards,<br>\nMarkus<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-08T19:54:18.740Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a>,<\/p>\n<p>You\u2019re welcome, glad that helped.<\/p>\n<p>Currently we do not allow the modification of a running sweep. A sweep is inherently tied to its config, so once it is set there is no way to change it. You can however, create a new sweep and seed it with existing runs, see <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs\">here<\/a>. Hope this helps.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-09T08:13:38.558Z",
                "Answer_body":"<p>Thanks for your kind reply <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-08T08:14:38.630Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1654627313182,
        "Question_original_content":"earli stop bad run sweep save time hello that topic commun hope post correct categori start explor sweep week univers project incred got new pytorch version support new appl silicon want try pro power exampl googlecolab fraction time want ask possibl stop bad run epoch report link run hope sweep look promis nice cancel run start new instead thank marku develop tool machin learn",
        "Question_preprocessed_content":"earli stop bad run sweep save time hello that topic commun hope post correct categori start explor sweep week univers project incred got new pytorch version support new appl silicon want try pro power exampl googlecolab fraction time want ask possibl stop bad run epoch report link run look promis nice cancel run start new instead thank marku develop tool machin learn",
        "Question_gpt_summary_original":"The user is exploring sweeps for a university project and wants to know if it is possible to stop bad runs after a few epochs to save time. They have provided a report showing a run that does not look promising and would like to cancel it and start a new one instead.",
        "Question_gpt_summary":"user explor sweep univers project want know possibl stop bad run epoch save time provid report show run look promis like cancel start new instead",
        "Answer_original_content":"markuskarn thank write question support earli termin sweep refer doc cover earli stop trigger agent stop current run get set hyperparamet try link exampl sweep configur refer set configur requir review feedback write thread review work close regard mohammad",
        "Answer_preprocessed_content":"thank write question support earli termin sweep refer doc cover earli stop trigger agent stop current run get set hyperparamet try link exampl sweep configur refer set configur requir review feedback write thread review work close regard mohammad",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the user's challenge. The solution is to use early termination of sweeps, which is supported by the platform. The reference document provided in the discussion covers this feature. When the early stopping is triggered, the agent stops the current run and gets the next set of hyperparameters to try. The discussion also provides a link to an example sweep configuration for reference.",
        "Answer_gpt_summary":"solut discuss provid solut user challeng solut us earli termin sweep support platform refer document provid discuss cover featur earli stop trigger agent stop current run get set hyperparamet try discuss provid link exampl sweep configur refer"
    },
    {
        "Question_title":"Is it possible to predict in sagemaker without using s3",
        "Question_body":"<p>I have a .pkl which I would like to put into production. I would like to do a daily query of my SQL server and do a prediction on about 1000 rows. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">documentation<\/a> implies I have to load the daily data into s3. Is there a way around this? It should be able to fit in memory no problem. <\/p>\n\n<p>The answer to \" <a href=\"https:\/\/stackoverflow.com\/questions\/48319893\/is-there-some-kind-of-persistent-local-storage-in-aws-sagemaker-model-training\">is there some kind of persistent local storage in aws sagemaker model training?<\/a> \" says that \"<em>The notebook instance is coming with a local EBS (5GB) that you can use to copy some data into it and run the fast development iterations without copying the data every time from S3.<\/em>\" The 5GB could be enough but I am not sure you can actually run from a notebook in this manner. If I set up a VPN could I just query using pyodbc?<\/p>\n\n<p>Is there sagemaker integration with AWS Lambda? That in combination with a docker container would suit my needs.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1531871216253,
        "Question_favorite_count":null,
        "Question_last_edit_time":1531893078607,
        "Question_score":2.0,
        "Question_view_count":1576.0,
        "Answer_body":"<p>While you need to to specify a s3 \"folder\" as input, this folder can contain only a dummy file. \nAlso if you bring your own docker container for training like in <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">this example<\/a>, you can do pretty much everthing in it. So you could do your daily query inside your docker container, because they have access to the internet. <\/p>\n\n<p>Also inside this container you have access to all the other aws services. Your access is defined by the role you're passing to your training job.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51391639",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1531919252070,
        "Question_original_content":"possibl predict pkl like product like daili queri sql server predict row document impli load daili data wai abl fit memori problem answer kind persist local storag model train sai notebook instanc come local eb us copi data run fast develop iter copi data time sure actual run notebook manner set vpn queri pyodbc integr aw lambda combin docker contain suit need",
        "Question_preprocessed_content":"possibl predict pkl like product like daili queri sql server predict row document impli load daili data wai abl fit memori problem answer kind persist local storag model train sai notebook instanc come local eb us copi data run fast develop iter copi data time sure actual run notebook manner set vpn queri pyodbc integr aw lambda combin docker contain suit need",
        "Question_gpt_summary_original":"The user wants to put a .pkl file into production and do a daily query of their SQL server to predict on about 1000 rows. However, the documentation implies that the daily data needs to be loaded into s3, which the user wants to avoid. They are unsure if they can run from a notebook with local EBS storage or query using pyodbc with a VPN. They are also wondering if there is sagemaker integration with AWS Lambda, which would suit their needs.",
        "Question_gpt_summary":"user want pkl file product daili queri sql server predict row document impli daili data need load user want avoid unsur run notebook local eb storag queri pyodbc vpn wonder integr aw lambda suit need",
        "Answer_original_content":"need specifi folder input folder contain dummi file bring docker contain train like exampl pretti everth daili queri insid docker contain access internet insid contain access aw servic access defin role pass train job",
        "Answer_preprocessed_content":"need specifi folder input folder contain dummi file bring docker contain train like exampl pretti everth daili queri insid docker contain access internet insid contain access aw servic access defin role pass train job",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. The user can specify a dummy file in the s3 folder as input and bring their own docker container for training to do the daily query inside the container, which has access to the internet and all other AWS services. \n\nThere is no mention of any solution related to using a notebook with local EBS storage or querying using pyodbc with a VPN or sagemaker integration with AWS Lambda.",
        "Answer_gpt_summary":"possibl solut mention discuss user specifi dummi file folder input bring docker contain train daili queri insid contain access internet aw servic mention solut relat notebook local eb storag queri pyodbc vpn integr aw lambda"
    },
    {
        "Question_title":"I can not install \"git-lfs\" on aws sagemaker notebook instance",
        "Question_body":"<p>I Can not run <code>apt to install git-lfs<\/code> on sagemaker notebook instance. I want to run git commands in my notebook.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640732204667,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":489.0,
        "Answer_body":"<p>use the following commands to install git-lfs<\/p>\n<pre><code>!curl -s https:\/\/packagecloud.io\/install\/repositories\/github\/git-lfs\/script.rpm.sh | sudo bash\n\n!sudo yum install git-lfs -y\n\n!git lfs install\n<\/code><\/pre>\n<p>that should make it work<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70513398",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1640732355180,
        "Question_original_content":"instal git lf notebook instanc run apt instal git lf notebook instanc want run git command notebook",
        "Question_preprocessed_content":"instal notebook instanc run notebook instanc want run git command notebook",
        "Question_gpt_summary_original":"The user is facing a challenge in installing \"git-lfs\" on their AWS Sagemaker notebook instance, which is preventing them from running git commands in their notebook.",
        "Question_gpt_summary":"user face challeng instal git lf notebook instanc prevent run git command notebook",
        "Answer_original_content":"us follow command instal git lf curl http packagecloud instal repositori github git lf script rpm sudo bash sudo yum instal git lf git lf instal work",
        "Answer_preprocessed_content":"us follow command instal work",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the challenge by suggesting the user to use the following commands to install \"git-lfs\" on their AWS Sagemaker notebook instance:\n\n!curl -s https:\/\/packagecloud.io\/install\/repositories\/github\/git-lfs\/script.rpm.sh | sudo bash\n\n!sudo yum install git-lfs -y\n\n!git lfs install\n\nThese commands should enable the user to run git commands in their notebook.",
        "Answer_gpt_summary":"solut discuss provid solut challeng suggest user us follow command instal git lf notebook instanc curl http packagecloud instal repositori github git lf script rpm sudo bash sudo yum instal git lf git lf instal command enabl user run git command notebook"
    },
    {
        "Question_title":"How to use fitted_model.forecast() for AutoML forecasting model?",
        "Question_body":"<p>Say I have a dataset with a monthly granularity with the following columns:<\/p>\n\n<ul>\n<li>Timestamp<\/li>\n<li>Issues (i.e. number of GitHub issues)<\/li>\n<\/ul>\n\n<p>There is data for each month for all of 2016-2019, so I divide the data accordingly.<\/p>\n\n<ul>\n<li><code>training_data<\/code>: 2016-2017<\/li>\n<li><code>validation_data<\/code>: 2018<\/li>\n<li><code>holdout_data<\/code>: 2019<\/li>\n<\/ul>\n\n<p>If I have a <code>fitted_model<\/code> that is a <code>ForecastingPipelineWrapper<\/code> which is the best run from AutoML where I passed gave it <code>training_data<\/code> and <code>validation_data<\/code>.<\/p>\n\n<p>Looking at the <a href=\"https:\/\/gist.github.com\/swanderz\/c68514d955bebf20f48ffd91aabdc55d\" rel=\"nofollow noreferrer\">ForecastingPipelineWrapper class docstring<\/a> documentation only serves to confuse me more. What is <code>X_past<\/code>, <code>X_future<\/code>, and <code>Y_future<\/code>?<\/p>\n\n<p>How do I use the above dataframes with <code>fitted_model.forecast()<\/code> to manually validate model fit on the <code>holdout_data<\/code> dataframe?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586997104997,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1587136317380,
        "Question_score":2.0,
        "Question_view_count":284.0,
        "Answer_body":"<p>The following notebook illustrates how to leverage y_past, x_past, y_future, x_future, and fitted_model.forecast in the bottom half, 'Forecasting away from training data'. <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-high-frequency\/auto-ml-forecasting-function.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-high-frequency\/auto-ml-forecasting-function.ipynb<\/a><\/p>\n\n<p>The notebook will be a much better guide to grasping these concepts than perhaps a docstring doc. Should you have any more questions or need clarity, let us know!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61240809",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1587062122060,
        "Question_original_content":"us fit model forecast automl forecast model dataset monthli granular follow column timestamp issu number github issu data month divid data accordingli train data valid data holdout data fit model forecastingpipelinewrapp best run automl pass gave train data valid data look forecastingpipelinewrapp class docstr document serv confus past futur futur us datafram fit model forecast manual valid model fit holdout data datafram",
        "Question_preprocessed_content":"us automl forecast model dataset monthli granular follow column timestamp issu data month divid data accordingli best run automl pass gave look forecastingpipelinewrapp class docstr document serv confus us datafram manual valid model fit datafram",
        "Question_gpt_summary_original":"The user is facing challenges in understanding how to use the fitted_model.forecast() function for an AutoML forecasting model. They have a dataset with monthly granularity and are dividing the data into training_data, validation_data, and holdout_data. The confusion arises from the ForecastingPipelineWrapper class docstring documentation, specifically regarding the X_past, X_future, and Y_future dataframes, and how to use them with the fitted_model.forecast() function to validate the model fit on the holdout_data dataframe.",
        "Question_gpt_summary":"user face challeng understand us fit model forecast function automl forecast model dataset monthli granular divid data train data valid data holdout data confus aris forecastingpipelinewrapp class docstr document specif past futur futur datafram us fit model forecast function valid model fit holdout data datafram",
        "Answer_original_content":"follow notebook illustr leverag past past futur futur fit model forecast half forecast awai train data http github com azur machinelearningnotebook blob master us autom machin learn forecast high frequenc auto forecast function ipynb notebook better guid grasp concept docstr doc question need clariti let know",
        "Answer_preprocessed_content":"follow notebook illustr leverag half forecast awai train data notebook better guid grasp concept docstr doc question need clariti let know",
        "Answer_gpt_summary_original":"Solution: The discussion suggests referring to a notebook that illustrates how to use y_past, x_past, y_future, x_future, and fitted_model.forecast in the 'Forecasting away from training data' section. The notebook provides a better guide to understanding these concepts than the docstring documentation.",
        "Answer_gpt_summary":"solut discuss suggest refer notebook illustr us past past futur futur fit model forecast forecast awai train data section notebook provid better guid understand concept docstr document"
    },
    {
        "Question_title":"Organizations using MLflow - Emerton Data",
        "Question_body":"Hi !\n\n\nAt Emerton Data, we are big fans of MLflow and are using it in our project to industrialize AI models and data projects.\n\n\nHappy to be one of the mlflow supporter and would be glad to appear on your website as an organization using MLFlow.\u00a0\n\n\nCheers,\n\n\n\nYannick LEO\nDirector Data Science\n\n16 avenue Hoche\u00a0\u00a0\n75008 Paris\u00a0\nM + 33 6 38 21 33 99\nT + 33 1 53 75 38 75\nyanni...@emerton-data.com\u00a0|\u00a0http:\/\/www.emerton-data.com",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593271817000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":30.0,
        "Answer_body":"Hello Yannick,\n\n\nWe will be happy to include you as supporters and users of mlflow.\u00a0\n\n\nCan you send me your preferred logo? We will include in the list.\n\n\nThanks for being a fan and supporter of MLflow.\n\n\nCheers\nJules\u00a0\n\n\nSent from my iPhone\nPardon the dumb thumb typos :)\n\n\nOn Jun 27, 2020, at 12:32 PM, Yannick Leo <yanni...@emerton-data.com> wrote:\n\n\n\ufeffHi !\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/a52afb80-ce39-4985-8e13-ef601a2d4edfn%40googlegroups.com.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/9WHVX1YBK_A",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-27T17:33:39",
                "Answer_body":"Hello Yannick,\n\n\nWe will be happy to include you as supporters and users of mlflow.\u00a0\n\n\nCan you send me your preferred logo? We will include in the list.\n\n\nThanks for being a fan and supporter of MLflow.\n\n\nCheers\nJules\u00a0\n\n\nSent from my iPhone\nPardon the dumb thumb typos :)\n\n\nOn Jun 27, 2020, at 12:32 PM, Yannick Leo <yanni...@emerton-data.com> wrote:\n\n\n\ufeffHi !\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/a52afb80-ce39-4985-8e13-ef601a2d4edfn%40googlegroups.com."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"organ emerton data emerton data big fan project industri model data project happi support glad appear websit organ cheer yannick leo director data scienc avenu hoch pari yanni emerton data com http emerton data com",
        "Question_preprocessed_content":"organ emerton data emerton data big fan project industri model data project happi support glad appear websit organ cheer yannick leo director data scienc avenu hoch pari",
        "Question_gpt_summary_original":"The given text does not mention any challenges faced by the user. It is a message from Yannick LEO, Director Data Science at Emerton Data, expressing their support for MLflow and their interest in being listed as an organization using MLflow.",
        "Question_gpt_summary":"given text mention challeng face user messag yannick leo director data scienc emerton data express support list organ",
        "Answer_original_content":"hello yannick happi includ support user send prefer logo includ list thank fan support cheer jule sent iphon pardon dumb thumb typo jun yannick leo wrote receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user aafb efadedfn googlegroup com",
        "Answer_preprocessed_content":"hello yannick happi includ support user send prefer logo includ list thank fan support cheer jule sent iphon pardon dumb thumb typo jun yannick leo wrote receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit",
        "Answer_gpt_summary_original":"There are no solutions mentioned in the given text as it is a message expressing support for MLflow and interest in being listed as an organization using MLflow.",
        "Answer_gpt_summary":"solut mention given text messag express support list organ"
    },
    {
        "Question_title":"Validation error on Role name when running AWS SageMaker linear-learner locally",
        "Question_body":"<p>I'm trying to build a machine learning model locally using AWS SageMaker, but I got a validation error on IAM Role name. Although it's the exact role name that I created on the console.<\/p>\n<p>This is my code<\/p>\n<pre><code>    import boto3\n    import sagemaker\n    from sagemaker import get_execution_role\n    from sagemaker.amazon.amazon_estimator import image_uris\n    from sagemaker.amazon.amazon_estimator import RecordSet\n\n    sess = sagemaker.Session()\n\n\n    bucket = sagemaker.Session().default_bucket()\n    prefix = 'sagemaker\/ccard19'\n\n    role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access '\n\n    container = image_uris.retrieve('linear-learner',boto3.Session().region_name)\n    \n    # Some other code\n\n   linear = sagemaker.LinearLearner(role=role,\n                                               instance_count=1,\n                                               instance_type='ml.m4.xlarge',\n                                               predictor_type='binary_classifier')\n  \n  # Some other code\n\n  ### Fit the classifier\n  linear.fit([train_records,val_records,test_records], wait=True, logs='All')\n\n<\/code><\/pre>\n<p>And this is the error message<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: 1 validation error detected: Value 'arn:aws:iam::949010940542:role\/SageMaker-Full-Access ' at 'roleArn' failed to satisfy constraint: Member must satisfy regular expression pattern: ^arn:aws[a-z\\-]*:iam::\\d{12}:role\/?[a-zA-Z_0-9+=,.@\\-_\/]+$\n<\/code><\/pre>\n<p>Any Help please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617256728180,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":560.0,
        "Answer_body":"<p>You have <strong>space<\/strong> in the name. It should be:<\/p>\n<pre><code>role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access'\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66899120",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1617256809808,
        "Question_original_content":"valid error role run linear learner local try build machin learn model local got valid error iam role exact role creat consol code import boto import import execut role amazon amazon estim import imag uri amazon amazon estim import recordset sess session bucket session default bucket prefix ccard role arn aw iam role access contain imag uri retriev linear learner boto session region code linear linearlearn role role instanc count instanc type xlarg predictor type binari classifi code fit classifi linear fit train record val record test record wait true log error messag clienterror error occur validationexcept call createtrainingjob oper valid error detect valu arn aw iam role access rolearn fail satisfi constraint member satisfi regular express pattern arn aw iam role help",
        "Question_preprocessed_content":"valid error role run local try build machin learn model local got valid error iam role exact role creat consol code error messag help",
        "Question_gpt_summary_original":"The user encountered a validation error on IAM Role name when trying to build a machine learning model locally using AWS SageMaker. The error message indicated that the value of the roleArn failed to satisfy the regular expression pattern. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount valid error iam role try build machin learn model local error messag indic valu rolearn fail satisfi regular express pattern user seek help resolv issu",
        "Answer_original_content":"space role arn aw iam role access",
        "Answer_preprocessed_content":"space",
        "Answer_gpt_summary_original":"Solution: The user should remove the space in the IAM Role name and use the correct format: 'arn:aws:iam::account-id:role\/role-name'.",
        "Answer_gpt_summary":"solut user remov space iam role us correct format arn aw iam account role role"
    },
    {
        "Question_title":"How to optimize for multiple metrics in Optuna",
        "Question_body":"<p>How do I optimize for multiple metrics simultaneously inside the <code>objective<\/code> function of Optuna. For example, I am training an LGBM classifier and want to find the best hyperparameter set for all common classification metrics like F1, precision, recall, accuracy, AUC, etc.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    # Train\n    gbm = lgb.train(param, dtrain)\n\n    preds = gbm.predict(X_test)\n    pred_labels = np.rint(preds)\n    # Calculate metrics\n    accuracy = sklearn.metrics.accuracy_score(y_test, pred_labels)\n    recall = metrics.recall_score(pred_labels, y_test)\n    precision = metrics.precision_score(pred_labels, y_test)\n    f1 = metrics.f1_score(pred_labels, y_test, pos_label=1)\n\n    ...\n<\/code><\/pre>\n<p>How do I do it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630917852487,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1630917952870,
        "Question_score":2.0,
        "Question_view_count":1887.0,
        "Answer_body":"<p>After defining the grid and fitting the model with these params and generate predictions, calculate all metrics you want to optimize for:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    param_grid = {&quot;n_estimators&quot;: trial.suggest_int(&quot;n_estimators&quot;, 2000, 10000, step=200)}\n    clf = lgbm.LGBMClassifier(objective='binary', **param_grid)\n    clf.fit(X_train, y_train)\n    preds = clf.predict(X_valid)\n    probs = clf.predict_proba(X_valid)\n \n    # Metrics\n    f1 = sklearn.metrics.f1_score(y_valid, press)\n    accuracy = ...\n    precision = ...\n    recall = ...\n    logloss = ...\n<\/code><\/pre>\n<p>and return them in the order you want:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    ...\n\n    return f1, logloss, accuracy, precision, recall\n<\/code><\/pre>\n<p>Then, in the study object, specify whether you want to minimize or maximize each metric to <code>directions<\/code> like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>study = optuna.create_study(directions=['maximize', 'minimize', 'maximize', 'maximize', 'maximize'])\n\nstudy.optimize(objective, n_trials=100)\n<\/code><\/pre>\n<p>For more details, see <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/20_recipes\/002_multi_objective.html#sphx-glr-tutorial-20-recipes-002-multi-objective-py\" rel=\"nofollow noreferrer\">Multi-objective Optimization with Optuna<\/a> in the documentation.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1659853446112,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69071684",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1630917852487,
        "Question_original_content":"optim multipl metric optim multipl metric simultan insid object function exampl train lgbm classifi want best hyperparamet set common classif metric like precis recal accuraci auc def object trial train gbm lgb train param dtrain pred gbm predict test pred label rint pred calcul metric accuraci sklearn metric accuraci score test pred label recal metric recal score pred label test precis metric precis score pred label test metric score pred label test po label",
        "Question_preprocessed_content":"optim multipl metric optim multipl metric simultan insid function exampl train lgbm classifi want best hyperparamet set common classif metric like precis recal accuraci auc",
        "Question_gpt_summary_original":"The user is facing a challenge of optimizing for multiple metrics simultaneously inside the objective function of Optuna while training an LGBM classifier. The user wants to find the best hyperparameter set for all common classification metrics like F1, precision, recall, accuracy, AUC, etc.",
        "Question_gpt_summary":"user face challeng optim multipl metric simultan insid object function train lgbm classifi user want best hyperparamet set common classif metric like precis recal accuraci auc",
        "Answer_original_content":"defin grid fit model param gener predict calcul metric want optim def object trial param grid estim trial suggest int estim step clf lgbm lgbmclassifi object binari param grid clf fit train train pred clf predict valid prob clf predict proba valid metric sklearn metric score valid press accuraci precis recal logloss return order want def object trial return logloss accuraci precis recal studi object specifi want minim maxim metric direct like studi creat studi direct maxim minim maxim maxim maxim studi optim object trial detail multi object optim document",
        "Answer_preprocessed_content":"defin grid fit model param gener predict calcul metric want optim return order want studi object specifi want minim maxim metric like detail optim document",
        "Answer_gpt_summary_original":"The solution suggested in the discussion is to define the grid and fit the model with the parameters, generate predictions, and calculate all the metrics that need to be optimized for. Then, return the metrics in the order they need to be optimized for and specify whether to minimize or maximize each metric in the study object using the directions parameter. The documentation link for multi-objective optimization with Optuna is also provided for more details.",
        "Answer_gpt_summary":"solut suggest discuss defin grid fit model paramet gener predict calcul metric need optim return metric order need optim specifi minim maxim metric studi object direct paramet document link multi object optim provid detail"
    },
    {
        "Question_title":"\"dvc add -external S3:\/\/mybucket\/data.csv\" is failing with access error even after giving correct remote cache configurations",
        "Question_body":"<p>Hi All,<\/p>\n<p>Am connecting to remote S3 for data and also setting remote dvc cache in same S3.<br>\nFollowing is configure file,<\/p>\n<pre><code class=\"lang-ini\">[core]\n    remote = s3remote\n[cache]\n    s3 = s3cache\n['remote \"s3remote\"']\n    url = S3:\/\/dvc-example\n    endpointurl = http:\/\/localhost:9000\/\n    access_key_id = user\n    secret_access_key = password\n    use_ssl = false\n['remote \"s3cache\"']\n    url = s3:\/\/dvc-example\/cache\n\tendpointurl = http:\/\/localhost:9000\/\n    access_key_id = user\n    secret_access_key = password\n    use_ssl = false\n<\/code><\/pre>\n<p>Am able to push and pull from remote repository to local.<br>\nBut when I try to add external data by configuring cache, am getting error.<br>\nBoth s3cache, s3remote has same credentials, then why is it failing when I add external data in dvc?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/844a3dfabb3e006bfe4f8e15222af4a0236fc610.png\" data-download-href=\"\/uploads\/short-url\/iSi5wqipNKK3zGD6gzqzzcaTV28.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/844a3dfabb3e006bfe4f8e15222af4a0236fc610.png\" alt=\"image\" data-base62-sha1=\"iSi5wqipNKK3zGD6gzqzzcaTV28\" width=\"690\" height=\"32\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/844a3dfabb3e006bfe4f8e15222af4a0236fc610_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1553\u00d773 5.72 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Any help would be much appreciated.<\/p>\n<blockquote>\n<p>Also posted in <a href=\"https:\/\/stackoverflow.com\/questions\/67104752\" rel=\"noopener nofollow ugc\">https:\/\/stackoverflow.com\/questions\/67104752<\/a><\/p>\n<\/blockquote>",
        "Question_answer_count":9,
        "Question_comment_count":0,
        "Question_creation_time":1618473589784,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":8.0,
        "Question_view_count":1385.0,
        "Answer_body":"<p>Can you share the output of <code>dvc doctor<\/code> and the full traceback of the failing command when runned with <code>-v<\/code> (eg <code>dvc add -v --external s3:\/\/&lt;blah&gt;<\/code>)<\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/isidentical\">@isidentical<\/a> ,<\/p>\n<ul>\n<li>\n<p><strong>Here is out put of dvc doctor<\/strong><\/p>\n<p>$ dvc doctor<br>\nDVC version: 2.0.17 (exe)<\/p>\n<\/li>\n<\/ul>\n<hr>\n<p>Platform: Python 3.8.8 on Windows-10-10.0.17763-SP0<br>\nSupports: All remotes<br>\nCache types: <a href=\"https:\/\/error.dvc.org\/no-dvc-cache\" rel=\"noopener nofollow ugc\">https:\/\/error.dvc.org\/no-dvc-cache<\/a><br>\nCaches: local, s3<br>\nRemotes: s3<br>\nWorkspace directory: NTFS on C:<br>\nRepo: dvc, git<\/p>\n<ul>\n<li>\n<strong>Here are the logs<\/strong>.<\/li>\n<\/ul>\n<pre><code class=\"lang-python\">Traceback (most recent call last):\n  File \"dvc\\main.py\", line 55, in main\n  File \"dvc\\command\\add.py\", line 21, in run\n  File \"dvc\\repo\\__init__.py\", line 49, in wrapper\n  File \"dvc\\repo\\scm_context.py\", line 14, in run\n  File \"dvc\\repo\\add.py\", line 123, in add\n  File \"dvc\\repo\\add.py\", line 195, in _process_stages\n  File \"dvc\\stage\\__init__.py\", line 437, in save\n  File \"dvc\\stage\\__init__.py\", line 451, in save_outs\n  File \"dvc\\output\\base.py\", line 280, in save\n  File \"dvc\\output\\base.py\", line 208, in exists\n  File \"dvc\\fs\\s3.py\", line 233, in exists\n  File \"dvc\\fs\\s3.py\", line 263, in isfile\n  File \"dvc\\fs\\s3.py\", line 284, in _list_paths\n  File \"boto3\\resources\\collection.py\", line 83, in __iter__\n  File \"boto3\\resources\\collection.py\", line 166, in pages\n  File \"botocore\\paginate.py\", line 255, in __iter__\n  File \"botocore\\paginate.py\", line 332, in _make_request\n  File \"botocore\\client.py\", line 357, in _api_call\n  File \"botocore\\client.py\", line 676, in _make_api_call\nbotocore.exceptions.ClientError: An error occurred (InvalidAccessKeyId) when calling the ListObjects operation: The AWS Access Key Id you provided does not exist in our records.\n<\/code><\/pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/299cdad6d06f3c460b6649ce67d79c9027e9eae5.png\" data-download-href=\"\/uploads\/short-url\/5W7DrGlsLno5Th0YXNbiBNQni85.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/299cdad6d06f3c460b6649ce67d79c9027e9eae5.png\" alt=\"image\" data-base62-sha1=\"5W7DrGlsLno5Th0YXNbiBNQni85\" width=\"690\" height=\"287\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/299cdad6d06f3c460b6649ce67d79c9027e9eae5_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1761\u00d7734 31.1 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div>.<\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/veeresh\">@veeresh<\/a>, I think there\u2019s a confusion here:<\/p>\n<p>You are using <code>s3:\/\/dataset\/<\/code> in your <code>add --external<\/code> command. I think that DVC tries to connect to a bucket called <code>dataset<\/code> and to use your default AWS credentials (e.g. in <code>~\/.aws\/<\/code>, as configured for AWS-CLI). Probably the bucket exists but it\u2019s someone else\u2019s, and either you don\u2019t have default AWS creds or they don\u2019t work with that bucket.<\/p>\n<p>To use your configured S3 remote as target, use the following format:<\/p>\n<pre><code class=\"lang-auto\">$ dvc add --external remote:\/\/s3remote\/path\/to\/dataset\n<\/code><\/pre>\n<p>Please let us know if that helps. I\u2019ll update <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\">https:\/\/dvc.org\/doc\/user-guide\/managing-external-data<\/a> to clarify on this meanwhile.<\/p>\n<p>BTW, you may be interested in another \u2014 usually better \u2014 way to add external data. See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-the-cache\">https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-the-cache<\/a>. Thanks<\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a> ,<\/p>\n<p>Its the same error I get even if I pass full dataset path.<br>\nI\u2019m passing the data path along with bucket name like below. Its still the same issue\/error.<\/p>\n<ul>\n<li>dvc add -v --external s3:\/\/dataset\/wine-quality.csv<br>\nDoes the above command also use locally configured AWS S3 configuration and not DVC remote configurations.<\/li>\n<\/ul>\n<p>config,<br>\n[core]<br>\nremote = s3remote<br>\n[cache]<br>\ns3 = s3cache<br>\n[\u2018remote \u201cs3remote\u201d\u2019]<br>\nurl = S3:\/\/dataset<br>\nendpointurl = http:\/\/{XYZ}:9000\/<br>\naccess_key_id = user<br>\nsecret_access_key = password<br>\nuse_ssl = false<br>\n[\u2018remote \u201cs3cache\u201d\u2019]<br>\nurl = s3:\/\/dataset\/cache\/<br>\nendpointurl = http:\/\/{XYZ}:9000\/<br>\naccess_key_id = user<br>\nsecret_access_key = password<br>\nuse_ssl = false<\/p>\n<p>This is my Minio bucket structure I have,<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/e9427f622d6a5828e9a505bad6b60b02124f1fa6.png\" data-download-href=\"\/uploads\/short-url\/xhvIHHW0SgEQkzQ8gT7962TmRCu.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/e9427f622d6a5828e9a505bad6b60b02124f1fa6_2_690x298.png\" alt=\"image\" data-base62-sha1=\"xhvIHHW0SgEQkzQ8gT7962TmRCu\" width=\"690\" height=\"298\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/e9427f622d6a5828e9a505bad6b60b02124f1fa6_2_690x298.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/e9427f622d6a5828e9a505bad6b60b02124f1fa6_2_1035x447.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/e9427f622d6a5828e9a505bad6b60b02124f1fa6.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/e9427f622d6a5828e9a505bad6b60b02124f1fa6_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1125\u00d7487 11.8 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>PS: I\u2019m using remote hosted Minio server to connect.<\/p>. <p>FYI,<br>\nRemote pull and push works as follows, but external add is not working for same cache and remote configuration.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/31f22a6b8f8acf73fcabc7d8bb0e97d0f14492ae.png\" data-download-href=\"\/uploads\/short-url\/77QeuglIGiJVXi9ro3F3E3I8urc.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/31f22a6b8f8acf73fcabc7d8bb0e97d0f14492ae_2_690x442.png\" alt=\"image\" data-base62-sha1=\"77QeuglIGiJVXi9ro3F3E3I8urc\" width=\"690\" height=\"442\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/31f22a6b8f8acf73fcabc7d8bb0e97d0f14492ae_2_690x442.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/31f22a6b8f8acf73fcabc7d8bb0e97d0f14492ae.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/31f22a6b8f8acf73fcabc7d8bb0e97d0f14492ae.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/31f22a6b8f8acf73fcabc7d8bb0e97d0f14492ae_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1011\u00d7648 154 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>. <p><a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a> ,<\/p>\n<p>The use case am trying to implement is to track the external data store (s3) remotely by dvc without storing\/downloading datasets locally.<br>\nThe link you mentioned ( <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add?&amp;_ga=2.37973290.800977337.1618466859-1686287924.1617853529#example-transfer-to-the-cache\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-the-cache<\/a>) stores data into local system right?<br>\nPlease let me know if you have any suggestions for this use case other than add --external?<\/p>\n<p>PS: I read ( Example: Transfer to remote storage , --to-remote ),But even here I see data(from https url) is pushed\/added to remote and tracked.<\/p>. <aside class=\"quote no-group\" data-username=\"veeresh\" data-post=\"5\" data-topic=\"726\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/v\/df705f\/40.png\" class=\"avatar\"> veeresh:<\/div>\n<blockquote>\n<p>I\u2019m passing the data path along with bucket name like below. Its still the same issue\/error.<br>\n<code>dvc add -v --external s3:\/\/dataset\/wine-quality.csv<\/code><br>\nDoes the above command also use locally configured AWS S3 configuration and not DVC remote configurations.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Yep, it seems you\u2019re still trying to connect to some S3 bucket called <code>dataset<\/code> on AWS (not in your Minio server). Please try the format I mentioned, here updated with the full path:<\/p>\n<pre><code class=\"lang-auto\">$ dvc add --external remote:\/\/s3remote\/dataset\/wine-quality.csv\n<\/code><\/pre>\n<p>Everything else looks good in your setup.<\/p>. <aside class=\"quote no-group\" data-username=\"veeresh\" data-post=\"7\" data-topic=\"726\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/v\/df705f\/40.png\" class=\"avatar\"> veeresh:<\/div>\n<blockquote>\n<p>The use case am trying to implement is to track the external data store (s3) remotely by dvc without storing\/downloading datasets locally.<\/p>\n<\/blockquote>\n<\/aside>\n<p>The other option is <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage\">https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage<\/a> as you discovered (also available with <a href=\"https:\/\/dvc.org\/doc\/command-reference\/import-url#example-transfer-to-remote-storage\"><code>import-url<\/code><\/a>). This is a form of bootstrapping your repo with some external data that you don\u2019t want locally now, but that some other system with a clone of the project will be able to actually download and process on that environment.<\/p>\n<p>I think that <code>add --external<\/code> (using an external cache) is the only method currently available that ensures the data never gets to the \u201clocal\u201d environment (on any machine with a repo clone). Note that still a copy of the data may be created in the external cache, if the Minio\/S3 file system doesn\u2019t support reflinks. And if it supports symlinks or hardlinks instead, those need to be configured explicitly in the project before using <code>add --external<\/code> (see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a>).<\/p>. <p>Thanks <a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a> , It worked <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-add-external-s3-mybucket-data-csv-is-failing-with-access-error-even-after-giving-correct-remote-cache-configurations\/726",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-15T08:27:50.608Z",
                "Answer_body":"<p>Can you share the output of <code>dvc doctor<\/code> and the full traceback of the failing command when runned with <code>-v<\/code> (eg <code>dvc add -v --external s3:\/\/&lt;blah&gt;<\/code>)<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-15T10:36:32.451Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/isidentical\">@isidentical<\/a> ,<\/p>\n<ul>\n<li>\n<p><strong>Here is out put of dvc doctor<\/strong><\/p>\n<p>$ dvc doctor<br>\nDVC version: 2.0.17 (exe)<\/p>\n<\/li>\n<\/ul>\n<hr>\n<p>Platform: Python 3.8.8 on Windows-10-10.0.17763-SP0<br>\nSupports: All remotes<br>\nCache types: <a href=\"https:\/\/error.dvc.org\/no-dvc-cache\" rel=\"noopener nofollow ugc\">https:\/\/error.dvc.org\/no-dvc-cache<\/a><br>\nCaches: local, s3<br>\nRemotes: s3<br>\nWorkspace directory: NTFS on C:<br>\nRepo: dvc, git<\/p>\n<ul>\n<li>\n<strong>Here are the logs<\/strong>.<\/li>\n<\/ul>\n<pre><code class=\"lang-python\">Traceback (most recent call last):\n  File \"dvc\\main.py\", line 55, in main\n  File \"dvc\\command\\add.py\", line 21, in run\n  File \"dvc\\repo\\__init__.py\", line 49, in wrapper\n  File \"dvc\\repo\\scm_context.py\", line 14, in run\n  File \"dvc\\repo\\add.py\", line 123, in add\n  File \"dvc\\repo\\add.py\", line 195, in _process_stages\n  File \"dvc\\stage\\__init__.py\", line 437, in save\n  File \"dvc\\stage\\__init__.py\", line 451, in save_outs\n  File \"dvc\\output\\base.py\", line 280, in save\n  File \"dvc\\output\\base.py\", line 208, in exists\n  File \"dvc\\fs\\s3.py\", line 233, in exists\n  File \"dvc\\fs\\s3.py\", line 263, in isfile\n  File \"dvc\\fs\\s3.py\", line 284, in _list_paths\n  File \"boto3\\resources\\collection.py\", line 83, in __iter__\n  File \"boto3\\resources\\collection.py\", line 166, in pages\n  File \"botocore\\paginate.py\", line 255, in __iter__\n  File \"botocore\\paginate.py\", line 332, in _make_request\n  File \"botocore\\client.py\", line 357, in _api_call\n  File \"botocore\\client.py\", line 676, in _make_api_call\nbotocore.exceptions.ClientError: An error occurred (InvalidAccessKeyId) when calling the ListObjects operation: The AWS Access Key Id you provided does not exist in our records.\n<\/code><\/pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/299cdad6d06f3c460b6649ce67d79c9027e9eae5.png\" data-download-href=\"\/uploads\/short-url\/5W7DrGlsLno5Th0YXNbiBNQni85.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/299cdad6d06f3c460b6649ce67d79c9027e9eae5.png\" alt=\"image\" data-base62-sha1=\"5W7DrGlsLno5Th0YXNbiBNQni85\" width=\"690\" height=\"287\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/299cdad6d06f3c460b6649ce67d79c9027e9eae5_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1761\u00d7734 31.1 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div>.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-15T17:32:40.570Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/veeresh\">@veeresh<\/a>, I think there\u2019s a confusion here:<\/p>\n<p>You are using <code>s3:\/\/dataset\/<\/code> in your <code>add --external<\/code> command. I think that DVC tries to connect to a bucket called <code>dataset<\/code> and to use your default AWS credentials (e.g. in <code>~\/.aws\/<\/code>, as configured for AWS-CLI). Probably the bucket exists but it\u2019s someone else\u2019s, and either you don\u2019t have default AWS creds or they don\u2019t work with that bucket.<\/p>\n<p>To use your configured S3 remote as target, use the following format:<\/p>\n<pre><code class=\"lang-auto\">$ dvc add --external remote:\/\/s3remote\/path\/to\/dataset\n<\/code><\/pre>\n<p>Please let us know if that helps. I\u2019ll update <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\">https:\/\/dvc.org\/doc\/user-guide\/managing-external-data<\/a> to clarify on this meanwhile.<\/p>\n<p>BTW, you may be interested in another \u2014 usually better \u2014 way to add external data. See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-the-cache\">https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-the-cache<\/a>. Thanks<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-16T02:25:21.682Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a> ,<\/p>\n<p>Its the same error I get even if I pass full dataset path.<br>\nI\u2019m passing the data path along with bucket name like below. Its still the same issue\/error.<\/p>\n<ul>\n<li>dvc add -v --external s3:\/\/dataset\/wine-quality.csv<br>\nDoes the above command also use locally configured AWS S3 configuration and not DVC remote configurations.<\/li>\n<\/ul>\n<p>config,<br>\n[core]<br>\nremote = s3remote<br>\n[cache]<br>\ns3 = s3cache<br>\n[\u2018remote \u201cs3remote\u201d\u2019]<br>\nurl = S3:\/\/dataset<br>\nendpointurl = http:\/\/{XYZ}:9000\/<br>\naccess_key_id = user<br>\nsecret_access_key = password<br>\nuse_ssl = false<br>\n[\u2018remote \u201cs3cache\u201d\u2019]<br>\nurl = s3:\/\/dataset\/cache\/<br>\nendpointurl = http:\/\/{XYZ}:9000\/<br>\naccess_key_id = user<br>\nsecret_access_key = password<br>\nuse_ssl = false<\/p>\n<p>This is my Minio bucket structure I have,<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/e9427f622d6a5828e9a505bad6b60b02124f1fa6.png\" data-download-href=\"\/uploads\/short-url\/xhvIHHW0SgEQkzQ8gT7962TmRCu.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/e9427f622d6a5828e9a505bad6b60b02124f1fa6_2_690x298.png\" alt=\"image\" data-base62-sha1=\"xhvIHHW0SgEQkzQ8gT7962TmRCu\" width=\"690\" height=\"298\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/e9427f622d6a5828e9a505bad6b60b02124f1fa6_2_690x298.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/e9427f622d6a5828e9a505bad6b60b02124f1fa6_2_1035x447.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/e9427f622d6a5828e9a505bad6b60b02124f1fa6.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/e9427f622d6a5828e9a505bad6b60b02124f1fa6_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1125\u00d7487 11.8 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>PS: I\u2019m using remote hosted Minio server to connect.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-16T02:28:18.586Z",
                "Answer_body":"<p>FYI,<br>\nRemote pull and push works as follows, but external add is not working for same cache and remote configuration.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/31f22a6b8f8acf73fcabc7d8bb0e97d0f14492ae.png\" data-download-href=\"\/uploads\/short-url\/77QeuglIGiJVXi9ro3F3E3I8urc.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/31f22a6b8f8acf73fcabc7d8bb0e97d0f14492ae_2_690x442.png\" alt=\"image\" data-base62-sha1=\"77QeuglIGiJVXi9ro3F3E3I8urc\" width=\"690\" height=\"442\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/31f22a6b8f8acf73fcabc7d8bb0e97d0f14492ae_2_690x442.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/31f22a6b8f8acf73fcabc7d8bb0e97d0f14492ae.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/31f22a6b8f8acf73fcabc7d8bb0e97d0f14492ae.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/31f22a6b8f8acf73fcabc7d8bb0e97d0f14492ae_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1011\u00d7648 154 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-16T02:44:44.743Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a> ,<\/p>\n<p>The use case am trying to implement is to track the external data store (s3) remotely by dvc without storing\/downloading datasets locally.<br>\nThe link you mentioned ( <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add?&amp;_ga=2.37973290.800977337.1618466859-1686287924.1617853529#example-transfer-to-the-cache\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-the-cache<\/a>) stores data into local system right?<br>\nPlease let me know if you have any suggestions for this use case other than add --external?<\/p>\n<p>PS: I read ( Example: Transfer to remote storage , --to-remote ),But even here I see data(from https url) is pushed\/added to remote and tracked.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-16T02:57:04.568Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"veeresh\" data-post=\"5\" data-topic=\"726\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/v\/df705f\/40.png\" class=\"avatar\"> veeresh:<\/div>\n<blockquote>\n<p>I\u2019m passing the data path along with bucket name like below. Its still the same issue\/error.<br>\n<code>dvc add -v --external s3:\/\/dataset\/wine-quality.csv<\/code><br>\nDoes the above command also use locally configured AWS S3 configuration and not DVC remote configurations.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Yep, it seems you\u2019re still trying to connect to some S3 bucket called <code>dataset<\/code> on AWS (not in your Minio server). Please try the format I mentioned, here updated with the full path:<\/p>\n<pre><code class=\"lang-auto\">$ dvc add --external remote:\/\/s3remote\/dataset\/wine-quality.csv\n<\/code><\/pre>\n<p>Everything else looks good in your setup.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-16T03:05:24.985Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"veeresh\" data-post=\"7\" data-topic=\"726\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/v\/df705f\/40.png\" class=\"avatar\"> veeresh:<\/div>\n<blockquote>\n<p>The use case am trying to implement is to track the external data store (s3) remotely by dvc without storing\/downloading datasets locally.<\/p>\n<\/blockquote>\n<\/aside>\n<p>The other option is <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage\">https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage<\/a> as you discovered (also available with <a href=\"https:\/\/dvc.org\/doc\/command-reference\/import-url#example-transfer-to-remote-storage\"><code>import-url<\/code><\/a>). This is a form of bootstrapping your repo with some external data that you don\u2019t want locally now, but that some other system with a clone of the project will be able to actually download and process on that environment.<\/p>\n<p>I think that <code>add --external<\/code> (using an external cache) is the only method currently available that ensures the data never gets to the \u201clocal\u201d environment (on any machine with a repo clone). Note that still a copy of the data may be created in the external cache, if the Minio\/S3 file system doesn\u2019t support reflinks. And if it supports symlinks or hardlinks instead, those need to be configured explicitly in the project before using <code>add --external<\/code> (see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a>).<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-16T04:52:22.598Z",
                "Answer_body":"<p>Thanks <a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a> , It worked <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"add extern mybucket data csv fail access error give correct remot cach configur connect remot data set remot cach follow configur file core remot sremot cach scach remot sremot url exampl endpointurl http localhost access kei user secret access kei password us ssl fals remot scach url exampl cach endpointurl http localhost access kei user secret access kei password us ssl fals abl push pull remot repositori local try add extern data configur cach get error scach sremot credenti fail add extern data imag help appreci post http stackoverflow com question",
        "Question_preprocessed_content":"add extern fail access error give correct remot cach configur connect remot data set remot cach follow configur file abl push pull remot repositori local try add extern data configur cach get error cach remot credenti fail add extern data imag help appreci post",
        "Question_gpt_summary_original":"The user is facing an access error while trying to add external data to DVC using the command \"dvc add -external S3:\/\/mybucket\/data.csv\" even after configuring the remote cache correctly. The user is able to push and pull from the remote repository to local, but adding external data is failing. The user has provided the configuration file and is seeking help to resolve the issue.",
        "Question_gpt_summary":"user face access error try add extern data command add extern mybucket data csv configur remot cach correctli user abl push pull remot repositori local ad extern data fail user provid configur file seek help resolv issu",
        "Answer_original_content":"share output doctor traceback fail command run add extern isident doctor doctor version ex platform python window support remot cach type http error org cach cach local remot workspac directori ntf repo git log traceback recent file main line main file command add line run file repo init line wrapper file repo scm context line run file repo add line add file repo add line process stage file stage init line save file stage init line save out file output base line save file output base line exist file line exist file line isfil file line list path file boto resourc collect line iter file boto resourc collect line page file botocor pagin line iter file botocor pagin line request file botocor client line api file botocor client line api botocor except clienterror error occur invalidaccesskeyid call listobject oper aw access kei provid exist record imag veeresh think there confus dataset add extern command think tri connect bucket call dataset us default aw credenti aw configur aw cli probabl bucket exist els dont default aw cred dont work bucket us configur remot target us follow format add extern remot sremot path dataset let know help ill updat http org doc user guid manag extern data clarifi btw interest usual better wai add extern data http org doc command refer add exampl transfer cach thank jorgeorpinel error pass dataset path pass data path bucket like issu error add extern dataset wine qualiti csv command us local configur aw configur remot configur config core remot sremot cach scach remot sremot url dataset endpointurl http xyz access kei user secret access kei password us ssl fals remot scach url dataset cach endpointurl http xyz access kei user secret access kei password us ssl fals minio bucket structur imag remot host minio server connect fyi remot pull push work follow extern add work cach remot configur imag jorgeorpinel us case try implement track extern data store remot store download dataset local link mention http org doc command refer add exampl transfer cach store data local right let know suggest us case add extern read exampl transfer remot storag remot data http url push ad remot track veeresh pass data path bucket like issu error add extern dataset wine qualiti csv command us local configur aw configur remot configur yep your try connect bucket call dataset aw minio server try format mention updat path add extern remot sremot dataset wine qualiti csv look good setup veeresh us case try implement track extern data store remot store download dataset local option http org doc command refer add exampl transfer remot storag discov avail import url form bootstrap repo extern data dont want local clone project abl actual download process environ think add extern extern cach method current avail ensur data get local environ machin repo clone note copi data creat extern cach minio file doesnt support reflink support symlink hardlink instead need configur explicitli project add extern http org doc user guid larg dataset optim thank jorgeorpinel work",
        "Answer_preprocessed_content":"share output traceback fail command run doctor doctor version platform python support remot cach type cach local remot workspac directori ntf repo git log imag think there confus command think tri connect bucket call us default aw credenti probabl bucket exist els dont default aw cred dont work bucket us configur remot target us follow format let know help ill updat clarifi btw interest usual better wai add extern data thank error pass dataset path pass data path bucket like add command us local configur aw configur remot configur config core remot remot cach cach remot remot url endpointurl user password fals remot cach url endpointurl user password fals minio bucket structur imag remot host minio server connect fyi remot pull push work follow extern add work cach remot configur imag us case try implement track extern data store remot dataset local link mention store data local right let know suggest us case add read data remot track veeresh pass data path bucket like command us local configur aw configur remot configur yep your try connect bucket call aw try format mention updat path look good setup veeresh us case try implement track extern data store remot dataset local option discov form bootstrap repo extern data dont want local clone project abl actual download process environ think method current avail ensur data get local environ note copi data creat extern cach file doesnt support reflink support symlink hardlink instead need configur explicitli project thank work",
        "Answer_gpt_summary_original":"Solution:\n- The user was using the wrong format for the external data path. They were using `s3:\/\/dataset\/` instead of `remote:\/\/s3remote\/path\/to\/dataset`.\n- Another option suggested was to use `add --external` to track external data without storing\/downloading datasets locally.\n- The user was also informed that `add --external` is the only method currently available that ensures the data never gets to the \u201clocal\u201d environment.",
        "Answer_gpt_summary":"solut user wrong format extern data path dataset instead remot sremot path dataset option suggest us add extern track extern data store download dataset local user inform add extern method current avail ensur data get local environ"
    },
    {
        "Question_title":"Difference between tracking_uri and the backend store uri in MLFLOW",
        "Question_body":"<p>I am using Mlflow for my project hosting it in an EC2 instance. I was wondering in MlFlow what is the difference between the backend_store_uri we set when we launch the server and the trarcking_uri ?<\/p>\n<p>Thanks,<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629231900840,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":682.0,
        "Answer_body":"<p><code>tracking_uri<\/code> is the URL of the MLflow server (remote, or built-in in Databricks) that will be used to log metadata &amp; model (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/quickstart.html#launch-a-tracking-server-on-a-remote-machine\" rel=\"nofollow noreferrer\">doc<\/a>).  In your case, this will be the URL pointing to your EC2 instance that should be configured in programs that will log parameters into your server.<\/p>\n<p><code>backend_store_uri<\/code> - is used by MLflow server to configure where to store this data - on filesystem, in SQL-compatible database, etc. (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#cmdoption-mlflow-server-backend-store-uri\" rel=\"nofollow noreferrer\">doc<\/a>). If you use SQL database, then you also need to provide the <code>--default-artifact-root<\/code> option to point where to store generated artifacts (images, model files, etc.)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68823606",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1629269923972,
        "Question_original_content":"differ track uri backend store uri project host instanc wonder differ backend store uri set launch server trarck uri thank",
        "Question_preprocessed_content":"differ backend store uri project host instanc wonder differ set launch server thank",
        "Question_gpt_summary_original":"The user is facing a challenge in understanding the difference between the backend_store_uri and the tracking_uri in MLFlow while hosting it on an EC2 instance for their project.",
        "Question_gpt_summary":"user face challeng understand differ backend store uri track uri host instanc project",
        "Answer_original_content":"track uri url server remot built databrick log metadata model doc case url point instanc configur program log paramet server backend store uri server configur store data filesystem sql compat databas doc us sql databas need provid default artifact root option point store gener artifact imag model file",
        "Answer_preprocessed_content":"url server log metadata model case url point instanc configur program log paramet server server configur store data filesystem databas us sql databas need provid option point store gener artifact",
        "Answer_gpt_summary_original":"Solution: The discussion provides a clear explanation of the difference between the backend_store_uri and the tracking_uri in MLFlow. The tracking_uri is the URL of the MLflow server that will be used to log metadata and models, while the backend_store_uri is used by the MLflow server to configure where to store this data. If a SQL-compatible database is used, the --default-artifact-root option should also be provided to point where to store generated artifacts. No further solutions are mentioned in the discussion.",
        "Answer_gpt_summary":"solut discuss provid clear explan differ backend store uri track uri track uri url server log metadata model backend store uri server configur store data sql compat databas default artifact root option provid point store gener artifact solut mention discuss"
    },
    {
        "Question_title":"Adding a {serve} metagraph to existing Tensorflow model",
        "Question_body":"<p>The situation:<\/p>\n\n<p>I've already created several models, trained over several days each, that we're ready to move from local testing to a serving environment.<\/p>\n\n<p>The models were saved using the function<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def save_graph_to_file(sess, graph, graph_file_name):\n    \"\"\"Saves an graph to file, creating a valid quantized one if necessary.\"\"\"\n    output_graph_def = graph_util.convert_variables_to_constants(sess, graph.as_graph_def(), [final_tensor_name])\n    with gfile.FastGFile(graph_file_name, 'wb') as f:\n        f.write(output_graph_def.SerializeToString())\n<\/code><\/pre>\n\n<p>Now when attempting to deploy to a serving environment (Sagemaker, using a correct directory structure and file naming convention), the system returns<\/p>\n\n<pre><code>2019-06-04 22:38:53.794056: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\n2019-06-04 22:38:53.798096: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:259] SavedModel load for tags { serve }; Status: fail. Took 83297 microseconds.\n2019-06-04 22:38:53.798132: E tensorflow_serving\/util\/retrier.cc:37] Loading servable: {name: model version: 1} failed: Not found: Could not find meta graph def matching supplied tags: { serve }. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`\n<\/code><\/pre>\n\n<p>All I have are the <code>*.pb<\/code> files and their label textfiles. These work lovely across multiple computers in local environments. <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def load_graph(model_file):\n    \"\"\"\n    Code from v1.6.0 of Tensorflow's label_image.py example\n    \"\"\"\n    graph = tf.Graph()\n    graph_def = tf.GraphDef()\n    with open(model_file, \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n    with graph.as_default():\n        tf.import_graph_def(graph_def)\n    return graph\n\ninputLayer = \"Mul\"\noutputLayer = \"final_result\"\ninputName = \"import\/\" + inputLayer\noutputName = \"import\/\" + outputLayer\ngraph = load_graph(modelPath)\ninputOperation = graph.get_operation_by_name(inputName)\noutputOperation = graph.get_operation_by_name(outputName)\nwith tf.Session(graph= graph) as sess:\n    # ... make a tensor t\n    results = sess.run(outputOperation.outputs[0], {\n        inputOperation.outputs[0]: t\n    })\n    # lovely functional results here\n<\/code><\/pre>\n\n<p>All I want to do is to take these existing files, add the \"serve\" tag needed, and re-save them, but everything I see seems to be related to doing this from scratch.<\/p>\n\n<p>I tried to use the builder to append a graph to a model like so:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Load the graph\ngraph = load_graph(modelPath)\nimport shutil\nif os.path.exists(exportDir):\n    shutil.rmtree(exportDir)\n# Add the serving metagraph tag\nbuilder = tf.saved_model.builder.SavedModelBuilder(exportDir)\nfrom tensorflow.saved_model import tag_constants\nwith tf.Session(graph= graph) as sess:\n    builder.add_meta_graph_and_variables(sess, [tag_constants.SERVING, tag_constants.GPU], strip_default_attrs= True)\nbuilder.save()\nprint(\"Built a SavedModel\")\n<\/code><\/pre>\n\n<p>but got the same error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559778036413,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":1140.0,
        "Answer_body":"<p>Finally solved it. This contains some S3 specific code and S3 instance calls (the <code>!<\/code> commands) but you should pretty much be able to slice that out to run this.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#!python3\n\"\"\"\nAssumes we've defined:\n\n- A directory for our working files to live in, CONTAINER_DIR\n- an arbitrary integer VERSION_INT\n- We have established local and S3 paths for our model and their labels as variables, particularly `modelLabel` and `modelPath`\n\"\"\"\n\n# Create a versioned path for the models to live in\n# See https:\/\/stackoverflow.com\/a\/54014480\/1877527\nexportDir = os.path.join(CONTAINER_DIR, VERSION_INT)\nif os.path.exists(exportDir):\n    shutil.rmtree(exportDir)\nos.mkdir(exportDir)\nimport tensorflow as tf\ndef load_graph(model_file, returnElements= None):\n    \"\"\"\n    Code from v1.6.0 of Tensorflow's label_image.py example\n    \"\"\"\n    graph = tf.Graph()\n    graph_def = tf.GraphDef()\n    with open(model_file, \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n    returns = None\n    with graph.as_default():\n        returns = tf.import_graph_def(graph_def, return_elements= returnElements)\n    if returnElements is None:\n        return graph\n    return graph, returns\n\n# Add the serving metagraph tag\n# We need the inputLayerName; in Inception we're feeding the resized tensor\n# corresponding to resized_input_tensor_name\n# May be able to get away with auto-determining this if not using Inception,\n# but for Inception this is the 11th layer\ninputLayerName = \"Mul:0\"\n# Load the graph\nif inputLayerName is None:\n    graph = load_graph(modelPath)\n    inputTensor = None\nelse:\n    graph, returns = load_graph(modelPath, returnElements= [inputLayerName])\n    inputTensor = returns[0]\nwith tf.Session(graph= graph) as sess:\n    # Read the layers\n    try:\n        from tensorflow.compat.v1.saved_model import simple_save\n    except (ModuleNotFoundError, ImportError):\n        from tensorflow.saved_model import simple_save\n    with graph.as_default():\n        layers = [n.name for n in graph.as_graph_def().node]\n        outName = layers.pop() + \":0\"\n        if inputLayerName is None:\n            inputLayerName = layers.pop(0) + \":0\"\n    print(\"Checking outlayer\", outName)\n    outLayer = tf.get_default_graph().get_tensor_by_name(outName)\n    if inputTensor is None:\n        print(\"Checking inlayer\", inputLayerName)\n        inputTensor = tf.get_default_graph().get_tensor_by_name(inputLayerName)\n    inputs = {\n        inputLayerName: inputTensor\n    }\n    outputs = {\n        outName: outLayer\n    }\n    simple_save(sess, exportDir, inputs, outputs)\nprint(\"Built a SavedModel\")\n# Put the model label into the artifact dir\nmodelLabelDest = os.path.join(exportDir, \"saved_model.txt\")\n!cp {modelLabel} {modelLabelDest}\n# Prep for serving\nimport datetime as dt\nmodelArtifact = f\"livemodel_{dt.datetime.now().timestamp()}.tar.gz\"\n# Copy the version directory here to package\n!cp -R {exportDir} .\/\n# gziptar it\n!tar -czvf {modelArtifact} {VERSION_INT}\n# Shove it back to S3 for serving\n!aws s3 cp {modelArtifact} {bucketPath}\nshutil.rmtree(VERSION_INT) # Cleanup\nshutil.rmtree(exportDir) # Cleanup\n<\/code><\/pre>\n\n<p>This model is then deployable as a Sagemaker endpoint (and any other Tensorflow serving environment)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1571867961112,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56469341",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1560534456823,
        "Question_original_content":"ad serv metagraph exist tensorflow model situat creat model train dai readi local test serv environ model save function def save graph file sess graph graph file save graph file creat valid quantiz necessari output graph def graph util convert variabl constant sess graph graph def final tensor gfile fastgfil graph file write output graph def serializetostr attempt deploi serv environ correct directori structur file name convent return extern org tensorflow tensorflow save model reader read meta graph tag serv extern org tensorflow tensorflow save model loader savedmodel load tag serv statu fail took microsecond tensorflow serv util retrier load servabl model version fail meta graph def match suppli tag serv inspect avail tag set savedmodel us savedmodel cli save model cli file label textfil work love multipl comput local environ def load graph model file code tensorflow label imag exampl graph graph graph def graphdef open model file graph def parsefromstr read graph default import graph def graph def return graph inputlay mul outputlay final result inputnam import inputlay outputnam import outputlay graph load graph modelpath inputoper graph oper inputnam outputoper graph oper outputnam session graph graph sess tensor result sess run outputoper output inputoper output love function result want exist file add serv tag need save relat scratch tri us builder append graph model like load graph graph load graph modelpath import shutil path exist exportdir shutil rmtree exportdir add serv metagraph tag builder save model builder savedmodelbuild exportdir tensorflow save model import tag constant session graph graph sess builder add meta graph variabl sess tag constant serv tag constant gpu strip default attr true builder save print built savedmodel got error",
        "Question_preprocessed_content":"ad metagraph exist tensorflow model situat creat model train dai readi local test serv environ model save function attempt deploi serv environ return file label textfil work love multipl comput local environ want exist file add serv tag need relat scratch tri us builder append graph model like got error",
        "Question_gpt_summary_original":"The user has encountered challenges when attempting to deploy their existing Tensorflow models to a serving environment. The system returns an error message indicating that it could not find the meta graph def matching the supplied tags. The user has tried to add the \"serve\" tag to the existing files using the builder, but still encountered the same error.",
        "Question_gpt_summary":"user encount challeng attempt deploi exist tensorflow model serv environ return error messag indic meta graph def match suppli tag user tri add serv tag exist file builder encount error",
        "Answer_original_content":"final solv contain specif code instanc call command pretti abl slice run python assum defin directori work file live contain dir arbitrari integ version int establish local path model label variabl particularli modellabel modelpath creat version path model live http stackoverflow com exportdir path join contain dir version int path exist exportdir shutil rmtree exportdir mkdir exportdir import tensorflow def load graph model file returnel code tensorflow label imag exampl graph graph graph def graphdef open model file graph def parsefromstr read return graph default return import graph def graph def return element returnel returnel return graph return graph return add serv metagraph tag need inputlayernam incept feed resiz tensor correspond resiz input tensor abl awai auto determin incept incept layer inputlayernam mul load graph inputlayernam graph load graph modelpath inputtensor graph return load graph modelpath returnel inputlayernam inputtensor return session graph graph sess read layer try tensorflow compat save model import simpl save modulenotfounderror importerror tensorflow save model import simpl save graph default layer graph graph def node outnam layer pop inputlayernam inputlayernam layer pop print check outlay outnam outlay default graph tensor outnam inputtensor print check inlay inputlayernam inputtensor default graph tensor inputlayernam input inputlayernam inputtensor output outnam outlay simpl save sess exportdir input output print built savedmodel model label artifact dir modellabeldest path join exportdir save model txt modellabel modellabeldest prep serv import datetim modelartifact livemodel datetim timestamp tar copi version directori packag exportdir gziptar tar czvf modelartifact version int shove serv aw modelartifact bucketpath shutil rmtree version int cleanup shutil rmtree exportdir cleanup model deploy endpoint tensorflow serv environ",
        "Answer_preprocessed_content":"final solv contain specif code instanc call pretti abl slice run model deploy endpoint",
        "Answer_gpt_summary_original":"Solution:\nThe user has shared a code snippet that can be used to deploy a Tensorflow model to a serving environment. The code involves creating a versioned path for the models to live in, loading the graph, adding the serving metagraph tag, reading the layers, and preparing for serving. The model can then be deployed as a Sagemaker endpoint or any other Tensorflow serving environment.",
        "Answer_gpt_summary":"solut user share code snippet deploi tensorflow model serv environ code involv creat version path model live load graph ad serv metagraph tag read layer prepar serv model deploi endpoint tensorflow serv environ"
    },
    {
        "Question_title":"Objects Not Visible Within S3 Bucket for GroundTruth Labeling Job",
        "Question_body":"<p>I am currently creating a GroundTruth Labeling job, and am following the tutorial\n<a href=\"https:\/\/www.youtube.com\/watch?v=_FPI6KjDlCI&amp;t=210s\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=_FPI6KjDlCI&amp;t=210s<\/a>\nI have created the same bucket <code>ground-truth-example-labeling-job<\/code> and uploaded jpg files within the bucket. Within this tutorial, under Select S3 bucket or resource, they were able to go within the S3 Bucket and access the jpg files inside.<\/p>\n<p>However, I am able to go inside the <code>ground-truth-example-labeling-job<\/code> bucket, but no jpg files are visible for me to select. The entire bucket is empty with nothing to select.<\/p>\n<p>Is this a permissions settings problem?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sdXZw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sdXZw.png\" alt=\"enter image description here\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/s10qk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s10qk.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649824909493,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":66.0,
        "Answer_body":"<p>You cannot select the files.<\/p>\n<p>But if you have a folder within a bucket then you can select that folder which consists of the input data.<\/p>\n<p>In the video they selected the bucket but not the files.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71851981",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1651560562667,
        "Question_original_content":"object visibl bucket groundtruth label job current creat groundtruth label job follow tutori http youtub com watch fpikjdlci creat bucket ground truth exampl label job upload jpg file bucket tutori select bucket resourc abl bucket access jpg file insid abl insid ground truth exampl label job bucket jpg file visibl select entir bucket select permiss set problem",
        "Question_preprocessed_content":"object visibl bucket groundtruth label job current creat groundtruth label job follow tutori creat bucket upload jpg file bucket tutori select bucket resourc abl bucket access jpg file insid abl insid bucket jpg file visibl select entir bucket select permiss set problem",
        "Question_gpt_summary_original":"The user is facing a challenge in accessing jpg files within an S3 bucket for a GroundTruth Labeling job, despite having uploaded the files. The entire bucket appears to be empty, and the user is unsure if this is due to a permissions settings problem.",
        "Question_gpt_summary":"user face challeng access jpg file bucket groundtruth label job despit have upload file entir bucket appear user unsur permiss set problem",
        "Answer_original_content":"select file folder bucket select folder consist input data video select bucket file",
        "Answer_preprocessed_content":"select file folder bucket select folder consist input data video select bucket file",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that the user should create a folder within the S3 bucket and upload the jpg files into that folder. Then, the user can select the folder as the input data for the GroundTruth Labeling job. It is also mentioned that in a video tutorial, the bucket was selected but not the files, which may have caused the issue.",
        "Answer_gpt_summary":"solut discuss suggest user creat folder bucket upload jpg file folder user select folder input data groundtruth label job mention video tutori bucket select file caus issu"
    },
    {
        "Question_title":"Dvc error \"file not owned by user\"",
        "Question_body":"<p>Hi,<br>\nI am configuring dvc on shared system.<br>\nThe ubuntu system has multiple user ids as logins.<\/p>\n<p>The folder dataset1 is copied by one user  \u201cuser1\u201d.<br>\nNow when I am doing \u201cdvc add dataset1\u201d, its showing as<br>\n\u201cfile not owned by user\u201d.<\/p>\n<p>I have rwx permissions on the dataset1. I verified using getfacl.<br>\nCan anyone please guide how can i do \u201cdvc add\u201d without getting error. I dont want to do as root user as later during git push we would need username for tracking.<\/p>\n<p>Kindly help please.<\/p>\n<p>Thanks &amp; Regards<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1612845083215,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":261.0,
        "Answer_body":"<p><a class=\"mention\" href=\"\/u\/ruchita\">@Ruchita<\/a>, could you please share the traceback by running <code>dvc add -v<\/code>? Thanks.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-error-file-not-owned-by-user\/661",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-09T10:38:52.813Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/ruchita\">@Ruchita<\/a>, could you please share the traceback by running <code>dvc add -v<\/code>? Thanks.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error file own user configur share ubuntu multipl user id login folder dataset copi user user add dataset show file own user rwx permiss dataset verifi getfacl guid add get error dont want root user later git push need usernam track kindli help thank regard",
        "Question_preprocessed_content":"error file own user configur share ubuntu multipl user id login folder dataset copi user user add dataset show file own user rwx permiss dataset verifi getfacl guid add get error dont want root user later git push need usernam track kindli help thank regard",
        "Question_gpt_summary_original":"The user is encountering an error while configuring dvc on a shared Ubuntu system with multiple user IDs. The error message \"file not owned by user\" appears when trying to add a folder copied by another user. The user has rwx permissions on the folder and wants to avoid using root user for tracking purposes during git push. The user is seeking guidance on how to resolve the error.",
        "Question_gpt_summary":"user encount error configur share ubuntu multipl user id error messag file own user appear try add folder copi user user rwx permiss folder want avoid root user track purpos git push user seek guidanc resolv error",
        "Answer_original_content":"ruchita share traceback run add thank",
        "Answer_preprocessed_content":"share traceback run thank",
        "Answer_gpt_summary_original":"Solutions provided: No solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut provid solut mention discuss"
    },
    {
        "Question_title":"Request to add TVS in Organizations using & contributing to MLflow",
        "Question_body":"Hi Team,\n\nAt TVS Motor we use MLflow extensively for ML model tracking, projects and registry. In the past, I was a contributor to MLflow as well.\n\nSo, can we get the TVS Motor listed in the \u201cOrganizations using and contributing to MLflow\u201d section, please?\n\n\n\n\n\u00a0\n\n\u00a0Thanks,\nNaga",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1627294324000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":57.0,
        "Answer_body":"Dear MLflow team, reinitiating our request to add www.tvsmotor.com listed as\u00a0 \u201cOrganizations using and contributing to MLflow\u201d section. our teams use MLflow extensively for ML model tracking, projects and registry.\u00a0\ncheers.\n\n\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/ReyGdDjmCeo",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-13T10:20:20",
                "Answer_body":"Dear MLflow team, reinitiating our request to add www.tvsmotor.com listed as\u00a0 \u201cOrganizations using and contributing to MLflow\u201d section. our teams use MLflow extensively for ML model tracking, projects and registry.\u00a0\ncheers.\n\n\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"request add tv organ contribut team tv motor us extens model track project registri past contributor tv motor list organ contribut section thank naga",
        "Question_preprocessed_content":"request add tv organ contribut team tv motor us extens model track project registri past contributor tv motor list organ contribut section thank naga",
        "Question_gpt_summary_original":"The user is requesting to add TVS Motor to the \"Organizations using and contributing to MLflow\" section, as they use MLflow extensively for ML model tracking, projects, and registry.",
        "Question_gpt_summary":"user request add tv motor organ contribut section us extens model track project registri",
        "Answer_original_content":"dear team reiniti request add tvsmotor com list organ contribut section team us extens model track project registri cheer",
        "Answer_preprocessed_content":"dear team reiniti request add list organ contribut section team us extens model track project registri cheer",
        "Answer_gpt_summary_original":"Solution: The user is requesting to add TVS Motor to the \"Organizations using and contributing to MLflow\" section, as they use MLflow extensively for ML model tracking, projects, and registry. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut user request add tv motor organ contribut section us extens model track project registri solut mention discuss"
    },
    {
        "Question_title":"Sagemaker Studio encountered an error when creating your project(github and codepipeline template)",
        "Question_body":"I trying tutorial on \"MLOps template for model building, training, and deployment with third-party Git repositories using CodePipeline\". But I am getting error as shown in image",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658949790257,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":68.0,
        "Answer_body":"Hello. It seems like you are having permission problems according to the snapshot you provided. If you head to the Cloudformation service, you will probably get a better understanding of where the tamplate is failing. Make sure to have followed the prerequisites and check out this section.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOCKdskABQumCC7OnzBZR4g\/sagemaker-studio-encountered-an-error-when-creating-your-project-github-and-codepipeline-template",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-28T07:44:00.283Z",
                "Answer_score":1,
                "Answer_body":"Hello. It seems like you are having permission problems according to the snapshot you provided. If you head to the Cloudformation service, you will probably get a better understanding of where the tamplate is failing. Make sure to have followed the prerequisites and check out this section.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1658994240283,
        "Question_original_content":"studio encount error creat project github codepipelin templat try tutori mlop templat model build train deploy parti git repositori codepipelin get error shown imag",
        "Question_preprocessed_content":"studio encount error creat project try tutori mlop templat model build train deploy git repositori codepipelin get error shown imag",
        "Question_gpt_summary_original":"The user encountered an error while trying to follow a tutorial on MLOps template for model building, training, and deployment with third-party Git repositories using CodePipeline. The error occurred when creating a project in Sagemaker Studio.",
        "Question_gpt_summary":"user encount error try follow tutori mlop templat model build train deploy parti git repositori codepipelin error occur creat project studio",
        "Answer_original_content":"hello like have permiss problem accord snapshot provid head cloudform servic probabl better understand tamplat fail sure follow prerequisit check section",
        "Answer_preprocessed_content":"hello like have permiss problem accord snapshot provid head cloudform servic probabl better understand tamplat fail sure follow prerequisit check section",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that the user may have permission problems and recommends checking the Cloudformation service to get a better understanding of where the template is failing. The user is also advised to ensure that they have followed the prerequisites and check out a specific section. No specific solution is provided, but the discussion provides guidance on how to troubleshoot the issue.",
        "Answer_gpt_summary":"solut discuss suggest user permiss problem recommend check cloudform servic better understand templat fail user advis ensur follow prerequisit check specif section specif solut provid discuss provid guidanc troubleshoot issu"
    },
    {
        "Question_title":"Set Github as Artifacts location",
        "Question_body":"Im testing MLFLOW 1.0.0. I run MLFLOW in docker container, myArifacts location is Minio bucket (Minio is running in its own docker container), backend store is Postgres ( running in its own docker container); I train my model in Jupiter notebook ( also in its own docker container). Is it possible to use Github as an artifact location?\n\n\nThank you.",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1561645149000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":20.0,
        "Answer_body":"No, you can\u2019t use GitHub by default, but it may be possible to write an artifact store plugin that does that. Our idea was that artifacts can be very large, however (multi-gigabyte models or multi-terabyte datasets), so it might not make sense to use GitHub.\n\n\nMatei\n\n\n\nOn Jun 27, 2019, at 11:19 AM, SoniaK <sofia....@8451.com> wrote:\n\n\nIm testing MLFLOW 1.0.0. I run MLFLOW in docker container, myArifacts location is Minio bucket (Minio is running in its own docker container), backend store is Postgres ( running in its own docker container); I train my model in Jupiter notebook ( also in its own docker container). Is it possible to use Github as an artifact location?\n\n\nThank you.\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/59f7455b-4b7b-41f5-8058-6bb231bb0807%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.. Hi, I think you should use the right tool for the right job. GitHub is for sharing code.\nThx\n\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/F25604B5-40AA-4907-960B-C588609BC72A%40databricks.com.\n\ue5d3. Thank you, Matei and Zahir.\n\n\nOn Thursday, June 27, 2019 at 2:36:20 PM UTC-5, zahir hamroune wrote:\nHi, I think you should use the right tool for the right job. GitHub is for sharing code.\nThx\n\n\n\nOn 27 Jun 2019, at 21:34, Matei Zaharia <ma...@databricks.com> wrote:\n\n\nNo, you can\u2019t use GitHub by default, but it may be possible to write an artifact store plugin that does that. Our idea was that artifacts can be very large, however (multi-gigabyte models or multi-terabyte datasets), so it might not make sense to use GitHub.\n\n\nMatei\n\n\n\nOn Jun 27, 2019, at 11:19 AM, SoniaK <sofia...@8451.com> wrote:\n\n\nIm testing MLFLOW 1.0.0. I run MLFLOW in docker container, myArifacts location is Minio bucket (Minio is running in its own docker container), backend store is Postgres ( running in its own docker container); I train my model in Jupiter notebook ( also in its own docker container). Is it possible to use Github as an artifact location?\n\n\nThank you.\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow...@googlegroups.com.\n\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/59f7455b-4b7b-41f5-8058-6bb231bb0807%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.\n\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow...@googlegroups.com.\n\ue5d3. Hi Zahir.\u00a0 If GitHub is not the right tool for the job (I don't disagree with that), what would you say IS the right tool for persisting artifacts?\n\nThanks in advance!\n\n\nOn Thursday, June 27, 2019 at 2:36:20 PM UTC-5, zahir hamroune wrote:\n\ue5d3. You can use a shared filesystem or blob store, such as AWS S3, Azure Blob Storage, HDFS, or a POSIX file system set up through NFS.\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/9bd4f3f5-4574-473e-8fd9-81361945f52f%40googlegroups.com.\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/QKRO33wr3hM",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-06-27T15:34:12",
                "Answer_body":"No, you can\u2019t use GitHub by default, but it may be possible to write an artifact store plugin that does that. Our idea was that artifacts can be very large, however (multi-gigabyte models or multi-terabyte datasets), so it might not make sense to use GitHub.\n\n\nMatei\n\n\n\nOn Jun 27, 2019, at 11:19 AM, SoniaK <sofia....@8451.com> wrote:\n\n\nIm testing MLFLOW 1.0.0. I run MLFLOW in docker container, myArifacts location is Minio bucket (Minio is running in its own docker container), backend store is Postgres ( running in its own docker container); I train my model in Jupiter notebook ( also in its own docker container). Is it possible to use Github as an artifact location?\n\n\nThank you.\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/59f7455b-4b7b-41f5-8058-6bb231bb0807%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2019-06-27T15:36:20",
                "Answer_body":"Hi, I think you should use the right tool for the right job. GitHub is for sharing code.\nThx\n\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/F25604B5-40AA-4907-960B-C588609BC72A%40databricks.com.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-06-27T16:03:55",
                "Answer_body":"Thank you, Matei and Zahir.\n\n\nOn Thursday, June 27, 2019 at 2:36:20 PM UTC-5, zahir hamroune wrote:\nHi, I think you should use the right tool for the right job. GitHub is for sharing code.\nThx\n\n\n\nOn 27 Jun 2019, at 21:34, Matei Zaharia <ma...@databricks.com> wrote:\n\n\nNo, you can\u2019t use GitHub by default, but it may be possible to write an artifact store plugin that does that. Our idea was that artifacts can be very large, however (multi-gigabyte models or multi-terabyte datasets), so it might not make sense to use GitHub.\n\n\nMatei\n\n\n\nOn Jun 27, 2019, at 11:19 AM, SoniaK <sofia...@8451.com> wrote:\n\n\nIm testing MLFLOW 1.0.0. I run MLFLOW in docker container, myArifacts location is Minio bucket (Minio is running in its own docker container), backend store is Postgres ( running in its own docker container); I train my model in Jupiter notebook ( also in its own docker container). Is it possible to use Github as an artifact location?\n\n\nThank you.\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow...@googlegroups.com.\n\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/59f7455b-4b7b-41f5-8058-6bb231bb0807%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.\n\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow...@googlegroups.com.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-07-01T18:14:36",
                "Answer_body":"Hi Zahir.\u00a0 If GitHub is not the right tool for the job (I don't disagree with that), what would you say IS the right tool for persisting artifacts?\n\nThanks in advance!\n\n\nOn Thursday, June 27, 2019 at 2:36:20 PM UTC-5, zahir hamroune wrote:\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-07-02T06:37:31",
                "Answer_body":"You can use a shared filesystem or blob store, such as AWS S3, Azure Blob Storage, HDFS, or a POSIX file system set up through NFS.\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/9bd4f3f5-4574-473e-8fd9-81361945f52f%40googlegroups.com.\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"set github artifact locat test run docker contain myarifact locat minio bucket minio run docker contain backend store postgr run docker contain train model jupit notebook docker contain possibl us github artifact locat thank",
        "Question_preprocessed_content":"set github artifact locat test run docker contain myarifact locat minio bucket backend store postgr train model jupit notebook possibl us github artifact locat thank",
        "Question_gpt_summary_original":"The user is facing a challenge of whether it is possible to use Github as an artifact location while testing MLFLOW 1.0.0. They are currently running MLFLOW, Minio, Postgres, and Jupiter notebook in separate docker containers, with Minio serving as the current artifact location.",
        "Question_gpt_summary":"user face challeng possibl us github artifact locat test current run minio postgr jupit notebook separ docker contain minio serv current artifact locat",
        "Answer_original_content":"us github default possibl write artifact store plugin idea artifact larg multi gigabyt model multi terabyt dataset sens us github matei jun soniak wrote test run docker contain myarifact locat minio bucket minio run docker contain backend store postgr run docker contain train model jupit notebook docker contain possibl us github artifact locat thank receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user bbbb googlegroup com option visit http group googl com optout think us right tool right job github share code thx view discuss web visit http group googl com msgid user cbca databrick com thank matei zahir thursdai june utc zahir hamroun wrote think us right tool right job github share code thx jun matei zaharia wrote us github default possibl write artifact store plugin idea artifact larg multi gigabyt model multi terabyt dataset sens us github matei jun soniak wrote test run docker contain myarifact locat minio bucket minio run docker contain backend store postgr run docker contain train model jupit notebook docker contain possibl us github artifact locat thank receiv messag subscrib googl group user group unsubscrib group stop receiv email send email googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user bbbb googlegroup com option visit http group googl com optout receiv messag subscrib googl group user group unsubscrib group stop receiv email send email googlegroup com zahir github right tool job disagre right tool persist artifact thank advanc thursdai june utc zahir hamroun wrote us share filesystem blob store aw azur blob storag hdf posix file set nf receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user bdff googlegroup com",
        "Answer_preprocessed_content":"us github default possibl write artifact store plugin idea artifact larg sens us github matei jun soniak wrote test run docker contain myarifact locat minio bucket backend store postgr train model jupit notebook possibl us github artifact locat thank receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit think us right tool right job github share code thx view discuss web visit thank matei zahir thursdai june zahir hamroun wrote think us right tool right job github share code thx jun matei zaharia wrote us github default possibl write artifact store plugin idea artifact larg sens us github matei jun soniak wrote test run docker contain myarifact locat minio bucket backend store postgr train model jupit notebook possibl us github artifact locat thank receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit receiv messag subscrib googl group group unsubscrib group stop receiv email send email zahir github right tool job right tool persist artifact thank advanc thursdai june zahir hamroun wrote us share filesystem blob store aw azur blob storag hdf posix file set nf receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit",
        "Answer_gpt_summary_original":"The discussion suggests that it is not possible to use Github as an artifact location by default, but it may be possible to write an artifact store plugin that does that. However, it might not make sense to use Github as artifacts can be very large. The participants suggest using a shared filesystem or blob store, such as AWS S3, Azure Blob Storage, HDFS, or a POSIX file system set up through NFS, as a better alternative for persisting artifacts.",
        "Answer_gpt_summary":"discuss suggest possibl us github artifact locat default possibl write artifact store plugin sens us github artifact larg particip suggest share filesystem blob store aw azur blob storag hdf posix file set nf better altern persist artifact"
    },
    {
        "Question_title":"Comment s\u00e9lectionner Standard_DS11_v2",
        "Question_body":"Bonjour\nJe suis le cours en ligne concernant l'impl\u00e9mentation d'algorithmes de machine learning\nA l'\u00e9tape Create compute resources\nhttps:\/\/docs.microsoft.com\/en-us\/learn\/modules\/use-automated-machine-learning\/create-compute\n\nOn me demande Search for and select Standard_DS11_v2\n\nHors, l'interface me dit que je n'ai pas les quotas disponibles.\nJ'utilise l'offre d'essai \u00e0 200 USD.\nComment faire pour que cela fonctionne ?\nCordialement\nThibaut",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1638368598000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@ThibautJacquin-3972 For a free account only 200$ credit is available and not all compute can be created or selected because of this limitation. You can choose a lower priced VM and proceed with the creation of compute or upgrade to a pay-as-you-go account for your subscription and select the required compute type. I hope this helps.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/647767\/comment-selectionner-standard-ds11-v2.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-02T08:09:58.51Z",
                "Answer_score":0,
                "Answer_body":"@ThibautJacquin-3972 For a free account only 200$ credit is available and not all compute can be created or selected because of this limitation. You can choose a lower priced VM and proceed with the creation of compute or upgrade to a pay-as-you-go account for your subscription and select the required compute type. I hope this helps.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-12-02T10:05:54.843Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nI am not a French speaker, below is my translation from machine:\n\nHello\nI am taking the online course on the implementation of machine learning algorithms\nAt the Create compute resources step\nhttps:\/\/docs.microsoft.com\/en-us\/learn\/modules\/use-automated-machine-learning\/create-compute\n\n\nI am asked Search for and select Standard_DS11_v2\n\n\nHowever, the interface tells me that I do not have the quotas available.\nI am using the $ 200 trial offer.\nHow do I make it work?\nRegards\nThibaut\n\nAs the note: \"Compute instances and clusters are based on standard Azure virtual machine images. For this module, the Standard_DS11_v2 image is recommended to achieve the optimal balance of cost and performance. If your subscription has a quota that does not include this image, choose an alternative image; but bear in mind that a larger image may incur higher cost and a smaller image may not be sufficient to complete the tasks. Alternatively, ask your Azure administrator to extend your quota.\"\n\nYou do not need to select Standard_DS11_v2 for this, could you please try other similar image to see if that works( there is no effect for your lab)? There maybe some limitations from your administration.\n\nHope this will help. Please let us know if any further queries.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-02T14:18:35.637Z",
                "Answer_score":0,
                "Answer_body":"Thanks for you replies !\nIndeed, I can't select another image !\nFrom my understanding I have to ask to expand my quota !",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1638432598510,
        "Question_original_content":"comment slectionn standard bonjour sui cour lign concern implment algorithm machin learn tape creat comput resourc http doc microsoft com learn modul us autom machin learn creat comput demand search select standard hor interfac dit que pa le quota dispon utilis offr essai usd comment fair pour que cela fonctionn cordial thibaut",
        "Question_preprocessed_content":"comment slectionn bonjour sui cour lign concern implment algorithm machin learn tape creat comput resourc demand search select hor interfac dit que pa le quota dispon utilis offr essai usd comment fair pour que cela fonctionn cordial thibaut",
        "Question_gpt_summary_original":"The user is facing challenges in selecting Standard_DS11_v2 while creating compute resources for an online course on machine learning implementation. The interface is indicating that the user does not have the required quotas available, despite using the $200 trial offer. The user is seeking guidance on how to resolve this issue.",
        "Question_gpt_summary":"user face challeng select standard creat comput resourc onlin cours machin learn implement interfac indic user requir quota avail despit trial offer user seek guidanc resolv issu",
        "Answer_original_content":"thibautjacquin free account credit avail comput creat select limit choos lower price proce creation comput upgrad pai account subscript select requir comput type hope help answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"free account credit avail comput creat select limit choos lower price proce creation comput upgrad account subscript select requir comput type hope help answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Choose a lower priced VM and proceed with the creation of compute.\n2. Upgrade to a pay-as-you-go account for the subscription and select the required compute type.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss choos lower price proce creation comput upgrad pai account subscript select requir comput type person opinion bias includ summari"
    },
    {
        "Question_title":"Deployment of Multiple Models to Container Instance Fails in Azure DevOps",
        "Question_body":"Hi Team,\n\nI am trying to deploy 2 ML models ( which is registered in Model Registry ) to Azure Container Instance using DevOps Release pipeline using AZ CLI ML extension\n\nMy ACI Configuration is :\n\ncontainerResourceRequirements: cpu: 1 memoryInGB: 4 computeType: ACI\n\nInference Config :\n\nentryScript: score.py runtime: python condaFile: conda_dependencies.yml extraDockerfileSteps: schemaFile: sourceDirectory: enableGpu: False baseImage: baseImageRegistry:\n\nAll score.py, conda_dependencies.yml, aciDeploymentConfig.yml is placed in a flattened directory which is publised in to DevOps pipeline artifcat and looks like\n\n\n\n\n\nDevOps Deploy command looks like\n\naz ml model deploy -g $(ml.resourceGroup) -w $(ml.workspace) --name $(service.name.staging) -f .\/model.json -m \"GloVe:4\" --dc aciDeploymentConfig.yml --ic inferenceConfig.yml --overwrite --debug\n\nAlso i have set the working directory as the folder where all above files are placed. something like\n\n$(System.DefaultWorkingDirectory)\/_Symptom-Code-Indexing\/symptom_model\/a\n\nIts getting in to an exception as\n\n2020-06-08T12:50:27.9202657Z \"error\": {\n2020-06-08T12:50:27.9208361Z \"message\": \"Received bad response from Model Management Service:\\nResponse Code: 400\\nHeaders: {'Date': 'Mon, 08 Jun 2020 12:50:27 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d', 'x-ms-client-request-id': '823e8483923846b1958c08ffaba074ff', 'x-ms-client-session-id': '62e84a29-b77c-456f-9d91-5ca6be26f79c', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{\\\"code\\\":\\\"BadRequest\\\",\\\"statusCode\\\":400,\\\"message\\\":\\\"The request is invalid.\\\",\\\"details\\\":[{\\\"code\\\":\\\"InvalidOverwriteRequest\\\",\\\"message\\\":\\\"Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.\\\"}],\\\"correlation\\\":{\\\"RequestId\\\":\\\"823e8483923846b1958c08ffaba074ff\\\"}}'\"\n2020-06-08T12:50:27.9212109Z }\n2020-06-08T12:50:27.9212376Z }}\n2020-06-08T12:50:27.9213437Z {'Azure-cli-ml Version': '1.6.0', 'Error': WebserviceException:\n2020-06-08T12:50:27.9214158Z Message: Received bad response from Model Management Service:\n2020-06-08T12:50:27.9214688Z Response Code: 400\n2020-06-08T12:50:27.9217800Z Headers: {'Date': 'Mon, 08 Jun 2020 12:50:27 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d', 'x-ms-client-request-id': '823e8483923846b1958c08ffaba074ff', 'x-ms-client-session-id': '62e84a29-b77c-456f-9d91-5ca6be26f79c', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\n2020-06-08T12:50:27.9222115Z Content: b'{\"code\":\"BadRequest\",\"statusCode\":400,\"message\":\"The request is invalid.\",\"details\":[{\"code\":\"InvalidOverwriteRequest\",\"message\":\"Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.\"}],\"correlation\":{\"RequestId\":\"823e8483923846b1958c08ffaba074ff\"}}'\n2020-06-08T12:50:27.9223705Z InnerException None\n2020-06-08T12:50:27.9224049Z ErrorResponse\n2020-06-08T12:50:27.9224320Z {\n2020-06-08T12:50:27.9224617Z \"error\": {\n2020-06-08T12:50:27.9229025Z \"message\": \"Received bad response from Model Management Service:\\nResponse Code: 400\\nHeaders: {'Date': 'Mon, 08 Jun 2020 12:50:27 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d', 'x-ms-client-request-id': '823e8483923846b1958c08ffaba074ff', 'x-ms-client-session-id': '62e84a29-b77c-456f-9d91-5ca6be26f79c', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{\\\"code\\\":\\\"BadRequest\\\",\\\"statusCode\\\":400,\\\"message\\\":\\\"The request is invalid.\\\",\\\"details\\\":[{\\\"code\\\":\\\"InvalidOverwriteRequest\\\",\\\"message\\\":\\\"Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.\\\"}],\\\"correlation\\\":{\\\"RequestId\\\":\\\"823e8483923846b1958c08ffaba074ff\\\"}}'\"\n2020-06-08T12:50:27.9230782Z }\n2020-06-08T12:50:27.9230908Z }}\n2020-06-08T12:50:27.9231134Z Event: Cli.PostExecute [<function AzCliLogging.deinit_cmd_metadata_logging at 0x7fea2ca1f730>]\n2020-06-08T12:50:27.9231431Z az_command_data_logger : exit code: 1\n2020-06-08T12:50:27.9275693Z telemetry.save : Save telemetry record of length 7390 in cache\n2020-06-08T12:50:27.9280735Z telemetry.check : Negative: The \/home\/vsts\/work\/_temp\/.azclitask\/telemetry.txt was modified at 2020-06-08 12:47:41.161160, which in less than 600.000000 s\n2020-06-08T12:50:27.9290480Z command ran in 55.735 seconds.\n2020-06-08T12:50:28.1525434Z ##[error]Script failed with exit code: 1\n2020-06-08T12:50:28.1536650Z [command]\/opt\/hostedtoolcache\/Python\/3.6.10\/x64\/bin\/az account clear\n2020-06-08T12:50:29.9078943Z ##[section]Finishing: Deploy Model to ACI\n\nBut when i tried to Deploy it using Python SDK it works as well. Is there any permission issues or login to be set before using DevOps Release. I have not done any sort of login in my DevOps Build pipeline.\n\nAny pointers on what is going wrong here ? It would be really helpful.\n\nThanks,\nSrijith",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592223294357,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"They're actively answering Devops question in dedicated forums here.\n\n\n\n\nhttps:\/\/developercommunity.visualstudio.com\/spaces\/21\/index.html\n\n\n\n\n--please don't forget to Accept as answer if the reply is helpful--\n\nRegards, Dave Patrick ....\nMicrosoft Certified Professional\nMicrosoft MVP [Windows Server] Datacenter Management\n\n\n\n\nDisclaimer: This posting is provided \"AS IS\" with no warranties or guarantees, and confers no rights.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/36104\/deployment-of-multiple-models-to-container-instanc.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-15T12:17:00.34Z",
                "Answer_score":1,
                "Answer_body":"They're actively answering Devops question in dedicated forums here.\n\n\n\n\nhttps:\/\/developercommunity.visualstudio.com\/spaces\/21\/index.html\n\n\n\n\n--please don't forget to Accept as answer if the reply is helpful--\n\nRegards, Dave Patrick ....\nMicrosoft Certified Professional\nMicrosoft MVP [Windows Server] Datacenter Management\n\n\n\n\nDisclaimer: This posting is provided \"AS IS\" with no warranties or guarantees, and confers no rights.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1592223420340,
        "Question_original_content":"deploy multipl model contain instanc fail azur devop team try deploi model regist model registri azur contain instanc devop releas pipelin cli extens aci configur containerresourcerequir cpu memoryingb computetyp aci infer config entryscript score runtim python condafil conda depend yml extradockerfilestep schemafil sourcedirectori enablegpu fals baseimag baseimageregistri score conda depend yml acideploymentconfig yml place flatten directori publis devop pipelin artifcat look like devop deploi command look like model deploi resourcegroup workspac servic stage model json glove acideploymentconfig yml inferenceconfig yml overwrit debug set work directori folder file place like defaultworkingdirectori symptom code index symptom model get except error messag receiv bad respons model manag servic nrespons code nheader date mon jun gmt content type applic json transfer encod chunk connect aliv request context appid cid dee eea client request ebcffabaff client session cabefc api support version preview strict transport secur max ag includesubdomain preload ncontent code badrequest statuscod messag request invalid detail code invalidoverwriterequest messag invalid overwrit request updat contain resourc requir dn label deploy type delet redeploi servic correl requestid ebcffabaff azur cli version error webserviceexcept messag receiv bad respons model manag servic respons code header date mon jun gmt content type applic json transfer encod chunk connect aliv request context appid cid dee eea client request ebcffabaff client session cabefc api support version preview strict transport secur max ag includesubdomain preload content code badrequest statuscod messag request invalid detail code invalidoverwriterequest messag invalid overwrit request updat contain resourc requir dn label deploy type delet redeploi servic correl requestid ebcffabaff innerexcept errorrespons error messag receiv bad respons model manag servic nrespons code nheader date mon jun gmt content type applic json transfer encod chunk connect aliv request context appid cid dee eea client request ebcffabaff client session cabefc api support version preview strict transport secur max ag includesubdomain preload ncontent code badrequest statuscod messag request invalid detail code invalidoverwriterequest messag invalid overwrit request updat contain resourc requir dn label deploy type delet redeploi servic correl requestid ebcffabaff event cli postexecut command data logger exit code telemetri save save telemetri record length cach telemetri check neg home vst work temp azclitask telemetri txt modifi command ran second error script fail exit code command opt hostedtoolcach python bin account clear section finish deploi model aci tri deploi python sdk work permiss issu login set devop releas sort login devop build pipelin pointer go wrong help thank srijith",
        "Question_preprocessed_content":"deploy multipl model contain instanc fail azur devop team try deploi model azur contain instanc devop releas pipelin cli extens aci configur containerresourcerequir cpu memoryingb computetyp aci infer config entryscript runtim python condafil extradockerfilestep schemafil sourcedirectori enablegpu fals baseimag baseimageregistri place flatten directori publis devop pipelin artifcat look like devop deploi command look like model deploi glove set work directori folder file place like get except error request overwrit request updat contain resourc requir dn label deploy type delet redeploi content correl requestid ffaba innerexcept errorrespons request overwrit request updat contain resourc requir dn label deploy type delet redeploi event exit code save telemetri record length cach neg modifi command ran second error script fail exit code account clear section finish deploi model aci tri deploi python sdk work permiss issu login set devop releas sort login devop build pipelin pointer go wrong help thank srijith",
        "Question_gpt_summary_original":"The user is encountering an exception while trying to deploy two ML models to Azure Container Instance using DevOps Release pipeline using AZ CLI ML extension. The error message suggests that there is an invalid overwrite request and the container resource requirements, DNS name label, or deployment type cannot be updated. The user is unsure if there are any permission issues or login requirements that need to be set before using DevOps Release.",
        "Question_gpt_summary":"user encount except try deploi model azur contain instanc devop releas pipelin cli extens error messag suggest invalid overwrit request contain resourc requir dn label deploy type updat user unsur permiss issu login requir need set devop releas",
        "Answer_original_content":"activ answer devop question dedic forum http developercommun visualstudio com space index html forget accept answer repli help regard dave patrick microsoft certifi profession microsoft mvp window server datacent manag disclaim post provid warranti guarante confer right",
        "Answer_preprocessed_content":"activ answer devop question dedic forum forget accept answer repli regard dave patrick microsoft certifi profession microsoft mvp datacent manag disclaim post provid warranti guarante confer right",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Why AWS Lambda Internel Server Error 500 but successfully \/invocations POST 200 in Endpoint SageMaker?",
        "Question_body":"<pre><code>import os\nimport io\nimport boto3\nimport json\nimport csv\n\n\n# grab environment variables\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\n# grab runtime client\nruntime = boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    # Load data from POST request\n    data = json.loads(json.dumps(event))\n    \n    # Grab the payload\n    payload = data['body']\n    \n    # Invoke the model. In this case the data type is a JSON but can be other things such as a CSV\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                   ContentType='application\/json',\n                                   Body=payload)\n    \n    # Get the body of the response from the model\n    result = response['Body'].read().decode()\n\n    # Return it along with the status code of 200 meaning this was succesful \n    return {\n        'statusCode': 200,\n        'body': result\n    }\n<\/code><\/pre>\n<p><strong>response from AWS Lambda<\/strong><\/p>\n<pre><code>{\n  &quot;errorMessage&quot;: &quot;'body'&quot;,\n  &quot;errorType&quot;: &quot;KeyError&quot;,\n  &quot;stackTrace&quot;: [\n    [\n      &quot;\/var\/task\/lambda_function.py&quot;,\n      18,\n      &quot;lambda_handler&quot;,\n      &quot;payload = data['body']&quot;\n    ]\n  ]\n}\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/h8wvA.png\" rel=\"nofollow noreferrer\">response from Postman 500 Internal Server Error<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/cuknX.png\" rel=\"nofollow noreferrer\">but successfully invoke POST 200 in SageMaker Endpoint<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1626360722473,
        "Question_favorite_count":null,
        "Question_last_edit_time":1626360827356,
        "Question_score":0.0,
        "Question_view_count":277.0,
        "Answer_body":"<p>The issue is when you are trying to parse your payload with data['body']. The data is not being passed in the format that the endpoint is expecting. Use the following code snippet to properly format\/serialize your data for the endpoint. Also to make all this clearer make sure to check for your payload type to make sure you have not serialized again by accident.<\/p>\n<pre><code>    data = json.loads(json.dumps(event))\n    payload = json.dumps(data)\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                       ContentType='application\/json',\n                                       Body=payload)\n    result = json.loads(response['Body'].read().decode())\n<\/code><\/pre>\n<p>I work for AWS &amp; my opinions are my own<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1626979396583,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68396088",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1626975923230,
        "Question_original_content":"aw lambda internel server error successfulli invoc post endpoint import import import boto import json import csv grab environ variabl endpoint environ endpoint grab runtim client runtim boto client runtim def lambda handler event context load data post request data json load json dump event grab payload payload data bodi invok model case data type json thing csv respons runtim invok endpoint endpointnam endpoint contenttyp applic json bodi payload bodi respons model result respons bodi read decod return statu code mean succes return statuscod bodi result respons aw lambda errormessag bodi errortyp keyerror stacktrac var task lambda function lambda handler payload data bodi respons postman intern server error successfulli invok post endpoint",
        "Question_preprocessed_content":"aw lambda internel server error successfulli post endpoint respons aw lambda respons postman intern server error successfulli invok post endpoint",
        "Question_gpt_summary_original":"The user is facing a challenge where they are receiving an Internal Server Error 500 response from AWS Lambda when using a POST request with a JSON payload. The error is caused by a KeyError in the Lambda function, specifically with the 'body' parameter. However, the user is able to successfully invoke a POST request with a JSON payload to the SageMaker Endpoint and receive a 200 response.",
        "Question_gpt_summary":"user face challeng receiv intern server error respons aw lambda post request json payload error caus keyerror lambda function specif bodi paramet user abl successfulli invok post request json payload endpoint receiv respons",
        "Answer_original_content":"issu try pars payload data bodi data pass format endpoint expect us follow code snippet properli format serial data endpoint clearer sure check payload type sure serial accid data json load json dump event payload json dump data respons runtim invok endpoint endpointnam endpoint contenttyp applic json bodi payload result json load respons bodi read decod work aw opinion",
        "Answer_preprocessed_content":"issu try pars payload data data pass format endpoint expect us follow code snippet properli data endpoint clearer sure check payload type sure serial accid work aw opinion",
        "Answer_gpt_summary_original":"Solution: The discussion provides a code snippet that can be used to properly format and serialize the data for the endpoint. The user should use this code snippet to parse the payload and check for the payload type to ensure that it has not been serialized again by accident.",
        "Answer_gpt_summary":"solut discuss provid code snippet properli format serial data endpoint user us code snippet pars payload check payload type ensur serial accid"
    },
    {
        "Question_title":"ML for object search",
        "Question_body":"<p>I'm trying to find a way to build ML using AWS, preferably using their services such as SageMaker and not just EC2, for object detection in images using an image as input.<\/p>\n\n<p>AWS Rekognition offers Image Comparison and Object detection APIs, but they are not exactly what I'm looking for, the comparison works only with faces and not objects and object detection is too basic.<\/p>\n\n<p>AlibabCloud has that functionality as a service (<a href=\"https:\/\/www.alibabacloud.com\/product\/imagesearch\" rel=\"nofollow noreferrer\">https:\/\/www.alibabacloud.com\/product\/imagesearch<\/a>) but I would like to use something similar on AWS, rather than Alibaba.<\/p>\n\n<p>How would I go about and build something like this?<\/p>\n\n<p>Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1583445060257,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1586898520163,
        "Question_score":2.0,
        "Question_view_count":86.0,
        "Answer_body":"<p><em>edited 03\/08\/2020 to add pointers for visual search<\/em><\/p>\n\n<p>Since you seem interested both in the tasks of <strong>object detection<\/strong> (input an image, and return bounding boxes with object classes) and <strong>visual search<\/strong> (input an image and return relevant images) let me give you pointers for both :)<\/p>\n\n<p>For <strong>object detection<\/strong> you have 3 options:<\/p>\n\n<ul>\n<li><strong>Using the managed service <a href=\"https:\/\/aws.amazon.com\/rekognition\/custom-labels-features\/?nc1=h_ls\" rel=\"nofollow noreferrer\">Amazon Rekognition Custom Labels<\/a><\/strong>. The key benefits of this service is that (1) it doesn't require writing ML code, as the service runs autoML internally to find the best model, (2) it is very flexible in terms of interaction (SDKs, console), data loading and annotation and (3) it can work even with small datasets (typically a few hundred images or less).<\/li>\n<li><strong>Using SageMaker Object Detection model<\/strong> (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection.html\" rel=\"nofollow noreferrer\">documentation<\/a>, <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_birds\/object_detection_birds.ipynb\" rel=\"nofollow noreferrer\">demo<\/a>). In this option, the model is also already written (SSD architecture with Resnet or VGG backbone) and you just need to choose  or tune <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection-api-config.html\" rel=\"nofollow noreferrer\">hyperparameters<\/a><\/li>\n<li><strong>Using your own model on Amazon SageMaker<\/strong>. This could be your own code in docker, or code from an ML framework in a SageMaker ML Framework container. There are such containers for <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html\" rel=\"nofollow noreferrer\">Pytorch<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html\" rel=\"nofollow noreferrer\">Tensorflow<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_mxnet.html\" rel=\"nofollow noreferrer\">MXNet<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_chainer.html\" rel=\"nofollow noreferrer\">Chainer<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html\" rel=\"nofollow noreferrer\">Sklearn<\/a>. In terms of model code, I recommend considering <a href=\"https:\/\/gluon-cv.mxnet.io\/\" rel=\"nofollow noreferrer\"><strong><code>gluoncv<\/code><\/strong><\/a>, a compact python computer vision toolkit (based on mxnet backend) that comes with <a href=\"https:\/\/gluon-cv.mxnet.io\/model_zoo\/detection.html\" rel=\"nofollow noreferrer\">many state-of-the-art models<\/a> and <a href=\"https:\/\/gluon-cv.mxnet.io\/build\/examples_detection\/index.html\" rel=\"nofollow noreferrer\">tutorials<\/a> for object detection<\/li>\n<\/ul>\n\n<p>The task of <strong>visual search<\/strong> requires more customization, since you need to provide the info of (1) what you define as search relevancy (eg is it visual similarity? or object complementarity? etc) and (2) the collection among which to search. If all you need is visual similarity, a popular option is to transform images into vectors with a pre-trained neural network and run kNN search between the query image and the collection of transformed images. There are 2 tutos showing how to build such systems on AWS here:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/visual-search-on-aws-part-1-engine-implementation-with-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">Blog Post Visual Search on AWS<\/a> (MXNet resnet embeddings +\nSageMaker kNN)<\/li>\n<li><a href=\"https:\/\/thomasdelteil.github.io\/VisualSearch_MXNet\/\" rel=\"nofollow noreferrer\">Visual Search on MMS demo<\/a> (MXNet resnet\nembeddings + HNSW kNN on AWS Fargate)<\/li>\n<\/ul>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":1583703139203,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60554471",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1583500304256,
        "Question_original_content":"object search try wai build aw prefer servic object detect imag imag input aw rekognit offer imag comparison object detect api exactli look comparison work face object object detect basic alibabcloud function servic http alibabacloud com product imagesearch like us similar aw alibaba build like thank",
        "Question_preprocessed_content":"object search try wai build aw prefer servic object detect imag imag input aw rekognit offer imag comparison object detect api exactli look comparison work face object object detect basic alibabcloud function servic like us similar aw alibaba build like thank",
        "Question_gpt_summary_original":"The user is facing challenges in building machine learning (ML) using AWS for object detection in images. Although AWS Rekognition offers Image Comparison and Object detection APIs, they are not suitable for the user's requirements. The user is looking for a service similar to AlibabaCloud's Image Search, but on AWS. The user is seeking guidance on how to build ML for object search on AWS.",
        "Question_gpt_summary":"user face challeng build machin learn aw object detect imag aw rekognit offer imag comparison object detect api suitabl user requir user look servic similar alibabacloud imag search aw user seek guidanc build object search aw",
        "Answer_original_content":"edit add pointer visual search interest task object detect input imag return bound box object class visual search input imag return relev imag let pointer object detect option manag servic amazon rekognit custom label kei benefit servic requir write code servic run automl intern best model flexibl term interact sdk consol data load annot work small dataset typic imag object detect model document demo option model written ssd architectur resnet vgg backbon need choos tune hyperparamet model code docker code framework framework contain contain pytorch tensorflow mxnet chainer sklearn term model code recommend consid gluoncv compact python vision toolkit base mxnet backend come state art model tutori object detect task visual search requir custom need provid info defin search relev visual similar object complementar collect search need visual similar popular option transform imag vector pre train neural network run knn search queri imag collect transform imag tuto show build system aw blog post visual search aw mxnet resnet embed knn visual search mm demo mxnet resnet embed hnsw knn aw fargat",
        "Answer_preprocessed_content":"edit add pointer visual search interest task object detect visual search let pointer object detect option manag servic amazon rekognit custom label kei benefit servic requir write code servic run automl intern best model flexibl term interact data load annot work small dataset object detect model option model written need choos tune hyperparamet model code docker code framework framework contain contain pytorch tensorflow mxnet chainer sklearn term model code recommend consid compact python vision toolkit come model tutori object detect task visual search requir custom need provid info defin search relev collect search need visual similar popular option transform imag vector neural network run knn search queri imag collect transform imag tuto show build system aw blog post visual search aw visual search mm demo",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Using Amazon Rekognition Custom Labels, which is a managed service that runs autoML internally to find the best model for object detection. It is flexible in terms of interaction, data loading, and annotation, and can work even with small datasets.\n2. Using SageMaker Object Detection model, which is a pre-written model with SSD architecture and Resnet or VGG backbone. The user can choose or tune hyperparameters.\n3. Using the user's own model on Amazon SageMaker, which could be their own code in docker or code from an ML framework in a SageMaker ML Framework container.\n4. For visual search, transforming images into vectors with a pre-trained neural network and running kNN search",
        "Answer_gpt_summary":"possibl solut mention discuss amazon rekognit custom label manag servic run automl intern best model object detect flexibl term interact data load annot work small dataset object detect model pre written model ssd architectur resnet vgg backbon user choos tune hyperparamet user model code docker code framework framework contain visual search transform imag vector pre train neural network run knn search"
    },
    {
        "Question_title":"Outgoing mail for sagemaker labeling job",
        "Question_body":"When having made a labeling job on Ground Truth, an outgoing mail should be sent to team member, but in my case, mail not be sent with no error message.\n\nin case no private team created (the first job creation) : mail can be sent. (set up a team during job creation)\nin case a private team already set up: mail cannot be sent. (select a existing team during job creation)\n\nI think policies of the job role might not be enough, for example, cognito policy. How can I make sure the cause of the error?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662016903588,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":36.0,
        "Answer_body":"To Successfully create a SageMaker Labeling Job you will need the following Permission Policies applied within your account:\n\nThe IAM entity you have used to create the job will need permissions outlined in the \"Permissions Required to Use the Amazon SageMaker Ground Truth Console\" [1]\nYour Labelling Job Role will need SageMakerFullAccess [2]\n\nWith these permissions in place your job should create successfully.\n\nLinks to documentation provided by AWS:\n\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security_iam_id-based-policy-examples.html#console-permissions\n\n[2] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonSageMakerFullAccess",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqbxtiU_kSe-GoPcj6g0pzg\/outgoing-mail-for-sagemaker-labeling-job",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-02T14:53:23.761Z",
                "Answer_score":0,
                "Answer_body":"To Successfully create a SageMaker Labeling Job you will need the following Permission Policies applied within your account:\n\nThe IAM entity you have used to create the job will need permissions outlined in the \"Permissions Required to Use the Amazon SageMaker Ground Truth Console\" [1]\nYour Labelling Job Role will need SageMakerFullAccess [2]\n\nWith these permissions in place your job should create successfully.\n\nLinks to documentation provided by AWS:\n\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security_iam_id-based-policy-examples.html#console-permissions\n\n[2] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonSageMakerFullAccess",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1662130403760,
        "Question_original_content":"outgo mail label job have label job ground truth outgo mail sent team member case mail sent error messag case privat team creat job creation mail sent set team job creation case privat team set mail sent select exist team job creation think polici job role exampl cognito polici sure caus error",
        "Question_preprocessed_content":"outgo mail label job have label job ground truth outgo mail sent team member case mail sent error messag case privat team creat mail sent case privat team set mail sent think polici job role exampl cognito polici sure caus error",
        "Question_gpt_summary_original":"The user is facing challenges with outgoing mail for a labeling job on Ground Truth. The user is unable to send an outgoing mail to team members when a private team is already set up, but can send the mail when no private team is created. The user suspects that the job role policies, such as the cognito policy, may not be sufficient and is unsure how to determine the cause of the error.",
        "Question_gpt_summary":"user face challeng outgo mail label job ground truth user unabl send outgo mail team member privat team set send mail privat team creat user suspect job role polici cognito polici suffici unsur determin caus error",
        "Answer_original_content":"successfulli creat label job need follow permiss polici appli account iam entiti creat job need permiss outlin permiss requir us ground truth consol label job role need fullaccess permiss place job creat successfulli link document provid aw http doc aw amazon com latest secur iam base polici exampl html consol permiss http doc aw amazon com latest secur iam awsmanpol html secur iam awsmanpol amazonfullaccess",
        "Answer_preprocessed_content":"successfulli creat label job need follow permiss polici appli account iam entiti creat job need permiss outlin permiss requir us ground truth consol label job role need fullaccess permiss place job creat successfulli link document provid aw",
        "Answer_gpt_summary_original":"Solution: The user needs to ensure that the IAM entity used to create the job has the necessary permissions outlined in the \"Permissions Required to Use the Amazon SageMaker Ground Truth Console\" and that the labeling job role has SageMakerFullAccess. AWS documentation links are provided to help the user understand the required permissions. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut user need ensur iam entiti creat job necessari permiss outlin permiss requir us ground truth consol label job role fullaccess aw document link provid help user understand requir permiss solut mention discuss"
    },
    {
        "Question_title":"ClusterIdentityNotFound when submitting experiment.",
        "Question_body":"When I'm submitting my experiment fom notebook, experiment is queing for a long time then I get as error:\n\nAzureMLCompute job failed.\nClusterIdentityNotFound: Identity of the specified\nmanaged compute <hidden cluster location> is not found\n\n\n\n\nI've updated all azure ml packages and restarted cluster, deleted, recreating, ... Nothing seems to be working.\n\nWhat Should I do?",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1633943984543,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, are you by any chance using a low priority VM? If so, can you try selecting 'dedicated' as priority to verify? Also, ensure that you are following the steps outlined in this document for creating a compute cluster. In the advanced settings, ensure to assign a managed identity and specify a system-assigned identity or user-assigned identity.\n\n\n\n\n--- Kindly Accept Answer if the information provided helps. Thanks.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/585373\/clusteridentitynotfound-when-submitting-experiment.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-11T21:32:50.467Z",
                "Answer_score":1,
                "Answer_body":"Hi, are you by any chance using a low priority VM? If so, can you try selecting 'dedicated' as priority to verify? Also, ensure that you are following the steps outlined in this document for creating a compute cluster. In the advanced settings, ensure to assign a managed identity and specify a system-assigned identity or user-assigned identity.\n\n\n\n\n--- Kindly Accept Answer if the information provided helps. Thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-10-13T07:35:27.65Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nThank you for you answer.\nIt fixed my issue.\n\nHave a great day. :)\n\n\n\n\nBest regards.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-16T10:25:02.78Z",
                "Answer_score":0,
                "Answer_body":"Thanks for the answer @GiftA-MSFT . May i ask if this problem is happening for both compute clusters and compute instances? We are experiencing the same problem when trying to create an automated regression machine learning experiment by using a compute instance: Virtual machine size\nStandard_DS3_v2 (4 cores, 14 GB RAM, 28 GB disk)\nProcessing unit\nCPU - General purpose\nAny help much appreciated\nKind regards",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1633987970467,
        "Question_original_content":"clusteridentitynotfound submit experi submit experi fom notebook experi que long time error comput job fail clusteridentitynotfound ident specifi manag comput updat packag restart cluster delet recreat work",
        "Question_preprocessed_content":"clusteridentitynotfound submit experi submit experi fom notebook experi que long time error comput job fail clusteridentitynotfound ident specifi manag comput updat packag restart cluster delet recreat work",
        "Question_gpt_summary_original":"The user is encountering an error message \"ClusterIdentityNotFound\" when submitting an experiment from their notebook. The experiment is queuing for a long time before failing with the error message \"AzureMLCompute job failed\". The user has tried updating all Azure ML packages, restarting the cluster, and recreating it, but none of these solutions have worked. The user is seeking advice on what to do next.",
        "Question_gpt_summary":"user encount error messag clusteridentitynotfound submit experi notebook experi queu long time fail error messag comput job fail user tri updat packag restart cluster recreat solut work user seek advic",
        "Answer_original_content":"chanc low prioriti try select dedic prioriti verifi ensur follow step outlin document creat comput cluster advanc set ensur assign manag ident specifi assign ident user assign ident kindli accept answer inform provid help thank",
        "Answer_preprocessed_content":"chanc low prioriti try select dedic prioriti verifi ensur follow step outlin document creat comput cluster advanc set ensur assign manag ident specifi ident ident kindli accept answer inform provid help thank",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Check if the user is using a low priority VM and try selecting 'dedicated' as priority to verify.\n2. Ensure that the user is following the steps outlined in the document for creating a compute cluster.\n3. In the advanced settings, assign a managed identity and specify a system-assigned identity or user-assigned identity.\n\nNo personal opinions or biases are included in the response.",
        "Answer_gpt_summary":"possibl solut mention discuss check user low prioriti try select dedic prioriti verifi ensur user follow step outlin document creat comput cluster advanc set assign manag ident specifi assign ident user assign ident person opinion bias includ respons"
    },
    {
        "Question_title":"Python in AzureML fail to pass dataframe without changes",
        "Question_body":"<p>when trying to pass data without doing anything in python, getting this error:<\/p>\n\n<pre><code>Error 0085: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\nCaught exception while executing function: Traceback (most recent call last):\n  File \"C:\\server\\invokepy.py\", line 175, in batch\n    rutils.RUtils.DataFrameToRFile(outlist[i], outfiles[i])\n  File \"C:\\server\\RReader\\rutils.py\", line 28, in DataFrameToRFile\n    rwriter.write_attribute_list(attributes)\n  File \"C:\\server\\RReader\\rwriter.py\", line 59, in write_attribute_list\n    self.write_object(value);\n  File \"C:\\server\\RReader\\rwriter.py\", line 121, in write_object\n    write_function(flags, value.values())\n  File \"C:\\server\\RReader\\rwriter.py\", line 104, in write_objects\n    self.write_object(value)\n  File \"C:\\server\\RReader\\rwriter.py\", line 121, in write_object\n    write_function(flags, value.values())\n  File \"C:\\server\\RReader\\rwriter.py\", line 71, in write_integers\n    self.write_integer(value)\n  File \"C:\\server\\RReader\\rwriter.py\", line 147, in write_integer\n    self.writer.WriteInt32(value)\n  File \"C:\\server\\RReader\\BinaryIO\\binarywriter.py\", line 26, in WriteInt32\n    self.WriteData(self.Int32Format, data)\n  File \"C:\\server\\RReader\\BinaryIO\\binarywriter.py\", line 14, in WriteData\n    self.stream.write(pack(format, data))\nerror: cannot convert argument to integer\n\n---------- End of error message from Python  interpreter  ----------\nStart time: UTC 05\/26\/2016 13:16:01\nEnd time: UTC 05\/26\/2016 13:16:13\n<\/code><\/pre>\n\n<p>here is the data i'm trying to pass:\n<a href=\"https:\/\/i.stack.imgur.com\/ysG36.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ysG36.png\" alt=\"the data\"><\/a><\/p>\n\n<p>here is the experiment:\n<a href=\"https:\/\/i.stack.imgur.com\/vgdSn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vgdSn.png\" alt=\"the experiment\"><\/a><\/p>\n\n<p>and the python code:\n<a href=\"https:\/\/i.stack.imgur.com\/LRSAE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LRSAE.png\" alt=\"python code\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1464269301867,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":739.0,
        "Answer_body":"<p>after talking to Microsoft support, the problem was that the \"Execute Python Script\" module cannot return empty values.\nthis can be solved by adding a \"Clean Missing Data\" module before reading it from python:\n<a href=\"https:\/\/i.stack.imgur.com\/BzCUZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BzCUZ.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37462268",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1468139774183,
        "Question_original_content":"python fail pass datafram chang try pass data python get error error follow error occur script evalu view output log inform start error messag python interpret caught except execut function traceback recent file server invokepi line batch rutil rutil dataframetorfil outlist outfil file server rreader rutil line dataframetorfil rwriter write attribut list attribut file server rreader rwriter line write attribut list self write object valu file server rreader rwriter line write object write function flag valu valu file server rreader rwriter line write object self write object valu file server rreader rwriter line write object write function flag valu valu file server rreader rwriter line write integ self write integ valu file server rreader rwriter line write integ self writer writeint valu file server rreader binaryio binarywrit line writeint self writedata self intformat data file server rreader binaryio binarywrit line writedata self stream write pack format data error convert argument integ end error messag python interpret start time utc end time utc data try pass experi python code",
        "Question_preprocessed_content":"python fail pass datafram chang try pass data python get error data try pass experi python code",
        "Question_gpt_summary_original":"The user encountered an error when trying to pass data without making any changes in Python in AzureML. The error message suggests that there was an issue with converting an argument to an integer. The user provided a screenshot of the data and experiment, as well as the Python code used.",
        "Question_gpt_summary":"user encount error try pass data make chang python error messag suggest issu convert argument integ user provid screenshot data experi python code",
        "Answer_original_content":"talk microsoft support problem execut python script modul return valu solv ad clean miss data modul read python",
        "Answer_preprocessed_content":"talk microsoft support problem execut python script modul return valu solv ad clean miss data modul read python",
        "Answer_gpt_summary_original":"Solution: The issue was that the \"Execute Python Script\" module cannot return empty values. This can be solved by adding a \"Clean Missing Data\" module before reading it from Python.",
        "Answer_gpt_summary":"solut issu execut python script modul return valu solv ad clean miss data modul read python"
    },
    {
        "Question_title":"SageMaker Experiments Deletion Help Needed",
        "Question_body":"Hi Friends,\n\nI have deleted everything in Sagemaker - but support is asking me to delete the experiments that are still in my account : they sent me a link to follow\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/experiments-cleanup.html\n\nbut I have no idea how to complete this task -- does anyone know in terms that someone who has no idea what this means - can follow and achieve this task\n\nyou have no idea how much it would mean to me for any assistance",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1668037196721,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":44.0,
        "Answer_body":"You need to use a computer with Python, the SageMaker SDK installed, and AWS credentials with enough permissions for that account configured. If you are already using SageMaker Studio, that should work.\n\nUse the second method. Create a file (Menu File -> New -> Python File). Rename it as cleanup_experiments.py(right click on the file on top and select Rename Python File), then paste the code in the documentation (those three sections, one after another). Save the file and open a terminal (Menu File -> New -> Terminal). Navigate to the directory where you saved the file and execute the command python cleanup_experiments.py",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFMzl26gfQna8sAZcCDJw_Q\/sage-maker-experiments-deletion-help-needed",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-10T09:00:37.516Z",
                "Answer_score":1,
                "Answer_body":"You need to use a computer with Python, the SageMaker SDK installed, and AWS credentials with enough permissions for that account configured. If you are already using SageMaker Studio, that should work.\n\nUse the second method. Create a file (Menu File -> New -> Python File). Rename it as cleanup_experiments.py(right click on the file on top and select Rename Python File), then paste the code in the documentation (those three sections, one after another). Save the file and open a terminal (Menu File -> New -> Terminal). Navigate to the directory where you saved the file and execute the command python cleanup_experiments.py",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1668070837516,
        "Question_original_content":"experi delet help need friend delet support ask delet experi account sent link follow http doc aw amazon com latest experi cleanup html idea complet task know term idea mean follow achiev task idea mean assist",
        "Question_preprocessed_content":"experi delet help need friend delet support ask delet experi account sent link follow idea complet task know term idea mean follow achiev task idea mean assist",
        "Question_gpt_summary_original":"The user has deleted everything in SageMaker but is now facing challenges in deleting experiments that are still in their account. They have been provided with a link by support to follow for guidance, but they are unsure of how to complete the task and are seeking assistance.",
        "Question_gpt_summary":"user delet face challeng delet experi account provid link support follow guidanc unsur complet task seek assist",
        "Answer_original_content":"need us python sdk instal aw credenti permiss account configur studio work us second method creat file menu file new python file renam cleanup experi right click file select renam python file past code document section save file open termin menu file new termin navig directori save file execut command python cleanup experi",
        "Answer_preprocessed_content":"need us python sdk instal aw credenti permiss account configur studio work us second method creat file renam click file select renam python file past code document save file open termin navig directori save file execut command python",
        "Answer_gpt_summary_original":"Solution: The user needs to use a computer with Python, the SageMaker SDK installed, and AWS credentials with enough permissions for that account configured. They should create a file, rename it as cleanup_experiments.py, paste the code in the documentation, save the file, and open a terminal. Then, they should navigate to the directory where they saved the file and execute the command python cleanup_experiments.py.",
        "Answer_gpt_summary":"solut user need us python sdk instal aw credenti permiss account configur creat file renam cleanup experi past code document save file open termin navig directori save file execut command python cleanup experi"
    },
    {
        "Question_title":"How to Publish an Azure Bot",
        "Question_body":"<p>Just learning how to use <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/bot-service\/?view=azure-bot-service-3.0\" rel=\"nofollow noreferrer\">Azure Bot Service<\/a> and <code>Azure Bot Framework<\/code>. I created a Bot in Azure portal following <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/bot-service\/bot-service-quickstart?view=azure-bot-service-3.0\" rel=\"nofollow noreferrer\">this<\/a> Official Azure tutorial. Does this bot need to be published somewhere? I read somewhere that you <code>Build--&gt;Test--&gt;Publish--&gt;Evaluate<\/code>. I've tested it in Azure portal itself as explained <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/bot-service\/bot-service-quickstart?view=azure-bot-service-3.0\" rel=\"nofollow noreferrer\">here<\/a>. Not sure about the Publish part of it.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1526657122620,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":844.0,
        "Answer_body":"<p>How do you intend to use your bot? Azure Bots work by connecting them to existing channels like Skype, Facebook Messenger, SMS, etc or making REST calls from a custom application.<\/p>\n\n<p>However you can also reach your bot directly from: <code>https:\/\/webchat.botframework.com\/embed\/YOUR_BOT_ID?t=YOUR_TOKEN_HERE<\/code><\/p>\n\n<p>You can embed it on any web page with this HTML tag:<\/p>\n\n<pre><code>&lt;iframe src=\"https:\/\/webchat.botframework.com\/embed\/YOUR_BOT_ID?t=YOUR_TOKEN_HERE\"&gt;&lt;\/iframe&gt;\n<\/code><\/pre>\n\n<p>Please note that both of these methods expose your token and would allow other developers to add your bot to their pages as well.<\/p>\n\n<p>Bot ID is the name of your bot and you can get the token from the portal by going to your bot and choosing \"Channel\" blade and then clicking the \"Get bot embed codes\" link.<\/p>\n\n<p>Edit: I went ahead and wrote a blog post on this topic <a href=\"https:\/\/medium.com\/@joelatwar\/how-to-embed-your-azure-web-app-bot-in-any-web-page-120dfda91fdc\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@joelatwar\/how-to-embed-your-azure-web-app-bot-in-any-web-page-120dfda91fdc<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1527282754360,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50414639",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1526659876240,
        "Question_original_content":"publish azur bot learn us azur bot servic azur bot framework creat bot azur portal follow offici azur tutori bot need publish read build test publish evalu test azur portal explain sure publish",
        "Question_preprocessed_content":"publish azur bot learn us azur bot servic creat bot azur portal follow offici azur tutori bot need publish read test azur portal explain sure publish",
        "Question_gpt_summary_original":"The user is facing challenges in understanding how to publish an Azure Bot created using Azure Bot Service and Azure Bot Framework, despite following the official Azure tutorial. They are unsure if the bot needs to be published somewhere and are seeking clarification on the Build-Test-Publish-Evaluate process. The user has tested the bot in the Azure portal but is uncertain about the publishing aspect.",
        "Question_gpt_summary":"user face challeng understand publish azur bot creat azur bot servic azur bot framework despit follow offici azur tutori unsur bot need publish seek clarif build test publish evalu process user test bot azur portal uncertain publish aspect",
        "Answer_original_content":"intend us bot azur bot work connect exist channel like skype facebook messeng sm make rest call custom applic reach bot directli http webchat botframework com emb bot token emb web page html tag note method expos token allow develop add bot page bot bot token portal go bot choos channel blade click bot emb code link edit went ahead wrote blog post topic http medium com joelatwar emb azur web app bot web page dfdafdc",
        "Answer_preprocessed_content":"intend us bot azur bot work connect exist channel like skype facebook messeng sm make rest call custom applic reach bot directli emb web page html tag note method expos token allow develop add bot page bot bot token portal go bot choos channel blade click bot emb code link edit went ahead wrote blog post topic",
        "Answer_gpt_summary_original":"Solutions provided:\n- The user can connect the bot to existing channels like Skype, Facebook Messenger, SMS, etc or make REST calls from a custom application.\n- The bot can be reached directly from a web page using an HTML tag or a URL that exposes the bot's token. \n- The user can get the token from the Azure portal by going to the bot and choosing the \"Channel\" blade and then clicking the \"Get bot embed codes\" link. \n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"solut provid user connect bot exist channel like skype facebook messeng sm rest call custom applic bot reach directli web page html tag url expos bot token user token azur portal go bot choos channel blade click bot emb code link person opinion bias includ respons"
    },
    {
        "Question_title":"XGBoost Model in AWS-Sagemaker Fails with no error message",
        "Question_body":"<p>I'm trying to get a model using the XGBoost classifier in AWS-Sagemaker.  I'm following the abalone example, but when I run it to build the training job it states InProgress 3 times and then just states Failed.  Where do I go to find why it failed?  <\/p>\n\n<p>I've double checked the parameters and made sure the input and output files and directories in S3 were correct.  I know there is permission to read and write because when setting up the data for train\/validate\/test I read and write to S3 with no problems.<\/p>\n\n<pre><code>print(status)\nwhile status !='Completed' and status!='Failed':\n    time.sleep(60)\n    status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n    print(status)\n<\/code><\/pre>\n\n<p>That is the code where the print statements come from.  Is there something I can add to receive a better error message?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1562946273023,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":204.0,
        "Answer_body":"<p>The problem occurred was that the file sent for predictions was csv but the XGBoost settings were set to receive libsvm.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57010184",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1564494294316,
        "Question_original_content":"xgboost model aw fail error messag try model xgboost classifi aw follow abalon exampl run build train job state inprogress time state fail fail doubl check paramet sure input output file directori correct know permiss read write set data train valid test read write problem print statu statu complet statu fail time sleep statu client train job trainingjobnam job trainingjobstatu print statu code print statement come add receiv better error messag",
        "Question_preprocessed_content":"xgboost model aw fail error messag try model xgboost classifi follow abalon exampl run build train job state inprogress time state fail fail doubl check paramet sure input output file directori correct know permiss read write set data read write problem code print statement come add receiv better error messag",
        "Question_gpt_summary_original":"The user is facing challenges while trying to build a training job using the XGBoost classifier in AWS-Sagemaker. The job fails without any error message, and the user is unable to determine the reason for the failure. The user has checked the parameters and ensured that the input and output files and directories in S3 are correct. The user is also able to read and write to S3 without any problems. The user is seeking help to receive a better error message.",
        "Question_gpt_summary":"user face challeng try build train job xgboost classifi aw job fail error messag user unabl determin reason failur user check paramet ensur input output file directori correct user abl read write problem user seek help receiv better error messag",
        "Answer_original_content":"problem occur file sent predict csv xgboost set set receiv libsvm",
        "Answer_preprocessed_content":"problem occur file sent predict csv xgboost set set receiv libsvm",
        "Answer_gpt_summary_original":"Solution: The issue was identified as a mismatch between the file format sent for predictions and the XGBoost settings. The XGBoost classifier was set to receive libsvm format, but the file sent was in csv format. The solution is to ensure that the file format matches the XGBoost settings.",
        "Answer_gpt_summary":"solut issu identifi mismatch file format sent predict xgboost set xgboost classifi set receiv libsvm format file sent csv format solut ensur file format match xgboost set"
    },
    {
        "Question_title":"Remotely execute ClearML task using local-only repo",
        "Question_body":"<p>I want to execute ClearML task remotely. According to docs there are 2 options: 1) execute single python file; 2) ClearML would identify that script is part of repo, that repo will be cloned and installed into docker and executed on the worker.<\/p>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<p>I think it is somewhat extending scenario 1, where not a single file is passed for execution but whole directory with file in it.<\/p>\n<p>PS: i understand reproducibility concerns that arise, but repo is really not accessible from worker :(<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653499480970,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":25.0,
        "Answer_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<blockquote>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<\/blockquote>\n<p>well, no :( if your code is a single script, then yes ClearML would store the entire script, then the worker will reproduce it on the remote machine. But if your code base is composed of more than a single file, then why not use git? it is free hosted by GitHub, Bitbucket, GitLab etc.<\/p>\n<p>In theory this is doable and if you feel the need, I urge you to PR this feature. Basically you would store the entire folder as an artifact (ClearML will auto zip it for you), then the agent needs to unzip the artifact and run it. The main issue would be that cloning the Task will not clone the artifact...<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72381916",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1659992618003,
        "Question_original_content":"remot execut task local repo want execut task remot accord doc option execut singl python file identifi script repo repo clone instal docker execut worker second scenario assum repo remot url access worker isn case possibl pack local repo send remot execut think somewhat extend scenario singl file pass execut directori file understand reproduc concern aris repo access worker thank advanc",
        "Question_preprocessed_content":"remot execut task repo want execut task remot accord doc option execut singl python file identifi script repo repo clone instal docker execut worker second scenario assum repo remot url access worker isn case possibl pack local repo send remot execut think somewhat extend scenario singl file pass execut directori file understand reproduc concern aris repo access worker thank advanc",
        "Question_gpt_summary_original":"The user wants to execute a ClearML task remotely, but faces a challenge when the repo is not accessible by the worker. The two options provided by ClearML are to execute a single python file or to clone and install the repo into docker and execute it on the worker. However, the user wonders if it is possible to pack the local repo and send it for remote execution, similar to passing a directory with a file for execution. The user acknowledges the reproducibility concerns but states that the repo is not accessible from the worker.",
        "Question_gpt_summary":"user want execut task remot face challeng repo access worker option provid execut singl python file clone instal repo docker execut worker user wonder possibl pack local repo send remot execut similar pass directori file execut user acknowledg reproduc concern state repo access worker",
        "Answer_original_content":"disclaim team member second scenario assum repo remot url access worker isn case possibl pack local repo send remot execut code singl script ye store entir script worker reproduc remot machin code base compos singl file us git free host github bitbucket gitlab theori doabl feel need urg featur basic store entir folder artifact auto zip agent need unzip artifact run main issu clone task clone artifact",
        "Answer_preprocessed_content":"disclaim team member second scenario assum repo remot url access worker isn case possibl pack local repo send remot execut code singl script ye store entir script worker reproduc remot machin code base compos singl file us git free host github bitbucket gitlab theori doabl feel need urg featur basic store entir folder artifact agent need unzip artifact run main issu clone task clone",
        "Answer_gpt_summary_original":"Solution: The discussion mentions two possible solutions. The first solution is to execute a single python file or to clone and install the repo into docker and execute it on the worker, which are the options provided by ClearML. The second solution is to use git to store the entire code base, which is free and hosted by GitHub, Bitbucket, GitLab, etc. However, if the user wants to pack the local repo and send it for remote execution, it is not currently possible with ClearML. Although, in theory, it is doable and the user can submit a feature request to ClearML to store the entire folder as an artifact, which will be auto-zipped by ClearML, and then the agent needs to unzip the artifact and",
        "Answer_gpt_summary":"solut discuss mention possibl solut solut execut singl python file clone instal repo docker execut worker option provid second solut us git store entir code base free host github bitbucket gitlab user want pack local repo send remot execut current possibl theori doabl user submit featur request store entir folder artifact auto zip agent need unzip artifact"
    },
    {
        "Question_title":"batch predictions in GCP Vertex AI",
        "Question_body":"<p>While trying out batch predictions in GCP Vertex AI for an AutoML model, the batch prediction results span over several files(which is not convenient from a user perspective). If it would have been a single batch prediction result file i.e. covering all the records in a single file, it would make the procedure much more simple.<\/p>\n<p>For instance, I had 5585 records in my input dataset file. The batch prediction results comprise of 21 files wherein each file has records in the range of 200-300, thus, covering 5585 records altogether.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635004730777,
        "Question_favorite_count":null,
        "Question_last_edit_time":1635191683160,
        "Question_score":2.0,
        "Question_view_count":1708.0,
        "Answer_body":"<p>Batch predictions on an image, text,video,tabular AutoML model, runs the jobs using distributed processing which means the data is distributed among an arbitrary cluster of virtual machines and is processed in an unpredictable order because of which you will get the prediction results stored across various files in Cloud Storage. Since the batch prediction output files are not generated with the same order as an input file, a feature request has been raised and you can track the update on this request from this <a href=\"https:\/\/issuetracker.google.com\/issues\/202080076\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n<p>We cannot provide an ETA at this moment but you can follow the progress in the issue tracker and you can \u2018STAR\u2019 the issue to receive automatic updates and give it traction by referring to this <a href=\"https:\/\/developers.google.com\/issue-tracker\/guides\/subscribe#starring_an_issue\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n<p>However, if you are doing batch prediction for a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/batch-predictions#batch_request_input\" rel=\"nofollow noreferrer\">tabular AutoML model<\/a>, there you have the option to choose the BigQuery as storage where all the prediction output will be stored in a single table and then you can export the table data to a single CSV file.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1635092457400,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69689785",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1635085080550,
        "Question_original_content":"batch predict gcp try batch predict gcp automl model batch predict result span file conveni user perspect singl batch predict result file cover record singl file procedur simpl instanc record input dataset file batch predict result compris file file record rang cover record altogeth",
        "Question_preprocessed_content":"batch predict gcp try batch predict gcp automl model batch predict result span file singl batch predict result file cover record singl file procedur simpl instanc record input dataset file batch predict result compris file file record rang cover record altogeth",
        "Question_gpt_summary_original":"The user encountered challenges with batch predictions in GCP Vertex AI for an AutoML model, as the batch prediction results were split into multiple files instead of a single file, which made the procedure less convenient. The user had 5585 records in their input dataset file, and the batch prediction results comprised of 21 files with each file containing records in the range of 200-300, covering all 5585 records.",
        "Question_gpt_summary":"user encount challeng batch predict gcp automl model batch predict result split multipl file instead singl file procedur conveni user record input dataset file batch predict result compris file file contain record rang cover record",
        "Answer_original_content":"batch predict imag text video tabular automl model run job distribut process mean data distribut arbitrari cluster virtual machin process unpredict order predict result store file cloud storag batch predict output file gener order input file featur request rais track updat request link provid eta moment follow progress issu tracker star issu receiv automat updat traction refer link batch predict tabular automl model option choos bigqueri storag predict output store singl tabl export tabl data singl csv file",
        "Answer_preprocessed_content":"batch predict imag text video tabular automl model run job distribut process mean data distribut arbitrari cluster virtual machin process unpredict order predict result store file cloud storag batch predict output file gener order input file featur request rais track updat request link provid eta moment follow progress issu tracker star issu receiv automat updat traction refer link batch predict tabular automl model option choos bigqueri storag predict output store singl tabl export tabl data singl csv file",
        "Answer_gpt_summary_original":"Solution:\n- Batch predictions on an AutoML model in GCP Vertex AI are processed using distributed processing, which results in the prediction output files being stored across various files in Cloud Storage. A feature request has been raised to generate the batch prediction output files in the same order as the input file, but there is no ETA for this feature yet. The user can track the progress of this feature request and give it traction by referring to the provided links.\n- For batch prediction on a tabular AutoML model, the user can choose BigQuery as storage, where all the prediction output will be stored in a single table, and then export the table data to a single CSV file.",
        "Answer_gpt_summary":"solut batch predict automl model gcp process distribut process result predict output file store file cloud storag featur request rais gener batch predict output file order input file eta featur user track progress featur request traction refer provid link batch predict tabular automl model user choos bigqueri storag predict output store singl tabl export tabl data singl csv file"
    },
    {
        "Question_title":"Why does my ML model deployment in Azure Container Instance still fail with \"current service state: Transitioning\"?",
        "Question_body":"<p>I am using Azure Machine Learning Service to deploy a ML model as web service.<\/p>\n<p>I <a href=\"https:\/\/stackoverflow.com\/a\/55281703\/4240413\">registered a <code>model<\/code><\/a> and now would like to deploy it as an ACI web service as in <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#aci\" rel=\"nofollow noreferrer\">the guide<\/a>.<\/p>\n<p>To do so I define<\/p>\n<pre><code>from azureml.core.webservice import Webservice, AciWebservice\nfrom azureml.core.image import ContainerImage\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores=4, \n                      memory_gb=32, \n                      tags={&quot;data&quot;: &quot;text&quot;,  &quot;method&quot; : &quot;NB&quot;}, \n                      description='Predict something')\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>image_config = ContainerImage.image_configuration(execution_script=&quot;score.py&quot;, \n                      docker_file=&quot;Dockerfile&quot;,\n                      runtime=&quot;python&quot;, \n                      conda_file=&quot;myenv.yml&quot;)\n<\/code><\/pre>\n<p>and create an image with<\/p>\n<pre><code>image = ContainerImage.create(name = &quot;scorer-image&quot;,\n                      models = [model],\n                      image_config = image_config,\n                      workspace = ws\n                      )\n<\/code><\/pre>\n<p>Image creation succeeds with<\/p>\n<blockquote>\n<p>Creating image Image creation operation finished for image\nscorer-image:5, operation &quot;Succeeded&quot;<\/p>\n<\/blockquote>\n<p>Also, troubleshooting the image by running it locally on an Azure VM with<\/p>\n<pre><code>sudo docker run -p 8002:5001 myscorer0588419434.azurecr.io\/scorer-image:5\n<\/code><\/pre>\n<p>allows me to run (locally) queries successfully against <code>http:\/\/localhost:8002\/score<\/code>.<\/p>\n<p>However, deployment with<\/p>\n<pre><code>service_name = 'scorer-svc'\nservice = Webservice.deploy_from_image(deployment_config = aciconfig,\n                                        image = image,\n                                        name = service_name,\n                                        workspace = ws)\n<\/code><\/pre>\n<p>fails with<\/p>\n<blockquote>\n<p>Creating service<br \/>\nRunning.<br \/>\nFailedACI service creation operation finished, operation &quot;Failed&quot;<br \/>\nService creation polling reached terminal state, current service state: Transitioning<br \/>\nService creation polling reached terminal state, unexpected response received. Transitioning<\/p>\n<\/blockquote>\n<p>I tried setting in the <code>aciconfig<\/code> more generous <code>memory_gb<\/code>, but to no avail: the deployment stays in a <em>transitioning<\/em> state (like in the image below if monitored on the Azure portal):\n<a href=\"https:\/\/i.stack.imgur.com\/gCjI3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gCjI3.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Also, running <code>service.get_logs()<\/code> gives me<\/p>\n<blockquote>\n<p>WebserviceException: Received bad response from Model Management\nService: Response Code: 404<\/p>\n<\/blockquote>\n<p>What could possibly be the culprit?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1553557082573,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":1597618502196,
        "Question_score":4.0,
        "Question_view_count":3489.0,
        "Answer_body":"<p>If ACI deployment fails, one solution is trying to allocate <em>less<\/em> resources, e.g.<\/p>\n\n<pre><code>aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                  memory_gb=8, \n                  tags={\"data\": \"text\",  \"method\" : \"NB\"}, \n                  description='Predict something')\n<\/code><\/pre>\n\n<p>While the error messages thrown are not particularly informative, this is actually clearly stated in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/container-instances\/container-instances-region-availability\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>When a region is under heavy load, you may experience a failure when\n  deploying instances. To mitigate such a deployment failure, try\n  deploying instances with lower resource settings [...]<\/p>\n<\/blockquote>\n\n<p>The documentation also states which are the maximum values of the CPU\/RAM resources available in the different regions (at the time of writing, requiring a deployment with <code>memory_gb=32<\/code> would likely fail in all regions because of insufficient resources).<\/p>\n\n<p>Upon requiring less resources, deployment should succeed with <\/p>\n\n<blockquote>\n  <p>Creating service<br>\n  Running......................................................<br>\n  SucceededACI service creation operation finished, operation<br>\n  \"Succeeded\" Healthy<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1554115865523,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55347910",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1553818018048,
        "Question_original_content":"model deploy azur contain instanc fail current servic state transit servic deploi model web servic regist model like deploi aci web servic guid defin core webservic import webservic aciwebservic core imag import containerimag aciconfig aciwebservic deploi configur cpu core memori tag data text method descript predict imag config containerimag imag configur execut script score docker file dockerfil runtim python conda file myenv yml creat imag imag containerimag creat scorer imag model model imag config imag config workspac imag creation succe creat imag imag creation oper finish imag scorer imag oper succeed troubleshoot imag run local azur sudo docker run myscor azurecr scorer imag allow run local queri successfulli http localhost score deploy servic scorer svc servic webservic deploi imag deploy config aciconfig imag imag servic workspac fail creat servic run failedaci servic creation oper finish oper fail servic creation poll reach termin state current servic state transit servic creation poll reach termin state unexpect respons receiv transit tri set aciconfig gener memori avail deploy stai transit state like imag monitor azur portal run servic log give webserviceexcept receiv bad respons model manag servic respons code possibl culprit",
        "Question_preprocessed_content":"model deploy azur contain instanc fail current servic state transit servic deploi model web servic regist like deploi aci web servic guid defin creat imag imag creation succe creat imag imag creation oper finish imag oper succeed troubleshoot imag run local azur allow run queri successfulli deploy fail creat servic run failedaci servic creation oper finish oper fail servic creation poll reach termin state current servic state transit servic creation poll reach termin state unexpect respons receiv transit tri set gener avail deploy stai transit state run give webserviceexcept receiv bad respons model manag servic respons code possibl culprit",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a machine learning model as a web service using Azure Machine Learning Service. The image creation is successful, and the image can be run locally, but the deployment fails with a \"transitioning\" state. The user has tried increasing the memory_gb in the aciconfig, but the deployment still fails. Additionally, running service.get_logs() gives a WebserviceException with a response code of 404.",
        "Question_gpt_summary":"user face challeng deploi machin learn model web servic servic imag creation success imag run local deploy fail transit state user tri increas memori aciconfig deploy fail addition run servic log give webserviceexcept respons code",
        "Answer_original_content":"aci deploy fail solut try alloc resourc aciconfig aciwebservic deploi configur cpu core memori tag data text method descript predict error messag thrown particularli inform actual clearli state document region heavi load experi failur deploi instanc mitig deploy failur try deploi instanc lower resourc set document state maximum valu cpu ram resourc avail differ region time write requir deploy memori like fail region insuffici resourc requir resourc deploy succe creat servic run succeededaci servic creation oper finish oper succeed healthi",
        "Answer_preprocessed_content":"aci deploy fail solut try alloc resourc error messag thrown particularli inform actual clearli state document region heavi load experi failur deploi instanc mitig deploy failur try deploi instanc lower resourc set document state maximum valu resourc avail differ region requir resourc deploy succe creat servic succeededaci servic creation oper finish oper succeed healthi",
        "Answer_gpt_summary_original":"One solution suggested in the discussion is to allocate less resources when deploying the machine learning model as a web service using Azure Machine Learning Service. The documentation states that when a region is under heavy load, deploying instances with lower resource settings can mitigate deployment failure. The user can try deploying instances with lower resource settings, such as reducing the memory_gb value in the aciconfig. This solution has been successful in resolving the deployment failure issue.",
        "Answer_gpt_summary":"solut suggest discuss alloc resourc deploi machin learn model web servic servic document state region heavi load deploi instanc lower resourc set mitig deploy failur user try deploi instanc lower resourc set reduc memori valu aciconfig solut success resolv deploy failur issu"
    },
    {
        "Question_title":"Keep track of all the parameters of spark-submit",
        "Question_body":"<p>I have a team where many member has permission to submit Spark tasks to YARN (the resource management) by command line. It's hard to track who is using how much cores, who is using how much memory...e.g. Now I'm looking for a software, framework or something could help me monitor the parameters that each member used. It will be a bridge between client and YARN. Then I could used it to filter the submit commands.<\/p>\n\n<p>I did take a look at <a href=\"http:\/\/www.mlflow.org\" rel=\"nofollow noreferrer\">mlflow<\/a> and I really like the MLFlow Tracking but it was designed for ML training process. I wonder if there is an alternative for my purpose? Or there is any other solution for the problem.<\/p>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562750084187,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":93.0,
        "Answer_body":"<p>My recommendation would be to build such a tool yourself as its not too complicated,\nhave a wrapper script to spark submit which logs the usage in a DB and after the spark job finishes the wrapper will know to release information. could be done really easily.\nIn addition you can even block new spark submits if your team already asked for too much information.<\/p>\n\n<p>And as you build it your self its really flexible as you can even create \"sub teams\" or anything you want.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56967364",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1562766864887,
        "Question_original_content":"track paramet spark submit team member permiss submit spark task yarn resourc manag command line hard track core memori look softwar framework help monitor paramet member bridg client yarn filter submit command look like track design train process wonder altern purpos solut problem thank",
        "Question_preprocessed_content":"track paramet team member permiss submit spark task yarn command line hard track core look softwar framework help monitor paramet member bridg client yarn filter submit command look like track design train process wonder altern purpos solut problem thank",
        "Question_gpt_summary_original":"The user is facing challenges in tracking the usage of cores and memory by team members who submit Spark tasks to YARN via command line. They are looking for a software or framework that can help monitor the parameters used by each member and act as a bridge between the client and YARN. The user has explored MLFlow Tracking but is seeking alternatives or other solutions to address the problem.",
        "Question_gpt_summary":"user face challeng track usag core memori team member submit spark task yarn command line look softwar framework help monitor paramet member act bridg client yarn user explor track seek altern solut address problem",
        "Answer_original_content":"recommend build tool complic wrapper script spark submit log usag spark job finish wrapper know releas inform easili addit block new spark submit team ask inform build self flexibl creat sub team want",
        "Answer_preprocessed_content":"recommend build tool complic wrapper script spark submit log usag spark job finish wrapper know releas inform easili addit block new spark submit team ask inform build self flexibl creat sub team want",
        "Answer_gpt_summary_original":"Solution: One solution suggested in the discussion is to build a tool that can act as a bridge between the client and YARN. The tool can be built by creating a wrapper script to spark submit which logs the usage in a database and releases information after the spark job finishes. This tool can also block new spark submits if the team has already asked for too much information. Building the tool provides flexibility to create \"sub teams\" or any other customization required. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut solut suggest discuss build tool act bridg client yarn tool built creat wrapper script spark submit log usag databas releas inform spark job finish tool block new spark submit team ask inform build tool provid flexibl creat sub team custom requir solut mention discuss"
    },
    {
        "Question_title":"Can the list of custom jobs in vertex AI custom seen in the UI?",
        "Question_body":"<p>I have created a custom job with<\/p>\n<pre><code>gcloud ai custom-jobs create --region=us-west1 --display-name=test-job --config=trainjob.yaml\n<\/code><\/pre>\n<p>where <code>trainjob.yaml<\/code> is<\/p>\n<pre><code>workerPoolSpecs:\n  machineSpec:\n    machineType: n1-standard-4\n  replicaCount: 1\n  containerSpec:\n    imageUri: eu.gcr.io\/myproject\/myimage\n<\/code><\/pre>\n<p>I can see the list of the job via<\/p>\n<pre><code>gcloud ai custom-jobs list --region=us-west1\n<\/code><\/pre>\n<p>. Can this list seen in the UI? For AI Platform product there is jobs but I don't see anything like this in Vertex AI<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657900255003,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":68.0,
        "Answer_body":"<p>I don't know if it is exactly what you are looking for, but you can see the custom training jobs details using the UI at <code>Console<\/code> &gt; <code>Vertex AI<\/code> &gt; <code>Training<\/code> &gt; <code>Custom Jobs<\/code> or following the next <a href=\"https:\/\/console.cloud.google.com\/vertex-ai\/training\/custom-jobs\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1658160312116,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72996624",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1658154433088,
        "Question_original_content":"list custom job custom seen creat custom job gcloud custom job creat region west displai test job config trainjob yaml trainjob yaml workerpoolspec machinespec machinetyp standard replicacount containerspec imageuri gcr myproject myimag list job gcloud custom job list region west list seen platform product job like",
        "Question_preprocessed_content":"list custom job custom seen creat custom job list job list seen platform product job like",
        "Question_gpt_summary_original":"The user has created a custom job in Vertex AI using a YAML configuration file and can view the list of jobs using a command-line interface. However, the user is unsure if this list can be viewed in the Vertex AI user interface, as they cannot find a similar feature to the \"jobs\" section in the AI Platform product.",
        "Question_gpt_summary":"user creat custom job yaml configur file view list job command line interfac user unsur list view user interfac similar featur job section platform product",
        "Answer_original_content":"know exactli look custom train job detail consol train custom job follow link",
        "Answer_preprocessed_content":"know exactli look custom train job detail follow link",
        "Answer_gpt_summary_original":"Solution: The user can view the list of custom training jobs in the Vertex AI user interface by navigating to \"Console > Vertex AI > Training > Custom Jobs\" or by following the provided link.",
        "Answer_gpt_summary":"solut user view list custom train job user interfac navig consol train custom job follow provid link"
    },
    {
        "Question_title":"Facing Problems in mlflow deployment on windows server",
        "Question_body":"I am new to mlflow and finding its windows deployment extremely challenging. Has anyone been able to successfully deploy models with mlflow on windows machine ??",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1568860989000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":7.0,
        "Answer_body":"Maybe you can use docker container .\n\n\nOn Thu, 19 Sep 2019, 12:13 babar ali, <bac...@gmail.com> wrote:\n\nI am new to mlflow and finding its windows deployment extremely challenging. Has anyone been able to successfully deploy models with mlflow on windows machine ??\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/5e8a0cee-7833-48d3-9fed-a61c3db8c349%40googlegroups.com.. Hi Babar.\u00a0\n\n\nUnfortunately mlflow windows support is limited to experiment tracking for now (we would appreciate contributions).\n\ue5d3. Just to add to that, you may want to consider Docker on Windows as a way to run the packaged models. MLflow provides a command to package models as a Docker container already. The simple built-in model server in mlflow serve is not designed to run on Windows right now.\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/2c536152-711c-4be5-b79b-45530411ac14%40googlegroups.com.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/lovl7Ns6x0Y",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-09-20T06:19:56",
                "Answer_body":"Maybe you can use docker container .\n\n\nOn Thu, 19 Sep 2019, 12:13 babar ali, <bac...@gmail.com> wrote:\n\nI am new to mlflow and finding its windows deployment extremely challenging. Has anyone been able to successfully deploy models with mlflow on windows machine ??\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/5e8a0cee-7833-48d3-9fed-a61c3db8c349%40googlegroups.com."
            },
            {
                "Answer_creation_time":"2019-09-20T12:45:11",
                "Answer_body":"Hi Babar.\u00a0\n\n\nUnfortunately mlflow windows support is limited to experiment tracking for now (we would appreciate contributions).\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-09-22T19:27:11",
                "Answer_body":"Just to add to that, you may want to consider Docker on Windows as a way to run the packaged models. MLflow provides a command to package models as a Docker container already. The simple built-in model server in mlflow serve is not designed to run on Windows right now.\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/2c536152-711c-4be5-b79b-45530411ac14%40googlegroups.com."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"face problem deploy window server new find window deploy extrem challeng abl successfulli deploi model window machin",
        "Question_preprocessed_content":"face problem deploy window server new find window deploy extrem challeng abl successfulli deploi model window machin",
        "Question_gpt_summary_original":"The user is facing challenges in deploying mlflow on a Windows server and is seeking advice from others who have successfully deployed models with mlflow on a Windows machine.",
        "Question_gpt_summary":"user face challeng deploi window server seek advic successfulli deploi model window machin",
        "Answer_original_content":"mayb us docker contain thu sep babar ali wrote new find window deploy extrem challeng abl successfulli deploi model window machin receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user eace fed acdbc googlegroup com babar unfortun window support limit experi track appreci contribut add want consid docker window wai run packag model provid command packag model docker contain simpl built model server serv design run window right receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user googlegroup com",
        "Answer_preprocessed_content":"mayb us docker contain thu sep babar ali wrote new find window deploy extrem challeng abl successfulli deploi model window machin receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit babar unfortun window support limit experi track add want consid docker window wai run packag model provid command packag model docker contain simpl model server serv design run window right receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use Docker on Windows as a way to run the packaged models. MLflow provides a command to package models as a Docker container already. The simple built-in model server in mlflow serve is not designed to run on Windows right now.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us docker window wai run packag model provid command packag model docker contain simpl built model server serv design run window right"
    },
    {
        "Question_title":"Sagemaker doesn't inference in an async manner",
        "Question_body":"<p>I've deployed a custom model with an async endpoint. I want to process video files with it because videos can have ~5-10 minutes I can't load all frames to memory. Of course, I want to make an inference on each frame.\nI've written<br \/>\n<code>input_fn<\/code> - download video file from s3 using boto and creates generator which loads video frames with a given batch size - return a generator - written with OpenCV<br \/>\n<code>predict_fn<\/code> - iterate over generator batched frames and generate prediction using model - save prediction in list<br \/>\n<code>output_fn<\/code> - transform prediction into json format, gzip all to reduce the size<\/p>\n<p>Endpoint works well, but the problem is concurrency. The sagemaker endpoint processes request after request (from cloudwatch and s3 save file time). I don't know why this happens.\nmax_concurrent_invocations_per_instance is set to 1000. Other settings from PyTorch serving are as follows:<\/p>\n<pre><code>SAGEMAKER_MODEL_SERVER_TIMEOUT: 100000\nSAGEMAKER_TS_MAX_BATCH_DELAY: 10000\nSAGEMAKER_TS_BATCH_SIZE: 1000\nSAGEMAKER_TS_MAX_WORKERS: 4\nSAGEMAKER_TS_RESPONSE_TIMEOUT: 100000\n<\/code><\/pre>\n<p>And still, it doesn't work. So how can I create an async inference endpoint with PyTorch to get concurrency?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653569251380,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":109.0,
        "Answer_body":"<p>The concurrency settings for TorchServe DLC are controlled by such mechanisms as # of workers, which can be set by defining the appropriate variables, such as <code>SAGEMAKER_TS_*<\/code>, and <code>SAGEMAKER_MODEL_*<\/code> (see, e.g., <a href=\"https:\/\/github.com\/pytorch\/serve\/blob\/master\/docs\/configuration.md\" rel=\"nofollow noreferrer\">this page<\/a> for details on their meaning and implications).<\/p>\n<p>While the latter are agnostic to any particular serving stack and are defined in the <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\" rel=\"nofollow noreferrer\">SageMaker Inference Toolkit<\/a>, the former are TorchServe-specific and are defined in <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\" rel=\"nofollow noreferrer\">TorchServe Inference Toolkit<\/a>. Moreover, since the TorchServe Inference Toolkit is built on top of the SageMaker Inference Toolkit, there is a non-trivial interplay between these two sets of params.<\/p>\n<p>Thus you may also want to experiment with such params as, e.g., <code>SAGEMAKER_MODEL_SERVER_WORKERS<\/code> to properly set up the concurrency setting of the SageMaker Async Endpoint.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72392070",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1653766025390,
        "Question_original_content":"infer async manner deploi custom model async endpoint want process video file video minut load frame memori cours want infer frame written input download video file boto creat gener load video frame given batch size return gener written opencv predict iter gener batch frame gener predict model save predict list output transform predict json format gzip reduc size endpoint work problem concurr endpoint process request request cloudwatch save file time know happen max concurr invoc instanc set set pytorch serv follow model server timeout max batch delai batch size max worker respons timeout work creat async infer endpoint pytorch concurr",
        "Question_preprocessed_content":"infer async manner deploi custom model async endpoint want process video file video minut load frame memori cours want infer frame written download video file boto creat gener load video frame given batch size return gener written opencv iter gener batch frame gener predict model save predict list transform predict json format gzip reduc size endpoint work problem concurr endpoint process request request know happen set set pytorch serv follow work creat async infer endpoint pytorch concurr",
        "Question_gpt_summary_original":"The user has deployed a custom model with an async endpoint on Sagemaker to process video files with multiple frames. However, the Sagemaker endpoint processes requests one after the other, causing concurrency issues. The user has tried adjusting the settings, but the problem persists. The user is seeking a solution to create an async inference endpoint with PyTorch to achieve concurrency.",
        "Question_gpt_summary":"user deploi custom model async endpoint process video file multipl frame endpoint process request caus concurr issu user tri adjust set problem persist user seek solut creat async infer endpoint pytorch achiev concurr",
        "Answer_original_content":"concurr set torchserv dlc control mechan worker set defin appropri variabl model page detail mean implic agnost particular serv stack defin infer toolkit torchserv specif defin torchserv infer toolkit torchserv infer toolkit built infer toolkit non trivial interplai set param want experi param model server worker properli set concurr set async endpoint",
        "Answer_preprocessed_content":"concurr set torchserv dlc control mechan worker set defin appropri variabl agnost particular serv stack defin infer toolkit defin torchserv infer toolkit torchserv infer toolkit built infer toolkit interplai set param want experi param properli set concurr set async endpoint",
        "Answer_gpt_summary_original":"Solution: The user can experiment with adjusting the concurrency settings for TorchServe DLC by defining appropriate variables such as <code>SAGEMAKER_TS_*<\/code> and <code>SAGEMAKER_MODEL_*<\/code>. Additionally, the user can experiment with adjusting the <code>SAGEMAKER_MODEL_SERVER_WORKERS<\/code> parameter to properly set up the concurrency setting of the SageMaker Async Endpoint.",
        "Answer_gpt_summary":"solut user experi adjust concurr set torchserv dlc defin appropri variabl model addition user experi adjust model server worker paramet properli set concurr set async endpoint"
    },
    {
        "Question_title":"How to custom code an inference pipeline in AWS sagemaker?",
        "Question_body":"<p>I am building a time series usecase to automate the preprocess and retrain tasks.At first the data is preprocessed using numpy, pandas, statsmodels etc &amp; later a machine learning algorithm is applied to make predictions.\nThe reason for using inference pipeline is that it reuses the same preprocess code for training and inference. I have checked the examples given by AWS sagemaker team with spark and sci-kit learn. In both the examples they use a sci-kit learn container to fit &amp; transform their preprocess code. Should I also have to create a container which is not needed in my use case as I am not using any sci-kit-learn code? <\/p>\n\n<p>Can someone give me a custom example of using these pipelines? Any help is appreciated!<\/p>\n\n<p><strong>Sources looked into:<\/strong><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline<\/a>\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1574408498230,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1186.0,
        "Answer_body":"<p>Apologies for the late response.<\/p>\n\n<p>Below is some documentation on inference pipelines:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html<\/a>\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-real-time.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-real-time.html<\/a><\/p>\n\n<blockquote>\n  <p>Should I also have to create a container which is not needed in my use case as I am not using any sci-kit-learn code?<\/p>\n<\/blockquote>\n\n<p>Your container is an encapsulation of the environment needed for your custom code needed to run properly. Based on the requirements listed above, <code>numpy, pandas, statsmodels etc &amp; later a machine learning algorithm<\/code>, I would create a container if you wish to isolate your dependencies or modify an existing predefined SageMaker container, such as the scikit-learn one, and add your dependencies into that.<\/p>\n\n<blockquote>\n  <p>Can someone give me a custom example of using these pipelines? Any help is appreciated!<\/p>\n<\/blockquote>\n\n<p>Unfortunately, the two example notebooks referenced above are the only examples utilizing inference pipelines. The biggest hurdle most likely is creating containers that fulfill the preprocessing and prediction task you are seeking and then combining those two together into the inference pipeline.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1579974118672,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58989610",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1575505485728,
        "Question_original_content":"custom code infer pipelin build time seri usecas autom preprocess retrain task data preprocess numpi panda statsmodel later machin learn algorithm appli predict reason infer pipelin reus preprocess code train infer check exampl given team spark sci kit learn exampl us sci kit learn contain fit transform preprocess code creat contain need us case sci kit learn code custom exampl pipelin help appreci sourc look http github com awslab amazon exampl tree master python sdk scikit learn infer pipelin http github com awslab amazon exampl tree master advanc function infer pipelin sparkml blazingtext dbpedia",
        "Question_preprocessed_content":"custom code infer pipelin build time seri usecas autom preprocess retrain data preprocess numpi panda statsmodel later machin learn algorithm appli predict reason infer pipelin reus preprocess code train infer check exampl given team spark learn exampl us learn contain fit transform preprocess code creat contain need us case code custom exampl pipelin help appreci sourc look",
        "Question_gpt_summary_original":"The user is facing a challenge in custom coding an inference pipeline in AWS Sagemaker for a time series use case. They have checked examples provided by AWS Sagemaker team but are unsure if they need to create a container as they are not using any sci-kit-learn code. The user is seeking a custom example of using these pipelines.",
        "Question_gpt_summary":"user face challeng custom code infer pipelin time seri us case check exampl provid team unsur need creat contain sci kit learn code user seek custom exampl pipelin",
        "Answer_original_content":"apolog late respons document infer pipelin http doc aw amazon com latest infer pipelin html http doc aw amazon com latest infer pipelin real time html creat contain need us case sci kit learn code contain encapsul environ need custom code need run properli base requir list numpi panda statsmodel later machin learn algorithm creat contain wish isol depend modifi exist predefin contain scikit learn add depend custom exampl pipelin help appreci unfortun exampl notebook referenc exampl util infer pipelin biggest hurdl like creat contain fulfil preprocess predict task seek combin infer pipelin",
        "Answer_preprocessed_content":"apolog late respons document infer pipelin creat contain need us case code contain encapsul environ need custom code need run properli base requir list creat contain wish isol depend modifi exist predefin contain add depend custom exampl pipelin help appreci unfortun exampl notebook referenc exampl util infer pipelin biggest hurdl like creat contain fulfil preprocess predict task seek combin infer pipelin",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- The user can refer to the AWS Sagemaker documentation on inference pipelines to understand how to create them for their time series use case.\n- The user may need to create a container to encapsulate the environment needed for their custom code to run properly, even if they are not using any sci-kit-learn code.\n- The user can modify an existing predefined SageMaker container, such as the scikit-learn one, and add their dependencies into that.\n- Unfortunately, no custom example of using these pipelines is provided in the discussion.",
        "Answer_gpt_summary":"possibl solut mention discuss user refer document infer pipelin understand creat time seri us case user need creat contain encapsul environ need custom code run properli sci kit learn code user modifi exist predefin contain scikit learn add depend unfortun custom exampl pipelin provid discuss"
    },
    {
        "Question_title":"Cannot execute AWS Sagemaker Notebook",
        "Question_body":"<p>I cannot execute sagemaker notebook anymore.<br>\nThe following error occurs.<\/p>\n\n<pre><code>Failed to start kernel\nAn error occurred (ThrottlingException) when calling the CreateApp operation (reached max retries: 4): \nRate exceeded\n<\/code><\/pre>\n\n<p>I checked my app list and there are only two.\nOne app is trying to delete but never stops, this could be one of the problem.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/M0iqo.png\" rel=\"nofollow noreferrer\">image<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590373205720,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":965.0,
        "Answer_body":"<p>Happened to me too. Contact support and ask them to delete the kernel behind the scenes.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61994821",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1590563891208,
        "Question_original_content":"execut notebook execut notebook anymor follow error occur fail start kernel error occur throttlingexcept call createapp oper reach max retri rate exceed check app list app try delet stop problem imag",
        "Question_preprocessed_content":"execut notebook execut notebook anymor follow error occur check app list app try delet stop problem imag",
        "Question_gpt_summary_original":"The user is facing challenges in executing AWS Sagemaker Notebook due to a ThrottlingException error that occurs when calling the CreateApp operation. The user suspects that the issue could be related to an app that is trying to delete but never stops.",
        "Question_gpt_summary":"user face challeng execut notebook throttlingexcept error occur call createapp oper user suspect issu relat app try delet stop",
        "Answer_original_content":"happen contact support ask delet kernel scene",
        "Answer_preprocessed_content":"happen contact support ask delet kernel scene",
        "Answer_gpt_summary_original":"Solution: Contact AWS support and request them to delete the kernel behind the scenes.",
        "Answer_gpt_summary":"solut contact aw support request delet kernel scene"
    },
    {
        "Question_title":"\"The timestamp column must have valid timestamp entries.\" error when using `timestamp_split_column_name` arg in `AutoMLTabularTrainingJob.run`",
        "Question_body":"<p>From <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html#google.cloud.aiplatform.AutoMLTabularTrainingJob.run\" rel=\"nofollow noreferrer\">the docs<\/a> it says that<\/p>\n<blockquote>\n<p>The value of the key values of the key (the values in the column) must be in RFC 3339 date-time format, where time-offset = \u201cZ\u201d (e.g. 1985-04-12T23:20:50.52Z)<\/p>\n<\/blockquote>\n<p>The dataset that I'm pointing to is a CSV in cloud storage, where the data is in the format suggested by the docs:<\/p>\n<pre><code>$ gsutil cat gs:\/\/my-data.csv | head | xsv select TS_SPLIT_COL\nTS_SPLIT_COL\n2021-01-18T00:00:00.00Z\n2021-01-18T00:00:00.00Z\n2021-01-04T00:00:00.00Z\n2021-03-06T00:00:00.00Z\n2021-01-15T00:00:00.00Z\n2021-02-11T00:00:00.00Z\n2021-02-05T00:00:00.00Z\n2021-05-20T00:00:00.00Z\n2021-01-05T00:00:00.00Z\n<\/code><\/pre>\n<p>But I receive a <code>Training pipeline failed with error message: The timestamp column must have valid timestamp entries.<\/code> error when I try to run a training job<\/p>\n<p>EDIT: this should hopefully make it more reproducible<\/p>\n<p>data: <a href=\"https:\/\/pastebin.com\/qEDqvzX6\" rel=\"nofollow noreferrer\">https:\/\/pastebin.com\/qEDqvzX6<\/a><\/p>\n<p>Code I'm running:<\/p>\n<pre><code>from google.cloud import aiplatform\n\nPROJECT = &quot;my-project&quot;\nDATASET_ID = &quot;dataset-id&quot;  # points to CSV \n\naiplatform.init(project=PROJECT)\n\ndataset = aiplatform.TabularDataset(DATASET_ID)\n\njob = aiplatform.AutoMLTabularTrainingJob(\n    display_name=&quot;so-58454722&quot;,\n    optimization_prediction_type=&quot;classification&quot;,\n    optimization_objective=&quot;maximize-au-roc&quot;,\n)\n\nmodel = job.run(\n    dataset=dataset,\n    model_display_name=&quot;so-58454722&quot;,\n    target_column=&quot;Y&quot;,\n    training_fraction_split=0.8,\n    validation_fraction_split=0.1,\n    test_fraction_split=0.1,\n    timestamp_split_column_name=&quot;TS_SPLIT_COL&quot;,\n)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1647473614997,
        "Question_favorite_count":null,
        "Question_last_edit_time":1647534964220,
        "Question_score":0.0,
        "Question_view_count":157.0,
        "Answer_body":"<p>Try this timestamp format instead:<\/p>\n<p><code>2022-03-18T01:23:45.123456+00:00<\/code><\/p>\n<p>It uses <code>+00:00<\/code> instead of <code>Z<\/code> to specify timezone.<\/p>\n<p>This change will eliminate the &quot;The timestamp column must have valid timestamp entries.&quot; error<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71505415",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1647602028112,
        "Question_original_content":"timestamp column valid timestamp entri error timestamp split column arg automltabulartrainingjob run doc sai valu kei valu kei valu column rfc date time format time offset dataset point csv cloud storag data format suggest doc gsutil cat data csv head xsv select split col split col receiv train pipelin fail error messag timestamp column valid timestamp entri error try run train job edit hopefulli reproduc data http pastebin com qedqvzx code run googl cloud import aiplatform project project dataset dataset point csv aiplatform init project project dataset aiplatform tabulardataset dataset job aiplatform automltabulartrainingjob displai optim predict type classif optim object maxim roc model job run dataset dataset model displai target column train fraction split valid fraction split test fraction split timestamp split column split col",
        "Question_preprocessed_content":"timestamp column valid timestamp error arg doc sai valu kei valu kei rfc format dataset point csv cloud storag data format suggest doc receiv error try run train job edit hopefulli reproduc data code run",
        "Question_gpt_summary_original":"The user is encountering an error when running a training job using `AutoMLTabularTrainingJob.run` with the `timestamp_split_column_name` argument. The error message states that the timestamp column must have valid timestamp entries in RFC 3339 date-time format with time-offset = \u201cZ\u201d. The user's dataset is in the correct format, but they are still receiving the error.",
        "Question_gpt_summary":"user encount error run train job automltabulartrainingjob run timestamp split column argument error messag state timestamp column valid timestamp entri rfc date time format time offset user dataset correct format receiv error",
        "Answer_original_content":"try timestamp format instead us instead specifi timezon chang elimin timestamp column valid timestamp entri error",
        "Answer_preprocessed_content":"try timestamp format instead us instead specifi timezon chang elimin timestamp column valid timestamp error",
        "Answer_gpt_summary_original":"Solution: The discussion suggests using a different timestamp format that includes the timezone offset in the format of `2022-03-18T01:23:45.123456+00:00` instead of `Z`. This solution is expected to eliminate the error message and allow the user to run the training job successfully.",
        "Answer_gpt_summary":"solut discuss suggest differ timestamp format includ timezon offset format instead solut expect elimin error messag allow user run train job successfulli"
    },
    {
        "Question_title":"How do you see ALL predictors by influence not just the top predictors of AutoML training reports?",
        "Question_body":"The \"top predictors by influence\" in the training reports of AutoML regression models is very useful (see reference image), but I'm looking for a way to display all of the predictors, not just the top 10. Any way I can visualise this either in the training report or using the data tables themselves would be very useful.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635159990897,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, PowerBi is not currently supported here on Q&A. Please post your question on the PowerBI community forum for faster response. Thanks.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/602784\/how-do-you-see-all-predictors-by-influence-not-jus.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-26T02:26:26.563Z",
                "Answer_score":0,
                "Answer_body":"Hi, PowerBi is not currently supported here on Q&A. Please post your question on the PowerBI community forum for faster response. Thanks.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1635215186563,
        "Question_original_content":"predictor influenc predictor automl train report predictor influenc train report automl regress model us refer imag look wai displai predictor wai visualis train report data tabl us",
        "Question_preprocessed_content":"predictor influenc predictor automl train report predictor influenc train report automl regress model us look wai displai predictor wai visualis train report data tabl us",
        "Question_gpt_summary_original":"The user is facing a challenge with the AutoML regression model's training reports, which only display the top 10 predictors by influence. The user is looking for a way to visualize all predictors, and is seeking a solution either within the training report or using the data tables themselves.",
        "Question_gpt_summary":"user face challeng automl regress model train report displai predictor influenc user look wai visual predictor seek solut train report data tabl",
        "Answer_original_content":"powerbi current support post question powerbi commun forum faster respons thank kindli accept answer inform help thank",
        "Answer_preprocessed_content":"powerbi current support post question powerbi commun forum faster respons thank kindli accept answer inform help thank",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Merging of files in DVC-tracked directories",
        "Question_body":"<p>I just tested DVC with some dummy data and have run into a situation which would be less than ideal for production. Is there an elegant way out?<\/p>\n<h1>Situation<\/h1>\n<ul>\n<li>The workspace has only one DVC-tracked directory named \u201cdata\/\u201d<\/li>\n<li>There are two git branches. In both of them, we added\/removed separate files to \u201cdata\/\u201d<\/li>\n<li>Now, we want to merge both branches<\/li>\n<\/ul>\n<h1>Expected outcome<\/h1>\n<ul>\n<li>Unless there are conflicts, a simple git merge yields the union of the file operations in both branches<\/li>\n<\/ul>\n<h1>Actual outcome<\/h1>\n<ul>\n<li>A git conflict in data.dcv. I can\u2019t really merge, but only pick the data version in one of the branches<\/li>\n<\/ul>\n<p>Given that the command \u201cdvc diff\u201d shows some very useful output, is there a way to merge both data versions semi-automatic? I have read the page <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/merge-conflicts\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/how-to\/merge-conflicts<\/a>, but this only mentions the \u201cappend-only\u201d strategy, not even mentioning dvc diff :(.<\/p>\n<p>P.s.: As a side question: Can \u201cdvc diff\u201d detect and highlight renamed files (since they have the same hash value)?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1608643957319,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":942.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/gebbissimo\">@gebbissimo<\/a> !<\/p>\n<p>As I understand those two datasets have modified and deleted data, not only new unique additions?<\/p>\n<p>As noted in <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/merge-conflicts\">https:\/\/dvc.org\/doc\/user-guide\/how-to\/merge-conflicts<\/a> we have a merge-driver that is able to automatically merge append-only datasets, but currently lacks functionality for a more involed conflict resolution <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_frowning_face.png?v=9\" title=\":slightly_frowning_face:\" class=\"emoji\" alt=\":slightly_frowning_face:\"> We do plan on adding support for it in the future, so it would be great to figure out the scenario you have and functionality you expect from it.<\/p>. <p>\/\/EDIT: Ah, I believe the issue is that it\u2019s impossible to distinguish the following two situations without keeping track of the file operations history, or?<\/p>\n<p>Situation 1<br>\ncommon_branch: A-AB<br>\nbranch1: ABC (add C)<br>\nbranch2: A (delete B)<br>\nexpected merge: AC<\/p>\n<p>Situation 2<br>\ncommon_branch: A<br>\nbranch1: AB-ABC (add B,C)<br>\nbranch2: AD-A (add D, then delete D)<br>\nexpected merge: ABC<\/p>\n<p>Thus, in such a case, one would need to decide for each file in DVC DIFF separately, whether to add or delete it. Hmm\u2026 I wonder how git LFS deals with such scenarios?<\/p>\n<p>\/\/EDIT2: Would it be possible to detect the difference of each branch w.r.t. the common git ancestor node and then get the union of those two differences? At least in the example above it should work, or?<\/p>\n<hr>\n<p>P.s.: The other thing is that file renaming currently shows up in DVC DIFF &lt;another_git_branch&gt; as:<\/p>\n<p>Added: (new filename after renaming)<br>\nDeleted: (old filename)<\/p>\n<p>Would it be possible to group this information into a new category Renamed? Would make the overview more helpful in my view.<\/p>\n<p>Thanks again!<\/p>. <p>Got it. So clearly those kinds of scenarios would be hard to do automatically. And we need to provide a convenient way to resolve conflicts in your datasets by hand. We will be introducing it in the future.<\/p>\n<blockquote>\n<p>I wonder how git LFS deals with such scenarios?<\/p>\n<\/blockquote>\n<p>It doesn\u2019t store datasets as a whole, rather tracking each individual file, so the conflicts with added\/deleted files are resolved the same way regular git does. With modified files though, it will prompt you into an editor with a old and new hash of that file. We hope to be able to provide something more handy, especially considering that in our scenarios datasets might consist of many millions of files.<\/p>\n<aside class=\"quote no-group\" data-username=\"gebbissimo\" data-post=\"3\" data-topic=\"599\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/g\/e0b2c6\/40.png\" class=\"avatar\"> gebbissimo:<\/div>\n<blockquote>\n<p>\/\/EDIT2: Would it be possible to detect the difference of each branch w.r.t. the common git ancestor node and then get the union of those two differences? At least in the example above it should work, or?<\/p>\n<\/blockquote>\n<\/aside>\n<p>Yep, there will be some common strategies to choose from for our merge-driver. Right now it only supports unions of append-only dirs, which might get adjusted to be smarter  in the scenarios you\u2019ve described. The current implementation of merge-driver is more of a POC that we\u2019ve created to cover a basic scenario and were waiting for users like you to request additional features <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<aside class=\"quote no-group\" data-username=\"gebbissimo\" data-post=\"3\" data-topic=\"599\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/g\/e0b2c6\/40.png\" class=\"avatar\"> gebbissimo:<\/div>\n<blockquote>\n<p>Would it be possible to group this information into a new category Renamed? Would make the overview more helpful in my view.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Great idea! Mind creating a feature request on <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\">https:\/\/github.com\/iterative\/dvc\/issues<\/a> , please?<\/p>. <p>Great answer, thanks. Opened two new feature requests. Can probably close this thread now.<\/p>\n<p>for the \u201cRenamed\u201d group: <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/5150\" rel=\"noopener nofollow ugc\">https:\/\/github.com\/iterative\/dvc\/issues\/5150<\/a><\/p>\n<p>for the merging strategy based on the union of the differences: <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/5151\" rel=\"noopener nofollow ugc\">https:\/\/github.com\/iterative\/dvc\/issues\/5151<\/a><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/merging-of-files-in-dvc-tracked-directories\/599",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-22T18:56:44.088Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/gebbissimo\">@gebbissimo<\/a> !<\/p>\n<p>As I understand those two datasets have modified and deleted data, not only new unique additions?<\/p>\n<p>As noted in <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/merge-conflicts\">https:\/\/dvc.org\/doc\/user-guide\/how-to\/merge-conflicts<\/a> we have a merge-driver that is able to automatically merge append-only datasets, but currently lacks functionality for a more involed conflict resolution <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_frowning_face.png?v=9\" title=\":slightly_frowning_face:\" class=\"emoji\" alt=\":slightly_frowning_face:\"> We do plan on adding support for it in the future, so it would be great to figure out the scenario you have and functionality you expect from it.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-22T20:47:05.039Z",
                "Answer_body":"<p>\/\/EDIT: Ah, I believe the issue is that it\u2019s impossible to distinguish the following two situations without keeping track of the file operations history, or?<\/p>\n<p>Situation 1<br>\ncommon_branch: A-AB<br>\nbranch1: ABC (add C)<br>\nbranch2: A (delete B)<br>\nexpected merge: AC<\/p>\n<p>Situation 2<br>\ncommon_branch: A<br>\nbranch1: AB-ABC (add B,C)<br>\nbranch2: AD-A (add D, then delete D)<br>\nexpected merge: ABC<\/p>\n<p>Thus, in such a case, one would need to decide for each file in DVC DIFF separately, whether to add or delete it. Hmm\u2026 I wonder how git LFS deals with such scenarios?<\/p>\n<p>\/\/EDIT2: Would it be possible to detect the difference of each branch w.r.t. the common git ancestor node and then get the union of those two differences? At least in the example above it should work, or?<\/p>\n<hr>\n<p>P.s.: The other thing is that file renaming currently shows up in DVC DIFF &lt;another_git_branch&gt; as:<\/p>\n<p>Added: (new filename after renaming)<br>\nDeleted: (old filename)<\/p>\n<p>Would it be possible to group this information into a new category Renamed? Would make the overview more helpful in my view.<\/p>\n<p>Thanks again!<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-22T21:33:30.388Z",
                "Answer_body":"<p>Got it. So clearly those kinds of scenarios would be hard to do automatically. And we need to provide a convenient way to resolve conflicts in your datasets by hand. We will be introducing it in the future.<\/p>\n<blockquote>\n<p>I wonder how git LFS deals with such scenarios?<\/p>\n<\/blockquote>\n<p>It doesn\u2019t store datasets as a whole, rather tracking each individual file, so the conflicts with added\/deleted files are resolved the same way regular git does. With modified files though, it will prompt you into an editor with a old and new hash of that file. We hope to be able to provide something more handy, especially considering that in our scenarios datasets might consist of many millions of files.<\/p>\n<aside class=\"quote no-group\" data-username=\"gebbissimo\" data-post=\"3\" data-topic=\"599\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/g\/e0b2c6\/40.png\" class=\"avatar\"> gebbissimo:<\/div>\n<blockquote>\n<p>\/\/EDIT2: Would it be possible to detect the difference of each branch w.r.t. the common git ancestor node and then get the union of those two differences? At least in the example above it should work, or?<\/p>\n<\/blockquote>\n<\/aside>\n<p>Yep, there will be some common strategies to choose from for our merge-driver. Right now it only supports unions of append-only dirs, which might get adjusted to be smarter  in the scenarios you\u2019ve described. The current implementation of merge-driver is more of a POC that we\u2019ve created to cover a basic scenario and were waiting for users like you to request additional features <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<aside class=\"quote no-group\" data-username=\"gebbissimo\" data-post=\"3\" data-topic=\"599\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/g\/e0b2c6\/40.png\" class=\"avatar\"> gebbissimo:<\/div>\n<blockquote>\n<p>Would it be possible to group this information into a new category Renamed? Would make the overview more helpful in my view.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Great idea! Mind creating a feature request on <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\">https:\/\/github.com\/iterative\/dvc\/issues<\/a> , please?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-22T22:00:00.977Z",
                "Answer_body":"<p>Great answer, thanks. Opened two new feature requests. Can probably close this thread now.<\/p>\n<p>for the \u201cRenamed\u201d group: <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/5150\" rel=\"noopener nofollow ugc\">https:\/\/github.com\/iterative\/dvc\/issues\/5150<\/a><\/p>\n<p>for the merging strategy based on the union of the differences: <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/5151\" rel=\"noopener nofollow ugc\">https:\/\/github.com\/iterative\/dvc\/issues\/5151<\/a><\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"merg file track directori test dummi data run situat ideal product eleg wai situat workspac track directori name data git branch ad remov separ file data want merg branch expect outcom conflict simpl git merg yield union file oper branch actual outcom git conflict data dcv merg pick data version branch given command diff show us output wai merg data version semi automat read page http org doc user guid merg conflict mention append strategi mention diff question diff detect highlight renam file hash valu",
        "Question_preprocessed_content":"merg file track directori test dummi data run situat ideal product eleg wai situat workspac track directori name data git branch separ file data want merg branch expect outcom conflict simpl git merg yield union file oper branch actual outcom git conflict merg pick data version branch given command diff show us output wai merg data version read page mention strategi mention diff question diff detect highlight renam file",
        "Question_gpt_summary_original":"The user encountered a challenge while testing DVC with dummy data. The workspace has only one DVC-tracked directory named \"data\/\" and there are two git branches. In both branches, separate files were added\/removed to \"data\/\". The user wants to merge both branches, but a git conflict in data.dcv occurred, and the user can only pick the data version in one of the branches. The user is looking for a way to merge both data versions semi-automatically using \"dvc diff,\" but the documentation only mentions the \"append-only\" strategy. The user also has a side question about whether \"dvc diff\" can detect and highlight renamed files.",
        "Question_gpt_summary":"user encount challeng test dummi data workspac track directori name data git branch branch separ file ad remov data user want merg branch git conflict data dcv occur user pick data version branch user look wai merg data version semi automat diff document mention append strategi user question diff detect highlight renam file",
        "Answer_original_content":"gebbissimo understand dataset modifi delet data new uniqu addit note http org doc user guid merg conflict merg driver abl automat merg append dataset current lack function invol conflict resolut plan ad support futur great figur scenario function expect edit believ issu imposs distinguish follow situat keep track file oper histori situat common branch branch abc add branch delet expect merg situat common branch branch abc add branch add delet expect merg abc case need decid file diff separ add delet hmm wonder git lf deal scenario edit possibl detect differ branch common git ancestor node union differ exampl work thing file renam current show diff ad new filenam renam delet old filenam possibl group inform new categori renam overview help view thank got clearli kind scenario hard automat need provid conveni wai resolv conflict dataset hand introduc futur wonder git lf deal scenario doesnt store dataset track individu file conflict ad delet file resolv wai regular git modifi file prompt editor old new hash file hope abl provid handi especi consid scenario dataset consist million file gebbissimo edit possibl detect differ branch common git ancestor node union differ exampl work yep common strategi choos merg driver right support union append dir adjust smarter scenario youv describ current implement merg driver poc weve creat cover basic scenario wait user like request addit featur gebbissimo possibl group inform new categori renam overview help view great idea mind creat featur request http github com iter issu great answer thank open new featur request probabl close thread renam group http github com iter issu merg strategi base union differ http github com iter issu",
        "Answer_preprocessed_content":"understand dataset modifi delet data new uniqu addit note abl automat merg dataset current lack function invol conflict resolut plan ad support futur great figur scenario function expect believ issu imposs distinguish follow situat keep track file oper histori situat branch abc branch expect merg situat branch branch expect merg abc case need decid file diff separ add delet hmm wonder git lf deal scenario possibl detect differ branch common git ancestor node union differ exampl work thing file renam current show diff ad delet possibl group inform new categori renam overview help view thank got clearli kind scenario hard automat need provid conveni wai resolv conflict dataset hand introduc futur wonder git lf deal scenario doesnt store dataset track individu file conflict file resolv wai regular git modifi file prompt editor old new hash file hope abl provid handi especi consid scenario dataset consist million file gebbissimo possibl detect differ branch common git ancestor node union differ exampl work yep common strategi choos right support union dir adjust smarter scenario youv describ current implement poc weve creat cover basic scenario wait user like request addit featur gebbissimo possibl group inform new categori renam overview help view great idea mind creat featur request great answer thank open new featur request probabl close thread renam group merg strategi base union differ",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Manually deleted .dvc files -- these files still appear to be tracked by DVC",
        "Question_body":"<p>Hello,<\/p>\n<p>I have manually deleted .dvc files that were previously tracked (and the tracked files themselves), expecting this to remove them from tracking (I realise now I should have used dvc gc).<\/p>\n<p>My question is how do I now remove these from tracking? I cannot re-add then remove them properly as the original files are not there.<\/p>\n<p>I am syncing data in two machines, on the machine I deleted the files on it says everything is up to date but on my other machine I am getting cache errors for the now deleted items. I am confused as I have pushed the deletes of the .dvc files so I assumed this would prevent them being tracked on both machines. Are they tracked somewhere else? I do not have a dvc.yaml file.<\/p>\n<p>Thanks,<\/p>\n<p>Justin<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611306516441,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":210.0,
        "Answer_body":"<p>To answer this here as well, deleting the .dvc files and then committing the git changes will work for files which were tracked with <code>dvc add<\/code>. (But prior versions of the files will still be \u201ctracked\u201d in your older git history.)<\/p>\n<p>One other thing to note is that manually removing the .dvc files will still leave entries in your .gitignore that should also be removed. <code>dvc remove &lt;path_to_.dvc_file&gt;<\/code> can be used to both remove the file and cleanup the .gitignore entry.<\/p>\n<p>Regarding the cache errors, this was clarified in a separate email chain to be an unrelated issue - the errors were due to files\/directories which had not yet been completely <code>dvc push<\/code>ed from a coworker\u2019s machine to a remote.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/manually-deleted-dvc-files-these-files-still-appear-to-be-tracked-by-dvc\/634",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-22T12:06:01.345Z",
                "Answer_body":"<p>To answer this here as well, deleting the .dvc files and then committing the git changes will work for files which were tracked with <code>dvc add<\/code>. (But prior versions of the files will still be \u201ctracked\u201d in your older git history.)<\/p>\n<p>One other thing to note is that manually removing the .dvc files will still leave entries in your .gitignore that should also be removed. <code>dvc remove &lt;path_to_.dvc_file&gt;<\/code> can be used to both remove the file and cleanup the .gitignore entry.<\/p>\n<p>Regarding the cache errors, this was clarified in a separate email chain to be an unrelated issue - the errors were due to files\/directories which had not yet been completely <code>dvc push<\/code>ed from a coworker\u2019s machine to a remote.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"manual delet file file appear track hello manual delet file previous track track file expect remov track realis question remov track add remov properli origin file sync data machin machin delet file sai date machin get cach error delet item confus push delet file assum prevent track machin track yaml file thank justin",
        "Question_preprocessed_content":"manual delet file file appear track hello manual delet file previous track expect remov track question remov track remov properli origin file sync data machin machin delet file sai date machin get cach error delet item confus push delet file assum prevent track machin track yaml file thank justin",
        "Question_gpt_summary_original":"The user manually deleted .dvc files that were previously tracked, but they still appear to be tracked by DVC. The user is unsure how to remove them from tracking and is experiencing cache errors on another machine where the files were not deleted. The user has pushed the deletes of the .dvc files but is confused as to why they are still being tracked.",
        "Question_gpt_summary":"user manual delet file previous track appear track user unsur remov track experienc cach error machin file delet user push delet file confus track",
        "Answer_original_content":"answer delet file commit git chang work file track add prior version file track older git histori thing note manual remov file leav entri gitignor remov remov remov file cleanup gitignor entri cach error clarifi separ email chain unrel issu error file directori complet push cowork machin remot",
        "Answer_preprocessed_content":"answer delet file commit git chang work file track thing note manual remov file leav entri gitignor remov remov file cleanup gitignor entri cach error clarifi separ email chain unrel issu error complet cowork machin remot",
        "Answer_gpt_summary_original":"Solutions provided:\n- Manually deleting the .dvc files and then committing the git changes will work for files which were tracked with dvc add. However, prior versions of the files will still be \u201ctracked\u201d in your older git history.\n- Use \"dvc remove <path_to_.dvc_file>\" to both remove the file and cleanup the .gitignore entry.\n- The cache errors were clarified in a separate email chain to be an unrelated issue - the errors were due to files\/directories which had not yet been completely dvc pushed from a coworker\u2019s machine to a remote.\n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"solut provid manual delet file commit git chang work file track add prior version file track older git histori us remov remov file cleanup gitignor entri cach error clarifi separ email chain unrel issu error file directori complet push cowork machin remot person opinion bias includ respons"
    },
    {
        "Question_title":"1T dataset with distributed deep learning",
        "Question_body":"<p>Hello,<\/p>\n<p>I\u2019m currently evaluating DVC for a very large scale Deep Learning project.<\/p>\n<p>We have an ever growing dataset of +1M video files totaling to more than a Tb.<br>\nFrom this we do:<\/p>\n<ol>\n<li>preprocessing of each video file<\/li>\n<li>sub sample a subset of the original dataset, which will become the new training\/val\/test dataset. This dataset is currently at +400k video files totaling to 800G<\/li>\n<li>Train a distributed deep learning video classification model on aws with pytorch distributed<\/li>\n<\/ol>\n<p>I have already used DVC extensively but only in a more local-first approach where I could run the whole pipeline on my machine and would just run the same pipeline on more powerful servers.<\/p>\n<p>In my new use-case, I would like to scale the whole process on aws where all the data is on S3 buckets and steps of the pipeline is parallelized on light aws lambdas as much as possible.<br>\nI would also like to be able to launch the whole pipeline from my local machine (which would run all the processing on aws) but also on a CI job.<\/p>\n<p>Has anyone ever had such a challenging setup ?<br>\nI feel like it\u2019s the distributed parallel execution features that would be the most challenging to put in place with DVC.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1666885670842,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":107.0,
        "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/kwon-young_veesion\">@kwon-young_veesion<\/a>!<br>\nThis is indeed an advanced use case. And while I am unable to find an example that would cover this exact use case I can try to provide you with some information where to start looking.<\/p>\n<ol>\n<li>Data versioning:<br>\nI believe you could use two approaches here:<br>\na. using <code>external dependencies\/outputs<\/code> -  <a href=\"https:\/\/dvc.org\/doc\/user-guide\/data-management\/managing-external-data\">https:\/\/dvc.org\/doc\/user-guide\/data-management\/managing-external-data<\/a> - the problem with external outputs is that when you do <code>dvc checkout<\/code> you override existing remote file, so parallelization is problematic as it would need some kind of synchronization allowing to make sure that your lambda job has the proper data before starting an experiment that is using different version of the data.<br>\nb. using \u201cnormal\u201d repo and <code>dvc add --to-remote<\/code> to version it, and on lambda utilize <a href=\"https:\/\/dvc.org\/doc\/api-reference\/get_url\" class=\"inline-onebox\">get_url()<\/a> to obtain the data on lambda and process it the way you want.<\/li>\n<\/ol>\n<p>Regarding the parallelization:<br>\nI am not sure I understand - Do you want to parallelize the steps of a single pipeline or by parallelization you mean running, for example, few experiments simultaneously?<\/p>. <p>Hello,<\/p>\n<p>Thank you very much for your reply.<\/p>\n<blockquote>\n<p>the problem with external outputs is that when you do <code>dvc checkout<\/code> you override existing remote file<\/p>\n<\/blockquote>\n<p>It took me some time to understand this, but now I get it.<\/p>\n<p>I believe your second solution would be the most elegant.<br>\nUse DVC normally but never locally pull the whole dataset.<br>\nIt is a very interesting idea and I\u2019ll try to make a poc with it. Thanks!<\/p>\n<blockquote>\n<p>I am not sure I understand - Do you want to parallelize the steps of a single pipeline or by parallelization you mean running, for example, few experiments simultaneously?<\/p>\n<\/blockquote>\n<p>Well, I would need to parallelize the processing of each video, but we are already evaluating the use of <a href=\"http:\/\/ray.io\" rel=\"noopener nofollow ugc\">ray.io<\/a> that can manage workers on aws. I suppose with DVC we could wrap the execution of the scheduler that manage the parallelization on aws.<\/p>\n<p>For experiment, I already know how to use DVC experiments to parallelize them.<\/p>\n<p>Thank you very much for your thoughts!<\/p>. <p>Sure thing!<br>\nFeel free to ping us if any more questions arise!<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/1t-dataset-with-distributed-deep-learning\/1374",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-29T12:02:30.749Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/kwon-young_veesion\">@kwon-young_veesion<\/a>!<br>\nThis is indeed an advanced use case. And while I am unable to find an example that would cover this exact use case I can try to provide you with some information where to start looking.<\/p>\n<ol>\n<li>Data versioning:<br>\nI believe you could use two approaches here:<br>\na. using <code>external dependencies\/outputs<\/code> -  <a href=\"https:\/\/dvc.org\/doc\/user-guide\/data-management\/managing-external-data\">https:\/\/dvc.org\/doc\/user-guide\/data-management\/managing-external-data<\/a> - the problem with external outputs is that when you do <code>dvc checkout<\/code> you override existing remote file, so parallelization is problematic as it would need some kind of synchronization allowing to make sure that your lambda job has the proper data before starting an experiment that is using different version of the data.<br>\nb. using \u201cnormal\u201d repo and <code>dvc add --to-remote<\/code> to version it, and on lambda utilize <a href=\"https:\/\/dvc.org\/doc\/api-reference\/get_url\" class=\"inline-onebox\">get_url()<\/a> to obtain the data on lambda and process it the way you want.<\/li>\n<\/ol>\n<p>Regarding the parallelization:<br>\nI am not sure I understand - Do you want to parallelize the steps of a single pipeline or by parallelization you mean running, for example, few experiments simultaneously?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-31T09:56:12.248Z",
                "Answer_body":"<p>Hello,<\/p>\n<p>Thank you very much for your reply.<\/p>\n<blockquote>\n<p>the problem with external outputs is that when you do <code>dvc checkout<\/code> you override existing remote file<\/p>\n<\/blockquote>\n<p>It took me some time to understand this, but now I get it.<\/p>\n<p>I believe your second solution would be the most elegant.<br>\nUse DVC normally but never locally pull the whole dataset.<br>\nIt is a very interesting idea and I\u2019ll try to make a poc with it. Thanks!<\/p>\n<blockquote>\n<p>I am not sure I understand - Do you want to parallelize the steps of a single pipeline or by parallelization you mean running, for example, few experiments simultaneously?<\/p>\n<\/blockquote>\n<p>Well, I would need to parallelize the processing of each video, but we are already evaluating the use of <a href=\"http:\/\/ray.io\" rel=\"noopener nofollow ugc\">ray.io<\/a> that can manage workers on aws. I suppose with DVC we could wrap the execution of the scheduler that manage the parallelization on aws.<\/p>\n<p>For experiment, I already know how to use DVC experiments to parallelize them.<\/p>\n<p>Thank you very much for your thoughts!<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-31T13:13:43.757Z",
                "Answer_body":"<p>Sure thing!<br>\nFeel free to ping us if any more questions arise!<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"dataset distribut deep learn hello current evalu larg scale deep learn project grow dataset video file total preprocess video file sub sampl subset origin dataset new train val test dataset dataset current video file total train distribut deep learn video classif model aw pytorch distribut extens local approach run pipelin machin run pipelin power server new us case like scale process aw data bucket step pipelin parallel light aw lambda possibl like abl launch pipelin local machin run process aw job challeng setup feel like distribut parallel execut featur challeng place",
        "Question_preprocessed_content":"dataset distribut deep learn hello current evalu larg scale deep learn project grow dataset video file total preprocess video file sub sampl subset origin dataset new dataset dataset current video file total train distribut deep learn video classif model aw pytorch distribut extens approach run pipelin machin run pipelin power server new like scale process aw data bucket step pipelin parallel light aw lambda possibl like abl launch pipelin local machin job challeng setup feel like distribut parallel execut featur challeng place",
        "Question_gpt_summary_original":"The user is facing challenges in implementing a distributed deep learning project using DVC for a large dataset of over 1 million video files totaling more than a terabyte. The user needs to preprocess each video file, sub-sample a subset of the original dataset, and train a distributed deep learning video classification model on AWS with PyTorch distributed. The user wants to scale the entire process on AWS, where all the data is on S3 buckets, and the pipeline steps are parallelized on light AWS lambdas as much as possible. The user also wants to launch the whole pipeline from their local machine and on a CI job. The user feels that the most challenging aspect of implementing this setup with DVC is the distributed parallel execution features.",
        "Question_gpt_summary":"user face challeng implement distribut deep learn project larg dataset million video file total terabyt user need preprocess video file sub sampl subset origin dataset train distribut deep learn video classif model aw pytorch distribut user want scale entir process aw data bucket pipelin step parallel light aw lambda possibl user want launch pipelin local machin job user feel challeng aspect implement setup distribut parallel execut featur",
        "Answer_original_content":"hello kwon young veesion advanc us case unabl exampl cover exact us case try provid inform start look data version believ us approach extern depend output http org doc user guid data manag manag extern data problem extern output checkout overrid exist remot file parallel problemat need kind synchron allow sure lambda job proper data start experi differ version data normal repo add remot version lambda util url obtain data lambda process wai want parallel sure understand want parallel step singl pipelin parallel mean run exampl experi simultan hello thank repli problem extern output checkout overrid exist remot file took time understand believ second solut eleg us normal local pull dataset interest idea ill try poc thank sure understand want parallel step singl pipelin parallel mean run exampl experi simultan need parallel process video evalu us rai manag worker aw suppos wrap execut schedul manag parallel aw experi know us experi parallel thank thought sure thing feel free ping question aris",
        "Answer_preprocessed_content":"hello advanc us case unabl exampl cover exact us case try provid inform start look data version believ us approach problem extern output overrid exist remot file parallel problemat need kind synchron allow sure lambda job proper data start experi differ version data normal repo version lambda util obtain data lambda process wai want parallel sure understand want parallel step singl pipelin parallel mean run exampl experi simultan hello thank repli problem extern output overrid exist remot file took time understand believ second solut eleg us normal local pull dataset interest idea ill try poc thank sure understand want parallel step singl pipelin parallel mean run exampl experi simultan need parallel process video evalu us manag worker aw suppos wrap execut schedul manag parallel aw experi know us experi parallel thank thought sure thing feel free ping question aris",
        "Answer_gpt_summary_original":"Solutions provided:\n- Two approaches for data versioning: using external dependencies\/outputs or using a \"normal\" repo and dvc add --to-remote to version it, and on lambda utilize get_url() to obtain the data on lambda and process it the way you want.\n- Use DVC normally but never locally pull the whole dataset.\n- Evaluate the use of ray.io that can manage workers on AWS and wrap the execution of the scheduler that manages the parallelization on AWS with DVC.",
        "Answer_gpt_summary":"solut provid approach data version extern depend output normal repo add remot version lambda util url obtain data lambda process wai want us normal local pull dataset evalu us rai manag worker aw wrap execut schedul manag parallel aw"
    },
    {
        "Question_title":"Google Cloud Translate API",
        "Question_body":"Hi,i am use free trails of Google Cloud Translate send request on this URL  https:\/\/translation.googleapis.com\/language\/translate\/v2 with API key with body rowget error response kindly see below\"code\": 403,\n\"message\": \"The request is missing a valid API key.\",\n\"errors\":\n\n\"message\": \"The request is missing a valid API key.\",\n\"domain\": \"global\",\n\"reason\": \"forbidden\"\n\"status\": \"PERMISSION_DENIED\"",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1668394320000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":66.0,
        "Answer_body":"Hi, mjunaid,\n\nThe key you're using might not have the permission to use Translate APIs.\n\nTo fix this:\n\nGo to the Google Cloud Platform console\nChoose your project from the drop-down menu in the top bar\nGo to API & Services > Library\nSearch for Cloud Translation API and click on it\nEnable it\n\nGo to API & Services > Credentials\nSelect the key you are using in your Android App\nFrom the menu called Restrict key, choose Cloud Translation API\nSave your edit.\n\nNow the APIs should work properly.\n\nAdditionally, please note that the documentation\u00a0mentions that the structure of the HTTP method should be something like:\n\nhttps:\/\/translation.googleapis.com\/language\/translate\/v2?key=[yourAPIkey]&target=language\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-Translate-API\/td-p\/489124\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-15T09:16:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hi, mjunaid,\n\nThe key you're using might not have the permission to use Translate APIs.\n\nTo fix this:\n\nGo to the Google Cloud Platform console\nChoose your project from the drop-down menu in the top bar\nGo to API & Services > Library\nSearch for Cloud Translation API and click on it\nEnable it\n\nGo to API & Services > Credentials\nSelect the key you are using in your Android App\nFrom the menu called Restrict key, choose Cloud Translation API\nSave your edit.\n\nNow the APIs should work properly.\n\nAdditionally, please note that the documentation\u00a0mentions that the structure of the HTTP method should be something like:\n\nhttps:\/\/translation.googleapis.com\/language\/translate\/v2?key=[yourAPIkey]&target=language\n\nView solution in original post"
            },
            {
                "Answer_creation_time":"2022-11-15T09:16:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hi, mjunaid,\n\nThe key you're using might not have the permission to use Translate APIs.\n\nTo fix this:\n\nGo to the Google Cloud Platform console\nChoose your project from the drop-down menu in the top bar\nGo to API & Services > Library\nSearch for Cloud Translation API and click on it\nEnable it\n\nGo to API & Services > Credentials\nSelect the key you are using in your Android App\nFrom the menu called Restrict key, choose Cloud Translation API\nSave your edit.\n\nNow the APIs should work properly.\n\nAdditionally, please note that the documentation\u00a0mentions that the structure of the HTTP method should be something like:\n\nhttps:\/\/translation.googleapis.com\/language\/translate\/v2?key=[yourAPIkey]&target=language"
            },
            {
                "Answer_creation_time":"2022-11-15T22:20:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Thanks"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1668503760000,
        "Question_original_content":"googl cloud translat api us free trail googl cloud translat send request url http translat googleapi com languag translat api kei bodi rowget error respons kindli code messag request miss valid api kei error messag request miss valid api kei domain global reason forbidden statu permiss deni",
        "Question_preprocessed_content":"googl cloud translat api us free trail googl cloud translat send request url api kei bodi rowget error respons kindli code messag request miss valid api error messag request miss valid api domain global reason forbidden statu",
        "Question_gpt_summary_original":"The user encountered a challenge while using the free trial of Google Cloud Translate API. They received an error response stating that the request is missing a valid API key, with a code of 403 and a message of \"The request is missing a valid API key.\" The error was due to a permission denied issue.",
        "Question_gpt_summary":"user encount challeng free trial googl cloud translat api receiv error respons state request miss valid api kei code messag request miss valid api kei error permiss deni issu",
        "Answer_original_content":"mjunaid kei permiss us translat api fix googl cloud platform consol choos project drop menu bar api servic librari search cloud translat api click enabl api servic credenti select kei android app menu call restrict kei choos cloud translat api save edit api work properli addition note documentationment structur http method like http translat googleapi com languag translat kei yourapikei languag view solut origin post",
        "Answer_preprocessed_content":"mjunaid kei permiss us translat api fix googl cloud platform consol choos project menu bar api servic librari search cloud translat api click enabl api servic credenti select kei android app menu call restrict kei choos cloud translat api save edit api work properli addition note documentationment structur http method like view solut origin post",
        "Answer_gpt_summary_original":"Solution:\n\nThe error was due to a permission denied issue. To fix this, the user needs to enable the Cloud Translation API in the Google Cloud Platform console and select the key they are using in their Android App. From the menu called Restrict key, they should choose Cloud Translation API and save their edit. After this, the APIs should work properly.",
        "Answer_gpt_summary":"solut error permiss deni issu fix user need enabl cloud translat api googl cloud platform consol select kei android app menu call restrict kei choos cloud translat api save edit api work properli"
    },
    {
        "Question_title":"Allowing users to view GPU utilization in GCP Vertex AI training jobs",
        "Question_body":"<p>I am running custom training jobs using Google cloud Vertex AI. But when I enter a custom training job page, the GPU utilization display is not shown, instead, there is a message saying &quot;you don't have access to this data.&quot;\n<a href=\"https:\/\/i.stack.imgur.com\/t0D2M.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t0D2M.jpg\" alt=\"enter image description here\" \/><\/a>\nI would appreciate help finding the right IAM role which will allow me to view the GPU utilization.\nThanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660033318073,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":40.0,
        "Answer_body":"<p>You can use <a href=\"https:\/\/cloud.google.com\/monitoring\/access-control#mon_roles_desc\" rel=\"nofollow noreferrer\"><code>monitoring.viewer<\/code><\/a> IAM role to display both CPU and GPU utilization in GCP Vertex AI training jobs on top of <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/access-control#predefined-roles\" rel=\"nofollow noreferrer\"><code>aiplatform.viewer<\/code><\/a> IAM role.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jbqyl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jbqyl.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73288631",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1660101916403,
        "Question_original_content":"allow user view gpu util gcp train job run custom train job googl cloud enter custom train job page gpu util displai shown instead messag sai access data appreci help find right iam role allow view gpu util thank",
        "Question_preprocessed_content":"allow user view gpu util gcp train job run custom train job googl cloud enter custom train job page gpu util displai shown instead messag sai access appreci help find right iam role allow view gpu util thank",
        "Question_gpt_summary_original":"The user is facing challenges in viewing GPU utilization while running custom training jobs using Google cloud Vertex AI. The GPU utilization display is not shown and the user is unable to access the data. The user is seeking help in finding the right IAM role to view the GPU utilization.",
        "Question_gpt_summary":"user face challeng view gpu util run custom train job googl cloud gpu util displai shown user unabl access data user seek help find right iam role view gpu util",
        "Answer_original_content":"us monitor viewer iam role displai cpu gpu util gcp train job aiplatform viewer iam role",
        "Answer_preprocessed_content":"us iam role displai cpu gpu util gcp train job iam role",
        "Answer_gpt_summary_original":"Solution: The user can use the \"monitoring.viewer\" IAM role along with the \"aiplatform.viewer\" IAM role to display both CPU and GPU utilization in GCP Vertex AI training jobs.",
        "Answer_gpt_summary":"solut user us monitor viewer iam role aiplatform viewer iam role displai cpu gpu util gcp train job"
    },
    {
        "Question_title":"How can I save more metadata on an MLFlow model",
        "Question_body":"<p>I am trying to save a model to MLFlow, but as I have a custom prediction pipeline to retrieve data, I need to save extra metadata into the model.<\/p>\n<p>I tried using my custom signature class, which It does the job correctly and saves the model with the extra metadata in the MLModel file (YAML format). But when want to load the model from the MLFlow registry, the signature is not easy accesible.<\/p>\n<pre><code>mlflow.sklearn.log_model(model, &quot;model&quot;, signature = signature)\n<\/code><\/pre>\n<p>I've also tried to save an extra dictionary at the log_model function, but it saves it in the conda.yaml file:<\/p>\n<pre><code>mlflow.sklearn.log_model(model, &quot;model&quot;, {&quot;metadata1&quot;:&quot;value1&quot;, &quot;metadata2&quot;:&quot;value2&quot;})\n<\/code><\/pre>\n<p>Should I make my own flavour? Or my own Model inheritance? I've seen <a href=\"https:\/\/github1s.com\/mlflow\/mlflow\/blob\/HEAD\/mlflow\/pyfunc\/__init__.py\" rel=\"nofollow noreferrer\">here<\/a> that the PyFuncModel recieves some metadata class and an implementation to solve this, but I don't know where should I pass my own implementations to PyFuncModel on an experiment script. Here's a minimal example:<\/p>\n<pre><code>import mlflow\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\nmetadata_dic = {&quot;metadata1&quot;: &quot;value1&quot;, \n                &quot;metadata2&quot;: &quot;value2&quot;}\n\nX = np.array([[-2, -1, 0, 1, 2, 1],[-2, -1, 0, 1, 2, 1]]).T\ny = np.array([0, 0, 1, 1, 1, 0])\n\nX = pd.DataFrame(X, columns=[&quot;X1&quot;, &quot;X2&quot;])\ny = pd.DataFrame(y, columns=[&quot;y&quot;])\n\n\nmodel = LogisticRegression()\nmodel.fit(X, y)\n\nmlflow.sklearn.log_model(model, &quot;model&quot;)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1638276045390,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":323.0,
        "Answer_body":"<p>Finally, I made a class that contains every metadata and saved it as an model argument:<\/p>\n<pre><code>model = LogisticRegression()\nmodel.fit(X, y)\nmodel.metadata = ModelMetadata(**metadata_dic)\nmlflow.sklearn.log_model(model, &quot;model&quot;)\n<\/code><\/pre>\n<p>Here I lost the customizable <code>predict<\/code> process, but after reading the <code>MLFlow<\/code> documentation is not very clear how to proceed.<\/p>\n<p>If anyone finds a good approach It would be very appreciated.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70169519",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1638361888372,
        "Question_original_content":"save metadata model try save model custom predict pipelin retriev data need save extra metadata model tri custom signatur class job correctli save model extra metadata mlmodel file yaml format want load model registri signatur easi acces sklearn log model model model signatur signatur tri save extra dictionari log model function save conda yaml file sklearn log model model model metadata valu metadata valu flavour model inherit seen pyfuncmodel reciev metadata class implement solv know pass implement pyfuncmodel experi script minim exampl import import numpi import panda sklearn linear model import logisticregress metadata dic metadata valu metadata valu arrai arrai datafram column datafram column model logisticregress model fit sklearn log model model model",
        "Question_preprocessed_content":"save metadata model try save model custom predict pipelin retriev data need save extra metadata model tri custom signatur class job correctli save model extra metadata mlmodel file want load model registri signatur easi acces tri save extra dictionari function save file flavour model inherit seen pyfuncmodel reciev metadata class implement solv know pass implement pyfuncmodel experi script minim exampl",
        "Question_gpt_summary_original":"The user is facing challenges in saving extra metadata into an MLFlow model due to a custom prediction pipeline. The user has tried using a custom signature class and saving an extra dictionary at the log_model function, but the signature is not easily accessible when loading the model from the MLFlow registry. The user is considering making their own flavor or model inheritance and is seeking guidance on how to pass their own implementations to PyFuncModel on an experiment script.",
        "Question_gpt_summary":"user face challeng save extra metadata model custom predict pipelin user tri custom signatur class save extra dictionari log model function signatur easili access load model registri user consid make flavor model inherit seek guidanc pass implement pyfuncmodel experi script",
        "Answer_original_content":"final class contain metadata save model argument model logisticregress model fit model metadata modelmetadata metadata dic sklearn log model model model lost customiz predict process read document clear proce find good approach appreci",
        "Answer_preprocessed_content":"final class contain metadata save model argument lost customiz process read document clear proce find good approach appreci",
        "Answer_gpt_summary_original":"Solution: One solution mentioned in the discussion is to create a class that contains all the metadata and save it as a model argument. However, this approach may result in losing the customizable predict process. The user is seeking guidance on how to proceed with their own implementations to PyFuncModel on an experiment script. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut solut mention discuss creat class contain metadata save model argument approach result lose customiz predict process user seek guidanc proce implement pyfuncmodel experi script solut mention discuss"
    },
    {
        "Question_title":"Deployed an mlflow sklearn model to Azure ACI using Databrics how to get the prediction probabilities",
        "Question_body":"I have deployed the model to Azure Container Instance using model.pkl file along with conda.yaml and MLproject file created by mlflow. I can get the output class for input data by hitting the endpoint\u00a0http:\/\/1fa00837-1734-487c-998e-82b402c2451a.eastasia.azurecontainer.io\/score  \nhowever I am interested in getting the prediction probabilities that we get in scikit learn model.predict_proba(). Can anyone suggest how I can do that.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1570153632000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":14.0,
        "Answer_body":"You can use custom pyfunc model to call predict_proba instead of predict (or call both).\u00a0\n\n\nOn Thu, Oct 3, 2019 at 10:47 PM babar ali <bac...@gmail.com> wrote:\n\nI have deployed the model to Azure Container Instance using model.pkl file along with conda.yaml and MLproject file created by mlflow. I can get the output class for input data by hitting the endpoint\u00a0http:\/\/1fa00837-1734-487c-998e-82b402c2451a.eastasia.azurecontainer.io\/score  \nhowever I am interested in getting the prediction probabilities that we get in scikit learn model.predict_proba(). Can anyone suggest how I can do that.\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/e99e2539-e3dd-4859-a6d0-6c337cf6e07b%40googlegroups.com.. Thomas, I am confused by looking at the XGBoot example given since I have no understanding of the library. Is it possible for you to provide a snippet that I can use to score predict_proba() function in mlflow model.\n\n\nOn Friday, October 4, 2019 at 5:38:06 PM UTC, Tomas Nykodym wrote:\nYou can use custom pyfunc model to call predict_proba instead of predict (or call both).\u00a0\n\n\nOn Thu, Oct 3, 2019 at 10:47 PM babar ali <bac...@gmail.com> wrote:\n\nI have deployed the model to Azure Container Instance using model.pkl file along with conda.yaml and MLproject file created by mlflow. I can get the output class for input data by hitting the endpoint\u00a0http:\/\/1fa00837-1734-487c-998e-82b402c2451a.eastasia.azurecontainer.io\/score  \nhowever I am interested in getting the prediction probabilities that we get in scikit learn model.predict_proba(). Can anyone suggest how I can do that.\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow...@googlegroups.com.\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/0yQ7tWc1Pgc",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-10-04T13:38:06",
                "Answer_body":"You can use custom pyfunc model to call predict_proba instead of predict (or call both).\u00a0\n\n\nOn Thu, Oct 3, 2019 at 10:47 PM babar ali <bac...@gmail.com> wrote:\n\nI have deployed the model to Azure Container Instance using model.pkl file along with conda.yaml and MLproject file created by mlflow. I can get the output class for input data by hitting the endpoint\u00a0http:\/\/1fa00837-1734-487c-998e-82b402c2451a.eastasia.azurecontainer.io\/score  \nhowever I am interested in getting the prediction probabilities that we get in scikit learn model.predict_proba(). Can anyone suggest how I can do that.\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/e99e2539-e3dd-4859-a6d0-6c337cf6e07b%40googlegroups.com."
            },
            {
                "Answer_creation_time":"2019-10-07T07:48:55",
                "Answer_body":"Thomas, I am confused by looking at the XGBoot example given since I have no understanding of the library. Is it possible for you to provide a snippet that I can use to score predict_proba() function in mlflow model.\n\n\nOn Friday, October 4, 2019 at 5:38:06 PM UTC, Tomas Nykodym wrote:\nYou can use custom pyfunc model to call predict_proba instead of predict (or call both).\u00a0\n\n\nOn Thu, Oct 3, 2019 at 10:47 PM babar ali <bac...@gmail.com> wrote:\n\nI have deployed the model to Azure Container Instance using model.pkl file along with conda.yaml and MLproject file created by mlflow. I can get the output class for input data by hitting the endpoint\u00a0http:\/\/1fa00837-1734-487c-998e-82b402c2451a.eastasia.azurecontainer.io\/score  \nhowever I am interested in getting the prediction probabilities that we get in scikit learn model.predict_proba(). Can anyone suggest how I can do that.\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow...@googlegroups.com.\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"deploi sklearn model azur aci databr predict probabl deploi model azur contain instanc model pkl file conda yaml mlproject file creat output class input data hit endpointhttp bca eastasia azurecontain score interest get predict probabl scikit learn model predict proba suggest",
        "Question_preprocessed_content":"deploi sklearn model azur aci databr predict probabl deploi model azur contain instanc file mlproject file creat output class input data hit interest get predict probabl scikit learn suggest",
        "Question_gpt_summary_original":"The user has successfully deployed an mlflow sklearn model to Azure ACI using Databrics, but is facing a challenge in getting the prediction probabilities that are obtained in scikit learn model.predict_proba(). The user is seeking suggestions on how to obtain these probabilities.",
        "Question_gpt_summary":"user successfulli deploi sklearn model azur aci databr face challeng get predict probabl obtain scikit learn model predict proba user seek suggest obtain probabl",
        "Answer_original_content":"us custom pyfunc model predict proba instead predict thu oct babar ali wrote deploi model azur contain instanc model pkl file conda yaml mlproject file creat output class input data hit endpointhttp bca eastasia azurecontain score interest get predict probabl scikit learn model predict proba suggest receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user edd ccfeb googlegroup com thoma confus look xgboot exampl given understand librari possibl provid snippet us score predict proba function model fridai octob utc toma nykodym wrote us custom pyfunc model predict proba instead predict thu oct babar ali wrote deploi model azur contain instanc model pkl file conda yaml mlproject file creat output class input data hit endpointhttp bca eastasia azurecontain score interest get predict probabl scikit learn model predict proba suggest receiv messag subscrib googl group user group unsubscrib group stop receiv email send email googlegroup com",
        "Answer_preprocessed_content":"us custom pyfunc model instead predict thu oct babar ali wrote deploi model azur contain instanc file mlproject file creat output class input data hit interest get predict probabl scikit learn suggest receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit thoma confus look xgboot exampl given understand librari possibl provid snippet us score function model fridai octob utc toma nykodym wrote us custom pyfunc model instead predict thu oct babar ali wrote deploi model azur contain instanc file mlproject file creat output class input data hit interest get predict probabl scikit learn suggest receiv messag subscrib googl group group unsubscrib group stop receiv email send email",
        "Answer_gpt_summary_original":"Solution: The user can use a custom pyfunc model to call predict_proba instead of predict (or call both) to obtain the prediction probabilities.",
        "Answer_gpt_summary":"solut user us custom pyfunc model predict proba instead predict obtain predict probabl"
    },
    {
        "Question_title":"Reading a file from s3 to sagemaker on AWS gives 403 forbidden error, but other operations work on the file",
        "Question_body":"<p>This command:<\/p>\n\n<pre><code>BUCKET_TO_READ='my-bucket'\nFILE_TO_READ='myFile'\ndata_location = 's3:\/\/{}\/{}'.format(BUCKET_TO_READ, FILE_TO_READ)\ndf=pd.read_csv(data_location)\n<\/code><\/pre>\n\n<p>is failing with a <\/p>\n\n<pre><code>ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n\n<p>Error and I'm unable to figure out why.  That should work according to <a href=\"https:\/\/stackoverflow.com\/a\/50244897\/3763782\">https:\/\/stackoverflow.com\/a\/50244897\/3763782<\/a> <\/p>\n\n<p>Here are my permissions on the bucket:<\/p>\n\n<pre><code>            \"Action\": [\n                \"s3:ListMultipartUploadParts\",\n                \"s3:ListBucket\",\n                \"s3:GetObjectVersionTorrent\",\n                \"s3:GetObjectVersionTagging\",\n                \"s3:GetObjectVersionAcl\",\n                \"s3:GetObjectVersion\",\n                \"s3:GetObjectTorrent\",\n                \"s3:GetObjectTagging\",\n                \"s3:GetObjectAcl\",\n                \"s3:GetObject\"\n<\/code><\/pre>\n\n<p>And these commands work as expected: <\/p>\n\n<pre><code>role = get_execution_role()\nregion = boto3.Session().region_name\nprint(role)\nprint(region)\n\ns3 = boto3.resource('s3')\nbucket = s3.Bucket(BUCKET_TO_READ)\nprint(bucket.creation_date)\n\nfor my_bucket_object in bucket.objects.all():\n    print(my_bucket_object)\n    FILE_TO_READ = my_bucket_object.key\n    break\n\nobj = s3.Object(BUCKET_TO_READ, FILE_TO_READ)\nprint(obj)\n\n<\/code><\/pre>\n\n<p>All of those print statements worked just fine.  <\/p>\n\n<p>I'm not sure if it matters, but each file is within a folder, so my FILE_TO_READ looks like <code>folder\/file<\/code>.<\/p>\n\n<p>This command which should download the file to sagemaker also falied with a 403:<\/p>\n\n<pre><code>import boto3\ns3 = boto3.resource('s3')\ns3.Object(BUCKET_TO_READ, FILE_TO_READ).download_file(FILE_TO_READ)\n<\/code><\/pre>\n\n<p>This is also happening when I open a terminal and use <\/p>\n\n<pre><code>aws s3 cp AWSURI local_file_name\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1582298932137,
        "Question_favorite_count":null,
        "Question_last_edit_time":1582306425572,
        "Question_score":4.0,
        "Question_view_count":2322.0,
        "Answer_body":"<p>The reason was that we granted permission to the bucket not the objects.  That would be granting <code>\"Resource\": \"arn:aws:s3:::bucket-name\/\"<\/code> but not <code>\"Resource\": \"arn:aws:s3:::bucket-name\/*\"<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60341782",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1582382844412,
        "Question_original_content":"read file aw give forbidden error oper work file command bucket read bucket file read myfil data locat format bucket read file read read csv data locat fail clienterror error occur call headobject oper forbidden error unabl figur work accord http stackoverflow com permiss bucket action listmultipartuploadpart listbucket getobjectversiontorr getobjectversiontag getobjectversionacl getobjectvers getobjecttorr getobjecttag getobjectacl getobject command work expect role execut role region boto session region print role print region boto resourc bucket bucket bucket read print bucket creation date bucket object bucket object print bucket object file read bucket object kei break obj object bucket read file read print obj print statement work fine sure matter file folder file read look like folder file command download file fali import boto boto resourc object bucket read file read download file file read happen open termin us aw awsuri local file",
        "Question_preprocessed_content":"read file aw give forbidden error oper work file command fail error unabl figur work accord permiss bucket command work expect print statement work fine sure matter file folder look like command download file fali happen open termin us",
        "Question_gpt_summary_original":"The user is encountering a 403 forbidden error when trying to read a file from s3 to sagemaker on AWS. The user has checked their permissions on the bucket and found that they have the necessary permissions. Other operations on the file, such as listing and downloading, are working as expected. The file is located within a folder, and the same error occurs when trying to download the file using the AWS CLI.",
        "Question_gpt_summary":"user encount forbidden error try read file aw user check permiss bucket necessari permiss oper file list download work expect file locat folder error occur try download file aw cli",
        "Answer_original_content":"reason grant permiss bucket object grant resourc arn aw bucket resourc arn aw bucket",
        "Answer_preprocessed_content":"reason grant permiss bucket object grant",
        "Answer_gpt_summary_original":"Solution: The user needs to grant permission to the objects in the bucket by adding <code>\"Resource\": \"arn:aws:s3:::bucket-name\/*\"<\/code> to their permissions instead of just <code>\"Resource\": \"arn:aws:s3:::bucket-name\/\"<\/code>.",
        "Answer_gpt_summary":"solut user need grant permiss object bucket ad resourc arn aw bucket permiss instead resourc arn aw bucket"
    },
    {
        "Question_title":"DVC in a directory that is already git-controlled",
        "Question_body":"<p>I want to init dvc in a directory that is already under git.<\/p>\n<p>More precisely, I already have a git-controlled directory that contains all my pycharm projects, and want to initialize dvc in its subdirectory for a specific project.<\/p>\n<p>Am I going to run into trouble later?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1566477305073,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":481.0,
        "Answer_body":"<p>Not at all. This is the normal way that DVC is supposed to work.<\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/byoussin\">@byoussin<\/a>! Do you want to initialize it specifically for a subdirectory of the git-controlled project? I\u2019m not sure DVC can do that right now. Most likely if you run <code>dvc init<\/code> it will create <code>.dvc<\/code> in the root of the git-controlled directory.  <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a> can give more details for this scenario.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-in-a-directory-that-is-already-git-controlled\/208",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-08-22T13:49:46.682Z",
                "Answer_body":"<p>Not at all. This is the normal way that DVC is supposed to work.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-23T04:19:46.817Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/byoussin\">@byoussin<\/a>! Do you want to initialize it specifically for a subdirectory of the git-controlled project? I\u2019m not sure DVC can do that right now. Most likely if you run <code>dvc init<\/code> it will create <code>.dvc<\/code> in the root of the git-controlled directory.  <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a> can give more details for this scenario.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"directori git control want init directori git precis git control directori contain pycharm project want initi subdirectori specif project go run troubl later",
        "Question_preprocessed_content":"directori want init directori git precis directori contain pycharm project want initi subdirectori specif project go run troubl later",
        "Question_gpt_summary_original":"The user wants to initialize DVC in a subdirectory of a directory that is already under Git control, which contains all their PyCharm projects. They are concerned about potential issues that may arise in the future.",
        "Question_gpt_summary":"user want initi subdirectori directori git control contain pycharm project concern potenti issu aris futur",
        "Answer_original_content":"normal wai suppos work byoussin want initi specif subdirectori git control project sure right like run init creat root git control directori kuprus detail scenario",
        "Answer_preprocessed_content":"normal wai suppos work want initi specif subdirectori project sure right like run creat root directori detail scenario",
        "Answer_gpt_summary_original":"Solution: No solution provided.",
        "Answer_gpt_summary":"solut solut provid"
    },
    {
        "Question_title":"Image Classification in Azure Machine Learning",
        "Question_body":"<p>I'm preparing for the Azure Machine Learning exam, and here is a question confuses me.<\/p>\n<blockquote>\n<p>You are designing an Azure Machine Learning workflow. You have a\ndataset that contains two million large digital photographs. You plan\nto detect the presence of trees in the photographs. You need to ensure\nthat your model supports the following:<\/p>\n<p>Solution: You create a Machine\nLearning experiment that implements the Multiclass Decision Jungle\nmodule. Does this meet the goal?<\/p>\n<p>Solution: You create a Machine Learning experiment that implements the\nMulticlass Neural Network module. Does this meet the goal?<\/p>\n<\/blockquote>\n<p>The answer for the first question is No while for second is Yes, but I cannot understand why Multiclass Decision Jungle doesn't meet the goal since it is a classifier. Can someone explain to me the reason?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1559676822253,
        "Question_favorite_count":null,
        "Question_last_edit_time":1592644375060,
        "Question_score":1.0,
        "Question_view_count":538.0,
        "Answer_body":"<p>I suppose that this is part of a series of questions that present the same scenario. And there should be definitely some constraints in the scenario. \nMoreover if you have a look on the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/multiclass-neural-network\" rel=\"nofollow noreferrer\">Azure documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>However, recent research has shown that deep neural networks (DNN)\n  with many layers can be very effective in complex tasks such as image\n  or speech recognition. The successive layers are used to model\n  increasing levels of semantic depth.<\/p>\n<\/blockquote>\n\n<p>Thus, Azure recommends using Neural Networks for image classification. Remember, that the goal of the exam is to test your capacity to design data science solution <strong>using Azure<\/strong> so better to use their official documentation as a reference.<\/p>\n\n<p>And comparing to the other solutions:<\/p>\n\n<ol>\n<li>You create an Azure notebook that supports the Microsoft Cognitive\nToolkit.<\/li>\n<li>You create a Machine Learning experiment that implements\nthe Multiclass Decision Jungle module.<\/li>\n<li>You create an endpoint to the\nComputer vision API. <\/li>\n<li>You create a Machine Learning experiment that\nimplements the Multiclass Neural Network module.<\/li>\n<li>You create an Azure\nnotebook that supports the Microsoft Cognitive Toolkit.<\/li>\n<\/ol>\n\n<p>There are only 2 Azure ML Studio modules, and as the question is about constructing a <strong>workflow<\/strong> I guess we can only choose between them. (CNTK is actually the best solution as it allows constructing a deep neural network with ReLU whereas AML Studio doesn't, and API call is not about data science at all). <\/p>\n\n<p>Finally, I do agree with the other contributors that the question is absurd. Hope this helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56450223",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1559895314607,
        "Question_original_content":"imag classif prepar exam question confus design workflow dataset contain million larg digit photograph plan detect presenc tree photograph need ensur model support follow solut creat machin learn experi implement multiclass decis jungl modul meet goal solut creat machin learn experi implement multiclass neural network modul meet goal answer question second ye understand multiclass decis jungl meet goal classifi explain reason",
        "Question_preprocessed_content":"imag classif prepar exam question confus design workflow dataset contain million larg digit photograph plan detect presenc tree photograph need ensur model support follow solut creat machin learn experi implement multiclass decis jungl modul meet goal solut creat machin learn experi implement multiclass neural network modul meet goal answer question second ye understand multiclass decis jungl meet goal classifi explain reason",
        "Question_gpt_summary_original":"The user is preparing for the Azure Machine Learning exam and is confused about a question regarding the detection of trees in a dataset of two million digital photographs. They are unsure why the Multiclass Decision Jungle module does not meet the goal of detecting trees while the Multiclass Neural Network module does.",
        "Question_gpt_summary":"user prepar exam confus question detect tree dataset million digit photograph unsur multiclass decis jungl modul meet goal detect tree multiclass neural network modul",
        "Answer_original_content":"suppos seri question present scenario definit constraint scenario look azur document recent research shown deep neural network dnn layer effect complex task imag speech recognit success layer model increas level semant depth azur recommend neural network imag classif rememb goal exam test capac design data scienc solut azur better us offici document refer compar solut creat azur notebook support microsoft cognit toolkit creat machin learn experi implement multiclass decis jungl modul creat endpoint vision api creat machin learn experi implement multiclass neural network modul creat azur notebook support microsoft cognit toolkit studio modul question construct workflow guess choos cntk actual best solut allow construct deep neural network relu aml studio api data scienc final agre contributor question absurd hope help",
        "Answer_preprocessed_content":"suppos seri question present scenario definit constraint scenario look azur document recent research shown deep neural network layer effect complex task imag speech recognit success layer model increas level semant depth azur recommend neural network imag classif rememb goal exam test capac design data scienc solut azur better us offici document refer compar solut creat azur notebook support microsoft cognit toolkit creat machin learn experi implement multiclass decis jungl modul creat endpoint vision api creat machin learn experi implement multiclass neural network modul creat azur notebook support microsoft cognit toolkit studio modul question construct workflow guess choos final agre contributor question absurd hope help",
        "Answer_gpt_summary_original":"Solutions provided:\n- Azure documentation recommends using Neural Networks for image classification, which is why the Multiclass Neural Network module is more effective in detecting trees in the dataset.\n- There are only 2 Azure ML Studio modules available for constructing a workflow, and the Multiclass Decision Jungle module is not effective in this scenario. \n- CNTK is actually the best solution as it allows constructing a deep neural network with ReLU, whereas AML Studio doesn't. \n- API call is not about data science at all. \n\nNo personal opinions or biases were included in the summary.",
        "Answer_gpt_summary":"solut provid azur document recommend neural network imag classif multiclass neural network modul effect detect tree dataset studio modul avail construct workflow multiclass decis jungl modul effect scenario cntk actual best solut allow construct deep neural network relu aml studio api data scienc person opinion bias includ summari"
    },
    {
        "Question_title":"Trying to use the 'log_metric() function.",
        "Question_body":"I used\u00a0 'with mlflow.start_run()\u00a0 to log and show the params in the web ui and it works,\u00a0 when i try to use log_metric() i cant get it to show in the ui.\u00a0\n\n\nI used a variable called 'history' to store the values output from model.compile, then a function to extract the float value, and then pass that variable to log_metric()\u00a0\n\n\nmy 'history' variable is not getting anything stored in it though.",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1541184539000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":19.0,
        "Answer_body":"Can you try replacing the 'history' variable with something simpler like a literal number, say 10 and see if that value shows up in the UI?\n\n\nThen if that works, work backwards and confirm the variable you are passing is a simple numberic value.\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/67c34783-3f24-4fb8-a33b-981e596bee02%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.. I dont follow you.\u00a0 replace 'history' with what ?\n\ue5d3. Hi Paul,\n\n\nAndy's suggestion was to replace history['loss'], history['metrics'] with a static constant value and verify that the constant gets reported on UI. This will verify that the setup was correct and mlflow APIs are storing values appropriately and UI can read those.\u00a0\n\n\nSo your example would look like\n\u00a0 mlflow.log_metric(\"metric\", 10.12)\n\n\nIf you haven't tried it already,\u00a0quickstart example would also be a good one to try.\u00a0\n\n\nHowever, the error message in the picture suggests that \"history\" might be getting set to None at some point before reaching line 116.\n\n\nRegards,\n\n\n\nMani Parkhe\n\nma...@databricks.com\n\n\n\n\n\n\n\n\n\n\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/7dd6e1c6-9067-4df8-9beb-fb8191055889%40googlegroups.com.\n\ue5d3. \ue5d3. I was able to log an int an a float. what is the syntax to capture the real data and pass i to log_metric()\u00a0 ?\u00a0\n\ue5d3. Next step is to dig into the Keras API docs and figure out what functions Keras provides that you can use to access the metrics of the model that you're looking to log.\n\n\nAndy\n\n\n\ue5d3\n\ue5d3\n--\n\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/658d5364-fc30-4d27-bf90-3dae926ce169%40googlegroups.com.\n\ue5d3. A potentially handy reference per Andy's call out is\u00a0How to Use MLflow to Experiment a Keras Network Model: Binary Classification for Movie Reviews.\u00a0 HTH!\n\n\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CALEZFQzkuxgN7SDpWFaC8T_oLfmaR2eekandF6EToO%3DucJnRKQ%40mail.gmail.com.\n\ue5d3. thanks. but I have already seen that example and i tried to implement it into my specific needs and i still doesnt work.\u00a0 'history = model.compile()'\u00a0 my history variable keeps getting nothing stored into it.\u00a0\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/qZONNmbAASk",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-11-02T19:53:41",
                "Answer_body":"Can you try replacing the 'history' variable with something simpler like a literal number, say 10 and see if that value shows up in the UI?\n\n\nThen if that works, work backwards and confirm the variable you are passing is a simple numberic value.\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/67c34783-3f24-4fb8-a33b-981e596bee02%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2018-11-02T20:24:44",
                "Answer_body":"I dont follow you.\u00a0 replace 'history' with what ?\n\ue5d3"
            },
            {
                "Answer_creation_time":"2018-11-02T21:17:16",
                "Answer_body":"Hi Paul,\n\n\nAndy's suggestion was to replace history['loss'], history['metrics'] with a static constant value and verify that the constant gets reported on UI. This will verify that the setup was correct and mlflow APIs are storing values appropriately and UI can read those.\u00a0\n\n\nSo your example would look like\n\u00a0 mlflow.log_metric(\"metric\", 10.12)\n\n\nIf you haven't tried it already,\u00a0quickstart example would also be a good one to try.\u00a0\n\n\nHowever, the error message in the picture suggests that \"history\" might be getting set to None at some point before reaching line 116.\n\n\nRegards,\n\n\n\nMani Parkhe\n\nma...@databricks.com\n\n\n\n\n\n\n\n\n\n\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/7dd6e1c6-9067-4df8-9beb-fb8191055889%40googlegroups.com.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2018-11-02T22:33:04",
                "Answer_body":"\ue5d3"
            },
            {
                "Answer_creation_time":"2018-11-02T22:34:26",
                "Answer_body":"I was able to log an int an a float. what is the syntax to capture the real data and pass i to log_metric()\u00a0 ?\u00a0\n\ue5d3"
            },
            {
                "Answer_creation_time":"2018-11-02T23:48:20",
                "Answer_body":"Next step is to dig into the Keras API docs and figure out what functions Keras provides that you can use to access the metrics of the model that you're looking to log.\n\n\nAndy\n\n\n\ue5d3\n\ue5d3\n--\n\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/658d5364-fc30-4d27-bf90-3dae926ce169%40googlegroups.com.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2018-11-03T01:31:29",
                "Answer_body":"A potentially handy reference per Andy's call out is\u00a0How to Use MLflow to Experiment a Keras Network Model: Binary Classification for Movie Reviews.\u00a0 HTH!\n\n\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CALEZFQzkuxgN7SDpWFaC8T_oLfmaR2eekandF6EToO%3DucJnRKQ%40mail.gmail.com.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2018-11-03T17:26:01",
                "Answer_body":"thanks. but I have already seen that example and i tried to implement it into my specific needs and i still doesnt work.\u00a0 'history = model.compile()'\u00a0 my history variable keeps getting nothing stored into it.\u00a0\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"try us log metric function start run log param web work try us log metric variabl call histori store valu output model compil function extract float valu pass variabl log metric histori variabl get store",
        "Question_preprocessed_content":"try us function log param web work try us variabl call histori store valu output function extract float valu pass variabl histori variabl get store",
        "Question_gpt_summary_original":"The user is facing challenges while trying to use the 'log_metric() function. They were able to use 'with mlflow.start_run() to log and show the params in the web UI, but they are unable to get log_metric() to show in the UI. The user is using a variable called 'history' to store the values output from model.compile, but it is not getting anything stored in it.",
        "Question_gpt_summary":"user face challeng try us log metric function abl us start run log param web unabl log metric user variabl call histori store valu output model compil get store",
        "Answer_original_content":"try replac histori variabl simpler like liter number valu show work work backward confirm variabl pass simpl number valu receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user ebe googlegroup com option visit http group googl com optout dont follow replac histori paul andi suggest replac histori loss histori metric static constant valu verifi constant get report verifi setup correct api store valu appropri read exampl look like log metric metric haven tri quickstart exampl good try error messag pictur suggest histori get set point reach line regard mani parkh databrick com view discuss web visit http group googl com msgid user ddec beb googlegroup com abl log int float syntax captur real data pass log metric step dig kera api doc figur function kera provid us access metric model look log andi receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user daec googlegroup com potenti handi refer andi ishow us experi kera network model binari classif movi review hth view discuss web visit http group googl com msgid user calezfqzkuxgnsdpwfact olfmareekandfetoo ducjnrkq mail gmail com thank seen exampl tri implement specif need doesnt work histori model compil histori variabl keep get store",
        "Answer_preprocessed_content":"try replac histori variabl simpler like liter number valu show work work backward confirm variabl pass simpl number valu receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit dont follow replac histori paul andi suggest replac histori histori static constant valu verifi constant get report verifi setup correct api store valu appropri read exampl look like haven tri quickstart exampl good try error messag pictur suggest histori get set point reach line regard mani parkh view discuss web visit abl log int float syntax captur real data pass step dig kera api doc figur function kera provid us access metric model look log andi receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit potenti handi refer andi ishow us experi kera network model binari classif movi review hth view discuss web visit thank seen exampl tri implement specif need doesnt work histori histori variabl keep get store",
        "Answer_gpt_summary_original":"Solutions provided:\n- Replace the 'history' variable with a simple numeric value to verify that the setup was correct and mlflow APIs are storing values appropriately and UI can read those.\n- Dig into the Keras API docs and figure out what functions Keras provides that you can use to access the metrics of the model that you're looking to log.\n- Refer to the article \"How to Use MLflow to Experiment a Keras Network Model: Binary Classification for Movie Reviews\" for a potentially handy reference. \n\nNo solution was provided for the issue of the 'history' variable not getting anything stored in it.",
        "Answer_gpt_summary":"solut provid replac histori variabl simpl numer valu verifi setup correct api store valu appropri read dig kera api doc figur function kera provid us access metric model look log refer articl us experi kera network model binari classif movi review potenti handi refer solut provid issu histori variabl get store"
    },
    {
        "Question_title":"Sagemaker batch transform job failure for 'batchStrategy: MultiRecord' along with data processing",
        "Question_body":"<p>We are using SageMaker Batch Transform job and to fit as many records in a mini-batch as can fit within the <code>MaxPayloadInMB<\/code> limit, we are setting <code>BatchStrategy<\/code> to <code>MultiRecord<\/code> and <code>SplitType<\/code> to <code>Line<\/code>.<\/p>\n<p>Input to the SageMaker batch transform job is:<\/p>\n<pre><code>{&quot;requestBody&quot;: {&quot;data&quot;: {&quot;Age&quot;: 90, &quot;Experience&quot;: 26, &quot;Income&quot;: 30, &quot;Family&quot;: 3, &quot;CCAvg&quot;: 1}}, &quot;mName&quot;: &quot;loanprediction&quot;, &quot;mVersion&quot;: &quot;1&quot;, &quot;testFlag&quot;: &quot;false&quot;, &quot;environment&quot;: &quot;DEV&quot;, &quot;transactionId&quot;: &quot;5-687sdf87-0bc7e3cb3454dbf261ed1353&quot;, &quot;timestamp&quot;: &quot;2022-01-15T01:45:32.955Z&quot;}\n{&quot;requestBody&quot;: {&quot;data&quot;: {&quot;Age&quot;: 55, &quot;Experience&quot;: 26, &quot;Income&quot;: 450, &quot;Family&quot;: 3, &quot;CCAvg&quot;: 1}}, &quot;mName&quot;: &quot;loanprediction&quot;, &quot;mVersion&quot;: &quot;1&quot;, &quot;testFlag&quot;: &quot;false&quot;, &quot;environment&quot;: &quot;DEV&quot;, &quot;transactionId&quot;: &quot;5-69e22778-594916685f4ceca66c08bfbc&quot;, &quot;timestamp&quot;: &quot;2022-01-15T01:46:32.386Z&quot;}\n<\/code><\/pre>\n<p>This is the SageMaker batch transform job config:<\/p>\n<pre><code>apiVersion: sagemaker.aws.amazon.com\/v1\nkind: BatchTransformJob\nmetadata:\n        generateName: '...-batchtransform'\nspec:\n        batchStrategy: MultiRecord\n        dataProcessing:\n                JoinSource: Input\n                OutputFilter: $\n                inputFilter: $.requestBody\n        modelClientConfig:\n                invocationsMaxRetries: 0\n                invocationsTimeoutInSeconds: 3\n        mName: '..'\n        region: us-west-2\n        transformInput:\n                contentType: application\/json\n                dataSource:\n                        s3DataSource:\n                                s3DataType: S3Prefix\n                                s3Uri: s3:\/\/......\/part-\n                splitType: Line\n        transformOutput:\n                accept: application\/json\n                assembleWith: Line\n                kmsKeyId: '....'\n                s3OutputPath: s3:\/\/....\/batch_output\n        transformResources:\n                instanceCount: ..\n                instanceType: '..'\n<\/code><\/pre>\n<p>The SageMaker batch transform job fails with:<\/p>\n<p>Error in batch transform data-log -<\/p>\n<blockquote>\n<p>2022-01-27T00:55:39.781:[sagemaker logs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:<\/p>\n\n<p>2022-01-27T00:55:39.781:[sagemaker logs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:<\/p>\n400 Bad Request 2022-01-27T00:55:39.781:[sagemaker\nlogs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:\n<p>Failed to decode JSON object: Extra data: line 2 column 1 (char\n163)<\/p>\n<\/blockquote>\n<p><strong>Observation:<\/strong>\nThis issue occurs when we provide <code>batchStrategy: MultiRecord<\/code> in the manifest along with these data processing configs:<\/p>\n<pre><code>dataProcessing:\n        JoinSource: Input\n        OutputFilter: $\n        inputFilter: $.requestBody\n<\/code><\/pre>\n<p><strong>NOTE:<\/strong> If we put <code>batchStrategy: SingleRecord<\/code> along with the aforementioned data processing configs, it just works fine (job succeeds)!<\/p>\n<p><strong>Question:<\/strong> How can we achieve successful run with <code>batchStrategy: MultiRecord<\/code> along with the aforementioned data processing config?<\/p>\n<p>A successful output with <code>batchStrategy: SingleRecord<\/code> looks like this:<\/p>\n<blockquote>\n<p>{&quot;SageMakerOutput&quot;:{&quot;prediction&quot;:0},&quot;environment&quot;:&quot;DEV&quot;,&quot;transactionId&quot;:&quot;5-687sdf87-0bc7e3cb3454dbf261ed1353&quot;,&quot;mName&quot;:&quot;loanprediction&quot;,&quot;mVersion&quot;:&quot;1&quot;,&quot;requestBody&quot;:{&quot;data&quot;:{&quot;Age&quot;:90,&quot;CCAvg&quot;:1,&quot;Experience&quot;:26,&quot;Family&quot;:3,&quot;Income&quot;:30}},&quot;testFlag&quot;:&quot;false&quot;,&quot;timestamp&quot;:&quot;2022-01-15T01:45:32.955Z&quot;}\n{&quot;SageMakerOutput&quot;:{&quot;prediction&quot;:0},&quot;environment&quot;:&quot;DEV&quot;,&quot;transactionId&quot;:&quot;5-69e22778-594916685f4ceca66c08bfbc&quot;,&quot;mName&quot;:&quot;loanprediction&quot;,&quot;mVersion&quot;:&quot;1&quot;,&quot;requestBody&quot;:{&quot;data&quot;:{&quot;Age&quot;:55,&quot;CCAvg&quot;:1,&quot;Experience&quot;:26,&quot;Family&quot;:3,&quot;Income&quot;:450}},&quot;testFlag&quot;:&quot;false&quot;,&quot;timestamp&quot;:&quot;2022-01-15T01:46:32.386Z&quot;}\nRegion name \u2013 optional: Relevant resource ARN \u2013 optional:\narn:aws:sagemaker:us-west-2:435945521637:transform-job\/my-pipeline-9v28r-bat-e548fbfb125946528957e0f123456789<\/p>\n<\/blockquote>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1643344870580,
        "Question_favorite_count":null,
        "Question_last_edit_time":1643607224392,
        "Question_score":0.0,
        "Question_view_count":659.0,
        "Answer_body":"<p>When your input data is in JSON line format and you choose a SingleRecord BatchStrategy, your container will receive a single JSON payload body like below<\/p>\n<pre><code>{ &lt;some JSON data&gt; }\n<\/code><\/pre>\n<p>However, if you use MultiRecord, Batch transform will split your JSON line input (which might contain 100 lines for example) into multiple records (say 10 records) all sent at once to your container as shown below:<\/p>\n<pre><code>{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n.\n.\n.\n{ &lt;some JSON data&gt; }\n<\/code><\/pre>\n<p>Therefore your container should be able to handle such input for it to work. However, from the error message, I can see it is complaining about invalid JSON format as it reads the second row of the request.<\/p>\n<p>I also noticed that you have supplied <code>ContentType<\/code> and <code>AcceptType<\/code> as <code>application\/json<\/code> but instead should be <code>application\/jsonlines<\/code><\/p>\n<p>Could you please test your container to see if it can handle multiple JSON line records per single invocation.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70888883",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1643938529208,
        "Question_original_content":"batch transform job failur batchstrategi multirecord data process batch transform job fit record mini batch fit maxpayloadinmb limit set batchstrategi multirecord splittyp line input batch transform job requestbodi data ag experi incom famili ccavg mname loanpredict mversion testflag fals environ dev transactionid sdf bcecbdbf timestamp requestbodi data ag experi incom famili ccavg mname loanpredict mversion testflag fals environ dev transactionid fcecacbfbc timestamp batch transform job config apivers aw amazon com kind batchtransformjob metadata generatenam batchtransform spec batchstrategi multirecord dataprocess joinsourc input outputfilt inputfilt requestbodi modelclientconfig invocationsmaxretri invocationstimeoutinsecond mname region west transforminput contenttyp applic json datasourc sdatasourc sdatatyp sprefix suri splittyp line transformoutput accept applic json assemblewith line kmskeyid soutputpath batch output transformresourc instancecount instancetyp batch transform job fail error batch transform data log log ephemer dev loanpredict usw dev loanpredict pipelin fbb bcaaff txt log ephemer dev loanpredict usw dev loanpredict pipelin fbb bcaaff txt bad request log ephemer dev loanpredict usw dev loanpredict pipelin fbb bcaaff txt fail decod json object extra data line column char observ issu occur provid batchstrategi multirecord manifest data process config dataprocess joinsourc input outputfilt inputfilt requestbodi note batchstrategi singlerecord aforement data process config work fine job succe question achiev success run batchstrategi multirecord aforement data process config success output batchstrategi singlerecord look like output predict environ dev transactionid sdf bcecbdbf mname loanpredict mversion requestbodi data ag ccavg experi famili incom testflag fals timestamp output predict environ dev transactionid fcecacbfbc mname loanpredict mversion requestbodi data ag ccavg experi famili incom testflag fals timestamp region option relev resourc arn option arn aw west transform job pipelin bat efbfbef",
        "Question_preprocessed_content":"batch transform job failur batchstrategi multirecord data process batch transform job fit record fit limit set input batch transform job batch transform job config batch transform job fail error batch transform log log bad request log fail decod json object extra data line column observ issu occur provid manifest data process config note aforement data process config work fine question achiev success run aforement data process config success output look like region option relev resourc arn option",
        "Question_gpt_summary_original":"The user is encountering a failure in their SageMaker batch transform job when using the <code>batchStrategy: MultiRecord<\/code> configuration along with certain data processing configurations. The error message indicates a failure to decode a JSON object due to extra data. The user is seeking a solution to successfully run the job with the <code>batchStrategy: MultiRecord<\/code> configuration and the aforementioned data processing configurations. The job runs successfully when using the <code>batchStrategy: SingleRecord<\/code> configuration.",
        "Question_gpt_summary":"user encount failur batch transform job batchstrategi multirecord configur certain data process configur error messag indic failur decod json object extra data user seek solut successfulli run job batchstrategi multirecord configur aforement data process configur job run successfulli batchstrategi singlerecord configur",
        "Answer_original_content":"input data json line format choos singlerecord batchstrategi contain receiv singl json payload bodi like us multirecord batch transform split json line input contain line exampl multipl record record sent contain shown contain abl handl input work error messag complain invalid json format read second row request notic suppli contenttyp accepttyp applic json instead applic jsonlin test contain handl multipl json line record singl invoc",
        "Answer_preprocessed_content":"input data json line format choos singlerecord batchstrategi contain receiv singl json payload bodi like us multirecord batch transform split json line input multipl record sent contain shown contain abl handl input work error messag complain invalid json format read second row request notic suppli instead test contain handl multipl json line record singl invoc",
        "Answer_gpt_summary_original":"Solution:\n- The user should ensure that their container can handle multiple JSON line records per single invocation when using the <code>batchStrategy: MultiRecord<\/code> configuration.\n- The user should also ensure that they are using the correct <code>ContentType<\/code> and <code>AcceptType<\/code> which should be <code>application\/jsonlines<\/code> instead of <code>application\/json<\/code>.",
        "Answer_gpt_summary":"solut user ensur contain handl multipl json line record singl invoc batchstrategi multirecord configur user ensur correct contenttyp accepttyp applic jsonlin instead applic json"
    },
    {
        "Question_title":"Custom container not running under root account?",
        "Question_body":"A customer wants to enforce these rules in their custom SageMaker containers:\n\n\u2022\tProcesses running inside a container must run with a known UID\/GUID and never as root.\n\u2022\tAvoid using privilege escalation methods that grant root access (e.g. sudo)\n\n\nHow do we ensure this?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607710724000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":76.0,
        "Answer_body":"SageMaker requires that Docker containers run without privileged access. See: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-toolkits.html SageMaker Docker containers do not run in Privileged mode and have the following Linux capabilities removed: SETPCAP, SETFCAP, NET_RAW, MKNOD",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYAkZepq4SgyArKZCC7gT_A\/custom-container-not-running-under-root-account",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-18T15:45:37.000Z",
                "Answer_score":0,
                "Answer_body":"SageMaker requires that Docker containers run without privileged access. See: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-toolkits.html SageMaker Docker containers do not run in Privileged mode and have the following Linux capabilities removed: SETPCAP, SETFCAP, NET_RAW, MKNOD",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1608306337000,
        "Question_original_content":"custom contain run root account custom want enforc rule custom contain process run insid contain run known uid guid root avoid privileg escal method grant root access sudo ensur",
        "Question_preprocessed_content":"custom contain run root account custom want enforc rule custom contain process run insid contain run known root avoid privileg escal method grant root access ensur",
        "Question_gpt_summary_original":"The user is facing the challenge of enforcing rules in their custom SageMaker containers to ensure that processes running inside the container do not run as root and avoid using privilege escalation methods that grant root access. They are seeking guidance on how to achieve this.",
        "Question_gpt_summary":"user face challeng enforc rule custom contain ensur process run insid contain run root avoid privileg escal method grant root access seek guidanc achiev",
        "Answer_original_content":"requir docker contain run privileg access http doc aw amazon com latest amazon toolkit html docker contain run privileg mode follow linux capabl remov setpcap setfcap net raw mknod",
        "Answer_preprocessed_content":"requir docker contain run privileg access docker contain run privileg mode follow linux capabl remov setpcap setfcap mknod",
        "Answer_gpt_summary_original":"Solution: The user can enforce rules in their custom SageMaker containers by ensuring that Docker containers run without privileged access. SageMaker Docker containers do not run in Privileged mode and have the following Linux capabilities removed: SETPCAP, SETFCAP, NET_RAW, MKNOD.",
        "Answer_gpt_summary":"solut user enforc rule custom contain ensur docker contain run privileg access docker contain run privileg mode follow linux capabl remov setpcap setfcap net raw mknod"
    },
    {
        "Question_title":"\u201cFailure Exception: OSError: [Errno 30] Read-only file system\u201d when using AzureML in Python Azure Function",
        "Question_body":"Issue\nI am trying prepare and then submit a new experiment to Azure Machine Learning from an Azure Function in Python. I therefore register a new dataset for my Azure ML workspace, which contains the training data for my ML model using dataset.register(.... However, when I try to create this dataset with the following line of code\n\n dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)\n\nthen I get a Failure Exception: OSError: [Errno 30] Read-only file system ....\n\nIdeas\n1. I know that I shouldn't write to the file system from within an Azure function if possible. But I actually don't want to write anything to the local file system. I only want to create the dataset as a reference to my blob storage under datastore_path and then register this to my Azure Machine Learning workspace. But it seems that the method from_delimited_files is trying to write to the file system anyway (maybe some caching?).\n2. I also know that there is a temp folder in which writing temporary files is permitted. However, I belive I cannot really control where this method is writing data. I already tried changing the current working directory to this temp folder just before the function call using os.chdir(tempfile.gettempdir()), but that didn't help.\n\nAny other ideas? I don't think I am doing something particularly unusually...\n\nDetails\nI am using python 3.7 and azureml-sdk 1.9.0 and I can run the python script locally without problems. I currently deploy from VSCode using the Azure Functions extension version 0.23.0 (and an Azure DevOps pipeline for CI\/CD).\n\nHere is my full stack trace:\n\n Microsoft.Azure.WebJobs.Host.FunctionInvocationException: Exception while executing function: Functions.HttpTrigger_Train\n  ---> Microsoft.Azure.WebJobs.Script.Workers.Rpc.RpcException: Result: Failure\n Exception: OSError: [Errno 30] Read-only file system: '\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/bin\/deps.lock'\n Stack:   File \"\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py\", line 345, in _handle__invocation_request\n     self.__run_sync_func, invocation_id, fi.func, args)\n   File \"\/usr\/local\/lib\/python3.7\/concurrent\/futures\/thread.py\", line 57, in run\n     result = self.fn(*self.args, **self.kwargs)\n   File \"\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py\", line 480, in __run_sync_func\n     return func(**params)\n   File \"\/home\/site\/wwwroot\/HttpTrigger_Train\/__init__.py\", line 11, in main\n     train()\n   File \"\/home\/site\/wwwroot\/shared_code\/train.py\", line 70, in train\n     dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/_loggerfactory.py\", line 126, in wrapper\n     return func(*args, **kwargs)\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/dataset_factory.py\", line 308, in from_delimited_files\n     quoting=support_multi_line)\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/readers.py\", line 100, in read_csv\n     df = Dataflow._path_to_get_files_block(path, archive_options)\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/dataflow.py\", line 2387, in _path_to_get_files_block\n     return datastore_to_dataflow(path)\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py\", line 41, in datastore_to_dataflow\n     datastore, datastore_value = get_datastore_value(source)\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py\", line 83, in get_datastore_value\n     _set_auth_type(workspace)\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py\", line 134, in _set_auth_type\n     get_engine_api().set_aml_auth(SetAmlAuthMessageArgument(AuthType.SERVICEPRINCIPAL, json.dumps(auth)))\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py\", line 18, in get_engine_api\n     _engine_api = EngineAPI()\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py\", line 55, in __init__\n     self._message_channel = launch_engine()\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/engine.py\", line 300, in launch_engine\n     dependencies_path = runtime.ensure_dependencies()\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py\", line 141, in ensure_dependencies\n     with _FileLock(deps_lock_path, raise_on_timeout=timeout_exception):\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py\", line 113, in __enter__\n     self.acquire()\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py\", line 72, in acquire\n     self.lockfile = os.open(self.lockfile_path, os.O_CREAT | os.O_EXCL | os.O_RDWR)\n    \n    at Microsoft.Azure.WebJobs.Script.Description.WorkerFunctionInvoker.InvokeCore(Object[] parameters, FunctionInvocationContext context) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/Workers\/WorkerFunctionInvoker.cs:line 85\n    at Microsoft.Azure.WebJobs.Script.Description.FunctionInvokerBase.Invoke(Object[] parameters) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/FunctionInvokerBase.cs:line 85\n    at Microsoft.Azure.WebJobs.Script.Description.FunctionGenerator.Coerce[T](Task`1 src) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/FunctionGenerator.cs:line 225\n    at Microsoft.Azure.WebJobs.Host.Executors.FunctionInvoker`2.InvokeAsync(Object instance, Object[] arguments) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionInvoker.cs:line 52\n    at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.InvokeAsync(IFunctionInvoker invoker, ParameterHelper parameterHelper, CancellationTokenSource timeoutTokenSource, CancellationTokenSource functionCancellationTokenSource, Boolean throwOnTimeout, TimeSpan timerInterval, IFunctionInstance instance) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 587\n    at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithWatchersAsync(IFunctionInstanceEx instance, ParameterHelper parameterHelper, ILogger logger, CancellationTokenSource functionCancellationTokenSource) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 532\n    at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, ParameterHelper parameterHelper, IFunctionOutputDefinition outputDefinition, ILogger logger, CancellationTokenSource functionCancellationTokenSource) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 470\n    at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, FunctionStartedMessage message, FunctionInstanceLogEntry instanceLogEntry, ParameterHelper parameterHelper, ILogger logger, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 278\n    --- End of inner exception stack trace ---\n    at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, FunctionStartedMessage message, FunctionInstanceLogEntry instanceLogEntry, ParameterHelper parameterHelper, ILogger logger, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 325\n    at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.TryExecuteAsyncCore(IFunctionInstanceEx functionInstance, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 117",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1597403151470,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"The issue was an incompatible OS version in my virtual environment.\n\nA huge thanks goes to PramodValavala-MSFT for his idea to create a docker container! Following his suggestion, I suddenly got the following error message for the dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths) command:\n\nException: NotImplementedError: Unsupported Linux distribution debian 10.\n\nwhich reminded me of the following warning in the azure machine learning documentation:\n\nChoosing the predefined docker image 2.0-python3.7 (running Debian 9) instead of 3.0-python3.7 (running Debian 10) solved the issue (see https:\/\/hub.docker.com\/_\/microsoft-azure-functions-python).\n\nI suspect that the default virtual environment, which I was using originally, also ran on an incompatible OS.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/67126\/failure-exception-oserror-errno-30-read-only-file.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-08-16T22:21:17.787Z",
                "Answer_score":1,
                "Answer_body":"The issue was an incompatible OS version in my virtual environment.\n\nA huge thanks goes to PramodValavala-MSFT for his idea to create a docker container! Following his suggestion, I suddenly got the following error message for the dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths) command:\n\nException: NotImplementedError: Unsupported Linux distribution debian 10.\n\nwhich reminded me of the following warning in the azure machine learning documentation:\n\nChoosing the predefined docker image 2.0-python3.7 (running Debian 9) instead of 3.0-python3.7 (running Debian 10) solved the issue (see https:\/\/hub.docker.com\/_\/microsoft-azure-functions-python).\n\nI suspect that the default virtual environment, which I was using originally, also ran on an incompatible OS.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-08-24T14:54:39.657Z",
                "Answer_score":0,
                "Answer_body":"getting this error while doing the operation:\n\nravikiran@Azure:~$ az group create -l eastus -n AZURE\nTraceback (most recent call last):\nFile \"\/opt\/az\/lib\/python3.6\/site-packages\/azure\/cli\/core\/_session.py\", line 47, in load\nself.save()\nFile \"\/opt\/az\/lib\/python3.6\/site-packages\/azure\/cli\/core\/_session.py\", line 65, in save\nwith codecs_open(self.filename, 'w', encoding=self._encoding) as f:\nFile \"\/usr\/bin\/..\/..\/opt\/az\/lib\/python3.6\/codecs.py\", line 897, in open\nfile = builtins.open(filename, mode, buffering)\nOSError: [Errno 30] Read-only file system: '\/home\/ravikiran\/.azure\/az.sess'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\nFile \"\/opt\/az\/lib\/python3.6\/runpy.py\", line 193, in run_module_as_main\n\"main\", mod_spec)\nFile \"\/opt\/az\/lib\/python3.6\/runpy.py\", line 85, in run_code\nexec(code, run_globals)\nFile \"\/opt\/az\/lib\/python3.6\/site-packages\/azure\/cli\/main.py\", line 38, in <module>\naz_cli = get_default_cli()\nFile \"\/opt\/az\/lib\/python3.6\/site-packages\/azure\/cli\/core\/init.py\", line 920, in get_default_cli\nhelp_cls=AzCliHelp)\nFile \"\/opt\/az\/lib\/python3.6\/site-packages\/azure\/cli\/core\/init.py\", line 82, in init\nSESSION.load(os.path.join(azure_folder, 'az.sess'), max_age=3600)\nFile \"\/opt\/az\/lib\/python3.6\/site-packages\/azure\/cli\/core\/_session.py\", line 61, in load\nself.save()\nFile \"\/opt\/az\/lib\/python3.6\/site-packages\/azure\/cli\/core\/_session.py\", line 65, in save\nwith codecs_open(self.filename, 'w', encoding=self._encoding) as f:\nFile \"\/usr\/bin\/..\/..\/opt\/az\/lib\/python3.6\/codecs.py\", line 897, in open\nfile = builtins.open(filename, mode, buffering)\nOSError: [Errno 30] Read-only file system: '\/home\/ravikiran\/.azure\/az.ses",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1597616477787,
        "Question_original_content":"failur except oserror errno read file python azur function issu try prepar submit new experi azur function python regist new dataset workspac contain train data model dataset regist try creat dataset follow line code dataset dataset tabular delimit file path datastor path failur except oserror errno read file idea know shouldn write file azur function possibl actual want write local file want creat dataset refer blob storag datastor path regist workspac method delimit file try write file mayb cach know temp folder write temporari file permit beliv control method write data tri chang current work directori temp folder function chdir tempfil gettempdir help idea think particularli unusu detail python sdk run python script local problem current deploi vscode azur function extens version azur devop pipelin stack trace microsoft azur webjob host functioninvocationexcept except execut function function httptrigger train microsoft azur webjob script worker rpc rpcexcept result failur except oserror errno read file home site root python packag lib site packag dotnetcor bin dep lock stack file azur function host worker python linux azur function worker dispatch line handl invoc request self run sync func invoc func arg file usr local lib python concurr futur thread line run result self self arg self kwarg file azur function host worker python linux azur function worker dispatch line run sync func return func param file home site root httptrigger train init line main train file home site root share code train line train dataset dataset tabular delimit file path datastor path file home site root python packag lib site packag data loggerfactori line wrapper return func arg kwarg file home site root python packag lib site packag data dataset factori line delimit file quot support multi line file home site root python packag lib site packag dataprep api reader line read csv dataflow path file block path archiv option file home site root python packag lib site packag dataprep api dataflow line path file block return datastor dataflow path file home site root python packag lib site packag dataprep api datastor helper line datastor dataflow datastor datastor valu datastor valu sourc file home site root python packag lib site packag dataprep api datastor helper line datastor valu set auth type workspac file home site root python packag lib site packag dataprep api datastor helper line set auth type engin api set aml auth setamlauthmessageargu authtyp serviceprincip json dump auth file home site root python packag lib site packag dataprep api engineapi api line engin api engin api engineapi file home site root python packag lib site packag dataprep api engineapi api line init self messag channel launch engin file home site root python packag lib site packag dataprep api engineapi engin line launch engin depend path runtim ensur depend file home site root python packag lib site packag dotnetcor runtim line ensur depend filelock dep lock path rais timeout timeout except file home site root python packag lib site packag dotnetcor runtim line enter self acquir file home site root python packag lib site packag dotnetcor runtim line acquir self lockfil open self lockfil path creat excl rdwr microsoft azur webjob script descript workerfunctioninvok invokecor object paramet functioninvocationcontext context src azur function host src webjob script descript worker workerfunctioninvok line microsoft azur webjob script descript functioninvokerbas invok object paramet src azur function host src webjob script descript functioninvokerbas line microsoft azur webjob script descript functiongener coerc task src src azur function host src webjob script descript functiongener line microsoft azur webjob host executor functioninvok invokeasync object instanc object argument project azur webjob sdk rqmt src microsoft azur webjob host executor functioninvok line microsoft azur webjob host executor functionexecutor invokeasync ifunctioninvok invok parameterhelp parameterhelp cancellationtokensourc timeouttokensourc cancellationtokensourc functioncancellationtokensourc boolean throwontimeout timespan timerinterv ifunctioninst instanc project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line microsoft azur webjob host executor functionexecutor executewithwatchersasync ifunctioninstanceex instanc parameterhelp parameterhelp ilogg logger cancellationtokensourc functioncancellationtokensourc project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line microsoft azur webjob host executor functionexecutor executewithloggingasync ifunctioninstanceex instanc parameterhelp parameterhelp ifunctionoutputdefinit outputdefinit ilogg logger cancellationtokensourc functioncancellationtokensourc project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line microsoft azur webjob host executor functionexecutor executewithloggingasync ifunctioninstanceex instanc functionstartedmessag messag functioninstancelogentri instancelogentri parameterhelp parameterhelp ilogg logger cancellationtoken cancellationtoken project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line end inner except stack trace microsoft azur webjob host executor functionexecutor executewithloggingasync ifunctioninstanceex instanc functionstartedmessag messag functioninstancelogentri instancelogentri parameterhelp parameterhelp ilogg logger cancellationtoken cancellationtoken project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line microsoft azur webjob host executor functionexecutor tryexecuteasynccor ifunctioninstanceex functioninst cancellationtoken cancellationtoken project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line",
        "Question_preprocessed_content":"failur except oserror file python azur function issu try prepar submit new experi azur function python regist new dataset workspac contain train data model try creat dataset follow line code dataset failur except oserror file idea know shouldn write file azur function possibl actual want write local file want creat dataset refer blob storag regist workspac method try write file know temp folder write temporari file permit beliv control method write data tri chang current work directori temp folder function help idea think particularli detail python sdk run python script local problem current deploi vscode azur function extens version stack trace except execut function result failur except oserror file stack file line arg file line run result file line return func file line main train file line train dataset file line wrapper return func file line file line file line return file line datastor file line file line file line engineapi file line file line file line file line file line acquir paramet functioninvocationcontext context paramet src instanc object argument invok parameterhelp parameterhelp cancellationtokensourc timeouttokensourc cancellationtokensourc functioncancellationtokensourc boolean throwontimeout timespan timerinterv ifunctioninst instanc instanc parameterhelp parameterhelp ilogg logger cancellationtokensourc functioncancellationtokensourc instanc parameterhelp parameterhelp ifunctionoutputdefinit outputdefinit ilogg logger cancellationtokensourc functioncancellationtokensourc instanc functionstartedmessag messag functioninstancelogentri instancelogentri parameterhelp parameterhelp ilogg logger cancellationtoken cancellationtoken end inner except stack trace instanc functionstartedmessag messag functioninstancelogentri instancelogentri parameterhelp parameterhelp ilogg logger cancellationtoken cancellationtoken functioninst cancellationtoken cancellationtoken",
        "Question_gpt_summary_original":"The user is encountering a \"Failure Exception: OSError: [Errno 30] Read-only file system\" error when trying to create a dataset reference to their blob storage using Azure Machine Learning from an Azure Function in Python. They have tried changing the current working directory to the temp folder, but it did not help. The user is using python 3.7 and azureml-sdk 1.9.0 and can run the python script locally without problems.",
        "Question_gpt_summary":"user encount failur except oserror errno read file error try creat dataset refer blob storag azur function python tri chang current work directori temp folder help user python sdk run python script local problem",
        "Answer_original_content":"issu incompat version virtual environ huge thank goe pramodvalavala msft idea creat docker contain follow suggest suddenli got follow error messag dataset dataset tabular delimit file path datastor path command except notimplementederror unsupport linux distribut debian remind follow warn document choos predefin docker imag python run debian instead python run debian solv issu http hub docker com microsoft azur function python suspect default virtual environ origin ran incompat",
        "Answer_preprocessed_content":"issu incompat version virtual environ huge thank goe idea creat docker contain follow suggest suddenli got follow error messag dataset command except notimplementederror unsupport linux distribut debian remind follow warn document choos predefin docker imag instead solv issu suspect default virtual environ origin ran incompat",
        "Answer_gpt_summary_original":"Solution: The user was able to solve the issue by creating a docker container and choosing the predefined docker image 2.0-python3.7 (running Debian 9) instead of 3.0-python3.7 (running Debian 10). This solved the issue as the default virtual environment, which the user was using originally, also ran on an incompatible OS.",
        "Answer_gpt_summary":"solut user abl solv issu creat docker contain choos predefin docker imag python run debian instead python run debian solv issu default virtual environ user origin ran incompat"
    },
    {
        "Question_title":"Trouble modifying and saving dvc data file which lives outside the repo",
        "Question_body":"<p>I have a dvc setup where the git\/dvc repo, the data-file directory and the dvc-cache directory are all peers.<br>\ne.g.<br>\nmy-test-repo<br>\ntest-files<br>\nmy-test-dvc-cache<br>\nare all directories at the same level<\/p>\n<p>All the files under test-files were added using dvc add test-files<\/p>\n<p>my-test-repo\/test-files.dvc contains this line:<br>\npath: \u2026\/test-files<\/p>\n<p>.\/dvc\/config contains this line<br>\n[cache]<br>\ndir = \u2026\/\u2026\/my-test-dvc-cache<\/p>\n<p>I did a dvc push and all the file went to the specified remote.<br>\nI did a git clone and a dvc pull (on several different machines) and all the files came down in the directory structure specified above.<br>\nCode which runs from the repo and uses the files works.<\/p>\n<p>However, when I modify a data  file, I am having trouble saving it.<br>\nI changed a file in the test-files directory and dvc status shows that test-files is modified.<br>\n$ dvc status<br>\nunit_test_input.dvc:<br>\nchanged outs:<br>\nmodified:           \u2026\\test-files<br>\nchanged checksum<\/p>\n<p>When I do dvc commit it gives this error message:<br>\n$ dvc commit<br>\nERROR: failed to commit - unable to commit changed stage: \u2018test-files.dvc\u2019. Use <code>-f|--force<\/code> to force.<\/p>\n<p>So I entered dvc commit -f and it complains about files outside of the repo.<br>\nI read that  it is ok to have files outside the repo and the original push and pull operations worked fine.<\/p>\n<p>ERROR: unexpected error - Cmd(\u2018git\u2019) failed due to: exit code(128)<br>\ncmdline: git ls-files C:\\test-files<br>\nstderr: \u2018fatal: C:\\test-files: \u2018C:\\test-files\u2019 is outside repository at \u2018C:\\test-files\/my-test-repo\u2019\u2019<\/p>\n<p>It seems like dvc doesn\u2019t care that the data files are outside the repo, but the commit command is try to perform git commands on those files and git doesn\u2019t like them being outside the repo.<\/p>\n<p>Is that what\u2019s going on?<br>\nIs there something I can do about it?<\/p>",
        "Question_answer_count":22,
        "Question_comment_count":0,
        "Question_creation_time":1594430779787,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":2152.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/rsl\">@RSL<\/a> !<\/p>\n<p>Could you also show <code>$ dvc version<\/code> output, please?<\/p>\n<p>Also, could you show full log <code>dvc commit -f -v<\/code>, please?<\/p>\n<p>We do support files outside of repo, but I can\u2019t say that we really recommend it, because there are quite a few caveats (like isolation if you are on a shared server) that make it a bit tricky to get right. Could you elaborate on why you can\u2019t store your file in the repo?<\/p>\n<p>As to the workaround, could you try <code>dvc repro unit_test_input.dvc<\/code>?<\/p>. <p>Thanks for the quick reply. Here is the version output:<br>\ndvc version<br>\nDVC version: 1.1.7<br>\nPython version: 3.7.5<br>\nPlatform: Windows-10-10.0.18362-SP0<br>\nBinary: True<br>\nPackage: exe<br>\nSupported remotes: azure, gdrive, gs, hdfs, http, https, s3, ssh, oss<br>\nCache: reflink - not supported, hardlink - supported, symlink - not supported<br>\nFilesystem type (cache directory): (\u2018NTFS\u2019, \u2018C:\\\u2019)<br>\nRepo: dvc, git<br>\nFilesystem type (workspace): (\u2018NTFS\u2019, \u2018C:\\\u2019)<\/p>. <p>$ dvc commit -f<br>\nERROR: unexpected error - Cmd(\u2018git\u2019) failed due to: exit code(128)<br>\ncmdline: git ls-files C:\\Algo\\unit_test_input<br>\nstderr: \u2018fatal: C:\\Algo\\unit_test_input: \u2018C:\\Algo\\unit_test_input\u2019 is outside repository at \u2018C:\/Algo\/sw_algo\u2019\u2019<\/p>. <p>Here is the output of trying to use repo<\/p>\n<p>$ dvc repro unit_test_input.dvc<br>\nVerifying data sources in stage: \u2018unit_test_input.dvc\u2019<br>\nERROR: failed to reproduce \u2018unit_test_input.dvc\u2019: Cmd(\u2018git\u2019) failed due to: exit code(128)<br>\ncmdline: git ls-files C:\\TestData_Zmirror\\Algo\\unit_test_input<br>\nstderr: \u2018fatal: C:\\TestData_Zmirror\\Algo\\unit_test_input: \u2018C:\\TestData_Zmirror\\Algo\\unit_test_input\u2019 is outside repository at \u2018C:\/TestData_Zmirror\/Algo\/sw_algo\u2019\u2019<\/p>\n<p>Having any troubles? Hit us up at <a href=\"https:\/\/dvc.org\/support\" rel=\"nofollow noopener\">https:\/\/dvc.org\/support<\/a>, we are always happy to help!<\/p>. <p><a class=\"mention\" href=\"\/u\/rsl\">@RSL<\/a> Could you please run <code>dvc commit -f -v<\/code> (notice the <code>-v<\/code>)?<\/p>. <p>In regards to why the data lives outside:<br>\nThe data set is pretty large, and we want to have more than one git\/dvc repo access the same data set.<br>\nOtherwise we would have to make copies of it.<br>\nFor example, we have a  Jenkins server with two jobs;<br>\nOne to run the dev version of the code, one to run the release version of the code.<br>\nSo they check out different branches to get different code, but both branches point to the same copy of the data, and we don\u2019t have to have separate copies of the same data.<\/p>. <pre><code class=\"lang-auto\">$ dvc commit -f -v\n2020-07-10 18:47:10,826 DEBUG: Trying to spawn '['C:\\\\Program Files (x86)\\\\DVC (Data Version Control)\\\\dvc.exe', 'daemon', '-q', 'updater']'\n2020-07-10 18:47:16,692 DEBUG: Spawned '['C:\\\\Program Files (x86)\\\\DVC (Data Version Control)\\\\dvc.exe', 'daemon', '-q', 'updater']'\n2020-07-10 18:47:16,699 DEBUG: fetched: [(3,)]\n2020-07-10 18:47:17,520 DEBUG: Path 'C:\\TestData_Zmirror\\Algo\\unit_test_input' inode '3166988651231289851'\n2020-07-10 18:47:17,520 DEBUG: fetched: [('d2deb874d59d018befb6a630a9080efb', '40165274755', '3e19d2d4a064727a5a3ee41091cdb433.dir', '1594430389307446784')]\n2020-07-10 18:47:17,521 DEBUG: Computed stage: 'unit_test_input.dvc' md5: 'None'\n2020-07-10 18:47:17,521 DEBUG: 'md5' of stage: 'unit_test_input.dvc' changed.\n2020-07-10 18:47:17,587 DEBUG: fetched: [(2,)]\n2020-07-10 18:47:17,596 ERROR: unexpected error - Cmd('git') failed due to: exit code(128)\n  cmdline: git ls-files C:\\TestData_Zmirror\\Algo\\unit_test_input\n  stderr: 'fatal: C:\\TestData_Zmirror\\Algo\\unit_test_input: 'C:\\TestData_Zmirror\\Algo\\unit_test_input' is outside repository at 'C:\/TestData_Zmirror\/Algo\/sw_algo''\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"dvc\\main.py\", line 53, in main\n  File \"dvc\\command\\commit.py\", line 22, in run\n  File \"dvc\\repo\\__init__.py\", line 36, in wrapper\n  File \"dvc\\repo\\commit.py\", line 45, in commit\n  File \"dvc\\stage\\__init__.py\", line 380, in save\n  File \"dvc\\stage\\__init__.py\", line 391, in save_outs\n  File \"dvc\\output\\base.py\", line 262, in save\n  File \"dvc\\output\\base.py\", line 239, in ignore\n  File \"dvc\\scm\\git\\__init__.py\", line 277, in is_tracked\n  File \"site-packages\\git\\cmd.py\", line 542, in &lt;lambda&gt;\n  File \"site-packages\\git\\cmd.py\", line 1005, in _call_process\n  File \"site-packages\\git\\cmd.py\", line 822, in execute\ngit.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\n  cmdline: git ls-files C:\\TestData_Zmirror\\Algo\\unit_test_input\n  stderr: 'fatal: C:\\TestData_Zmirror\\Algo\\unit_test_input: 'C:\\TestData_Zmirror\\Algo\\unit_test_input' is outside repository at 'C:\/TestData_Zmirror\/Algo\/sw_algo''\n------------------------------------------------------------\n<\/code><\/pre>. <blockquote>\n<p>The data set is pretty large, and we want to have more than one git\/dvc repo access the same data set.<\/p>\n<\/blockquote>\n<p>That seems like a bad idea, because if someone will run <code>dvc checkout<\/code>, your file (accessed by multiple users\/apps) will change as well. This is what I meant by isolation caveats above.<\/p>\n<p>You don\u2019t have to copy it, just use shared cache directory and a link type (symlink or hardlink) <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a> .<\/p>. <p>So each repo winds up containing links to the same shared cache directory.<br>\nCan the shared cache directory live outside the dvc\/git repo?<\/p>. <p>Sure, take a look at <code>dvc cache dir<\/code> command. And <a href=\"https:\/\/dvc.org\/doc\/use-cases\/shared-development-server\">https:\/\/dvc.org\/doc\/use-cases\/shared-development-server<\/a> . <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>. <p>Thanks, this is very helpful.<br>\nI would like to ask a couple of clarifying questions to make sure I\u2019ve got the right idea now.<\/p>\n<p>First question: Does this scenario sound correct?<br>\nWe place the data files in the repo, and run dvc add <br>\nThe files are replaced by links to the (outside) cache, and are put in a .gitignore file so git doesn\u2019t track them.<br>\nDo a dvc push to get the files in the remote.<br>\nDo a dvc pull to get them to another machine.<br>\nA parallel git repo can also do a pull, and it will receive links to the same external cache.<br>\nThis scenario is for running tests in parallel on a machine which does not modify data files.<br>\nThat seems pretty straightforward.<\/p>\n<p>Second question:<br>\nOn a developers machine, what sequence should a developer follow to modify and push a dvc data file?<\/p>. <ol>\n<li>That\u2019s correct <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\">\n<\/li>\n<li>Similar to a regular git workflow. Modify -&gt; <code>dvc add<\/code>(or <code>dvc repro<\/code>) -&gt; git add\/commit the resulting metafiles -&gt; dvc push -&gt; git push.<\/li>\n<\/ol>. <p>I looked at the shared development server page.<br>\nIt looks like we could keep the existing cache setup, and just put the data files inside the repo.<br>\nAm I right about this?<\/p>. <p>But if we\u2019re using links we need to follow special procedures for do the modification; is that correct?<\/p>. <p>Also, we\u2019re not using pipelines. The data is modified by hand at this time. What would repo do if you just modified a file?<\/p>. <aside class=\"quote no-group\" data-username=\"RSL\" data-post=\"14\" data-topic=\"435\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/r\/bc8723\/40.png\" class=\"avatar\"> RSL:<\/div>\n<blockquote>\n<p>It looks like we could keep the existing cache setup, and just put the data files inside the repo.<br>\nAm I right about this?<\/p>\n<\/blockquote>\n<\/aside>\n<p>I think so, yes.<\/p>\n<aside class=\"quote no-group\" data-username=\"RSL\" data-post=\"15\" data-topic=\"435\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/r\/bc8723\/40.png\" class=\"avatar\"> RSL:<\/div>\n<blockquote>\n<p>But if we\u2019re using links we need to follow special procedures for do the modification; is that correct?<\/p>\n<\/blockquote>\n<\/aside>\n<p>It depends on the type of link (e.g. for reflinks you don\u2019t need to do anything, but reflinks are not supported everywhere and specifically don\u2019t seem supported on your fs, judging by the <code>dvc version<\/code> output). But yes, symlinks and hardlinks will be read-only, so in order to modify them you need to <code>dvc unprotect<\/code> them (it pretty much creates a copy that is ready to be edited). The rest of the workflow is the same. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=10\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>\n<aside class=\"quote no-group\" data-username=\"RSL\" data-post=\"16\" data-topic=\"435\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/r\/bc8723\/40.png\" class=\"avatar\"> RSL:<\/div>\n<blockquote>\n<p>Also, we\u2019re not using pipelines. The data is modified by hand at this time. What would repo do if you just modified a file?<\/p>\n<\/blockquote>\n<\/aside>\n<p>It would see the changes, same as with git. You could restore the commited version with <code>dvc checkout<\/code>, for example.<\/p>. <p>Thanks for the help, this information is very useful.<br>\nWe\u2019re now keeping the data files in the repo but have left the cache files in a dir which is a peer to the repo, and I am able to modify data files, add data files, use dvc to commit and push the changes, and successfully pull them elsewhere.<\/p>. <p>This question is a bit off topic for the thread, but I\u2019m not sure it\u2019s worthy of a whole new thread.<br>\nIs there a simple way to ask for history of changes to specific files?<br>\nAll these files are under the auspices of one .dvc file, and when I invoke dvc diff HEAD HEAD~ (for example) it just tells me the dvc file changed, but not which data files changed.<\/p>. <blockquote>\n<p>Is there a simple way to ask for history of changes to specific files?<\/p>\n<\/blockquote>\n<p>There is no special command, but you can simply use <code>git log<\/code> for specific <code>.dvc<\/code> file. Would that work for you?<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/trouble-modifying-and-saving-dvc-data-file-which-lives-outside-the-repo\/435",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-11T01:33:00.757Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/rsl\">@RSL<\/a> !<\/p>\n<p>Could you also show <code>$ dvc version<\/code> output, please?<\/p>\n<p>Also, could you show full log <code>dvc commit -f -v<\/code>, please?<\/p>\n<p>We do support files outside of repo, but I can\u2019t say that we really recommend it, because there are quite a few caveats (like isolation if you are on a shared server) that make it a bit tricky to get right. Could you elaborate on why you can\u2019t store your file in the repo?<\/p>\n<p>As to the workaround, could you try <code>dvc repro unit_test_input.dvc<\/code>?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-11T01:35:27.371Z",
                "Answer_body":"<p>Thanks for the quick reply. Here is the version output:<br>\ndvc version<br>\nDVC version: 1.1.7<br>\nPython version: 3.7.5<br>\nPlatform: Windows-10-10.0.18362-SP0<br>\nBinary: True<br>\nPackage: exe<br>\nSupported remotes: azure, gdrive, gs, hdfs, http, https, s3, ssh, oss<br>\nCache: reflink - not supported, hardlink - supported, symlink - not supported<br>\nFilesystem type (cache directory): (\u2018NTFS\u2019, \u2018C:\\\u2019)<br>\nRepo: dvc, git<br>\nFilesystem type (workspace): (\u2018NTFS\u2019, \u2018C:\\\u2019)<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-11T01:38:05.849Z",
                "Answer_body":"<p>$ dvc commit -f<br>\nERROR: unexpected error - Cmd(\u2018git\u2019) failed due to: exit code(128)<br>\ncmdline: git ls-files C:\\Algo\\unit_test_input<br>\nstderr: \u2018fatal: C:\\Algo\\unit_test_input: \u2018C:\\Algo\\unit_test_input\u2019 is outside repository at \u2018C:\/Algo\/sw_algo\u2019\u2019<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-11T01:39:38.809Z",
                "Answer_body":"<p>Here is the output of trying to use repo<\/p>\n<p>$ dvc repro unit_test_input.dvc<br>\nVerifying data sources in stage: \u2018unit_test_input.dvc\u2019<br>\nERROR: failed to reproduce \u2018unit_test_input.dvc\u2019: Cmd(\u2018git\u2019) failed due to: exit code(128)<br>\ncmdline: git ls-files C:\\TestData_Zmirror\\Algo\\unit_test_input<br>\nstderr: \u2018fatal: C:\\TestData_Zmirror\\Algo\\unit_test_input: \u2018C:\\TestData_Zmirror\\Algo\\unit_test_input\u2019 is outside repository at \u2018C:\/TestData_Zmirror\/Algo\/sw_algo\u2019\u2019<\/p>\n<p>Having any troubles? Hit us up at <a href=\"https:\/\/dvc.org\/support\" rel=\"nofollow noopener\">https:\/\/dvc.org\/support<\/a>, we are always happy to help!<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-11T01:43:08.856Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/rsl\">@RSL<\/a> Could you please run <code>dvc commit -f -v<\/code> (notice the <code>-v<\/code>)?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-11T01:45:25.263Z",
                "Answer_body":"<p>In regards to why the data lives outside:<br>\nThe data set is pretty large, and we want to have more than one git\/dvc repo access the same data set.<br>\nOtherwise we would have to make copies of it.<br>\nFor example, we have a  Jenkins server with two jobs;<br>\nOne to run the dev version of the code, one to run the release version of the code.<br>\nSo they check out different branches to get different code, but both branches point to the same copy of the data, and we don\u2019t have to have separate copies of the same data.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-11T01:47:56.466Z",
                "Answer_body":"<pre><code class=\"lang-auto\">$ dvc commit -f -v\n2020-07-10 18:47:10,826 DEBUG: Trying to spawn '['C:\\\\Program Files (x86)\\\\DVC (Data Version Control)\\\\dvc.exe', 'daemon', '-q', 'updater']'\n2020-07-10 18:47:16,692 DEBUG: Spawned '['C:\\\\Program Files (x86)\\\\DVC (Data Version Control)\\\\dvc.exe', 'daemon', '-q', 'updater']'\n2020-07-10 18:47:16,699 DEBUG: fetched: [(3,)]\n2020-07-10 18:47:17,520 DEBUG: Path 'C:\\TestData_Zmirror\\Algo\\unit_test_input' inode '3166988651231289851'\n2020-07-10 18:47:17,520 DEBUG: fetched: [('d2deb874d59d018befb6a630a9080efb', '40165274755', '3e19d2d4a064727a5a3ee41091cdb433.dir', '1594430389307446784')]\n2020-07-10 18:47:17,521 DEBUG: Computed stage: 'unit_test_input.dvc' md5: 'None'\n2020-07-10 18:47:17,521 DEBUG: 'md5' of stage: 'unit_test_input.dvc' changed.\n2020-07-10 18:47:17,587 DEBUG: fetched: [(2,)]\n2020-07-10 18:47:17,596 ERROR: unexpected error - Cmd('git') failed due to: exit code(128)\n  cmdline: git ls-files C:\\TestData_Zmirror\\Algo\\unit_test_input\n  stderr: 'fatal: C:\\TestData_Zmirror\\Algo\\unit_test_input: 'C:\\TestData_Zmirror\\Algo\\unit_test_input' is outside repository at 'C:\/TestData_Zmirror\/Algo\/sw_algo''\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"dvc\\main.py\", line 53, in main\n  File \"dvc\\command\\commit.py\", line 22, in run\n  File \"dvc\\repo\\__init__.py\", line 36, in wrapper\n  File \"dvc\\repo\\commit.py\", line 45, in commit\n  File \"dvc\\stage\\__init__.py\", line 380, in save\n  File \"dvc\\stage\\__init__.py\", line 391, in save_outs\n  File \"dvc\\output\\base.py\", line 262, in save\n  File \"dvc\\output\\base.py\", line 239, in ignore\n  File \"dvc\\scm\\git\\__init__.py\", line 277, in is_tracked\n  File \"site-packages\\git\\cmd.py\", line 542, in &lt;lambda&gt;\n  File \"site-packages\\git\\cmd.py\", line 1005, in _call_process\n  File \"site-packages\\git\\cmd.py\", line 822, in execute\ngit.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\n  cmdline: git ls-files C:\\TestData_Zmirror\\Algo\\unit_test_input\n  stderr: 'fatal: C:\\TestData_Zmirror\\Algo\\unit_test_input: 'C:\\TestData_Zmirror\\Algo\\unit_test_input' is outside repository at 'C:\/TestData_Zmirror\/Algo\/sw_algo''\n------------------------------------------------------------\n<\/code><\/pre>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-11T01:48:06.789Z",
                "Answer_body":"<blockquote>\n<p>The data set is pretty large, and we want to have more than one git\/dvc repo access the same data set.<\/p>\n<\/blockquote>\n<p>That seems like a bad idea, because if someone will run <code>dvc checkout<\/code>, your file (accessed by multiple users\/apps) will change as well. This is what I meant by isolation caveats above.<\/p>\n<p>You don\u2019t have to copy it, just use shared cache directory and a link type (symlink or hardlink) <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a> .<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-11T01:51:24.666Z",
                "Answer_body":"<p>So each repo winds up containing links to the same shared cache directory.<br>\nCan the shared cache directory live outside the dvc\/git repo?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-11T01:52:56.857Z",
                "Answer_body":"<p>Sure, take a look at <code>dvc cache dir<\/code> command. And <a href=\"https:\/\/dvc.org\/doc\/use-cases\/shared-development-server\">https:\/\/dvc.org\/doc\/use-cases\/shared-development-server<\/a> . <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-11T02:03:41.712Z",
                "Answer_body":"<p>Thanks, this is very helpful.<br>\nI would like to ask a couple of clarifying questions to make sure I\u2019ve got the right idea now.<\/p>\n<p>First question: Does this scenario sound correct?<br>\nWe place the data files in the repo, and run dvc add <br>\nThe files are replaced by links to the (outside) cache, and are put in a .gitignore file so git doesn\u2019t track them.<br>\nDo a dvc push to get the files in the remote.<br>\nDo a dvc pull to get them to another machine.<br>\nA parallel git repo can also do a pull, and it will receive links to the same external cache.<br>\nThis scenario is for running tests in parallel on a machine which does not modify data files.<br>\nThat seems pretty straightforward.<\/p>\n<p>Second question:<br>\nOn a developers machine, what sequence should a developer follow to modify and push a dvc data file?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-11T02:07:04.275Z",
                "Answer_body":"<ol>\n<li>That\u2019s correct <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\">\n<\/li>\n<li>Similar to a regular git workflow. Modify -&gt; <code>dvc add<\/code>(or <code>dvc repro<\/code>) -&gt; git add\/commit the resulting metafiles -&gt; dvc push -&gt; git push.<\/li>\n<\/ol>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-11T02:08:10.648Z",
                "Answer_body":"<p>I looked at the shared development server page.<br>\nIt looks like we could keep the existing cache setup, and just put the data files inside the repo.<br>\nAm I right about this?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-11T02:09:26.645Z",
                "Answer_body":"<p>But if we\u2019re using links we need to follow special procedures for do the modification; is that correct?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-11T02:10:40.174Z",
                "Answer_body":"<p>Also, we\u2019re not using pipelines. The data is modified by hand at this time. What would repo do if you just modified a file?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-11T02:15:27.573Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"RSL\" data-post=\"14\" data-topic=\"435\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/r\/bc8723\/40.png\" class=\"avatar\"> RSL:<\/div>\n<blockquote>\n<p>It looks like we could keep the existing cache setup, and just put the data files inside the repo.<br>\nAm I right about this?<\/p>\n<\/blockquote>\n<\/aside>\n<p>I think so, yes.<\/p>\n<aside class=\"quote no-group\" data-username=\"RSL\" data-post=\"15\" data-topic=\"435\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/r\/bc8723\/40.png\" class=\"avatar\"> RSL:<\/div>\n<blockquote>\n<p>But if we\u2019re using links we need to follow special procedures for do the modification; is that correct?<\/p>\n<\/blockquote>\n<\/aside>\n<p>It depends on the type of link (e.g. for reflinks you don\u2019t need to do anything, but reflinks are not supported everywhere and specifically don\u2019t seem supported on your fs, judging by the <code>dvc version<\/code> output). But yes, symlinks and hardlinks will be read-only, so in order to modify them you need to <code>dvc unprotect<\/code> them (it pretty much creates a copy that is ready to be edited). The rest of the workflow is the same. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=10\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>\n<aside class=\"quote no-group\" data-username=\"RSL\" data-post=\"16\" data-topic=\"435\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/r\/bc8723\/40.png\" class=\"avatar\"> RSL:<\/div>\n<blockquote>\n<p>Also, we\u2019re not using pipelines. The data is modified by hand at this time. What would repo do if you just modified a file?<\/p>\n<\/blockquote>\n<\/aside>\n<p>It would see the changes, same as with git. You could restore the commited version with <code>dvc checkout<\/code>, for example.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-14T03:17:48.578Z",
                "Answer_body":"<p>Thanks for the help, this information is very useful.<br>\nWe\u2019re now keeping the data files in the repo but have left the cache files in a dir which is a peer to the repo, and I am able to modify data files, add data files, use dvc to commit and push the changes, and successfully pull them elsewhere.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-14T03:20:49.024Z",
                "Answer_body":"<p>This question is a bit off topic for the thread, but I\u2019m not sure it\u2019s worthy of a whole new thread.<br>\nIs there a simple way to ask for history of changes to specific files?<br>\nAll these files are under the auspices of one .dvc file, and when I invoke dvc diff HEAD HEAD~ (for example) it just tells me the dvc file changed, but not which data files changed.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-14T06:14:37.436Z",
                "Answer_body":"<blockquote>\n<p>Is there a simple way to ask for history of changes to specific files?<\/p>\n<\/blockquote>\n<p>There is no special command, but you can simply use <code>git log<\/code> for specific <code>.dvc<\/code> file. Would that work for you?<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"troubl modifi save data file live outsid repo setup git repo data file directori cach directori peer test repo test file test cach directori level file test file ad add test file test repo test file contain line path test file config contain line cach dir test cach push file went specifi remot git clone pull differ machin file came directori structur specifi code run repo us file work modifi data file have troubl save chang file test file directori statu show test file modifi statu unit test input chang out modifi test file chang checksum commit give error messag commit error fail commit unabl commit chang stage test file us forc forc enter commit complain file outsid repo read file outsid repo origin push pull oper work fine error unexpect error cmd git fail exit code cmdline git file test file stderr fatal test file test file outsid repositori test file test repo like doesnt care data file outsid repo commit command try perform git command file git doesnt like outsid repo what go",
        "Question_preprocessed_content":"troubl modifi save data file live outsid repo setup git repo directori cach directori peer directori level file ad add contain line path contain line cach dir push file went specifi remot git clone pull file came directori structur specifi code run repo us file work modifi data file have troubl save chang file directori statu show modifi statu chang out modifi chang checksum commit give error messag commit error fail commit unabl commit chang stage us forc enter commit complain file outsid repo read file outsid repo origin push pull oper work fine error unexpect error cmd fail exit code cmdline git stderr fatal outsid repositori like doesnt care data file outsid repo commit command try perform git command file git doesnt like outsid repo what go",
        "Question_gpt_summary_original":"The user is having trouble modifying and saving a dvc data file that is located outside the repo. The dvc setup includes a git\/dvc repo, a data-file directory, and a dvc-cache directory, all at the same level. The user is able to push and pull files successfully, but when attempting to modify and commit a data file, they receive an error message stating that the file is outside the repo. The user suspects that dvc doesn't care about the files being outside the repo, but the commit command is trying to perform git commands on those files, which git doesn't allow. The user is seeking advice on how to resolve this issue.",
        "Question_gpt_summary":"user have troubl modifi save data file locat outsid repo setup includ git repo data file directori cach directori level user abl push pull file successfulli attempt modifi commit data file receiv error messag state file outsid repo user suspect care file outsid repo commit command try perform git command file git allow user seek advic resolv issu",
        "Answer_original_content":"rsl version output log commit support file outsid repo recommend caveat like isol share server bit tricki right elabor store file repo workaround try repro unit test input thank quick repli version output version version python version platform window binari true packag ex support remot azur gdrive hdf http http ssh oss cach reflink support hardlink support symlink support filesystem type cach directori ntf repo git filesystem type workspac ntf commit error unexpect error cmd git fail exit code cmdline git file algo unit test input stderr fatal algo unit test input algo unit test input outsid repositori algo algo output try us repo repro unit test input verifi data sourc stage unit test input error fail reproduc unit test input cmd git fail exit code cmdline git file testdata zmirror algo unit test input stderr fatal testdata zmirror algo unit test input testdata zmirror algo unit test input outsid repositori testdata zmirror algo algo have troubl hit http org support happi help rsl run commit notic regard data live outsid data set pretti larg want git repo access data set copi exampl jenkin server job run dev version code run releas version code check differ branch differ code branch point copi data dont separ copi data commit debug try spawn program file data version control ex daemon updat debug spawn program file data version control ex daemon updat debug fetch debug path testdata zmirror algo unit test input inod debug fetch ddebddbefbaaefb eddaaaeecdb dir debug comput stage unit test input debug stage unit test input chang debug fetch error unexpect error cmd git fail exit code cmdline git file testdata zmirror algo unit test input stderr fatal testdata zmirror algo unit test input testdata zmirror algo unit test input outsid repositori testdata zmirror algo algo traceback recent file main line main file command commit line run file repo init line wrapper file repo commit line commit file stage init line save file stage init line save out file output base line save file output base line ignor file scm git init line track file site packag git cmd line file site packag git cmd line process file site packag git cmd line execut git exc gitcommanderror cmd git fail exit code cmdline git file testdata zmirror algo unit test input stderr fatal testdata zmirror algo unit test input testdata zmirror algo unit test input outsid repositori testdata zmirror algo algo data set pretti larg want git repo access data set like bad idea run checkout file access multipl user app chang meant isol caveat dont copi us share cach directori link type symlink hardlink http org doc user guid larg dataset optim repo wind contain link share cach directori share cach directori live outsid git repo sure look cach dir command http org doc us case share develop server thank help like ask coupl clarifi question sure iv got right idea question scenario sound correct place data file repo run add file replac link outsid cach gitignor file git doesnt track push file remot pull machin parallel git repo pull receiv link extern cach scenario run test parallel machin modifi data file pretti straightforward second question develop machin sequenc develop follow modifi push data file that correct similar regular git workflow modifi add repro git add commit result metafil push git push look share develop server page look like exist cach setup data file insid repo right link need follow special procedur modif correct pipelin data modifi hand time repo modifi file rsl look like exist cach setup data file insid repo right think ye rsl link need follow special procedur modif correct depend type link reflink dont need reflink support specif dont support judg version output ye symlink hardlink read order modifi need unprotect pretti creat copi readi edit rest workflow rsl pipelin data modifi hand time repo modifi file chang git restor commit version checkout exampl thank help inform us keep data file repo left cach file dir peer repo abl modifi data file add data file us commit push chang successfulli pull question bit topic thread sure worthi new thread simpl wai ask histori chang specif file file auspic file invok diff head head exampl tell file chang data file chang simpl wai ask histori chang specif file special command simpli us git log specif file work",
        "Answer_preprocessed_content":"output log support file outsid repo recommend caveat bit tricki right elabor store file repo workaround try thank quick repli version output version version python version platform binari true packag ex support remot azur gdrive hdf http http ssh oss cach reflink support hardlink support symlink support filesystem type repo git filesystem type commit error unexpect error cmd fail exit code cmdline git stderr fatal outsid repositori output try us repo repro verifi data sourc stage error fail reproduc cmd fail exit code cmdline git stderr fatal outsid repositori have troubl hit happi help run regard data live outsid data set pretti larg want git repo access data set copi exampl jenkin server job run dev version code run releas version code check differ branch differ code branch point copi data dont separ copi data data set pretti larg want git repo access data set like bad idea run file chang meant isol caveat dont copi us share cach directori link type repo wind contain link share cach directori share cach directori live outsid repo sure look command thank help like ask coupl clarifi question sure iv got right idea question scenario sound correct place data file repo run add file replac link cach gitignor file git doesnt track push file remot pull machin parallel git repo pull receiv link extern cach scenario run test parallel machin modifi data file pretti straightforward second question develop machin sequenc develop follow modifi push data file that correct similar regular git workflow modifi git result metafil push git push look share develop server page look like exist cach setup data file insid repo right link need follow special procedur modif correct pipelin data modifi hand time repo modifi file rsl look like exist cach setup data file insid repo right think ye rsl link need follow special procedur modif correct depend type link ye symlink hardlink order modifi need rest workflow rsl pipelin data modifi hand time repo modifi file chang git restor commit version exampl thank help inform us keep data file repo left cach file dir peer repo abl modifi data file add data file us commit push chang successfulli pull question bit topic thread sure worthi new thread simpl wai ask histori chang specif file file auspic file invok diff head head tell file chang data file chang simpl wai ask histori chang specif file special command simpli us specif file work",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Use shared cache directory and a link type (symlink or hardlink) to avoid isolation caveats.\n- Place the data files in the repo, and run dvc add. The files are replaced by links to the (outside) cache, and are put in a .gitignore file so git doesn\u2019t track them. Do a dvc push to get the files in the remote. Do a dvc pull to get them to another machine. A parallel git repo can also do a pull, and it will receive links to the same external cache.\n- Keep the existing cache setup, and just put the data files inside the repo.\n- Use git log for specific .dvc file to ask for",
        "Answer_gpt_summary":"possibl solut mention discuss us share cach directori link type symlink hardlink avoid isol caveat place data file repo run add file replac link outsid cach gitignor file git doesnt track push file remot pull machin parallel git repo pull receiv link extern cach exist cach setup data file insid repo us git log specif file ask"
    },
    {
        "Question_title":"Once a model is trained, can the DAG be re-used for prediction?",
        "Question_body":"<p>Hi,<\/p>\n<p>The examples I reviewed in the documentation seem to describe how to define, share and reproduce a <strong>train<\/strong> model pipeline. Once we are happy with our trained model and we want to move it into production, what would be the recommended approach to use DVC to ensure the pipeline consistency between <strong>train<\/strong> and <strong>predict<\/strong>?<\/p>\n<p>I would like to re-use the DVC pipeline defined for training (feature engineering, processing,\u2026) to ensure consistency and proper usage. On the other hand, the pipeline would also be somewhat different (each individual model script would \u201cpredict\u201d instead on \u201ctraining\u201d).<\/p>\n<p>Is there a recommended solution ? Should I create additional variables to tell each script whether to train or predict ? What to make of the metric at the end?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1569232040969,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":476.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jcrousse\">@jcrousse<\/a> !<\/p>\n<p>Thank you for your patience <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> We don\u2019t have any recommended approach for that yet, but your idea with additional vars (i imagine it would be some env var, right? e.g. <code>PREDICT=1 dvc repro<\/code>) should work. That would work nicely if for your pipeline it is just a matter of flicking a switch to go from training to prediction, otherwise it might get tricky and you would have to build a separate pipeline or somehow figureout the way to make current one work with the help of some additional flags. Are you mostly talking about using that in kind of a \u201cproduction\u201d setting? Or will it be something that you would want to keep in your project?<\/p>\n<p>Thanks,<br>\nRuslan<\/p>. <p>Thanks for the answer <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><br>\nYes the use case is exactly to use the pipeline in production.<br>\nOnce the model DAG becomes a bit complex, we would like to have only one DAG definition (the DVC one) and not to re-produce a similar DAG outside of DVC.<br>\nOtherwise there is too much or a risk to create mistakes or inconsistencies between the two.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/once-a-model-is-trained-can-the-dag-be-re-used-for-prediction\/228",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-09-24T06:55:41.671Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jcrousse\">@jcrousse<\/a> !<\/p>\n<p>Thank you for your patience <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> We don\u2019t have any recommended approach for that yet, but your idea with additional vars (i imagine it would be some env var, right? e.g. <code>PREDICT=1 dvc repro<\/code>) should work. That would work nicely if for your pipeline it is just a matter of flicking a switch to go from training to prediction, otherwise it might get tricky and you would have to build a separate pipeline or somehow figureout the way to make current one work with the help of some additional flags. Are you mostly talking about using that in kind of a \u201cproduction\u201d setting? Or will it be something that you would want to keep in your project?<\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-09-26T09:34:48.533Z",
                "Answer_body":"<p>Thanks for the answer <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><br>\nYes the use case is exactly to use the pipeline in production.<br>\nOnce the model DAG becomes a bit complex, we would like to have only one DAG definition (the DVC one) and not to re-produce a similar DAG outside of DVC.<br>\nOtherwise there is too much or a risk to create mistakes or inconsistencies between the two.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"model train dag predict exampl review document defin share reproduc train model pipelin happi train model want product recommend approach us ensur pipelin consist train predict like us pipelin defin train featur engin process ensur consist proper usag hand pipelin somewhat differ individu model script predict instead train recommend solut creat addit variabl tell script train predict metric end",
        "Question_preprocessed_content":"model train dag predict exampl review document defin share reproduc train model pipelin happi train model want product recommend approach us ensur pipelin consist train predict like pipelin defin train ensur consist proper usag hand pipelin somewhat differ recommend solut creat addit variabl tell script train predict metric end",
        "Question_gpt_summary_original":"The user is seeking advice on how to use DVC to ensure consistency between the training and prediction pipelines for a trained model. They want to reuse the DVC pipeline defined for training, but note that the prediction pipeline would be somewhat different. The user is asking for a recommended solution and whether they should create additional variables to tell each script whether to train or predict, and what to make of the metric at the end.",
        "Question_gpt_summary":"user seek advic us ensur consist train predict pipelin train model want reus pipelin defin train note predict pipelin somewhat differ user ask recommend solut creat addit variabl tell script train predict metric end",
        "Answer_original_content":"jcrouss thank patienc dont recommend approach idea addit var imagin env var right predict repro work work nice pipelin matter flick switch train predict tricki build separ pipelin figureout wai current work help addit flag talk kind product set want project thank ruslan thank answer ye us case exactli us pipelin product model dag bit complex like dag definit produc similar dag outsid risk creat mistak inconsist",
        "Answer_preprocessed_content":"thank patienc dont recommend approach idea addit var work work nice pipelin matter flick switch train predict tricki build separ pipelin figureout wai current work help addit flag talk kind product set want project thank ruslan thank answer ye us case exactli us pipelin product model dag bit complex like dag definit similar dag outsid risk creat mistak inconsist",
        "Answer_gpt_summary_original":"The discussion does not provide any specific solutions for the challenge. However, the user suggests using additional variables, such as environment variables, to switch between training and prediction pipelines. The discussion also mentions the possibility of building a separate pipeline or using additional flags to make the current pipeline work. The user's goal is to have only one DAG definition using DVC to avoid inconsistencies.",
        "Answer_gpt_summary":"discuss provid specif solut challeng user suggest addit variabl environ variabl switch train predict pipelin discuss mention possibl build separ pipelin addit flag current pipelin work user goal dag definit avoid inconsist"
    },
    {
        "Question_title":"Azure Machine Learning - Memory Error while creating dataframe",
        "Question_body":"<p>I am getting memory error while creating simple dataframe read from CSV file on Azure Machine Learning using notebook VM as compute instance. The VM has config of DS 13 56gb RAM, 8vcpu, 112gb storage on Ubuntu (Linux (ubuntu 16.04). CSV file is 5gb file. <\/p>\n\n<pre><code>blob_service = BlockBlobService(account_name,account_key)\nblobstring = blob_service.get_blob_to_text(container,filepath).content\ndffinaldata = pd.read_csv(StringIO(blobstring), sep=',')\n<\/code><\/pre>\n\n<p>What I am doing wrong here ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579544749467,
        "Question_favorite_count":null,
        "Question_last_edit_time":1579556126092,
        "Question_score":1.0,
        "Question_view_count":507.0,
        "Answer_body":"<p>you need to provide the right encoding when calling get_blob_to_text, please refer to the <a href=\"https:\/\/github.com\/Azure\/azure-storage-python\/blob\/master\/samples\/blob\/block_blob_usage.py#L390\" rel=\"nofollow noreferrer\">sample<\/a>.<\/p>\n\n<p>The code below is what  normally use for reading data file in blob storages. Basically, you can use blob\u2019s url along with sas token and use a request method. However, You might want to edit the \u2018for loop\u2019 depending what types of data you have (e.g. csv, jpg, and etc).<\/p>\n\n<p>-- Python code below --<\/p>\n\n<pre><code>import requests\nfrom azure.storage.blob import BlockBlobService, BlobPermissions\nfrom azure.storage.blob.baseblobservice import BaseBlobService\nfrom datetime import datetime, timedelta\n\naccount_name = '&lt;account_name&gt;'\naccount_key = '&lt;account_key&gt;'\ncontainer_name = '&lt;container_name&gt;'\n\nblob_service=BlockBlobService(account_name,account_key)\ngenerator = blob_service.list_blobs(container_name)\n\nfor blob in generator:\n    url = f\"https:\/\/{account_name}.blob.core.windows.net\/{container_name}\"\n    service = BaseBlobService(account_name=account_name, account_key=account_key)\n    token = service.generate_blob_shared_access_signature(container_name, img_name, permission=BlobPermissions.READ, expiry=datetime.utcnow() + timedelta(hours=1),)\n    url_with_sas = f\"{url}?{token}\"\n    response = requests.get(url_with_sas)\n<\/code><\/pre>\n\n<p>Please follow the below link to read data on Azure Blob Storage.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1579586200472,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59829017",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1579583849896,
        "Question_original_content":"memori error creat datafram get memori error creat simpl datafram read csv file notebook comput instanc config ram vcpu storag ubuntu linux ubuntu csv file file blob servic blockblobservic account account kei blobstr blob servic blob text contain filepath content dffinaldata read csv stringio blobstr sep wrong",
        "Question_preprocessed_content":"memori error creat datafram get memori error creat simpl datafram read csv file notebook comput instanc config ram vcpu storag ubuntu csv file file wrong",
        "Question_gpt_summary_original":"The user is encountering a memory error while creating a dataframe from a 5GB CSV file on Azure Machine Learning using a notebook VM as a compute instance with a configuration of DS 13 56gb RAM, 8vcpu, 112gb storage on Ubuntu (Linux (ubuntu 16.04). The user is seeking assistance in identifying the issue.",
        "Question_gpt_summary":"user encount memori error creat datafram csv file notebook comput instanc configur ram vcpu storag ubuntu linux ubuntu user seek assist identifi issu",
        "Answer_original_content":"need provid right encod call blob text refer sampl code normal us read data file blob storag basic us blob url sa token us request method want edit loop depend type data csv jpg python code import request azur storag blob import blockblobservic blobpermiss azur storag blob baseblobservic import baseblobservic datetim import datetim timedelta account account kei contain blob servic blockblobservic account account kei gener blob servic list blob contain blob gener url http account blob core window net contain servic baseblobservic account account account kei account kei token servic gener blob share access signatur contain img permiss blobpermiss read expiri datetim utcnow timedelta hour url sa url token respons request url sa follow link read data azur blob storag http doc microsoft com azur machin learn access data",
        "Answer_preprocessed_content":"need provid right encod call refer sampl code normal us read data file blob storag basic us blob url sa token us request method want edit loop depend type data python code follow link read data azur blob storag",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the challenge by suggesting that the user needs to provide the right encoding when calling get_blob_to_text. The discussion also provides a Python code that can be used for reading data files in blob storages. The code uses blob's URL along with SAS token and a request method. However, the user might need to edit the 'for loop' depending on the types of data they have. The discussion also provides a link to the Microsoft documentation on how to access data on Azure Blob Storage.",
        "Answer_gpt_summary":"solut discuss provid solut challeng suggest user need provid right encod call blob text discuss provid python code read data file blob storag code us blob url sa token request method user need edit loop depend type data discuss provid link microsoft document access data azur blob storag"
    },
    {
        "Question_title":"Vertex AI batch predictions from file-list",
        "Question_body":"<p>I want to submit batch prediction job for a custom model (in my case it is torch model, but I think this is irrelevant in this case). So I read the documentation:\n<a href=\"https:\/\/i.stack.imgur.com\/WGa7T.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WGa7T.png\" alt=\"batch prediction from file-list\" \/><\/a><\/p>\n<p>But as there are no examples I cannot be sure what the schema of the json object which vertex ai will send to my model will be. Does someone have made this work ?<\/p>\n<p>My best guess is that the request will be with the following body:<\/p>\n<pre><code>{'instance' : &lt;b64-encoded-content-of-the-file&gt;}\n<\/code><\/pre>\n<p>But when I read the documentation (for other 'features' of vertex ai) I could imagine the following body as well:<\/p>\n<pre><code>{'instance': {'b64' : &lt;b64-encoded-content-of-the-file&gt;}}\n<\/code><\/pre>\n<p>Does somebody actually know ?<\/p>\n<p>Another thing I did is to make a 'fake-model' which returns the request it gets ... when I submit the batch-prediction job it actually finishes successfully but when I check the output file it is empty ... so ... I actually need help\/more time to think of other ways to decipher vertex ai docs.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625948141620,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":611.0,
        "Answer_body":"<p>Vertex AI <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#request_requirements\" rel=\"nofollow noreferrer\">custom container<\/a> should wrap a service with an endpoint (predict) for receiving a list of instances, each is a json serializable object<\/p>\n<pre><code>{'instances': [{'b64' : &lt;b64-encoded-content-of-the-file1&gt;}, {'b64' : &lt;b64-encoded-content-of-the-file1&gt;}, ...]}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68331232",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1626781802423,
        "Question_original_content":"batch predict file list want submit batch predict job custom model case torch model think irrelev case read document exampl sure schema json object send model work best guess request follow bodi instanc read document featur imagin follow bodi instanc somebodi actual know thing fake model return request get submit batch predict job actual finish successfulli check output file actual need help time think wai deciph doc thank advanc",
        "Question_preprocessed_content":"batch predict want submit batch predict job custom model read document exampl sure schema json object send model work best guess request follow bodi read document imagin follow bodi somebodi actual know thing return request get submit job actual finish successfulli check output file actual need time think wai deciph doc thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges in submitting a batch prediction job for a custom model in Vertex AI due to the lack of examples and uncertainty about the schema of the JSON object that Vertex AI will send to the model. The user's best guess is that the request will be with the body containing the b64-encoded content of the file, but there is also a possibility that the body will contain a nested JSON object. The user attempted to create a fake model to decipher the Vertex AI docs but encountered an issue where the output file was empty.",
        "Question_gpt_summary":"user face challeng submit batch predict job custom model lack exampl uncertainti schema json object send model user best guess request bodi contain encod content file possibl bodi contain nest json object user attempt creat fake model deciph doc encount issu output file",
        "Answer_original_content":"custom contain wrap servic endpoint predict receiv list instanc json serializ object instanc",
        "Answer_preprocessed_content":"custom contain wrap servic endpoint receiv list instanc json serializ object",
        "Answer_gpt_summary_original":"Solution: The solution provided in the discussion is that the custom container in Vertex AI should wrap a service with an endpoint (predict) for receiving a list of instances, each of which is a JSON serializable object. The JSON object should contain a list of instances, and each instance should contain the b64-encoded content of the file.",
        "Answer_gpt_summary":"solut solut provid discuss custom contain wrap servic endpoint predict receiv list instanc json serializ object json object contain list instanc instanc contain encod content file"
    },
    {
        "Question_title":"How do I make this IAM role error in aws sagemaker go away?",
        "Question_body":"<p>I suspect this has to more to do with IAM roles than Sagemaker.<\/p>\n\n<p>I'm following the example <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"noreferrer\">here<\/a><\/p>\n\n<p>Specifically, when it makes this call<\/p>\n\n<pre><code>tf_estimator.fit('s3:\/\/bucket\/path\/to\/training\/data')\n<\/code><\/pre>\n\n<p>I get this error<\/p>\n\n<pre><code>ClientError: An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:sts::013772784144:assumed-role\/AmazonSageMaker-ExecutionRole-20181022T195630\/SageMaker is not authorized to perform: iam:GetRole on resource: role SageMakerRole\n<\/code><\/pre>\n\n<p>My notebook instance has an IAM role attached to it.\nThat role has the <code>AmazonSageMakerFullAccess<\/code> policy. It also has a custom policy that looks like this<\/p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:GetObject\",\n            \"s3:PutObject\",\n            \"s3:DeleteObject\",\n            \"s3:ListBucket\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::*\"\n        ]\n    }\n]\n<\/code><\/pre>\n\n<p>}<\/p>\n\n<p>My input files and .py script is in an s3 bucket with the phrase <code>sagemaker<\/code> in it.<\/p>\n\n<p>What else am I missing?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1542853625410,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":6.0,
        "Question_view_count":8160.0,
        "Answer_body":"<p>If you're running the example code on a SageMaker notebook instance, you can use the execution_role which has the <code>AmazonSageMakerFullAccess<\/code> attached.<\/p>\n<pre><code>from sagemaker import get_execution_role\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n<\/code><\/pre>\n<p>And you can pass this role when initializing <code>tf_estimator<\/code>.\nYou can check out the example <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html\" rel=\"nofollow noreferrer\">here<\/a> for using <code>execution_role<\/code> with S3 on notebook instance.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1620293748347,
        "Answer_score":8.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53423061",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1543010904776,
        "Question_original_content":"iam role error awai suspect iam role follow exampl specif make estim fit bucket path train data error clienterror error occur accessdeni call getrol oper user arn aw st assum role amazon executionrol author perform iam getrol resourc role role notebook instanc iam role attach role amazonfullaccess polici custom polici look like version statement effect allow action getobject putobject deleteobject listbucket resourc arn aw input file script bucket phrase miss",
        "Question_preprocessed_content":"iam role error awai suspect iam role follow exampl specif make error notebook instanc iam role attach role polici custom polici look like input file script bucket phrase miss",
        "Question_gpt_summary_original":"The user is encountering an IAM role error when trying to run a call in AWS Sagemaker. The error message indicates that the user's role is not authorized to perform the GetRole operation on the SageMakerRole resource. The user's notebook instance has an IAM role attached to it with the AmazonSageMakerFullAccess policy and a custom policy that allows access to S3 resources. The user's input files and .py script are in an S3 bucket with the phrase \"sagemaker\" in it. The user is unsure of what else they may be missing.",
        "Question_gpt_summary":"user encount iam role error try run error messag indic user role author perform getrol oper role resourc user notebook instanc iam role attach amazonfullaccess polici custom polici allow access resourc user input file script bucket phrase user unsur miss",
        "Answer_original_content":"run exampl code notebook instanc us execut role amazonfullaccess attach import execut role session session role execut role pass role initi estim check exampl execut role notebook instanc",
        "Answer_preprocessed_content":"run exampl code notebook instanc us attach pass role initi check exampl notebook instanc",
        "Answer_gpt_summary_original":"Solution:\n- Use the `execution_role` with `AmazonSageMakerFullAccess` attached when running the example code on a SageMaker notebook instance.\n- Pass this role when initializing `tf_estimator`.\n- Check out the example provided in the link for using `execution_role` with S3 on a notebook instance.",
        "Answer_gpt_summary":"solut us execut role amazonfullaccess attach run exampl code notebook instanc pass role initi estim check exampl provid link execut role notebook instanc"
    },
    {
        "Question_title":"How to add r2 and adj r2 metric in linear regression model - AzureML Studio?",
        "Question_body":"I have trained a linear regression model in AzureML studio which was created in designer as pipeline.\n\nI could not able to see R square and adj-R square metric in Evaluate Model step.\n\nCould any throw thoughts how can I add these 2 metrics to my trained model\n\n\n\n\n\n\n\n\n\nThanks\nBhaskar",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1618843927707,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello,\n\nSorry for the confusing. Actually, Coefficient of determination, often referred to as R2, represents the predictive power of the model as a value between 0 and 1. Zero means the model is random (explains nothing); 1 means there is a perfect fit. However, caution should be used in interpreting R2 values, as low values can be entirely normal and high values can be suspect in Azure Machine Learning Designer.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/evaluate-model#metrics-for-regression-models\n\nRegards,\nYutong",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/362850\/how-to-add-r2-metric-in-linear-regression-model-az.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-20T00:19:19.74Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nSorry for the confusing. Actually, Coefficient of determination, often referred to as R2, represents the predictive power of the model as a value between 0 and 1. Zero means the model is random (explains nothing); 1 means there is a perfect fit. However, caution should be used in interpreting R2 values, as low values can be entirely normal and high values can be suspect in Azure Machine Learning Designer.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/evaluate-model#metrics-for-regression-models\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1618877959740,
        "Question_original_content":"add adj metric linear regress model studio train linear regress model studio creat design pipelin abl squar adj squar metric evalu model step throw thought add metric train model thank bhaskar",
        "Question_preprocessed_content":"add adj metric linear regress model studio train linear regress model studio creat design pipelin abl squar squar metric evalu model step throw thought add metric train model thank bhaskar",
        "Question_gpt_summary_original":"The user is facing a challenge in adding R square and adj-R square metric to their trained linear regression model in AzureML Studio. They are seeking advice on how to add these metrics to their model.",
        "Question_gpt_summary":"user face challeng ad squar adj squar metric train linear regress model studio seek advic add metric model",
        "Answer_original_content":"hello sorri confus actual coeffici determin refer repres predict power model valu zero mean model random explain mean perfect fit caution interpret valu low valu entir normal high valu suspect design http doc microsoft com azur machin learn algorithm modul refer evalu model metric regress model regard yutong",
        "Answer_preprocessed_content":"hello sorri confus actual coeffici determin refer repres predict power model valu zero mean model random mean perfect fit caution interpret valu low valu entir normal high valu suspect design regard yutong",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the challenge by suggesting the use of Coefficient of determination, also known as R2, to represent the predictive power of the model. The user is advised to refer to the Azure Machine Learning Designer documentation for more information on how to add this metric to their trained linear regression model.",
        "Answer_gpt_summary":"solut discuss provid solut challeng suggest us coeffici determin known repres predict power model user advis refer design document inform add metric train linear regress model"
    },
    {
        "Question_title":"Model Deployment Issues",
        "Question_body":"Hello,\n\n\nI've been trying to deploy a scikit-learn model (a super simple classification using the Iris data set) that I built and trained using the standard MLproject protocol. I have been trying to deploy in two different manners and have been getting errors both ways:\n\n\nServe the model using a local REST server\n\nTo serve the model, I have been using the following command:\nmlflow models serve -m mlruns\/0\/f7cad9db15134c2abaa6d2a8b208c505\/artifacts\/sk_models -h **.***.**.** -p 1234 --no-conda\nNOTE: The host flag is the correct IP, I just masked it for this post\n\nWhen I run this command, I get the following output:\n2019\/09\/03 14:36:16 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n2019\/09\/03 14:36:16 INFO mlflow.pyfunc.backend: === Running command 'gunicorn --timeout=60 -b **.***.**.**:1234 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'\nbash: gunicorn: command not found\n\nI have ensured that gunicorn is in fact installed:\nsudo gunicorn --version\ngunicorn (version 19.9.0)\n\nIs this an issue that anyone else has run into?\n\n\nDeploy the model to Sagemaker\n\nTo deploy onto Sagemaker, I built the model using MLProject. The docker image that I have uploaded to ECR is the image I used to run the project and generate the model. When I attempt to deploy the model, I am using the following python script (the XXXX are personal info I removed for the post):\n\nimport mlflow.sagemaker as mfs\n\n\nrun_id = '0'\nexperiment_id = 'f7cad9db15134c2abaa6d2a8b208c505'\nregion = 'us-east-1'\naws_id = 'XXXXXXX'\narn = 'XXXXXXXXXX'\nimage_url = 'XXXXXXX\/mlflow-sklearn-test:latest'\napp_name = 'iris-dt-1'\nmodel_uri = 'mlruns\/%s\/%s\/artifacts\/sk_models' % (run_id, experiment_id)\n\n\nmfs.deploy(app_name=app_name, model_uri=model_uri, region_name=region, mode='create', execution_role_arn=arn, image_url=image_url)\n\n\nWhen I run the script, I get the following error:\n\u00a0\n2019\/09\/03 14:50:53 INFO mlflow.sagemaker: Creating new endpoint with name: iris-dt-1 ...\nTraceback (most recent call last):\n\u00a0 File \"sagemaker_deployment.py\", line 12, in <module>\n\u00a0 \u00a0 mfs.deploy(app_name=app_name, model_uri=model_uri, region_name=region, mode='create', execution_role_arn=arn, image_url=image_url)\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/mlflow\/sagemaker\/__init__.py\", line 325, in deploy\n\u00a0 \u00a0 role=execution_role_arn, sage_client=sage_client)\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/mlflow\/sagemaker\/__init__.py\", line 628, in _create_sagemaker_endpoint\n\u00a0 \u00a0 sage_client=sage_client)\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/mlflow\/sagemaker\/__init__.py\", line 840, in _create_sagemaker_model\n\u00a0 \u00a0 model_response = sage_client.create_model(**create_model_args)\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/botocore\/client.py\", line 357, in _api_call\n\u00a0 \u00a0 return self._make_api_call(operation_name, kwargs)\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/botocore\/client.py\", line 661, in _make_api_call\n\u00a0 \u00a0 raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateModel operation: ECR image \"XXXXXXX\/mlflow-sklearn-test:latest\" is invalid.\n\n\nHas anyone experienced this error before? I have tried googling the answer and the only answer I could find was to add the\u00a0:latest\u00a0tag to the image URI, but I have already done this and I still get the error.\n\n\n\n\nThank you so much!!",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1567508364000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":41.0,
        "Answer_body":"Hey\u00a0Melanie,\u00a0\n\n\nFor (1) My only long-shot thought would be that maybe\u00a0its a python versioning issue, Could MLFlow be running on Python 3 and gunicorn be install on 2? Shouldn't impact bash, but it could be a problem. There's also a random suggestion to install gunicorn from source.\u00a0 Are you spinning that up in a virtual\u00a0environment just to control for all of the dependencies.\n\n\n\n\nFor (2) I think this is an issue we dealt with, two things to try:\nin ECR, when you go into the repository in the console, is \"latest\" the image tag that is listed? Are there any others? If not, I'd maybe try pushing a new image to the same repository with a new tag, and then reference that tag just to confirm.\nAlthough this doesn't explicitly state anything about permissions, there's a chance it's a sneaky IAM issue. In the CreateModel docs\u00a0(which mlflow is calling), it mentions that the user who is calling this endpoint must have permissions to assume the role to provide access to the artifacts or containers. This means the account that boto3 is attached to in python should have an sts:assumerole permission that assume a role with access to ECR (this is probably the right permission). I've attached the JSON example that you can give you could add to account to test the assume role permission.\u00a0\n\n\nLet me know if any of this helps!\n\n\n-Adam\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/36db1100-ba72-48f2-90b8-faa55c8e560b%40googlegroups.com.. Hi Melanie,\n\n\nWere you able to confirm that the ECR image URI you're referencing is listed in the ECR console? Did Adam's ECR suggestion help you resolve the issue?\n\n\nBest,\n\n\nCorey\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CAHbUTHtxn%3D382DaJMCgbn9gqOrQ8C-K8Re%2BJsg7XpiL92c0BDA%40mail.gmail.com.. Sorry for the slow reply. It turns out the issues I was having was due to the ECR permissions in place at the enterprise I work at. With some help of the AWS team, I was able to get all of those permissions squared away and everything is now working.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/QPGFSApJ4Io",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-09-04T00:48:20",
                "Answer_body":"Hey\u00a0Melanie,\u00a0\n\n\nFor (1) My only long-shot thought would be that maybe\u00a0its a python versioning issue, Could MLFlow be running on Python 3 and gunicorn be install on 2? Shouldn't impact bash, but it could be a problem. There's also a random suggestion to install gunicorn from source.\u00a0 Are you spinning that up in a virtual\u00a0environment just to control for all of the dependencies.\n\n\n\n\nFor (2) I think this is an issue we dealt with, two things to try:\nin ECR, when you go into the repository in the console, is \"latest\" the image tag that is listed? Are there any others? If not, I'd maybe try pushing a new image to the same repository with a new tag, and then reference that tag just to confirm.\nAlthough this doesn't explicitly state anything about permissions, there's a chance it's a sneaky IAM issue. In the CreateModel docs\u00a0(which mlflow is calling), it mentions that the user who is calling this endpoint must have permissions to assume the role to provide access to the artifacts or containers. This means the account that boto3 is attached to in python should have an sts:assumerole permission that assume a role with access to ECR (this is probably the right permission). I've attached the JSON example that you can give you could add to account to test the assume role permission.\u00a0\n\n\nLet me know if any of this helps!\n\n\n-Adam\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/36db1100-ba72-48f2-90b8-faa55c8e560b%40googlegroups.com."
            },
            {
                "Answer_creation_time":"2019-09-13T19:05:15",
                "Answer_body":"Hi Melanie,\n\n\nWere you able to confirm that the ECR image URI you're referencing is listed in the ECR console? Did Adam's ECR suggestion help you resolve the issue?\n\n\nBest,\n\n\nCorey\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CAHbUTHtxn%3D382DaJMCgbn9gqOrQ8C-K8Re%2BJsg7XpiL92c0BDA%40mail.gmail.com."
            },
            {
                "Answer_creation_time":"2019-09-13T20:05:54",
                "Answer_body":"Sorry for the slow reply. It turns out the issues I was having was due to the ECR permissions in place at the enterprise I work at. With some help of the AWS team, I was able to get all of those permissions squared away and everything is now working."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"model deploy issu hello try deploi scikit learn model super simpl classif iri data set built train standard mlproject protocol try deploi differ manner get error wai serv model local rest server serv model follow command model serv mlrun fcaddbcabaadabc artifact model conda note host flag correct mask post run command follow output info model cli select backend flavor python function info pyfunc backend run command gunicorn timeout gunicorn cmd arg pyfunc score server wsgi app bash gunicorn command ensur gunicorn fact instal sudo gunicorn version gunicorn version issu run deploi model deploi built model mlproject docker imag upload ecr imag run project gener model attempt deploi model follow python script person info remov post import mf run experi fcaddbcabaadabc region east aw arn imag url sklearn test latest app iri model uri mlrun artifact model run experi mf deploi app app model uri model uri region region mode creat execut role arn arn imag url imag url run script follow error info creat new endpoint iri traceback recent file deploy line mf deploi app app model uri model uri region region mode creat execut role arn arn imag url imag url file usr local lib python site packag init line deploi role execut role arn sage client sage client file usr local lib python site packag init line creat endpoint sage client sage client file usr local lib python site packag init line creat model model respons sage client creat model creat model arg file usr local lib python site packag botocor client line api return self api oper kwarg file usr local lib python site packag botocor client line api rais error class pars respons oper botocor except clienterror error occur validationexcept call createmodel oper ecr imag sklearn test latest invalid experienc error tri googl answer answer add latesttag imag uri error thank",
        "Question_preprocessed_content":"model deploy issu hello try deploi model built train standard mlproject protocol try deploi differ manner get error wai serv model local rest server serv model follow command model serv note host flag correct mask post run command follow output info select backend flavor info run command gunicorn bash gunicorn command ensur gunicorn fact instal sudo gunicorn gunicorn issu run deploi model deploi built model mlproject docker imag upload ecr imag run project gener model attempt deploi model follow python script import mf cad abaa region arn mode creat run script follow error info creat new endpoint traceback file line mode creat file line deploi file line file line file line return kwarg file line rais error occur call createmodel oper ecr imag invalid experienc error tri googl answer answer add latesttag imag uri error thank",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a scikit-learn model using the MLproject protocol. They have encountered errors while trying to serve the model using a local REST server and while trying to deploy the model to Sagemaker. The errors include \"gunicorn: command not found\" and \"ECR image is invalid\". The user has confirmed that gunicorn is installed and has added the \":latest\" tag to the image URI, but the errors persist.",
        "Question_gpt_summary":"user face challeng deploi scikit learn model mlproject protocol encount error try serv model local rest server try deploi model error includ gunicorn command ecr imag invalid user confirm gunicorn instal ad latest tag imag uri error persist",
        "Answer_original_content":"heymelani long shot thought maybeit python version issu run python gunicorn instal shouldn impact bash problem random suggest instal gunicorn sourc spin virtualenviron control depend think issu dealt thing try ecr repositori consol latest imag tag list mayb try push new imag repositori new tag refer tag confirm explicitli state permiss chanc sneaki iam issu createmodel doc call mention user call endpoint permiss assum role provid access artifact contain mean account boto attach python st assumerol permiss assum role access ecr probabl right permiss attach json exampl add account test assum role permiss let know help adam receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user faaceb googlegroup com melani abl confirm ecr imag uri referenc list ecr consol adam ecr suggest help resolv issu best corei view discuss web visit http group googl com msgid user cahbuthtxn ddajmcgbngqorqc kre bjsgxpilcbda mail gmail com sorri slow repli turn issu have ecr permiss place enterpris work help aw team abl permiss squar awai work",
        "Answer_preprocessed_content":"heymelani thought maybeit python version issu run python gunicorn instal shouldn impact bash problem random suggest instal gunicorn sourc spin virtualenviron control depend think issu dealt thing try ecr repositori consol latest imag tag list mayb try push new imag repositori new tag refer tag confirm explicitli state permiss chanc sneaki iam issu createmodel doc mention user call endpoint permiss assum role provid access artifact contain mean account boto attach python st assumerol permiss assum role access ecr attach json exampl add account test assum role permiss let know help adam receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit melani abl confirm ecr imag uri referenc list ecr consol adam ecr suggest help resolv issu best corei view discuss web visit sorri slow repli turn issu have ecr permiss place enterpris work help aw team abl permiss squar awai work",
        "Answer_gpt_summary_original":"Solutions provided:\n- For the error \"gunicorn: command not found\", one solution suggested is to check if there is a Python versioning issue and to install gunicorn from source. It is also recommended to spin up a virtual environment to control for all dependencies.\n- For the error \"ECR image is invalid\", two solutions were suggested. First, check if the ECR image URI is listed in the ECR console and try pushing a new image to the same repository with a new tag. Second, check if there is a sneaky IAM issue and ensure that the account that boto3 is attached to in Python has an sts:assumerole permission that assumes a role with access to ECR. \nNo solution was provided for the",
        "Answer_gpt_summary":"solut provid error gunicorn command solut suggest check python version issu instal gunicorn sourc recommend spin virtual environ control depend error ecr imag invalid solut suggest check ecr imag uri list ecr consol try push new imag repositori new tag second check sneaki iam issu ensur account boto attach python st assumerol permiss assum role access ecr solut provid"
    },
    {
        "Question_title":"Why did it take so long to create endpoint with AWS Sagemaker using Boto3?",
        "Question_body":"<p>It took 45 minutes to create my endpoint from the stored endpoint configuration. (I tested it and it works too). This is the first time that I've used boto3 to do this, whereas previously I just used the Sagemaker web GUI to create an endpoint from endpoint configuration.  Suggestions to my code are appreciated:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nsagemaker_client = boto3.client('sagemaker')\n\nresponse = sagemaker_client.create_endpoint(\n    EndpointName='sagemaker-tensorflow-x',\n    EndpointConfigName='sagemaker-tensorflow-x'\n)\n<\/code><\/pre>\n<p>Note: I've replaced the last part of my endpoint name with <code>x<\/code>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1598917151350,
        "Question_favorite_count":null,
        "Question_last_edit_time":1598923606240,
        "Question_score":2.0,
        "Question_view_count":373.0,
        "Answer_body":"<p>AWS has currently <a href=\"https:\/\/status.aws.amazon.com\/\" rel=\"nofollow noreferrer\">issues<\/a> with Sagemaker:<\/p>\n<blockquote>\n<p>Increased Error Rates and Latencies for Multiple API operations<\/p>\n<\/blockquote>\n<blockquote>\n<p>5:33 PM PDT We are investigating increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.<\/p>\n<\/blockquote>\n<blockquote>\n<p>6:04 PM PDT We are continuing to investigate increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rQHQC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rQHQC.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63679503",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1598923278260,
        "Question_original_content":"long creat endpoint boto took minut creat endpoint store endpoint configur test work time boto previous web gui creat endpoint endpoint configur suggest code appreci import boto client boto client respons client creat endpoint endpointnam tensorflow endpointconfignam tensorflow note replac endpoint",
        "Question_preprocessed_content":"long creat endpoint boto took minut creat endpoint store endpoint configur time boto previous web gui creat endpoint endpoint configur suggest code appreci note replac endpoint",
        "Question_gpt_summary_original":"The user encountered a challenge in creating an endpoint with AWS Sagemaker using Boto3, which took 45 minutes to complete. The user had previously used the Sagemaker web GUI to create an endpoint from endpoint configuration. The user is seeking suggestions to improve their code.",
        "Question_gpt_summary":"user encount challeng creat endpoint boto took minut complet user previous web gui creat endpoint endpoint configur user seek suggest improv code",
        "Answer_original_content":"aw current issu increas error rate latenc multipl api oper pdt investig increas error rate latenc createtrainingjob createhyperparametertuningjob createendpoint api oper east region previous creat job endpoint unaffect pdt continu investig increas error rate latenc createtrainingjob createhyperparametertuningjob createendpoint api oper east region previous creat job endpoint unaffect",
        "Answer_preprocessed_content":"aw current issu increas error rate latenc multipl api oper pdt investig increas error rate latenc createtrainingjob createhyperparametertuningjob createendpoint api oper region previous creat job endpoint unaffect pdt continu investig increas error rate latenc createtrainingjob createhyperparametertuningjob createendpoint api oper region previous creat job endpoint unaffect",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"What Compute Infrastructure do I have to use?",
        "Question_body":"<p>Where can you choose the device you\u2019d like to execute a SigOpt run or optimization on? Is there a default device?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644274389975,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":157.0,
        "Answer_body":"<p>You can execute the process that generates run and optimization data on any device with internet access. As depicted in the below image, SigOpt interacts with your modeling environment through API requests that pass modeling artifacts like parameters and metrics.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business5\/uploads\/sigopt\/original\/1X\/de54cf477686020abf1906d570f8516e88fbaa60.jpeg\" data-download-href=\"\/uploads\/short-url\/vIPHxPGojhkqGexuNYR6pOE7Cak.jpeg?dl=1\" title=\"image\"><img src=\"https:\/\/global.discourse-cdn.com\/business5\/uploads\/sigopt\/optimized\/1X\/de54cf477686020abf1906d570f8516e88fbaa60_2_690x368.jpeg\" alt=\"image\" data-base62-sha1=\"vIPHxPGojhkqGexuNYR6pOE7Cak\" width=\"690\" height=\"368\" srcset=\"https:\/\/global.discourse-cdn.com\/business5\/uploads\/sigopt\/optimized\/1X\/de54cf477686020abf1906d570f8516e88fbaa60_2_690x368.jpeg, https:\/\/global.discourse-cdn.com\/business5\/uploads\/sigopt\/optimized\/1X\/de54cf477686020abf1906d570f8516e88fbaa60_2_1035x552.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business5\/uploads\/sigopt\/original\/1X\/de54cf477686020abf1906d570f8516e88fbaa60.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business5\/uploads\/sigopt\/optimized\/1X\/de54cf477686020abf1906d570f8516e88fbaa60_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1286\u00d7686 124 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.sigopt.com\/t\/what-compute-infrastructure-do-i-have-to-use\/86",
        "Tool":"SigOpt",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-11T18:31:56.673Z",
                "Answer_body":"<p>You can execute the process that generates run and optimization data on any device with internet access. As depicted in the below image, SigOpt interacts with your modeling environment through API requests that pass modeling artifacts like parameters and metrics.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business5\/uploads\/sigopt\/original\/1X\/de54cf477686020abf1906d570f8516e88fbaa60.jpeg\" data-download-href=\"\/uploads\/short-url\/vIPHxPGojhkqGexuNYR6pOE7Cak.jpeg?dl=1\" title=\"image\"><img src=\"https:\/\/global.discourse-cdn.com\/business5\/uploads\/sigopt\/optimized\/1X\/de54cf477686020abf1906d570f8516e88fbaa60_2_690x368.jpeg\" alt=\"image\" data-base62-sha1=\"vIPHxPGojhkqGexuNYR6pOE7Cak\" width=\"690\" height=\"368\" srcset=\"https:\/\/global.discourse-cdn.com\/business5\/uploads\/sigopt\/optimized\/1X\/de54cf477686020abf1906d570f8516e88fbaa60_2_690x368.jpeg, https:\/\/global.discourse-cdn.com\/business5\/uploads\/sigopt\/optimized\/1X\/de54cf477686020abf1906d570f8516e88fbaa60_2_1035x552.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business5\/uploads\/sigopt\/original\/1X\/de54cf477686020abf1906d570f8516e88fbaa60.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business5\/uploads\/sigopt\/optimized\/1X\/de54cf477686020abf1906d570f8516e88fbaa60_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1286\u00d7686 124 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"comput infrastructur us choos devic youd like execut run optim default devic",
        "Question_preprocessed_content":"comput infrastructur us choos devic youd like execut run optim default devic",
        "Question_gpt_summary_original":"The user is facing challenges in selecting the device to execute a SigOpt run or optimization on, and is unsure if there is a default device available.",
        "Question_gpt_summary":"user face challeng select devic execut run optim unsur default devic avail",
        "Answer_original_content":"execut process gener run optim data devic internet access depict imag interact model environ api request pass model artifact like paramet metric imag",
        "Answer_preprocessed_content":"execut process gener run optim data devic internet access depict imag interact model environ api request pass model artifact like paramet metric imag",
        "Answer_gpt_summary_original":"Solution: The user can execute the process that generates run and optimization data on any device with internet access. There is no default device available.",
        "Answer_gpt_summary":"solut user execut process gener run optim data devic internet access default devic avail"
    },
    {
        "Question_title":"AWS SageMaker SparkML Schema Eroor: member.environment' failed to satisfy constraint",
        "Question_body":"<p>I am deploying a model onto AWS via Sagemaker:<\/p>\n\n<p>I set up my JSON schema as follow:<\/p>\n\n<pre><code>import json\nschema = {\n    \"input\": [\n        {\n            \"name\": \"V1\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V2\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V3\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V4\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V5\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V6\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V7\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V8\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V9\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V10\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V11\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V12\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V13\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V14\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V15\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V16\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V17\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V18\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V19\",\n            \"type\": \"double\"\n        }, \n                {\n            \"name\": \"V20\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V21\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V22\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V23\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V24\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V25\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V26\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V27\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V28\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"Amount\",\n            \"type\": \"double\"\n        },         \n    ],\n    \"output\": \n        {\n            \"name\": \"features\",\n            \"type\": \"double\",\n            \"struct\": \"vector\"\n        }\n}\nschema_json = json.dumps(schema)\nprint(schema_json)\n<\/code><\/pre>\n\n<p>And deployed as:<\/p>\n\n<pre><code>from sagemaker.model import Model\nfrom sagemaker.pipeline import PipelineModel\nfrom sagemaker.sparkml.model import SparkMLModel\n\nsparkml_data = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\n# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\nsparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\nxgb_model = Model(model_data=xgb_model.model_data, image=training_image)\n\nmodel_name = 'inference-pipeline-' + timestamp_prefix\nsm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])\n\n    endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I got the error as below:<\/p>\n\n<p>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: 1 validation error detected: Value '{SAGEMAKER_SPARKML_SCHEMA={\"input\": [{\"type\": \"double\", \"name\": \"V1\"}, {\"type\": \"double\", \"name\": \"V2\"}, {\"type\": \"double\", \"name\": \"V3\"}, {\"type\": \"double\", \"name\": \"V4\"}, {\"type\": \"double\", \"name\": \"V5\"}, {\"type\": \"double\", \"name\": \"V6\"}, {\"type\": \"double\", \"name\": \"V7\"}, {\"type\": \"double\", \"name\": \"V8\"}, {\"type\": \"double\", \"name\": \"V9\"}, {\"type\": \"double\", \"name\": \"V10\"}, {\"type\": \"double\", \"name\": \"V11\"}, {\"type\": \"double\", \"name\": \"V12\"}, {\"type\": \"double\", \"name\": \"V13\"}, {\"type\": \"double\", \"name\": \"V14\"}, {\"type\": \"double\", \"name\": \"V15\"}, {\"type\": \"double\", \"name\": \"V16\"}, {\"type\": \"double\", \"name\": \"V17\"}, {\"type\": \"double\", \"name\": \"V18\"}, {\"type\": \"double\", \"name\": \"V19\"}, {\"type\": \"double\", \"name\": \"V20\"}, {\"type\": \"double\", \"name\": \"V21\"}, {\"type\": \"double\", \"name\": \"V22\"}, {\"type\": \"double\", \"name\": \"V23\"}, {\"type\": \"double\", \"name\": \"V24\"}, {\"type\": \"double\", \"name\": \"V25\"}, {\"type\": \"double\", \"name\": \"V26\"}, {\"type\": \"double\", \"name\": \"V27\"}, {\"type\": \"double\", \"name\": \"V28\"}, {\"type\": \"double\", \"name\": \"Amount\"}], \"output\": {\"type\": \"double\", \"name\": \"features\", \"struct\": \"vector\"}}}' at 'containers.1**.<strong>member.environment' failed to satisfy constraint: Map value must satisfy constraint: [Member must have length less than or equal to 1024<\/strong>,** Member must have length greater than or equal to 0, Member must satisfy regular expression pattern: [\\S\\s]*]<\/p>\n\n<p>I try to reduce my features to 20 and it able to deploy. Just wondering how can I Pass the schema with 29 attributes?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590448983843,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":226.0,
        "Answer_body":"<p>I do not think the environment length of 1024 limit will be increased in a short time. To work around this, you could try to rebuild the spark ml container with the <code>SAGEMAKER_SPARKML_SCHEMA<\/code> env var:<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62012264",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1600448991412,
        "Question_original_content":"sparkml schema eroor member environ fail satisfi constraint deploi model aw set json schema follow import json schema input type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl output featur type doubl struct vector schema json json dump schema print schema json deploi model import model pipelin import pipelinemodel sparkml model import sparkmlmodel sparkml data format model bucket model kei prefix model tar pass schema defin environ variabl sparkml serv understand sparkml model sparkmlmodel model data sparkml data env sparkml schema schema json xgb model model model data xgb model model data imag train imag model infer pipelin timestamp prefix model pipelinemodel model role role model sparkml model xgb model endpoint infer pipelin timestamp prefix model deploi initi instanc count instanc type xlarg endpoint endpoint got error clienterror error occur validationexcept call createmodel oper valid error detect valu sparkml schema input type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl output type doubl featur struct vector contain member environ fail satisfi constraint map valu satisfi constraint member length equal member length greater equal member satisfi regular express pattern try reduc featur abl deploi wonder pass schema attribut",
        "Question_preprocessed_content":"sparkml schema eroor fail satisfi constraint deploi model aw set json schema follow deploi got error clienterror error occur call createmodel oper valid error detect valu output fail satisfi constraint map valu satisfi constraint try reduc featur abl deploi wonder pass schema attribut",
        "Question_gpt_summary_original":"The user encountered an error while deploying a model onto AWS via Sagemaker due to a validation error. The error occurred because the member.environment failed to satisfy the constraint, which requires the map value to have a length less than or equal to 1024. The user was able to deploy the model successfully after reducing the features to 20, but is unsure how to pass the schema with 29 attributes.",
        "Question_gpt_summary":"user encount error deploi model aw valid error error occur member environ fail satisfi constraint requir map valu length equal user abl deploi model successfulli reduc featur unsur pass schema attribut",
        "Answer_original_content":"think environ length limit increas short time work try rebuild spark contain sparkml schema env var http github com aw sparkml serv contain blob master readm run imag local",
        "Answer_preprocessed_content":"think environ length limit increas short time work try rebuild spark contain env var",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to rebuild the spark ml container with the SAGEMAKER_SPARKML_SCHEMA env var. However, there are no other solutions provided.",
        "Answer_gpt_summary":"solut possibl solut mention discuss rebuild spark contain sparkml schema env var solut provid"
    },
    {
        "Question_title":"Refactor columns and features in Azure Machine Learning",
        "Question_body":"<p>Is there any way I can make my dataset features in Azure ML into something else than what it already is? <\/p>\n\n<p>I found a dataset of the Titanic ship in the sample datasets which I would like to work with but all of my columns are either a numeric feature or string feature, but I would like to categorize these. Also is there any possibility to rename the columns within my model so it\u2019s more descriptive than what I initially got? I have no clue what SibSp means for instance.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1465481281330,
        "Question_favorite_count":null,
        "Question_last_edit_time":1465977179120,
        "Question_score":0.0,
        "Question_view_count":297.0,
        "Answer_body":"<p>What you are doing is essentially recreating this experiment made by Raja Iqbal for the Titanic dataset. I recommend you check that out here: <a href=\"http:\/\/gallery.cortanaintelligence.com\/Experiment\/Tutorial-Building-a-classification-model-in-Azure-ML-8?share=1\" rel=\"nofollow noreferrer\">http:\/\/gallery.cortanaintelligence.com\/Experiment\/Tutorial-Building-a-classification-model-in-Azure-ML-8?share=1<\/a><\/p>\n\n<p>To answer your question, the module you can drag to your canvas in order to make the features into categories; is the Edit Metadata module where you select the columns you want and change the \u201cunchanged\u201d into \u201cMake categorical\u201d within the Categorical-properties pane like in the image below:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/2NDht.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2NDht.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>You can also use the same module to make better sense from your columns by giving them a different column name. SibSp means SiblingSpouse like I have renamed it to in the image below:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Gm9Rr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Gm9Rr.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And at last you can assign the targeted value (survived) and make the field into a label for ease of use.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LyN0j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LyN0j.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37728314",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1465484184003,
        "Question_original_content":"refactor column featur wai dataset featur dataset titan ship sampl dataset like work column numer featur string featur like categor possibl renam column model descript initi got clue sibsp mean instanc",
        "Question_preprocessed_content":"refactor column featur wai dataset featur dataset titan ship sampl dataset like work column numer featur string featur like categor possibl renam column model descript initi got clue sibsp mean instanc",
        "Question_gpt_summary_original":"The user is facing challenges in Azure Machine Learning related to the dataset features. They want to categorize the numeric and string features and rename the columns to make them more descriptive. The user is also unsure about the meaning of some of the column names.",
        "Question_gpt_summary":"user face challeng relat dataset featur want categor numer string featur renam column descript user unsur mean column name",
        "Answer_original_content":"essenti recreat experi raja iqbal titan dataset recommend check http galleri cortanaintellig com experi tutori build classif model azur share answer question modul drag canva order featur categori edit metadata modul select column want chang unchang categor categor properti pane like imag us modul better sens column give differ column sibsp mean siblingspous like renam imag assign target valu surviv field label eas us",
        "Answer_preprocessed_content":"essenti recreat experi raja iqbal titan dataset recommend check answer question modul drag canva order featur categori edit metadata modul select column want chang unchang categor pane like imag us modul better sens column give differ column sibsp mean siblingspous like renam imag assign target valu field label eas us",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Use the Edit Metadata module to categorize the features by selecting the columns and changing the \"unchanged\" into \"Make categorical\" within the Categorical-properties pane.\n- Use the same module to rename the columns to make them more descriptive.\n- Assign the targeted value (survived) and make the field into a label for ease of use.\n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"possibl solut mention discuss us edit metadata modul categor featur select column chang unchang categor categor properti pane us modul renam column descript assign target valu surviv field label eas us person opinion bias includ respons"
    },
    {
        "Question_title":"Train an already trained model in Sagemaker and Huggingface without re-initialising",
        "Question_body":"<p>Let's say I have successfully trained a model on some training data for 10 epochs. How can I then access the very same model and train for a further 10 epochs?<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">In the docs<\/a> it suggests &quot;you need to specify a checkpoint output path through hyperparameters&quot; --&gt; how?<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># define my estimator the standard way\nhuggingface_estimator = HuggingFace(\n    entry_point='train.py',\n    source_dir='.\/scripts',\n    instance_type='ml.p3.2xlarge',\n    instance_count=1,\n    role=role,\n    transformers_version='4.10',\n    pytorch_version='1.9',\n    py_version='py38',\n    hyperparameters = hyperparameters,\n    metric_definitions=metric_definitions\n)\n\n# train the model\nhuggingface_estimator.fit(\n    {'train': training_input_path, 'test': test_input_path}\n)\n<\/code><\/pre>\n<p>If I run <code>huggingface_estimator.fit<\/code> again it will just start the whole thing over again and overwrite my previous training.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649787701300,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":110.0,
        "Answer_body":"<p>You can find the relevant checkpoint save\/load code in <a href=\"https:\/\/github.com\/huggingface\/notebooks\/blob\/main\/sagemaker\/05_spot_instances\/sagemaker-notebook.ipynb\" rel=\"nofollow noreferrer\">Spot Instances - Amazon SageMaker x Hugging Face Transformers<\/a>.<br \/>\n(The example enables Spot instances, but you can use on-demand).<\/p>\n<ol>\n<li>In hyperparameters you set: <code>'output_dir':'\/opt\/ml\/checkpoints'<\/code>.<\/li>\n<li>You define a <code>checkpoint_s3_uri<\/code> in the Estimator (which is unique to the series of jobs you'll run).<\/li>\n<li>You add code for train.py to support checkpointing:<\/li>\n<\/ol>\n<blockquote>\n<pre><code>from transformers.trainer_utils import get_last_checkpoint\n\n# check if checkpoint existing if so continue training\nif get_last_checkpoint(args.output_dir) is not None:\n    logger.info(&quot;***** continue training *****&quot;)\n    last_checkpoint = get_last_checkpoint(args.output_dir)\n    trainer.train(resume_from_checkpoint=last_checkpoint)\nelse:\n    trainer.train()\n<\/code><\/pre>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71847442",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1649831911830,
        "Question_original_content":"train train model huggingfac initialis let successfulli train model train data epoch access model train epoch doc suggest need specifi checkpoint output path hyperparamet defin estim standard wai huggingfac estim huggingfac entri point train sourc dir script instanc type xlarg instanc count role role transform version pytorch version version hyperparamet hyperparamet metric definit metric definit train model huggingfac estim fit train train input path test test input path run huggingfac estim fit start thing overwrit previou train",
        "Question_preprocessed_content":"train train model huggingfac let successfulli train model train data epoch access model train epoch doc suggest need specifi checkpoint output path hyperparamet run start thing overwrit previou train",
        "Question_gpt_summary_original":"The user wants to know how to access a previously trained model and train it for further epochs without re-initializing it. The user has found documentation suggesting the use of a checkpoint output path through hyperparameters, but is unsure how to implement it. The user is concerned that running the training process again will overwrite the previous training.",
        "Question_gpt_summary":"user want know access previous train model train epoch initi user document suggest us checkpoint output path hyperparamet unsur implement user concern run train process overwrit previou train",
        "Answer_original_content":"relev checkpoint save load code spot instanc hug face transform exampl enabl spot instanc us demand hyperparamet set output dir opt checkpoint defin checkpoint uri estim uniqu seri job run add code train support checkpoint transform trainer util import checkpoint check checkpoint exist continu train checkpoint arg output dir logger info continu train checkpoint checkpoint arg output dir trainer train resum checkpoint checkpoint trainer train",
        "Answer_preprocessed_content":"relev checkpoint code spot instanc hug face transform exampl enabl spot instanc us hyperparamet set defin estim add code support checkpoint",
        "Answer_gpt_summary_original":"Solution:\n- Set the output directory for checkpoints in hyperparameters.\n- Define a unique checkpoint S3 URI in the Estimator.\n- Add code to train.py to support checkpointing and continue training from the last checkpoint if it exists.",
        "Answer_gpt_summary":"solut set output directori checkpoint hyperparamet defin uniqu checkpoint uri estim add code train support checkpoint continu train checkpoint exist"
    },
    {
        "Question_title":"Run Sagemaker notebook instance and be able to close tab",
        "Question_body":"<p>I'm currently using Sagemaker notebook instance (not from Sagemaker Studio), and I want to run a notebook that is expected to take around 8 hours to finish. I want to leave it overnight, and see the output from each cell, the output is a combination of print statements and plots.<\/p>\n<p>Howevever, when I start running the notebook and make sure the initial cells run, I close the Jupyterlab tab in my browser, and some minutes after, I open it again to see how is it going, but the notebook is stopped.<\/p>\n<p>Is there any way where I can still use my notebook as it is, see the output from each cell (prints and plots) and do not have to keep the Jupyterlab tab open (turn my laptop off, etc)?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1646922683383,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1154.0,
        "Answer_body":"<p>Answering my own question.<\/p>\n<p>I ended up using Sagemaker Processing jobs for this. As initially suggested by the other answer. I found this library developed a few months ago: <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-run-notebook\" rel=\"nofollow noreferrer\">Sagemaker run notebook<\/a>, which helped still keep my notebook structure and cells as I had them, and be able to run it using Sagemaker run notebook using a bigger instance, and modifying the notebook in a smaller one.<\/p>\n<p>The output of each cell was saved, along the plots I had, in S3 as a jupyter notebook.<\/p>\n<p>I see that no constant support is given to the library, but you can fork it and make changes to it, and use it as per your requirements. For example, creating a docker container based on your needs.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71425842",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1662537228663,
        "Question_original_content":"run notebook instanc abl close tab current notebook instanc studio want run notebook expect hour finish want leav overnight output cell output combin print statement plot howevev start run notebook sure initi cell run close jupyterlab tab browser minut open go notebook stop wai us notebook output cell print plot jupyterlab tab open turn laptop",
        "Question_preprocessed_content":"run notebook instanc abl close tab current notebook instanc want run notebook expect hour finish want leav overnight output cell output combin print statement plot howevev start run notebook sure initi cell run close jupyterlab tab browser minut open go notebook stop wai us notebook output cell jupyterlab tab open",
        "Question_gpt_summary_original":"The user is facing challenges in running a Sagemaker notebook instance and leaving it to run overnight. The user wants to be able to see the output from each cell, which includes print statements and plots, without having to keep the Jupyterlab tab open. However, when the user closes the tab and opens it again later, the notebook is stopped. The user is seeking a solution to be able to use the notebook as intended without having to keep the tab open.",
        "Question_gpt_summary":"user face challeng run notebook instanc leav run overnight user want abl output cell includ print statement plot have jupyterlab tab open user close tab open later notebook stop user seek solut abl us notebook intend have tab open",
        "Answer_original_content":"answer question end process job initi suggest answer librari develop month ago run notebook help notebook structur cell abl run run notebook bigger instanc modifi notebook smaller output cell save plot jupyt notebook constant support given librari fork chang us requir exampl creat docker contain base need",
        "Answer_preprocessed_content":"answer question end process job initi suggest answer librari develop month ago run notebook help notebook structur cell abl run run notebook bigger instanc modifi notebook smaller output cell save plot jupyt notebook constant support given librari fork chang us requir exampl creat docker contain base need",
        "Answer_gpt_summary_original":"Solution: The user found a solution by using Sagemaker Processing jobs and a library called Sagemaker run notebook. The library helped the user to keep the notebook structure and cells as they were and run it using a bigger instance, and modifying the notebook in a smaller one. The output of each cell was saved, along with the plots, in S3 as a Jupyter notebook. The user mentioned that no constant support is given to the library, but it can be forked and modified based on the user's requirements, such as creating a docker container based on their needs.",
        "Answer_gpt_summary":"solut user solut process job librari call run notebook librari help user notebook structur cell run bigger instanc modifi notebook smaller output cell save plot jupyt notebook user mention constant support given librari fork modifi base user requir creat docker contain base need"
    },
    {
        "Question_title":"Tracking Data Provenance with DVC",
        "Question_body":"<p>I recently discovered DVC and am looking to replace my current shell script-based approach for downloading source datasets and building derived datasets with DVC. In my current process, I have a clear record of data provenance as my scripted pipelines begin with downloads of the source datasets from the web.<\/p>\n<p>The question I have is the following: does DVC provide functionality that allows me to capture data provenance somehow? Can I record the URL from which the data was originally sourced and bind that to metadata associated with the data file(s)? Or will I need to maintain scripts that allow me to easily reacquire the data from the web? In the documentation, it seems like the story begins with the source data already in hand.<\/p>\n<p>I would love to have functionality that allows me to easily reacquire the source data from the web if needed and verify that indeed the dataset is equivalent to the original form that was used in the original pipeline development (via hash comparison).<\/p>\n<p>As of yet, I\u2019m not quite seeing how I would accomplish this with existing DVC commands. Any pointers would be greatly appreciated!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1652228110629,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":170.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/diehl\">@diehl<\/a> , sorry for late response.<\/p>\n<p>If I understand your use case correctly, it looks like you might be interested in using <code>dvc import<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/import\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/command-reference\/import<\/a>) and\/or <code>dvc import-url<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/import-url\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/command-reference\/import-url<\/a>)<\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/daavoo\">@daavoo<\/a>, No worries!<\/p>\n<p>That sounds on point. Thank you!<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/tracking-data-provenance-with-dvc\/1186",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-18T10:50:48.093Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/diehl\">@diehl<\/a> , sorry for late response.<\/p>\n<p>If I understand your use case correctly, it looks like you might be interested in using <code>dvc import<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/import\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/command-reference\/import<\/a>) and\/or <code>dvc import-url<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/import-url\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/command-reference\/import-url<\/a>)<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-21T18:22:52.060Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/daavoo\">@daavoo<\/a>, No worries!<\/p>\n<p>That sounds on point. Thank you!<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"track data proven recent discov look replac current shell script base approach download sourc dataset build deriv dataset current process clear record data proven script pipelin begin download sourc dataset web question follow provid function allow captur data proven record url data origin sourc bind metadata associ data file need maintain script allow easili reacquir data web document like stori begin sourc data hand love function allow easili reacquir sourc data web need verifi dataset equival origin form origin pipelin develop hash comparison see accomplish exist command pointer greatli appreci",
        "Question_preprocessed_content":"track data proven recent discov look replac current shell approach download sourc dataset build deriv dataset current process clear record data proven script pipelin begin download sourc dataset web question follow provid function allow captur data proven record url data origin sourc bind metadata associ data file need maintain script allow easili reacquir data web document like stori begin sourc data hand love function allow easili reacquir sourc data web need verifi dataset equival origin form origin pipelin develop see accomplish exist command pointer greatli appreci",
        "Question_gpt_summary_original":"The user is facing challenges in using DVC to replace their current shell script-based approach for downloading source datasets and building derived datasets. They are unsure if DVC provides functionality to capture data provenance, such as recording the URL from which the data was originally sourced and binding it to metadata associated with the data file(s). The user would like to have functionality that allows them to easily reacquire the source data from the web if needed and verify that the dataset is equivalent to the original form that was used in the original pipeline development. They are seeking guidance on how to accomplish this with existing DVC commands.",
        "Question_gpt_summary":"user face challeng replac current shell script base approach download sourc dataset build deriv dataset unsur provid function captur data proven record url data origin sourc bind metadata associ data file user like function allow easili reacquir sourc data web need verifi dataset equival origin form origin pipelin develop seek guidanc accomplish exist command",
        "Answer_original_content":"diehl sorri late respons understand us case correctli look like interest import http org doc command refer import import url http org doc command refer import url daavoo worri sound point thank",
        "Answer_preprocessed_content":"sorri late respons understand us case correctli look like interest worri sound point thank",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are to use the DVC commands \"dvc import\" and \"dvc import-url\" to capture data provenance and record the URL from which the data was originally sourced. These commands can also be used to easily reacquire the source data from the web if needed and verify that the dataset is equivalent to the original form used in the pipeline development.",
        "Answer_gpt_summary":"possibl solut mention discuss us command import import url captur data proven record url data origin sourc command easili reacquir sourc data web need verifi dataset equival origin form pipelin develop"
    },
    {
        "Question_title":"Use an existing Gateway with Azure Machine Learning?",
        "Question_body":"<p>We want to access an onprem SQL database with an existing Gateway, is that possible in AML?  The tool only seems to allow creating new gateways.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1482505382510,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":39.0,
        "Answer_body":"<p>Confirmed that this is not possible, AML only allows use of AML-created gateways.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41303697",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1488397699940,
        "Question_original_content":"us exist gatewai want access onprem sql databas exist gatewai possibl aml tool allow creat new gatewai",
        "Question_preprocessed_content":"us exist gatewai want access onprem sql databas exist gatewai possibl aml tool allow creat new gatewai",
        "Question_gpt_summary_original":"The user is facing a challenge in accessing an on-premises SQL database with an existing Gateway in Azure Machine Learning. The tool only allows creating new gateways, and the user is unsure if it is possible to use an existing one.",
        "Question_gpt_summary":"user face challeng access premis sql databas exist gatewai tool allow creat new gatewai user unsur possibl us exist",
        "Answer_original_content":"confirm possibl aml allow us aml creat gatewai",
        "Answer_preprocessed_content":"confirm possibl aml allow us gatewai",
        "Answer_gpt_summary_original":"Solution: No solution provided. It is confirmed that it is not possible to use an existing gateway in Azure Machine Learning, and the tool only allows creating new gateways.",
        "Answer_gpt_summary":"solut solut provid confirm possibl us exist gatewai tool allow creat new gatewai"
    },
    {
        "Question_title":"SageMaker Tensorflow - how to write my serving_input_fn()",
        "Question_body":"<p>I'm pretty new to Tensorflow and SageMaker and I'm trying to figure out how to write my <code>serving_input_fn()<\/code>. I've tried a number of ways to do it, but to no avail. <\/p>\n\n<p>my input function has 3 feature columns: <code>amount_normalized, x_month and y_month<\/code>:<\/p>\n\n<pre><code>def construct_feature_columns():\n    amount_normalized = tf.feature_column.numeric_column(key='amount_normalized')\n    x_month = tf.feature_column.numeric_column(key='x_month')\n    y_month = tf.feature_column.numeric_column(key='y_month')\n    return set([amount_normalized, x_month, y_month])\n<\/code><\/pre>\n\n<p>I want to be able to call my deployed model using something like <code>deployed_model.predict([1.23,0.3,0.8])<\/code> <\/p>\n\n<p>Where the first element is <code>amount_normalized<\/code>, second is <code>x_month<\/code> third is <code>y_month<\/code><\/p>\n\n<p>I've tried this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(params):\n    feature_placeholders = {\n      key : tf.placeholder(tf.float32, [None]) \\\n        for key in FEATURES\n    }\nreturn tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholders)()\n<\/code><\/pre>\n\n<p>But all I get is:\n<code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\".<\/code><\/p>\n\n<p>Any help would be <strong>really<\/strong> appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1524856485467,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1337.0,
        "Answer_body":"<p>Posting this here in case anyone else has this issue.<\/p>\n\n<p>After a bunch of trial and error I managed to solve my issue by writing my serving input function like this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(hyperparameters):\n    feature_spec = {\n        key : tf.FixedLenFeature(shape=[], dtype = tf.float32) \\\n          for key in FEATURES\n    }\n    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)()\n<\/code><\/pre>\n\n<p>I can then call my deployed model by passing in a hash:<\/p>\n\n<pre><code>deployed_model.predict({\"amount_normalized\": 2.3, \"x_month\": 0.2, \"y_month\": -0.3})\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50068941",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1525019634972,
        "Question_original_content":"tensorflow write serv input pretti new tensorflow try figur write serv input tri number wai avail input function featur column normal month month def construct featur column normal featur column numer column kei normal month featur column numer column kei month month featur column numer column kei month return set normal month month want abl deploi model like deploi model predict element normal second month month tri featur normal month month def serv input param featur placehold kei placehold float kei featur return estim export build raw serv input receiv featur placehold error occur modelerror call invokeendpoint oper receiv server error model messag help appreci",
        "Question_preprocessed_content":"tensorflow write pretti new tensorflow try figur write tri number wai avail input function featur column want abl deploi model like element second tri help appreci",
        "Question_gpt_summary_original":"The user is facing challenges in writing the serving_input_fn() for their Tensorflow model in SageMaker. They have tried different methods but have not been successful. The input function has three feature columns, and the user wants to call the deployed model using specific values for each column. The user has attempted to write the serving_input_fn() using placeholders, but they are encountering an error when calling the model.",
        "Question_gpt_summary":"user face challeng write serv input tensorflow model tri differ method success input function featur column user want deploi model specif valu column user attempt write serv input placehold encount error call model",
        "Answer_original_content":"post case issu bunch trial error manag solv issu write serv input function like featur normal month month def serv input hyperparamet featur spec kei fixedlenfeatur shape dtype float kei featur return estim export build pars serv input receiv featur spec deploi model pass hash deploi model predict normal month month",
        "Answer_preprocessed_content":"post case issu bunch trial error manag solv issu write serv input function like deploi model pass hash",
        "Answer_gpt_summary_original":"Solution:\nThe user can write the serving_input_fn() using the provided code snippet. They can define the feature columns in the FEATURES list and create a feature_spec dictionary using tf.FixedLenFeature. They can then call the exported model by passing in a hash with specific values for each column.",
        "Answer_gpt_summary":"solut user write serv input provid code snippet defin featur column featur list creat featur spec dictionari fixedlenfeatur export model pass hash specif valu column"
    },
    {
        "Question_title":"Create a predictor from an endpoint in a different region",
        "Question_body":"<p>I have created an endpoint on us-east-1. try to create a predictor:<\/p>\n\n<pre><code>In [106]: sagemaker.predictor.RealTimePredictor(&lt;endpoint name&gt;)\n<\/code><\/pre>\n\n<p>and get<\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the DescribeEndpoint operation: \nCould not find endpoint \"arn:aws:sagemaker:us-east-2:&lt;account number&gt;:endpoint\/&lt;endpoint name&gt;\".\n<\/code><\/pre>\n\n<p>which is perfectly correct, since the endpoint is on us-east-1.  Probably I could change some defaults, but I'd rather not - I work on us-east-2 99% of the time.<\/p>\n\n<p>So, how can I set a different region when initializing the predictor?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1573497062720,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":478.0,
        "Answer_body":"<p>The (python) <code>Predictors<\/code> <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"nofollow noreferrer\">documentation<\/a> shows that you can pass a <code>Session<\/code> object. In turn, the <code>Session<\/code> can be <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.Session\" rel=\"nofollow noreferrer\">initialized<\/a> with a <em>client<\/em> and a <em>runtime client<\/em> - the former does everything except endpoint invocations, the latter does... endpoint invocations.<\/p>\n\n<p>Those clients are tied to specific regions. It seems like you should be able to set the runtime client region to match your endpoint, by manually instantiating it, while leaving the regular client alone (disclaimer here: I haven't tried this - if you do, let me\/us know how it goes :)).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58806807",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1573497881147,
        "Question_original_content":"creat predictor endpoint differ region creat endpoint east try creat predictor predictor realtimepredictor clienterror error occur validationexcept call describeendpoint oper endpoint arn aw east endpoint perfectli correct endpoint east probabl chang default work east time set differ region initi predictor",
        "Question_preprocessed_content":"creat predictor endpoint differ region creat endpoint try creat predictor perfectli correct endpoint probabl chang default work time set differ region initi predictor",
        "Question_gpt_summary_original":"The user is facing a challenge in creating a predictor from an endpoint in a different region. They have created an endpoint on us-east-1 but are unable to create a predictor on us-east-2 due to a validation error. The user is unsure how to set a different region when initializing the predictor.",
        "Question_gpt_summary":"user face challeng creat predictor endpoint differ region creat endpoint east unabl creat predictor east valid error user unsur set differ region initi predictor",
        "Answer_original_content":"python predictor document show pass session object turn session initi client runtim client endpoint invoc endpoint invoc client ti specif region like abl set runtim client region match endpoint manual instanti leav regular client disclaim haven tri let know goe",
        "Answer_preprocessed_content":"document show pass object turn initi client runtim client endpoint invoc endpoint invoc client ti specif region like abl set runtim client region match endpoint manual instanti leav regular client",
        "Answer_gpt_summary_original":"Solution: The user can pass a Session object to the Predictors documentation and initialize it with a client and a runtime client. The clients are tied to specific regions, so the user can set the runtime client region to match their endpoint by manually instantiating it while leaving the regular client alone. However, it is not clear if this solution has been tried and tested.",
        "Answer_gpt_summary":"solut user pass session object predictor document initi client runtim client client ti specif region user set runtim client region match endpoint manual instanti leav regular client clear solut tri test"
    },
    {
        "Question_title":"Reloading from checkpoing during AWS Sagemaker Training",
        "Question_body":"<p>Sagemaker is a great tool to train your models, and we save some money by using AWS spot instances. However, training jobs sometimes get stopped in the middle. We are using some mechanisms to continue from the latest checkpoint after a restart. See also the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">docs<\/a>.<\/p>\n<p>Still, how do you efficiently test such a mechanism? Can you trigger it yourself? Otherwise you have to wait until the spot instance actually \u00eds restarted.<\/p>\n<p>Also, are you expected to use the linked <code>checkpoint_s3_uri<\/code> argument or the <code>model_dir<\/code> for this? E.g. the <code>TensorFlow<\/code> estimator <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/sagemaker.tensorflow.html#tensorflow-estimator\" rel=\"nofollow noreferrer\">docs<\/a> seem to suggest something <code>model_dir<\/code>for checkpoints.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616592352363,
        "Question_favorite_count":null,
        "Question_last_edit_time":1616592807080,
        "Question_score":0.0,
        "Question_view_count":286.0,
        "Answer_body":"<p>Since you can't manually terminate a sagemaker instance, run an Amazon SageMaker Managed Spot training for a small number of epochs, Amazon SageMaker would have backed up your checkpoint files to S3. Check that checkpoints are there. Now run a second training run, but this time provide the first jobs\u2019 checkpoint location to <code>checkpoint_s3_uri<\/code>. Reference is <a href=\"https:\/\/towardsdatascience.com\/a-quick-guide-to-using-spot-instances-with-amazon-sagemaker-b9cfb3a44a68\" rel=\"nofollow noreferrer\">here<\/a>, this also answer your second question.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66782040",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1616662041463,
        "Question_original_content":"reload checkpo train great tool train model save monei aw spot instanc train job stop middl mechan continu latest checkpoint restart doc effici test mechan trigger wait spot instanc actual restart expect us link checkpoint uri argument model dir tensorflow estim doc suggest model dirfor checkpoint",
        "Question_preprocessed_content":"reload checkpo train great tool train model save monei aw spot instanc train job stop middl mechan continu latest checkpoint restart doc effici test mechan trigger wait spot instanc actual restart expect us link argument estim doc suggest checkpoint",
        "Question_gpt_summary_original":"The user is facing challenges with training jobs getting stopped in the middle while using AWS Sagemaker and spot instances. They are using mechanisms to continue from the latest checkpoint after a restart, but are unsure how to efficiently test this mechanism and whether to use the checkpoint_s3_uri or model_dir argument.",
        "Question_gpt_summary":"user face challeng train job get stop middl spot instanc mechan continu latest checkpoint restart unsur effici test mechan us checkpoint uri model dir argument",
        "Answer_original_content":"manual termin instanc run manag spot train small number epoch back checkpoint file check checkpoint run second train run time provid job checkpoint locat checkpoint uri refer answer second question",
        "Answer_preprocessed_content":"manual termin instanc run manag spot train small number epoch back checkpoint file check checkpoint run second train run time provid job checkpoint locat refer answer second question",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to run an Amazon SageMaker Managed Spot training for a small number of epochs and check if the checkpoint files are backed up to S3. Then, run a second training run and provide the first job's checkpoint location to the checkpoint_s3_uri argument. This solution can help test the mechanism of continuing from the latest checkpoint after a restart and also answers the question of whether to use the checkpoint_s3_uri or model_dir argument.",
        "Answer_gpt_summary":"solut possibl solut mention discuss run manag spot train small number epoch check checkpoint file back run second train run provid job checkpoint locat checkpoint uri argument solut help test mechan continu latest checkpoint restart answer question us checkpoint uri model dir argument"
    },
    {
        "Question_title":"How to correctly specify a private ACR Docker image in an Azure ML Pipeline?",
        "Question_body":"<p>I created a private Azure Container Registry, and pushed a docker image to that registry. I was trying to understand the correct way to access that registry in my pipeline, and my understanding was that I needed to set the following info in the run configuration:<\/p>\n\n<pre><code>        run_config.environment.docker.base_image = \"myprivateacr.azurecr.io\/mydockerimage:0.0.1\"\n        run_config.environment.docker.base_image_registry.username = \"MyPrivateACR\"\n        run_config.environment.docker.base_image_registry.password = \"&lt;the password for the registry&gt;\"\n<\/code><\/pre>\n\n<p>Let's assume that I correctly provided the username and password. Any idea why this didn't work? Or: is there an example of a pipeline notebook that uses a docker image that's in a private docker registry, and thus deals with this type of authentication issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1566428573660,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":278.0,
        "Answer_body":"<p>There's a separate address property for a custom image registry. Try specifying it this way:<\/p>\n\n<pre><code>run_config.environment.docker.base_image = \"mydockerimage:0.0.1\"\nrun_config.environment.docker.base_image_registry.address = \"myprivateacr.azurecr.io\"\nrun_config.environment.docker.base_image_registry.username = \"MyPrivateACR\"\nrun_config.environment.docker.base_image_registry.password = \"&lt;the password for the registry&gt;\"\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57600154",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1566481255416,
        "Question_original_content":"correctli specifi privat acr docker imag pipelin creat privat azur contain registri push docker imag registri try understand correct wai access registri pipelin understand need set follow info run configur run config environ docker base imag myprivateacr azurecr mydockerimag run config environ docker base imag registri usernam myprivateacr run config environ docker base imag registri password let assum correctli provid usernam password idea work exampl pipelin notebook us docker imag privat docker registri deal type authent issu",
        "Question_preprocessed_content":"correctli specifi privat acr docker imag pipelin creat privat azur contain registri push docker imag registri try understand correct wai access registri pipelin understand need set follow info run configur let assum correctli provid usernam password idea work exampl pipelin notebook us docker imag privat docker registri deal type authent issu",
        "Question_gpt_summary_original":"The user is facing challenges in correctly specifying a private Azure Container Registry (ACR) Docker image in an Azure ML Pipeline. Despite setting the correct username and password for the registry, the pipeline is not able to access the registry. The user is seeking help to resolve this authentication issue and is looking for an example of a pipeline notebook that deals with this type of issue.",
        "Question_gpt_summary":"user face challeng correctli specifi privat azur contain registri acr docker imag pipelin despit set correct usernam password registri pipelin abl access registri user seek help resolv authent issu look exampl pipelin notebook deal type issu",
        "Answer_original_content":"separ address properti custom imag registri try specifi wai run config environ docker base imag mydockerimag run config environ docker base imag registri address myprivateacr azurecr run config environ docker base imag registri usernam myprivateacr run config environ docker base imag registri password",
        "Answer_preprocessed_content":"separ address properti custom imag registri try specifi wai",
        "Answer_gpt_summary_original":"Solution: The user can try specifying the address property for a custom image registry in the Azure ML Pipeline notebook. The solution involves setting the correct username and password for the registry and specifying the address property in the following way:\n\nrun_config.environment.docker.base_image = \"mydockerimage:0.0.1\"\nrun_config.environment.docker.base_image_registry.address = \"myprivateacr.azurecr.io\"\nrun_config.environment.docker.base_image_registry.username = \"MyPrivateACR\"\nrun_config.environment.docker.base_image_registry.password = \"<the password for the registry>\"",
        "Answer_gpt_summary":"solut user try specifi address properti custom imag registri pipelin notebook solut involv set correct usernam password registri specifi address properti follow wai run config environ docker base imag mydockerimag run config environ docker base imag registri address myprivateacr azurecr run config environ docker base imag registri usernam myprivateacr run config environ docker base imag registri password"
    },
    {
        "Question_title":"Azure Machine Learning Compute quota?",
        "Question_body":"<p>The <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-manage-quotas#azure-machine-learning-compute\" rel=\"nofollow noreferrer\">Manage and request quotas for Azure resources<\/a> documentation page states that the default quota depends \"on your subscription offer type\". The quota doesn't show up in Azure web portal. Is there a way to find out current quota values using SDK, CLI, REST API?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570636386640,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":354.0,
        "Answer_body":"<p>You probably want to try something like this command : <\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>az vm list-usage --location eastus --out table\n<\/code><\/pre>\n\n<p>It would get you the core usage for the region, which is what is important for deployment of resources.<\/p>\n\n<p>Other choices (az + Powershell) are available <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/networking\/check-usage-against-limits\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Hope this helps!<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58307950",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1570669528590,
        "Question_original_content":"comput quota manag request quota azur resourc document page state default quota depend subscript offer type quota azur web portal wai current quota valu sdk cli rest api",
        "Question_preprocessed_content":"comput quota manag request quota azur resourc document page state default quota depend subscript offer type quota azur web portal wai current quota valu sdk cli rest api",
        "Question_gpt_summary_original":"The user is facing challenges in finding out the current quota values for Azure Machine Learning Compute using SDK, CLI, or REST API as the default quota depends on the subscription offer type and does not show up in the Azure web portal.",
        "Question_gpt_summary":"user face challeng find current quota valu comput sdk cli rest api default quota depend subscript offer type azur web portal",
        "Answer_original_content":"probabl want try like command list usag locat eastu tabl core usag region import deploy resourc choic powershel avail hope help",
        "Answer_preprocessed_content":"probabl want try like command core usag region import deploy resourc choic avail hope help",
        "Answer_gpt_summary_original":"Solution:\n- Use the command \"az vm list-usage --location eastus --out table\" to get the core usage for the region, which is important for deployment of resources. Other options are also available using az and Powershell.",
        "Answer_gpt_summary":"solut us command list usag locat eastu tabl core usag region import deploy resourc option avail powershel"
    },
    {
        "Question_title":"SigOpt run improvement",
        "Question_body":"<p>In the \u201ctrials vs best value\u201d visualization I see a lot of points with the same value. Does this mean that SigOpt is not finding any improvement for those runs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645558490644,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":149.0,
        "Answer_body":"<p>What that particular graph is showing is the best seen value, or trace, so far for all the completed iterations. This means that the graph will only go up when a new and better run has been found.<\/p>\n<p>The fact that there can be multiple runs in between improvements might seem a bit counter intuitive. However, this is a product of Bayesian optimization relaying on both exploration and exploitation to find a good optimum.<\/p>\n<p>To be the most efficient at finding a good optimum the optimization algorithms both need to explorer the search space, while also exploiting the optimums that we already know.<\/p>\n<p>So whenever we are not seeing a better run for a given iteration, so does not mean that the iteration has been wasted, this just means that we have learned that a given run is not a better optimum - this information is just as valuable for the algorithm.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.sigopt.com\/t\/sigopt-run-improvement\/94",
        "Tool":"SigOpt",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-31T12:05:48.236Z",
                "Answer_body":"<p>What that particular graph is showing is the best seen value, or trace, so far for all the completed iterations. This means that the graph will only go up when a new and better run has been found.<\/p>\n<p>The fact that there can be multiple runs in between improvements might seem a bit counter intuitive. However, this is a product of Bayesian optimization relaying on both exploration and exploitation to find a good optimum.<\/p>\n<p>To be the most efficient at finding a good optimum the optimization algorithms both need to explorer the search space, while also exploiting the optimums that we already know.<\/p>\n<p>So whenever we are not seeing a better run for a given iteration, so does not mean that the iteration has been wasted, this just means that we have learned that a given run is not a better optimum - this information is just as valuable for the algorithm.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"run improv trial best valu visual lot point valu mean find improv run",
        "Question_preprocessed_content":"run improv trial best valu visual lot point valu mean find improv run",
        "Question_gpt_summary_original":"The user is questioning whether SigOpt is not finding any improvement for certain runs, as they see many points with the same value in the \"trials vs best value\" visualization.",
        "Question_gpt_summary":"user question find improv certain run point valu trial best valu visual",
        "Answer_original_content":"particular graph show best seen valu trace far complet iter mean graph new better run fact multipl run improv bit counter intuit product bayesian optim relai explor exploit good optimum effici find good optimum optim algorithm need explor search space exploit optimum know see better run given iter mean iter wast mean learn given run better optimum inform valuabl algorithm",
        "Answer_preprocessed_content":"particular graph show best seen valu trace far complet iter mean graph new better run fact multipl run improv bit counter intuit product bayesian optim relai explor exploit good optimum effici find good optimum optim algorithm need explor search space exploit optimum know see better run given iter mean iter wast mean learn given run better optimum inform valuabl algorithm",
        "Answer_gpt_summary_original":"Solution: The discussion explains that the \"trials vs best value\" visualization shows the best seen value so far for all completed iterations. The optimization algorithm relies on both exploration and exploitation to find a good optimum, so there may be multiple runs between improvements. Not seeing a better run for a given iteration does not mean that the iteration has been wasted, as this information is valuable for the algorithm. Therefore, the solution is to continue running the optimization algorithm and trust the process.",
        "Answer_gpt_summary":"solut discuss explain trial best valu visual show best seen valu far complet iter optim algorithm reli explor exploit good optimum multipl run improv see better run given iter mean iter wast inform valuabl algorithm solut continu run optim algorithm trust process"
    },
    {
        "Question_title":"mlruns and artifact cleanup",
        "Question_body":"hi all. thanks for mlflow and the great info.\n\n\nim trying to clear-up some disk space on my tracking server. when i delete experiments in the ui, i notice the mlruns folder remains the same size. is there a standard way to clear out the artifacts for deleted experiments?\n\n\nthanks!\n\n\n-matt",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1580314344000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":22.0,
        "Answer_body":"actually, i think im deleting \"runs\" in the ui, but they still don't appear to be removing their artifacts from the filesystem\n\ue5d3. Hi Matt,\n\n\nGreat question! I'm in the process of reviewing\u00a0https:\/\/github.com\/mlflow\/mlflow\/pull\/2265, which proposes to introduce an `mlflow gc` command that will run against the FileStore and SqlAlchemyStore and hard delete artifacts\/metadata associated with deleted runs. MLflow uses soft deletion semantics for the \/runs\/delete API, so the run artifacts and metadata are not cleaned up automatically. The `mlflow gc` command is intended to be performed as an administrative task, enabling users to periodically clean up unused data.\n\n\nPlease let me\u00a0know if you have any questions about this!\n\n\nBest,\n\n\nCorey\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/6bf4cc17-cbef-4706-9892-00aa81128d5c%40googlegroups.com.. it looks like a very complete implementation. thanks for responding.\u00a0\n\n\ni found the branch and got a\nmlflow.exceptions.MlflowException: Not implemented yet\n\n\n\n\n\n\n\n(mlflow-gc) poojapan-mobl1:mlflow pinnermx$ mlflow gc\nTraceback (most recent call last):\n\u00a0 File \"\/Users\/pinnermx\/anaconda3\/envs\/mlflow-gc\/bin\/mlflow\", line 8, in <module>\n\u00a0 \u00a0 sys.exit(cli())\n\u00a0 File \"\/Users\/pinnermx\/anaconda3\/envs\/mlflow-gc\/lib\/python3.6\/site-packages\/click\/core.py\", line 764, in __call__\n\u00a0 \u00a0 return self.main(*args, **kwargs)\n\u00a0 File \"\/Users\/pinnermx\/anaconda3\/envs\/mlflow-gc\/lib\/python3.6\/site-packages\/click\/core.py\", line 717, in main\n\u00a0 \u00a0 rv = self.invoke(ctx)\n\u00a0 File \"\/Users\/pinnermx\/anaconda3\/envs\/mlflow-gc\/lib\/python3.6\/site-packages\/click\/core.py\", line 1137, in invoke\n\u00a0 \u00a0 return _process_result(sub_ctx.command.invoke(sub_ctx))\n\u00a0 File \"\/Users\/pinnermx\/anaconda3\/envs\/mlflow-gc\/lib\/python3.6\/site-packages\/click\/core.py\", line 956, in invoke\n\u00a0 \u00a0 return ctx.invoke(self.callback, **ctx.params)\n\u00a0 File \"\/Users\/pinnermx\/anaconda3\/envs\/mlflow-gc\/lib\/python3.6\/site-packages\/click\/core.py\", line 555, in invoke\n\u00a0 \u00a0 return callback(*args, **kwargs)\n\u00a0 File \"\/Users\/pinnermx\/anaconda3\/envs\/mlflow-gc\/lib\/python3.6\/site-packages\/mlflow\/cli.py\", line 301, in gc\n\u00a0 \u00a0 run_ids = backend_store._get_deleted_runs()\n\u00a0 File \"\/Users\/pinnermx\/anaconda3\/envs\/mlflow-gc\/lib\/python3.6\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 328, in _get_deleted_runs\n\u00a0 \u00a0 raise MlflowException(\"Not implemented yet\")\nmlflow.exceptions.MlflowException: Not implemented yet\n\n\n\n\n\n--matt\n\n\n\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/xVnEg6hfdi0",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-01-29T16:35:03",
                "Answer_body":"actually, i think im deleting \"runs\" in the ui, but they still don't appear to be removing their artifacts from the filesystem\n\ue5d3"
            },
            {
                "Answer_creation_time":"2020-01-29T17:56:34",
                "Answer_body":"Hi Matt,\n\n\nGreat question! I'm in the process of reviewing\u00a0https:\/\/github.com\/mlflow\/mlflow\/pull\/2265, which proposes to introduce an `mlflow gc` command that will run against the FileStore and SqlAlchemyStore and hard delete artifacts\/metadata associated with deleted runs. MLflow uses soft deletion semantics for the \/runs\/delete API, so the run artifacts and metadata are not cleaned up automatically. The `mlflow gc` command is intended to be performed as an administrative task, enabling users to periodically clean up unused data.\n\n\nPlease let me\u00a0know if you have any questions about this!\n\n\nBest,\n\n\nCorey\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/6bf4cc17-cbef-4706-9892-00aa81128d5c%40googlegroups.com."
            },
            {
                "Answer_creation_time":"2020-01-29T20:01:05",
                "Answer_body":"it looks like a very complete implementation. thanks for responding.\u00a0\n\n\ni found the branch and got a\nmlflow.exceptions.MlflowException: Not implemented yet\n\n\n\n\n\n\n\n(mlflow-gc) poojapan-mobl1:mlflow pinnermx$ mlflow gc\nTraceback (most recent call last):\n\u00a0 File \"\/Users\/pinnermx\/anaconda3\/envs\/mlflow-gc\/bin\/mlflow\", line 8, in <module>\n\u00a0 \u00a0 sys.exit(cli())\n\u00a0 File \"\/Users\/pinnermx\/anaconda3\/envs\/mlflow-gc\/lib\/python3.6\/site-packages\/click\/core.py\", line 764, in __call__\n\u00a0 \u00a0 return self.main(*args, **kwargs)\n\u00a0 File \"\/Users\/pinnermx\/anaconda3\/envs\/mlflow-gc\/lib\/python3.6\/site-packages\/click\/core.py\", line 717, in main\n\u00a0 \u00a0 rv = self.invoke(ctx)\n\u00a0 File \"\/Users\/pinnermx\/anaconda3\/envs\/mlflow-gc\/lib\/python3.6\/site-packages\/click\/core.py\", line 1137, in invoke\n\u00a0 \u00a0 return _process_result(sub_ctx.command.invoke(sub_ctx))\n\u00a0 File \"\/Users\/pinnermx\/anaconda3\/envs\/mlflow-gc\/lib\/python3.6\/site-packages\/click\/core.py\", line 956, in invoke\n\u00a0 \u00a0 return ctx.invoke(self.callback, **ctx.params)\n\u00a0 File \"\/Users\/pinnermx\/anaconda3\/envs\/mlflow-gc\/lib\/python3.6\/site-packages\/click\/core.py\", line 555, in invoke\n\u00a0 \u00a0 return callback(*args, **kwargs)\n\u00a0 File \"\/Users\/pinnermx\/anaconda3\/envs\/mlflow-gc\/lib\/python3.6\/site-packages\/mlflow\/cli.py\", line 301, in gc\n\u00a0 \u00a0 run_ids = backend_store._get_deleted_runs()\n\u00a0 File \"\/Users\/pinnermx\/anaconda3\/envs\/mlflow-gc\/lib\/python3.6\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 328, in _get_deleted_runs\n\u00a0 \u00a0 raise MlflowException(\"Not implemented yet\")\nmlflow.exceptions.MlflowException: Not implemented yet\n\n\n\n\n\n--matt\n\n\n\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"mlrun artifact cleanup thank great info try clear disk space track server delet experi notic mlrun folder remain size standard wai clear artifact delet experi thank matt",
        "Question_preprocessed_content":"mlrun artifact cleanup thank great info try disk space track server delet experi notic mlrun folder remain size standard wai clear artifact delet experi thank matt",
        "Question_gpt_summary_original":"The user is facing challenges in clearing up disk space on their tracking server as the mlruns folder remains the same size even after deleting experiments in the UI. They are seeking a standard way to clear out the artifacts for deleted experiments.",
        "Question_gpt_summary":"user face challeng clear disk space track server mlrun folder remain size delet experi seek standard wai clear artifact delet experi",
        "Answer_original_content":"actual think delet run appear remov artifact filesystem matt great question process reviewinghttp github com pull propos introduc command run filestor sqlalchemystor hard delet artifact metadata associ delet run us soft delet semant run delet api run artifact metadata clean automat command intend perform administr task enabl user period clean unus data let meknow question best corei receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user bfcc cbef aadc googlegroup com look like complet implement thank respond branch got except except implement poojapan mobl pinnermx traceback recent file user pinnermx anaconda env bin line sy exit cli file user pinnermx anaconda env lib python site packag click core line return self main arg kwarg file user pinnermx anaconda env lib python site packag click core line main self invok ctx file user pinnermx anaconda env lib python site packag click core line invok return process result sub ctx command invok sub ctx file user pinnermx anaconda env lib python site packag click core line invok return ctx invok self callback ctx param file user pinnermx anaconda env lib python site packag click core line invok return callback arg kwarg file user pinnermx anaconda env lib python site packag cli line run id backend store delet run file user pinnermx anaconda env lib python site packag store track file store line delet run rais except implement except except implement matt",
        "Answer_preprocessed_content":"actual think delet run appear remov artifact filesystem matt great question process propos introduc command run filestor sqlalchemystor hard delet associ delet run us soft delet semant api run artifact metadata clean automat command intend perform administr task enabl user period clean unus data let meknow question best corei receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit look like complet implement thank respond branch got implement pinnermx traceback file line file line return kwarg file line main file line invok return file line invok return file line invok return callback file line file line rais except implement matt",
        "Answer_gpt_summary_original":"Solution:\nA proposed solution is to introduce an `mlflow gc` command that will run against the FileStore and SqlAlchemyStore and hard delete artifacts\/metadata associated with deleted runs. The `mlflow gc` command is intended to be performed as an administrative task, enabling users to periodically clean up unused data. However, it appears that the implementation is not yet complete, as the user encountered an error when trying to run the command.",
        "Answer_gpt_summary":"solut propos solut introduc command run filestor sqlalchemystor hard delet artifact metadata associ delet run command intend perform administr task enabl user period clean unus data appear implement complet user encount error try run command"
    },
    {
        "Question_title":"Best Practice for CI with Run Cache?",
        "Question_body":"<p>I\u2019m having trouble understanding how to use the new <a href=\"https:\/\/dvc.org\/blog\/dvc-1-0-release#run-cache\" rel=\"nofollow noopener\">run cache<\/a> in a continuous integration context.<\/p>\n<p>From what I gathered it\u2019s supposed to work as follows:<\/p>\n<ul>\n<li>\n<code>git commit<\/code> &amp; <code>git push<\/code> some changes to my code<\/li>\n<li>The CI-Pipeline is started, it executes a <code>dvc repro<\/code> and trains the new model<\/li>\n<li>After model training, the CI-Pipeline executes a <code>dvc push --run-cache<\/code>\n<ul>\n<li>The newly trained model, and all other outputs tracked by dvc are pushed to the dvc remote<\/li>\n<\/ul>\n<\/li>\n<li>On my local machine, I execute <code>dvc pull --run-cache<\/code> (without any changes to my local <code>dvc.lock<\/code> file)\n<ul>\n<li>I\u2019m expecting the newly trained model and all other dvc outputs to appear in my working directory?<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>But instead of the files from the run cache appearing in my working dir, I still get the outputs corresponding to my (old, local) <code>dvc.lock<\/code> file.<\/p>\n<p>Am I missing something here, or am I using the commands wrong?<br>\nIn case my expectations are correct I can try to provide a minimal example or more debug information for my current setup.<\/p>\n<p>When I manually copy the (new) <code>dvc.lock<\/code> from the runner and paste it into my local machine, I\u2019m able to <code>dvc pull<\/code> from the dvc remote and get the expected outputs.<br>\nBut from what I understood, it shouldn\u2019t be necessary anmyore to <code>git add dvc.lock &amp;&amp; git commit<\/code> from inside the runner for every experiment?<\/p>\n<p>Thanks in advance,<br>\nRabefabi<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1594029291581,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":691.0,
        "Answer_body":"<p><a class=\"mention\" href=\"\/u\/rabefabi\">@rabefabi<\/a>, right now, you have to run <code>dvc repro<\/code> to change the lock-file and checkout from run-cache. But, before that, try running <code>dvc status<\/code> to see if anything\u2019s changed.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-for-ci-with-run-cache\/427",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-06T10:19:24.569Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/rabefabi\">@rabefabi<\/a>, right now, you have to run <code>dvc repro<\/code> to change the lock-file and checkout from run-cache. But, before that, try running <code>dvc status<\/code> to see if anything\u2019s changed.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"best practic run cach have troubl understand us new run cach continu integr context gather suppos work follow git commit git push chang code pipelin start execut repro train new model model train pipelin execut push run cach newli train model output track push remot local machin execut pull run cach chang local lock file expect newli train model output appear work directori instead file run cach appear work dir output correspond old local lock file miss command wrong case expect correct try provid minim exampl debug inform current setup manual copi new lock runner past local machin abl pull remot expect output understood shouldnt necessari anmyor git add lock git commit insid runner experi thank advanc rabefabi",
        "Question_preprocessed_content":"best practic run cach have troubl understand us new run cach continu integr context gather suppos work follow chang code start execut train new model model train execut newli train model output track push remot local machin execut expect newli train model output appear work directori instead file run cach appear work dir output correspond file miss command wrong case expect correct try provid minim exampl debug inform current setup manual copi runner past local machin abl remot expect output understood shouldnt necessari anmyor insid runner experi thank advanc rabefabi",
        "Question_gpt_summary_original":"The user is having trouble understanding how to use the new run cache in a continuous integration context. They are expecting the newly trained model and all other dvc outputs to appear in their working directory after executing \"dvc pull --run-cache\" on their local machine, but instead, they still get the outputs corresponding to their old, local \"dvc.lock\" file. The user is unsure if they are missing something or using the commands wrong. They have found a workaround by manually copying the new \"dvc.lock\" from the runner, but they are wondering if it is necessary to \"git add dvc.lock && git commit\" from inside the runner for every experiment.",
        "Question_gpt_summary":"user have troubl understand us new run cach continu integr context expect newli train model output appear work directori execut pull run cach local machin instead output correspond old local lock file user unsur miss command wrong workaround manual copi new lock runner wonder necessari git add lock git commit insid runner experi",
        "Answer_original_content":"rabefabi right run repro chang lock file checkout run cach try run statu anyth chang",
        "Answer_preprocessed_content":"right run chang checkout try run anyth chang",
        "Answer_gpt_summary_original":"Solution: The user can try running \"dvc status\" to see if anything has changed and then run \"dvc repro\" to change the lock-file and checkout from run-cache. There is no mention of any other solutions provided in the discussion.",
        "Answer_gpt_summary":"solut user try run statu chang run repro chang lock file checkout run cach mention solut provid discuss"
    },
    {
        "Question_title":"Embarrassingly parallel hyperparameter search via Azure + DataBricks + MLFlow",
        "Question_body":"<p>Conceptual question.  My company is pushing Azure + DataBricks.  I am trying to understand where this can take us.<\/p>\n<p>I am porting some work I've done locally to the Azure + Databricks platform.  I want to run an experiment with a large number of hyperparameter combinations using Azure + Databricks + MLfLow.  I am using PyTorch to implement my models.<\/p>\n<p>I have a cluster with 8 nodes.  I want to kick off the parameter search across all of the nodes in an embarrassingly parallel manner (one run per node, running independently).  Is this as simple as creating a MLflow project and then using the mlflow.projects.run command for each hyperparameter combination and Databricks + MLflow will take care of the rest?<\/p>\n<p>Is this technology capable of this?  I'm looking for some references I could use to make this happen.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1594133526057,
        "Question_favorite_count":null,
        "Question_last_edit_time":1594134725200,
        "Question_score":0.0,
        "Question_view_count":262.0,
        "Answer_body":"<p>The short answer is yes, it's possible, but won't be exactly as easy as running a single mlflow command. You can paralelize single-node workflows using spark Python UDFs, a good example of this is this <a href=\"https:\/\/pages.databricks.com\/rs\/094-YMS-629\/images\/Fine-Grained-Time-Series-Forecasting.html?_ga=2.64430959.1760852900.1593769579-972789996.1561118598\" rel=\"nofollow noreferrer\">notebook<\/a><\/p>\n<p>I'm not sure if this will work with pytorch, but there is hyperopt library that lets you parallelize search across parameters using Spark - it's integrated with mlflow and available in databricks ML runtime. I've been using it only with scikit-learn, but it may be <a href=\"https:\/\/docs.databricks.com\/applications\/machine-learning\/automl\/hyperopt\/hyperopt-model-selection.html\" rel=\"nofollow noreferrer\">worth checking out<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62778020",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1594986774772,
        "Question_original_content":"embarrassingli parallel hyperparamet search azur databrick conceptu question compani push azur databrick try understand port work local azur databrick platform want run experi larg number hyperparamet combin azur databrick pytorch implement model cluster node want kick paramet search node embarrassingli parallel manner run node run independ simpl creat project project run command hyperparamet combin databrick care rest technolog capabl look refer us happen",
        "Question_preprocessed_content":"embarrassingli parallel hyperparamet search azur databrick conceptu question compani push azur databrick try understand port work local azur databrick platform want run experi larg number hyperparamet combin azur databrick pytorch implement model cluster node want kick paramet search node embarrassingli parallel manner simpl creat project command hyperparamet combin databrick care rest technolog capabl look refer us happen",
        "Question_gpt_summary_original":"The user is facing the challenge of understanding how to use Azure + DataBricks + MLFlow to run an experiment with a large number of hyperparameter combinations in an embarrassingly parallel manner using PyTorch. They are unsure if creating an MLflow project and using the mlflow.projects.run command for each hyperparameter combination will work and are looking for references to help them achieve this.",
        "Question_gpt_summary":"user face challeng understand us azur databrick run experi larg number hyperparamet combin embarrassingli parallel manner pytorch unsur creat project project run command hyperparamet combin work look refer help achiev",
        "Answer_original_content":"short answer ye possibl won exactli easi run singl command paralel singl node workflow spark python udf good exampl notebook sure work pytorch hyperopt librari let parallel search paramet spark integr avail databrick runtim scikit learn worth check",
        "Answer_preprocessed_content":"short answer ye possibl won exactli easi run singl command paralel workflow spark python udf good exampl notebook sure work pytorch hyperopt librari let parallel search paramet spark integr avail databrick runtim worth check",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include parallelizing single-node workflows using Spark Python UDFs and using the hyperopt library to parallelize search across parameters using Spark, which is integrated with MLflow and available in Databricks ML runtime. However, it is not clear if these solutions will work with PyTorch.",
        "Answer_gpt_summary":"possibl solut mention discuss includ parallel singl node workflow spark python udf hyperopt librari parallel search paramet spark integr avail databrick runtim clear solut work pytorch"
    },
    {
        "Question_title":"List all remote paths of tracked files",
        "Question_body":"<p>For a given commit, I\u2019d like to get a list of the local paths &amp; corresponding remote paths of all files tracked by DVC, possibly limited to a certain directory. Is there not an easy way to do this?<\/p>\n<p>Currently looking at a workaround involving \u201cdvc list -R --dvc-only --rev  \u2026\u201d to get all tracked files, reading the md5 out of the corresponding .dvc files, and then using those to build the remote paths. Doesn\u2019t seem very elegant.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1630558728484,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":217.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jmiller\">@jmiller<\/a>! Might I ask what is the final purpose of this operation?<\/p>\n<p>There is not really a straightforward way to get all the remote paths of a repo, probably what you are already doing would be the easiest way.<\/p>\n<p>Usually, you would be accessing those files directly with either <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\" rel=\"noopener nofollow ugc\"><code>dvc get<\/code><\/a> or the python API <a href=\"https:\/\/dvc.org\/doc\/api-reference\/read\" rel=\"noopener nofollow ugc\"><code>dvc.api.read<\/code><\/a>.<\/p>\n<p>Not sure if this can help but you can use <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get#example-getting-the-storage-url-of-a-dvc-tracked-file\" rel=\"noopener nofollow ugc\"><code>dvc get --show-url<\/code><\/a> to \u201cprint the storage location (URL) of the target data\u201d. Alternatively, you can use the python API <a href=\"https:\/\/dvc.org\/doc\/api-reference\/get_url\" rel=\"noopener nofollow ugc\"><code>dvc.api.get_url<\/code><\/a>.<\/p>. <p>Thanks! Possibly I\u2019m not using dvc for its intended purposes. I want to load the data, as it was at a certain commit, into a cloud database and\/or data visualization tool. Doing this directly with the remote copy of the data (s3) seems to make the most sense, because it doesn\u2019t require me to upload another copy of the data to the cloud first.<\/p>\n<p>Currently my repo has local cache but eventually I\u2019d like to have all data stored in the cloud, not in a local workspace or cache at all, since the data is quite large.<\/p>. <p>From Python API <a href=\"https:\/\/dvc.org\/doc\/api-reference\/open\" rel=\"noopener nofollow ugc\"><code>dvc.api.open<\/code><\/a>:<\/p>\n<blockquote>\n<p>makes a direct connection to the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/add#supported-storage-types\" rel=\"noopener nofollow ugc\">remote storage<\/a> (except for Google Drive), so the file contents can be streamed.<\/p>\n<\/blockquote>\n<p>So using the API won\u2019t create a copy of the data, but <code>dvc get<\/code> will.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/list-all-remote-paths-of-tracked-files\/863",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-02T15:10:14.876Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jmiller\">@jmiller<\/a>! Might I ask what is the final purpose of this operation?<\/p>\n<p>There is not really a straightforward way to get all the remote paths of a repo, probably what you are already doing would be the easiest way.<\/p>\n<p>Usually, you would be accessing those files directly with either <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\" rel=\"noopener nofollow ugc\"><code>dvc get<\/code><\/a> or the python API <a href=\"https:\/\/dvc.org\/doc\/api-reference\/read\" rel=\"noopener nofollow ugc\"><code>dvc.api.read<\/code><\/a>.<\/p>\n<p>Not sure if this can help but you can use <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get#example-getting-the-storage-url-of-a-dvc-tracked-file\" rel=\"noopener nofollow ugc\"><code>dvc get --show-url<\/code><\/a> to \u201cprint the storage location (URL) of the target data\u201d. Alternatively, you can use the python API <a href=\"https:\/\/dvc.org\/doc\/api-reference\/get_url\" rel=\"noopener nofollow ugc\"><code>dvc.api.get_url<\/code><\/a>.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-02T15:57:47.726Z",
                "Answer_body":"<p>Thanks! Possibly I\u2019m not using dvc for its intended purposes. I want to load the data, as it was at a certain commit, into a cloud database and\/or data visualization tool. Doing this directly with the remote copy of the data (s3) seems to make the most sense, because it doesn\u2019t require me to upload another copy of the data to the cloud first.<\/p>\n<p>Currently my repo has local cache but eventually I\u2019d like to have all data stored in the cloud, not in a local workspace or cache at all, since the data is quite large.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-06T08:37:28.957Z",
                "Answer_body":"<p>From Python API <a href=\"https:\/\/dvc.org\/doc\/api-reference\/open\" rel=\"noopener nofollow ugc\"><code>dvc.api.open<\/code><\/a>:<\/p>\n<blockquote>\n<p>makes a direct connection to the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/add#supported-storage-types\" rel=\"noopener nofollow ugc\">remote storage<\/a> (except for Google Drive), so the file contents can be streamed.<\/p>\n<\/blockquote>\n<p>So using the API won\u2019t create a copy of the data, but <code>dvc get<\/code> will.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"list remot path track file given commit like list local path correspond remot path file track possibl limit certain directori easi wai current look workaround involv list rev track file read correspond file build remot path doesnt eleg",
        "Question_preprocessed_content":"list remot path track file given commit like list local path correspond remot path file track possibl limit certain directori easi wai current look workaround involv list track file read correspond file build remot path doesnt eleg",
        "Question_gpt_summary_original":"The user is facing a challenge in obtaining a list of local and remote paths of all files tracked by DVC for a given commit. They are currently using a workaround involving \"dvc list\" command to get all tracked files and then reading the md5 out of the corresponding .dvc files to build the remote paths, which they find to be inelegant.",
        "Question_gpt_summary":"user face challeng obtain list local remot path file track given commit current workaround involv list command track file read correspond file build remot path ineleg",
        "Answer_original_content":"jmiller ask final purpos oper straightforward wai remot path repo probabl easiest wai usual access file directli python api api read sure help us url print storag locat url target data altern us python api api url thank possibl intend purpos want load data certain commit cloud databas data visual tool directli remot copi data sens doesnt requir upload copi data cloud current repo local cach eventu like data store cloud local workspac cach data larg python api api open make direct connect remot storag googl drive file content stream api wont creat copi data",
        "Answer_preprocessed_content":"ask final purpos oper straightforward wai remot path repo probabl easiest wai usual access file directli python api sure help us print storag locat target data altern us python api thank possibl intend purpos want load data certain commit cloud databas data visual tool directli remot copi data sens doesnt requir upload copi data cloud current repo local cach eventu like data store cloud local workspac cach data larg python api make direct connect remot storag file content stream api wont creat copi data",
        "Answer_gpt_summary_original":"Solutions provided:\n- The user can use \"dvc get --show-url\" command to print the storage location (URL) of the target data or use the python API \"dvc.api.get_url\".\n- The user can use the Python API \"dvc.api.open\" to make a direct connection to the remote storage and stream the file contents without creating a copy of the data.\n- The current workaround of using \"dvc list\" command to get all tracked files and then reading the md5 out of the corresponding .dvc files to build the remote paths is the easiest way to get all remote paths of a repo. \n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"solut provid user us url command print storag locat url target data us python api api url user us python api api open direct connect remot storag stream file content creat copi data current workaround list command track file read correspond file build remot path easiest wai remot path repo person opinion bias includ respons"
    },
    {
        "Question_title":"Azure ML inference pipeline deployment authorization token error",
        "Question_body":"<p>When deploying a real-time inferencing pipeline in Azure ML (as per <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy#deploy-the-real-time-endpoint\" rel=\"nofollow noreferrer\">this<\/a> tutorial), I receive the below error. I've tried forcibly logging out using OAuth. Tried creating a new Azure workspace but continue to receive the same error.<\/p>\n\n<p>It looks like the tenant id causing the problem is example.onmicrosoft.com (72f988bf-86f1-41af-91ab-2d7cd011db47)<\/p>\n\n<hr>\n\n<p><em>Deploy: Failed on step CreateServiceFromModels. Details: AzureML service API error. Error calling ServiceCreate: {\"code\":\"Unauthorized\",\"statusCode\":401,\"message\":\"Unauthorized\",\"details\":[{\"code\":\"EmptyOrInvalidToken\",\"message\":\"Error: Service invocation failed!\\r\\nRequest: GET <a href=\"https:\/\/management.azure.com\/subscriptions\/subscription_id\/resourceGroups\/dev-rg\/providers\/Microsoft.MachineLearningServices\/workspaces\/dev-ws\/providers\/Microsoft.Authorization\/permissions?api-version=2015-07-01\" rel=\"nofollow noreferrer\">https:\/\/management.azure.com\/subscriptions\/subscription_id\/resourceGroups\/dev-rg\/providers\/Microsoft.MachineLearningServices\/workspaces\/dev-ws\/providers\/Microsoft.Authorization\/permissions?api-version=2015-07-01<\/a>\\r\\nStatus Code: 401 Unauthorized\\r\\nReason Phrase: Unauthorized\\r\\nResponse Body: {\\\"error\\\":{\\\"code\\\":\\\"InvalidAuthenticationTokenTenant\\\",\\\"message\\\":\\\"The access token is from the wrong issuer '<a href=\"https:\/\/sts.windows.net\/72f988bf-86f1-41af-91ab-2d7cd011db47\/\" rel=\"nofollow noreferrer\">https:\/\/sts.windows.net\/72f988bf-86f1-41af-91ab-2d7cd011db47\/<\/a>'. It must match the tenant '<a href=\"https:\/\/sts.windows.net\/correct_tenant_id\/\" rel=\"nofollow noreferrer\">https:\/\/sts.windows.net\/correct_tenant_id\/<\/a>' associated with this subscription. Please use the authority (URL) '<a href=\"https:\/\/login.windows.net\/correct_tenant_id\" rel=\"nofollow noreferrer\">https:\/\/login.windows.net\/correct_tenant_id<\/a>' to get the token. Note, if the subscription is transferred to another tenant there i<\/em><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1582302156893,
        "Question_favorite_count":null,
        "Question_last_edit_time":1582395411243,
        "Question_score":2.0,
        "Question_view_count":481.0,
        "Answer_body":"<p>I appear to have had User Access Administrator role only (in addition to Classic Service Administrator). As soon as I added myself to the Owner role in the Access Control (IAM) section of the Azure Portal, the deployment succeeded.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60342645",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1582556597823,
        "Question_original_content":"infer pipelin deploy author token error deploi real time inferenc pipelin tutori receiv error tri forcibl log oauth tri creat new azur workspac continu receiv error look like tenant caus problem exampl onmicrosoft com fbf dcddb deploi fail step createservicefrommodel detail servic api error error call servicecr code unauthor statuscod messag unauthor detail code emptyorinvalidtoken messag error servic invoc fail nrequest http manag azur com subscript subscript resourcegroup dev provid microsoft machinelearningservic workspac dev provid microsoft author permiss api version nstatu code unauthor nreason phrase unauthor nrespons bodi error code invalidauthenticationtokenten messag access token wrong issuer http st window net fbf dcddb match tenant http st window net correct tenant associ subscript us author url http login window net correct tenant token note subscript transfer tenant",
        "Question_preprocessed_content":"infer pipelin deploy author token error deploi inferenc pipelin receiv error tri forcibl log oauth tri creat new azur workspac continu receiv error look like tenant caus problem deploi fail step createservicefrommodel detail servic api error error call servicecr code unauthor statuscod messag unauthor detail code emptyorinvalidtoken messag error servic invoc code phrase bodi access token wrong issuer match tenant associ subscript us author token note subscript transfer tenant",
        "Question_gpt_summary_original":"The user is encountering an error when deploying a real-time inferencing pipeline in Azure ML, receiving an \"Unauthorized\" error message with details indicating an issue with the authorization token. The error message suggests that the access token is from the wrong issuer and must match the correct tenant ID associated with the subscription. The user has attempted to resolve the issue by logging out and creating a new Azure workspace, but the error persists.",
        "Question_gpt_summary":"user encount error deploi real time inferenc pipelin receiv unauthor error messag detail indic issu author token error messag suggest access token wrong issuer match correct tenant associ subscript user attempt resolv issu log creat new azur workspac error persist",
        "Answer_original_content":"appear user access administr role addit classic servic administr soon ad owner role access control iam section azur portal deploy succeed",
        "Answer_preprocessed_content":"appear user access administr role soon ad owner role access control section azur portal deploy succeed",
        "Answer_gpt_summary_original":"Solution: Adding the user to the Owner role in the Access Control (IAM) section of the Azure Portal resolved the issue.",
        "Answer_gpt_summary":"solut ad user owner role access control iam section azur portal resolv issu"
    },
    {
        "Question_title":"Azure OpenAI advantages",
        "Question_body":"We have a customer that is interested in the Azure OpenAI Service and had a few question:\n\nWhat are the advantages for using Azure OpenAI vs. OpenAI API.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648125540293,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@App-4824 Thanks for the question. The Azure OpenAI team always works to make the latest models available in Azure as soon as they are available from OpenAI. This is an active area of work for the team to tighten release date for future models.\nThe Azure service is backed by an SLA (which typically a customer's primary need for high availability, low latency), and the support that comes along with Azure services. This is the first commercialized model of its kind from any public cloud provider. This is unique advantage to be first to market and to work closely with customer to adopt\/embrace these models for production use.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/785866\/azure-openai-advantages.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-24T14:21:38.047Z",
                "Answer_score":0,
                "Answer_body":"@App-4824 Thanks for the question. The Azure OpenAI team always works to make the latest models available in Azure as soon as they are available from OpenAI. This is an active area of work for the team to tighten release date for future models.\nThe Azure service is backed by an SLA (which typically a customer's primary need for high availability, low latency), and the support that comes along with Azure services. This is the first commercialized model of its kind from any public cloud provider. This is unique advantage to be first to market and to work closely with customer to adopt\/embrace these models for production use.",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1648131698047,
        "Question_original_content":"azur openai advantag custom interest azur openai servic question advantag azur openai openai api",
        "Question_preprocessed_content":"azur openai advantag custom interest azur openai servic question advantag azur openai openai api",
        "Question_gpt_summary_original":"The user is interested in the advantages of using Azure OpenAI compared to OpenAI API.",
        "Question_gpt_summary":"user interest advantag azur openai compar openai api",
        "Answer_original_content":"app thank question azur openai team work latest model avail azur soon avail openai activ area work team tighten releas date futur model azur servic back sla typic custom primari need high avail low latenc support come azur servic commerci model kind public cloud provid uniqu advantag market work close custom adopt embrac model product us",
        "Answer_preprocessed_content":"thank question azur openai team work latest model avail azur soon avail openai activ area work team tighten releas date futur model azur servic back sla support come azur servic commerci model kind public cloud provid uniqu advantag market work close custom model product us",
        "Answer_gpt_summary_original":"Summary: The advantages of using Azure OpenAI compared to OpenAI API include the availability of the latest models as soon as they are released, an SLA for high availability and low latency, and support from Azure services. Additionally, Azure OpenAI is the first commercialized model of its kind from any public cloud provider, which provides a unique advantage to be first to market and work closely with customers to adopt and embrace these models for production use. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"summari advantag azur openai compar openai api includ avail latest model soon releas sla high avail low latenc support azur servic addition azur openai commerci model kind public cloud provid provid uniqu advantag market work close custom adopt embrac model product us solut mention discuss"
    },
    {
        "Question_title":"Can I limit the type of instances that data scientists can launch for training jobs in SageMaker?",
        "Question_body":"We want to limit the types of instances that our data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker. Is it possible to limit the instance size options available through SageMaker by using IAM policies, or another method? For example: Could we remove the ability to launch ml.p3.16xlarge instances?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603454458000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":426.0,
        "Answer_body":"Yes, you can limit the types of instances that are available for your data scientists to launch in SageMaker by using an IAM policy similar to the following one:\n\nNote: This example IAM policy allows SageMaker users to launch only Compute Optimized (ml.c5)-type training jobs and hyperparameter tuning jobs.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"EnforceInstanceType\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:CreateTrainingJob\",\n                \"sagemaker:CreateHyperParameterTuningJob\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"ForAllValues:StringLike\": {\n                    \"sagemaker:InstanceTypes\": [\"ml.c5.*\"]\n                }\n            }\n        }\n\n     ]\n}",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUd77APmdHTx-2FZCvZfS6Qg\/can-i-limit-the-type-of-instances-that-data-scientists-can-launch-for-training-jobs-in-sage-maker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-23T12:15:48.000Z",
                "Answer_score":0,
                "Answer_body":"Yes, you can limit the types of instances that are available for your data scientists to launch in SageMaker by using an IAM policy similar to the following one:\n\nNote: This example IAM policy allows SageMaker users to launch only Compute Optimized (ml.c5)-type training jobs and hyperparameter tuning jobs.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"EnforceInstanceType\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:CreateTrainingJob\",\n                \"sagemaker:CreateHyperParameterTuningJob\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"ForAllValues:StringLike\": {\n                    \"sagemaker:InstanceTypes\": [\"ml.c5.*\"]\n                }\n            }\n        }\n\n     ]\n}",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1603455348000,
        "Question_original_content":"limit type instanc data scientist launch train job want limit type instanc data scientist launch run train job hyperparamet tune job possibl limit instanc size option avail iam polici method exampl remov abil launch xlarg instanc",
        "Question_preprocessed_content":"limit type instanc data scientist launch train job want limit type instanc data scientist launch run train job hyperparamet tune job possibl limit instanc size option avail iam polici method exampl remov abil launch instanc",
        "Question_gpt_summary_original":"The user wants to limit the types of instances that data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker. They are seeking a way to restrict the instance size options available through SageMaker, such as removing the ability to launch ml.p3.16xlarge instances, using IAM policies or another method.",
        "Question_gpt_summary":"user want limit type instanc data scientist launch run train job hyperparamet tune job seek wai restrict instanc size option avail remov abil launch xlarg instanc iam polici method",
        "Answer_original_content":"ye limit type instanc avail data scientist launch iam polici similar follow note exampl iam polici allow user launch comput optim type train job hyperparamet tune job version statement sid enforceinstancetyp effect allow action createtrainingjob createhyperparametertuningjob resourc condit forallvalu stringlik instancetyp",
        "Answer_preprocessed_content":"ye limit type instanc avail data scientist launch iam polici similar follow note exampl iam polici allow user launch comput optim type train job hyperparamet tune job version statement resourc condit",
        "Answer_gpt_summary_original":"Solution: The user can limit the types of instances that data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker by using an IAM policy. The example IAM policy provided in the discussion allows SageMaker users to launch only Compute Optimized (ml.c5)-type training jobs and hyperparameter tuning jobs.",
        "Answer_gpt_summary":"solut user limit type instanc data scientist launch run train job hyperparamet tune job iam polici exampl iam polici provid discuss allow user launch comput optim type train job hyperparamet tune job"
    },
    {
        "Question_title":"`Model.deploy()` failing for AutoML with `400 'automatic_resources' is not supported for Model`",
        "Question_body":"<p>Trying to use the python SDK to deploy an AutoML model, but am recieving the error<\/p>\n<pre><code>google.api_core.exceptions.InvalidArgument: 400 'automatic_resources' is not supported for Model\n<\/code><\/pre>\n<p>Here's what I'm running:<\/p>\n<pre><code>from google.cloud import aiplatform\n\naiplatform.init(project=&quot;MY_PROJECT&quot;)\n\n# model is an AutoML tabular model\nmodel = aiplatform.Model(&quot;MY_MODEL_ID&quot;)\n\n# this fails\nmodel.deploy()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643912020147,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":149.0,
        "Answer_body":"<p>You encounter the <code>automatic_resources<\/code> because deploying models for AutoML Tables requires the user to define the machine type at model deployment. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#specify\" rel=\"nofollow noreferrer\">configure compute resources for prediction docs<\/a> for more information.<\/p>\n<blockquote>\n<p>If you want to use a custom-trained model or an AutoML tabular model\nto serve online predictions, you must specify a machine type when you\ndeploy the Model resource as a DeployedModel to an Endpoint.<\/p>\n<\/blockquote>\n<p>You should add defining of resources to your code for you to continue the deployment. Adding <code>machine_type<\/code> parameter should suffice. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#machine-types\" rel=\"nofollow noreferrer\">supported machine types docs<\/a> for complete list.<\/p>\n<p>See code below:<\/p>\n<pre><code>from google.cloud import aiplatform\n\nproject = &quot;your-project-id&quot;\nlocation = &quot;us-central1&quot;\nmodel_id = &quot;99999999&quot;\n\naiplatform.init(project=project, location=location)\nmodel_name = (f&quot;projects\/{project}\/locations\/{location}\/models\/{model_id}&quot;)\n\nmodel = aiplatform.Model(model_name=model_name)\n\nmodel.deploy(machine_type=&quot;n1-standard-4&quot;)\n\nmodel.wait()\nprint(model.display_name)\n<\/code><\/pre>\n<p>Output:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NHpJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NHpJF.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70976273",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1643942120972,
        "Question_original_content":"model deploi fail automl automat resourc support model try us python sdk deploi automl model reciev error googl api core except invalidargu automat resourc support model run googl cloud import aiplatform aiplatform init project project model automl tabular model model aiplatform model model fail model deploi",
        "Question_preprocessed_content":"fail automl try us python sdk deploi automl model reciev error run",
        "Question_gpt_summary_original":"The user is encountering an error while trying to deploy an AutoML model using the python SDK. The error message states that \"automatic_resources\" is not supported for the model, resulting in a 400 InvalidArgument error. The user has provided the code they are running, which includes initializing the project and model, but the deployment fails.",
        "Question_gpt_summary":"user encount error try deploi automl model python sdk error messag state automat resourc support model result invalidargu error user provid code run includ initi project model deploy fail",
        "Answer_original_content":"encount automat resourc deploi model automl tabl requir user defin machin type model deploy configur comput resourc predict doc inform want us custom train model automl tabular model serv onlin predict specifi machin type deploi model resourc deployedmodel endpoint add defin resourc code continu deploy ad machin type paramet suffic support machin type doc complet list code googl cloud import aiplatform project project locat central model aiplatform init project project locat locat model project project locat locat model model model aiplatform model model model model deploi machin type standard model wait print model displai output",
        "Answer_preprocessed_content":"encount deploi model automl tabl requir user defin machin type model deploy configur comput resourc predict doc inform want us model automl tabular model serv onlin predict specifi machin type deploi model resourc deployedmodel endpoint add defin resourc code continu deploy ad paramet suffic support machin type doc complet list code output",
        "Answer_gpt_summary_original":"Solution: The error message encountered by the user while deploying an AutoML model using the python SDK is due to the absence of defining machine type at model deployment. The solution is to add the `machine_type` parameter to the code and specify a supported machine type. The user can refer to the supported machine types documentation for a complete list. The code snippet provided in the discussion includes the necessary changes to the code.",
        "Answer_gpt_summary":"solut error messag encount user deploi automl model python sdk absenc defin machin type model deploy solut add machin type paramet code specifi support machin type user refer support machin type document complet list code snippet provid discuss includ necessari chang code"
    },
    {
        "Question_title":"AWS CLI: How to use variable to filter EFS",
        "Question_body":"<p>I want to use the value of the <code>DOMAIN_ID<\/code> variable to filter the EFS to get a FileSystemId. I used the commands below. The first command works and it stores the domain ID. The second one returns an empty list, even though the <code>DOMAIN_ID<\/code> variable is present.<\/p>\n<pre><code>DOMAIN_ID=$(aws sagemaker list-domains --query 'Domains[0].DomainId')\naws efs describe-file-systems --query 'FileSystems[?CreationToken==`$DOMAIN_ID`].FileSystemId'\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>[]\n<\/code><\/pre>\n<p>Expected output:<\/p>\n<pre><code>&lt;Some EFS identifier&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658155362930,
        "Question_favorite_count":null,
        "Question_last_edit_time":1658167919760,
        "Question_score":0.0,
        "Question_view_count":64.0,
        "Answer_body":"<p>This works (escaping backticks) -<\/p>\n<pre><code>aws efs describe-file-systems --query &quot;FileSystems[?CreationToken==\\`$DOMAIN_ID\\`].FileSystemId&quot;\n<\/code><\/pre>\n<p>You can also use describe-domain command instead -<\/p>\n<pre><code>$ DOMAIN_ID=$(aws sagemaker list-domains --query 'Domains[0].DomainId' | tr -d '&quot;')\n$ aws sagemaker describe-domain --domain-id $DOMAIN_ID --query 'HomeEfsFileSystemId'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73024189",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1658177973060,
        "Question_original_content":"aw cli us variabl filter ef want us valu domain variabl filter ef filesystemid command command work store domain second return list domain variabl present domain list domain queri domain domainid aw ef file system queri filesystem creationtoken domain filesystemid output expect output",
        "Question_preprocessed_content":"aw cli us variabl filter ef want us valu variabl filter ef filesystemid command command work store domain second return list variabl present output expect output",
        "Question_gpt_summary_original":"The user is facing a challenge in using a variable to filter EFS and get a FileSystemId. The first command works and stores the domain ID, but the second command returns an empty list even though the DOMAIN_ID variable is present. The expected output is a specific EFS identifier.",
        "Question_gpt_summary":"user face challeng variabl filter ef filesystemid command work store domain second command return list domain variabl present expect output specif ef identifi",
        "Answer_original_content":"work escap backtick aw ef file system queri filesystem creationtoken domain filesystemid us domain command instead domain list domain queri domain domainid domain domain domain queri homeefsfilesystemid",
        "Answer_preprocessed_content":"work us command instead",
        "Answer_gpt_summary_original":"Two solutions were provided in the discussion. The first solution involves using the `aws efs describe-file-systems` command with a query to filter the EFS and get the FileSystemId. The second solution involves using the `aws sagemaker describe-domain` command with the `--domain-id` option to get the HomeEfsFileSystemId.",
        "Answer_gpt_summary":"solut provid discuss solut involv aw ef file system command queri filter ef filesystemid second solut involv domain command domain option homeefsfilesystemid"
    },
    {
        "Question_title":"Can Vertex AI model monitoring job send a pubsub message instead of email?",
        "Question_body":"<p>I have been using <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/08-model-monitoring.ipynb\" rel=\"nofollow noreferrer\">this<\/a> example of creating a Vertex AI monitoring job. It sends an email.<\/p>\n<pre><code>alerting_config = vertex_ai_beta.ModelMonitoringAlertConfig(\nemail_alert_config=vertex_ai_beta.ModelMonitoringAlertConfig.EmailAlertConfig(\n    user_emails=NOTIFY_EMAILS\n)\n<\/code><\/pre>\n<p>Is there any way to instead send a Pubsub message?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1660144734353,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":86.0,
        "Answer_body":"<p>You can configure the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/model-monitoring\/using-model-monitoring#set-up-alerts\" rel=\"nofollow noreferrer\">alert to be sent to Cloud Logging<\/a>. To enable Cloud Logging alerts you have to set the <code>enableLogging<\/code> field on your <code>ModelMonitoringAlertConfig<\/code> configuration to <code>TRUE<\/code>. Then you can forward the logs to any service that Cloud Logging supports, Pub\/Sub is one of these.<\/p>\n<p>For this you\u2019ll need one of the following permissions:<\/p>\n<ul>\n<li>Owner (roles\/owner)<\/li>\n<li>Logging Admin (roles\/logging.admin)<\/li>\n<li>Logs Configuration Writer (roles\/logging.configWriter)<\/li>\n<\/ul>\n<p>Then you need to <a href=\"https:\/\/cloud.google.com\/logging\/docs\/export\/configure_export_v2#creating_sink\" rel=\"nofollow noreferrer\">create a sink<\/a>.<\/p>\n<p>After that you have created the sink you\u2019ll need to set the <a href=\"https:\/\/cloud.google.com\/logging\/docs\/export\/configure_export_v2#dest-auth\" rel=\"nofollow noreferrer\">destination permissions<\/a>.<\/p>\n<p>While Cloud Logging provides you with the ability to exclude logs from being ingested, you might want to consider keeping logs that help with supportability. Using these logs can help you quickly troubleshoot and identify issues with your applications.<\/p>\n<p>Logs routed to Pub\/Sub are generally available within seconds, with 99% of logs available in less than 60 seconds.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73308825",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1660246628340,
        "Question_original_content":"model monitor job send pubsub messag instead email exampl creat monitor job send email alert config vertex beta modelmonitoringalertconfig email alert config vertex beta modelmonitoringalertconfig emailalertconfig user email notifi email wai instead send pubsub messag",
        "Question_preprocessed_content":"model monitor job send pubsub messag instead email exampl creat monitor job send email wai instead send pubsub messag",
        "Question_gpt_summary_original":"The user is facing a challenge with the Vertex AI model monitoring job as it only sends email alerts and they are looking for a way to send Pubsub messages instead.",
        "Question_gpt_summary":"user face challeng model monitor job send email alert look wai send pubsub messag instead",
        "Answer_original_content":"configur alert sent cloud log enabl cloud log alert set enablelog field modelmonitoringalertconfig configur true forward log servic cloud log support pub sub youll need follow permiss owner role owner log admin role log admin log configur writer role log configwrit need creat sink creat sink youll need set destin permiss cloud log provid abil exclud log ingest want consid keep log help support log help quickli troubleshoot identifi issu applic log rout pub sub gener avail second log avail second",
        "Answer_preprocessed_content":"configur alert sent cloud log enabl cloud log alert set field configur forward log servic cloud log support youll need follow permiss owner log admin log configur writer need creat sink creat sink youll need set destin permiss cloud log provid abil exclud log ingest want consid keep log help support log help quickli troubleshoot identifi issu applic log rout gener avail second log avail second",
        "Answer_gpt_summary_original":"The solution provided in the discussion is to configure the alert to be sent to Cloud Logging and then forward the logs to Pub\/Sub. This can be done by setting the enableLogging field on the ModelMonitoringAlertConfig configuration to TRUE and creating a sink. The user will also need the necessary permissions to create a sink and set destination permissions. Logs routed to Pub\/Sub are generally available within seconds.",
        "Answer_gpt_summary":"solut provid discuss configur alert sent cloud log forward log pub sub set enablelog field modelmonitoringalertconfig configur true creat sink user need necessari permiss creat sink set destin permiss log rout pub sub gener avail second"
    }
]